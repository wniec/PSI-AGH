{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sieci neuronowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wstęp\n",
    "\n",
    "Celem laboratorium jest zapoznanie się z podstawami sieci neuronowych oraz uczeniem głębokim (*deep learning*). Zapoznasz się na nim z następującymi tematami:\n",
    "- treningiem prostych sieci neuronowych, w szczególności z:\n",
    "  - regresją liniową w sieciach neuronowych\n",
    "  - optymalizacją funkcji kosztu\n",
    "  - algorytmem spadku wzdłuż gradientu\n",
    "  - siecią typu Multilayer Perceptron (MLP)\n",
    "- frameworkiem PyTorch, w szczególności z:\n",
    "  - ładowaniem danych\n",
    "  - preprocessingiem danych\n",
    "  - pisaniem pętli treningowej i walidacyjnej\n",
    "  - walidacją modeli\n",
    "- architekturą i hiperaprametrami sieci MLP, w szczególności z:\n",
    "  - warstwami gęstymi (w pełni połączonymi)\n",
    "  - funkcjami aktywacji\n",
    "  - regularyzacją: L2, dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykorzystywane biblioteki\n",
    "\n",
    "Zaczniemy od pisania ręcznie prostych sieci w bibliotece Numpy, służącej do obliczeń numerycznych na CPU. Później przejdziemy do wykorzystywania frameworka PyTorch, służącego do obliczeń numerycznych na CPU, GPU oraz automatycznego różniczkowania, wykorzystywanego głównie do treningu sieci neuronowych.\n",
    "\n",
    "Wykorzystamy PyTorcha ze względu na popularność, łatwość instalacji i użycia, oraz dużą kontrolę nad niskopoziomowymi aspektami budowy i treningu sieci neuronowych. Framework ten został stworzony do zastosowań badawczych i naukowych, ale ze względu na wygodę użycia stał się bardzo popularny także w przemyśle. W szczególności całkowicie zdominował przetwarzanie języka naturalnego (NLP) oraz uczenie na grafach.\n",
    "\n",
    "Pierwszy duży framework do deep learningu, oraz obecnie najpopularniejszy, to TensorFlow, wraz z wysokopoziomową nakładką Keras. Są jednak szanse, że Google (autorzy) będzie go powoli porzucać na rzecz ich nowego frameworka JAX ([dyskusja](https://www.reddit.com/r/MachineLearning/comments/vfl57t/d_google_quietly_moving_its_products_from/), [artykuł Business Insidera](https://www.businessinsider.com/facebook-pytorch-beat-google-tensorflow-jax-meta-ai-2022-6?IR=T)), który jest bardzo świeżym, ale ciekawym narzędziem.\n",
    "\n",
    "Trzecia, ale znacznie mniej popularna od powyższych opcja to Apache MXNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konfiguracja własnego komputera\n",
    "\n",
    "Jeżeli korzystasz z własnego komputera, to musisz zainstalować trochę więcej bibliotek (Google Colab ma je już zainstalowane).\n",
    "\n",
    "Jeżeli nie masz GPU lub nie chcesz z niego korzystać, to wystarczy znaleźć odpowiednią komendę CPU [na stronie PyTorcha](https://pytorch.org/get-started/locally/). Dla Anacondy odpowiednia komenda została podana poniżej, dla pip'a znajdź ją na stronie.\n",
    "\n",
    "Jeżeli chcesz korzystać ze wsparcia GPU (na tym laboratorium nie będzie potrzebne, na kolejnych może przyspieszyć nieco obliczenia), to musi być to odpowiednio nowa karta NVidii, mająca CUDA compatibility ([lista](https://developer.nvidia.com/cuda-gpus)). Poza PyTorchem będzie potrzebne narzędzie NVidia CUDA w wersji 11.6 lub 11.7. Instalacja na Windowsie jest bardzo prosta (wystarczy ściągnąć plik EXE i zainstalować jak każdy inny program). Instalacja na Linuxie jest trudna i można względnie łatwo zepsuć sobie system, ale jeżeli chcesz spróbować, to [ten tutorial](https://www.youtube.com/results?search_query=nvidia+cuda+install+ubuntu+20.04) jest bardzo dobry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for conda users\n",
    "#!conda install -y pytorch torchvision -c pytorch -c conda-forge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Othm3C2lLAsj"
   },
   "source": [
    "## Wprowadzenie\n",
    "\n",
    "Zanim zaczniemy naszą przygodę z sieciami neuronowymi, przyjrzyjmy się prostemu przykładowi regresji liniowej na syntetycznych danych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rnJsfxbnLAsj"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "EaYpEXzBLAsl",
    "outputId": "2f8d2922-72f0-4d38-8548-d1262adf522e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1af6f8b7710>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "x = np.linspace(0, 1, 100)\n",
    "y = x + np.random.normal(scale=0.1, size=x.shape)\n",
    "\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEM_-yKELAsl"
   },
   "source": [
    "W przeciwieństwie do laboratorium 1, tym razem będziemy chcieli rozwiązać ten problem własnoręcznie, bez użycia wysokopoziomowego interfejsu Scikit-learn'a. W tym celu musimy sobie przypomnieć sformułowanie naszego **problemu optymalizacyjnego (optimization problem)**.\n",
    "\n",
    "W przypadku prostej regresji liniowej (1 zmienna) mamy model postaci $\\hat{y} = \\alpha x + \\beta$, z dwoma parametrami, których będziemy się uczyć. Miarą niedopasowania modelu o danych parametrach jest **funkcja kosztu (cost function)**, nazywana też funkcją celu. Najczęściej używa się **błędu średniokwadratowego (mean squared error, MSE)**:\n",
    "$$\\large\n",
    "MSE = \\frac{1}{N} \\sum_{i}^{N} (y - \\hat{y})^2\n",
    "$$\n",
    "\n",
    "Od jakich $\\alpha$ i $\\beta$ zacząć? W najprostszym wypadku wystarczy po prostu je wylosować jako niewielkie liczby zmiennoprzecinkowe.\n",
    "\n",
    "#### Zadanie 1 (0.5 punkt)\n",
    "\n",
    "Uzupełnij kod funkcji `mse`, obliczającej błąd średniokwadratowy. Wykorzystaj Numpy'a w celu wektoryzacji obliczeń dla wydajności."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RaA7Q46TLAsm",
    "outputId": "5c57fe58-1934-4d21-9a7b-d14e9a23140b"
   },
   "outputs": [],
   "source": [
    "def mse(y: np.ndarray, y_hat: np.ndarray) -> float:\n",
    "    return np.linalg.norm(y-y_hat)\n",
    "    # implement me!\n",
    "    # your_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "qSGfamGbLAsm",
    "outputId": "733ce15f-ae75-466b-e7ee-eb3c9d9d534c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 3.647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1af6fabbc90>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.random.rand()\n",
    "b = np.random.rand()\n",
    "print(f\"MSE: {mse(y, a * x + b):.3f}\")\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, a * x + b, color=\"g\", linewidth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y--E9Mp9LAsn"
   },
   "source": [
    "Losowe parametry radzą sobie nie najlepiej. Jak lepiej dopasować naszą prostą do danych? Zawsze możemy starać się wyprowadzić rozwiązanie analitycznie, i w tym wypadku nawet nam się uda. Jest to jednak szczególny i dość rzadki przypadek, a w szczególności nie będzie to możliwe w większych sieciach neuronowych.\n",
    "\n",
    "Potrzebna nam będzie **metoda optymalizacji (optimization method)**, dającą wartości parametrów minimalizujące dowolną różniczkowalną funkcję kosztu. Zdecydowanie najpopularniejszy jest tutaj **spadek wzdłuż gradientu (gradient descent)**.\n",
    "\n",
    "Metoda ta wywodzi się z prostych obserwacji, które tutaj przedstawimy. Bardziej szczegółowe rozwinięcie dla zainteresowanych: [sekcja 4.3 \"Deep Learning Book\"](https://www.deeplearningbook.org/contents/numerical.html), [ten praktyczny kurs](https://cs231n.github.io/optimization-1/), [analiza oryginalnej publikacji Cauchy'ego](https://www.math.uni-bielefeld.de/documenta/vol-ismp/40_lemarechal-claude.pdf) (oryginał w języku francuskim).\n",
    "\n",
    "Pochodna jest dokładnie równa granicy funkcji. Dla małego $\\epsilon$ można ją przybliżyć jako:\n",
    "$$\\large\n",
    "\\frac{f(x)}{dx} \\approx \\frac{f(x+\\epsilon) - f(x)}{\\epsilon}\n",
    "$$\n",
    "\n",
    "Przyglądając się temu równaniu widzimy, że: \n",
    "* dla funkcji rosnącej ($f(x+\\epsilon) > f(x)$) wyrażenie $\\frac{f(x)}{dx}$ będzie miało znak dodatni \n",
    "* dla funkcji malejącej ($f(x+\\epsilon) < f(x)$) wyrażenie $\\frac{f(x)}{dx}$ będzie miało znak ujemny \n",
    "\n",
    "Widzimy więc, że potrafimy wskazać kierunek zmniejszenia wartości funkcji, patrząc na znak pochodnej. Zaobserwowano także, że amplituda wartości w $\\frac{f(x)}{dx}$ jest tym większa, im dalej jesteśmy od minimum (maximum). Pochodna wyznacza więc, w jakim kierunku funkcja najszybciej rośnie, zaś przeciwny zwrot to ten, w którym funkcja najszybciej spada.\n",
    "\n",
    "Stosując powyższe do optymalizacji, mamy:\n",
    "$$\\large\n",
    "x_{t+1} = x_{t} -  \\alpha * \\frac{f(x)}{dx}\n",
    "$$\n",
    "\n",
    "$\\alpha$ to niewielka wartość (rzędu zwykle $10^{-5}$ - $10^{-2}$), wprowadzona, aby trzymać się założenia o małej zmianie parametrów ($\\epsilon$). Nazywa się ją **stałą uczącą (learning rate)** i jest zwykle najważniejszym hiperparametrem podczas nauki sieci.\n",
    "\n",
    "Metoda ta zakłada, że używamy całego zbioru danych do aktualizacji parametrów w każdym kroku, co nazywa się po prostu GD (od *gradient descent*) albo *full batch GD*. Wtedy każdy krok optymalizacji nazywa się **epoką (epoch)**.\n",
    "\n",
    "Im większa stała ucząca, tym większe nasze kroki podczas minimalizacji. Możemy więc uczyć szybciej, ale istnieje ryzyko, że będziemy \"przeskakiwać\" minima. Mniejsza stała ucząca to wolniejszy, ale dokładniejszy trening. Jednak nie zawsze ona pozwala osiągnąć lepsze wyniki, bo może okazać się, że utkniemy w minimum lokalnym. Można także zmieniać stałą uczącą podczas treningu, co nazywa się **learning rate scheduling (LR scheduling)**. Obrazowo:\n",
    "\n",
    "![learning_rate](http://www.bdhammel.com/assets/learning-rate/lr-types.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "496qEjkVLAso"
   },
   "source": [
    "![interactive LR](http://cdn-images-1.medium.com/max/640/1*eeIvlwkMNG1wSmj3FR6M2g.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYkyAHKzLAsp"
   },
   "source": [
    "Policzmy więc pochodną dla naszej funkcji kosztu MSE. Pochodną liczymy po parametrach naszego modelu, bo to właśnie ich chcemy dopasować tak, żeby koszt był jak najmniejszy:\n",
    "\n",
    "$$\\large\n",
    "MSE = \\frac{1}{N} \\sum_{i}^{N} (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "W powyższym wzorze tylko $y_i$ jest zależny od $a$ oraz $b$. Możemy wykorzystać tu regułę łańcuchową (*chain rule*) i policzyć pochodne po naszych parametrach w sposób następujący:\n",
    "\n",
    "$$\\large\n",
    "\\frac{\\text{d} MSE}{\\text{d} a} = \\frac{1}{N} \\sum_{i}^{N} \\frac{\\text{d} (y_i - \\hat{y_i})^2}{\\text{d} \\hat{y_i}} \\frac{\\text{d} \\hat{y_i}}{\\text{d} a}\n",
    "$$\n",
    "\n",
    "$$\\large\n",
    "\\frac{\\text{d} MSE}{\\text{d} b} = \\frac{1}{N} \\sum_{i}^{N} \\frac{\\text{d} (y_i - \\hat{y_i})^2}{\\text{d} \\hat{y_i}} \\frac{\\text{d} \\hat{y_i}}{\\text{d} b}\n",
    "$$\n",
    "\n",
    "Policzmy te pochodne po kolei:\n",
    "\n",
    "$$\\large\n",
    "\\frac{\\text{d} (y_i - \\hat{y_i})^2}{\\text{d} \\hat{y_i}} = -2 \\cdot (y_i - \\hat{y_i})\n",
    "$$\n",
    "\n",
    "$$\\large\n",
    "\\frac{\\text{d} \\hat{y_i}}{\\text{d} a} = x_i\n",
    "$$\n",
    "\n",
    "$$\\large\n",
    "\\frac{\\text{d} \\hat{y_i}}{\\text{d} b} = 1\n",
    "$$\n",
    "\n",
    "Łącząc powyższe wyniki dostaniemy:\n",
    "\n",
    "$$\\large\n",
    "\\frac{\\text{d} MSE}{\\text{d} a} = \\frac{-2}{N} \\sum_{i}^{N} (y_i - \\hat{y_i}) \\cdot {x_i}\n",
    "$$\n",
    "\n",
    "$$\\large\n",
    "\\frac{\\text{d} MSE}{\\text{d} b} = \\frac{-2}{N} \\sum_{i}^{N} (y_i - \\hat{y_i})\n",
    "$$\n",
    "\n",
    "Aktualizacja parametrów wygląda tak:\n",
    "\n",
    "$$\\large\n",
    "a' = a - \\alpha * \\left( \\frac{-2}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i) \\cdot x_i \\right)\n",
    "$$\n",
    "$$\\large\n",
    "b' = b - \\alpha * \\left( \\frac{-2}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i) \\right)\n",
    "$$\n",
    "\n",
    "Liczymy więc pochodną funkcji kosztu, a potem za pomocą reguły łańcuchowej \"cofamy się\", dochodząc do tego, jak każdy z parametrów wpływa na błąd i w jaki sposób powinniśmy go zmienić. Nazywa się to **propagacją wsteczną (backpropagation)** i jest podstawowym mechanizmem umożliwiającym naukę sieci neuronowych za pomocą spadku wzdłuż gradientu. Więcej możesz o tym przeczytać [tutaj](https://cs231n.github.io/optimization-2/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Zadanie 2 (1.0 punkt)\n",
    "\n",
    "Zaimplementuj funkcję realizującą jedną epokę treningową. Zauważ, że `x` oraz `y` są wektorami. Oblicz predykcję przy aktualnych parametrach oraz zaktualizuj je zgodnie z powyższymi wzorami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4qbdWOSULAsp",
    "outputId": "055607ae-87aa-470a-e6da-25682c82d470"
   },
   "outputs": [],
   "source": [
    "def optimize(\n",
    "    x: np.ndarray, y: np.ndarray, a: float, b: float, learning_rate: float = 0.1\n",
    "):\n",
    "    y_hat = a * x + b\n",
    "    errors = y - y_hat\n",
    "    a += 2 * learning_rate * np.sum(errors*x) / y.size\n",
    "    b += 2 * learning_rate * np.sum(errors) / y.size\n",
    "    # implement me!\n",
    "    # your_code\n",
    "    return a, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss:  3.6472251361878225\n",
      "step 100 loss:  1.1257529826088704\n",
      "step 200 loss:  1.0127760631481086\n",
      "step 300 loss:  1.0047290059979308\n",
      "step 400 loss:  1.0041859595159215\n",
      "step 500 loss:  1.004149458147209\n",
      "step 600 loss:  1.004147005334364\n",
      "step 700 loss:  1.0041468405136404\n",
      "step 800 loss:  1.0041468294382587\n",
      "step 900 loss:  1.0041468286940316\n",
      "final loss: 1.0041468286441209\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    loss = mse(y, a * x + b)\n",
    "    a, b = optimize(x, y, a, b)\n",
    "    if i % 100 == 0:\n",
    "        print(f\"step {i} loss: \", loss)\n",
    "\n",
    "print(\"final loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "xOgRcPC1LAsq",
    "outputId": "85b0b3e4-aa0d-467a-d8ff-5f01be17b243",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1af6fa35290>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.plot(x, a * x + b, color=\"g\", linewidth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOr2fWYpLAsq"
   },
   "source": [
    "Udało ci się wytrenować swoją pierwszą sieć neuronową. Czemu? Otóż neuron to po prostu wektor parametrów, a zwykle robimy iloczyn skalarny tych parametrów z wejściem. Dodatkowo na wyjście nakłada się **funkcję aktywacji (activation function)**, która przekształca wyjście. Tutaj takiej nie było, a właściwie była to po prostu funkcja identyczności.\n",
    "\n",
    "Oczywiście w praktyce korzystamy z odpowiedniego frameworka, który w szczególności:\n",
    "- ułatwia budowanie sieci, np. ma gotowe klasy dla warstw neuronów\n",
    "- ma zaimplementowane funkcje kosztu oraz ich pochodne\n",
    "- sam różniczkuje ze względu na odpowiednie parametry i aktualizuje je odpowiednio podczas treningu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJBYJabuLAsr"
   },
   "source": [
    "## Wprowadzenie do PyTorcha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EB-99XqhLAsr"
   },
   "source": [
    "PyTorch to w gruncie rzeczy narzędzie do algebry liniowej z [automatycznym rożniczkowaniem](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html), z możliwością przyspieszenia obliczeń z pomocą GPU. Na tych fundamentach zbudowany jest pełny framework do uczenia głębokiego. Można spotkać się ze stwierdzenie, że PyTorch to NumPy + GPU + opcjonalne różniczkowanie, co jest całkiem celne. Plus można łatwo debugować printem :)\n",
    "\n",
    "PyTorch używa dynamicznego grafu obliczeń, który sami definiujemy w kodzie. Takie podejście jest bardzo wygodne, elastyczne i pozwala na łatwe eksperymentowanie. Odbywa się to potencjalnie kosztem wydajności, ponieważ pozostawia kwestię optymalizacji programiście. Więcej na ten temat dla zainteresowanych na końcu laboratorium.\n",
    "\n",
    "Samo API PyTorcha bardzo przypomina Numpy'a, a podstawowym obiektem jest `Tensor`, klasa reprezentująca tensory dowolnego wymiaru. Dodatkowo niektóre tensory będą miały automatycznie obliczony gradient. Co ważne, tensor jest na pewnym urządzeniu, CPU lub GPU, a przenosić między nimi trzeba explicite.\n",
    "\n",
    "Najważniejsze moduły:\n",
    "- `torch` - podstawowe klasy oraz funkcje, np. `Tensor`, `from_numpy()`\n",
    "- `torch.nn` - klasy związane z sieciami neuronowymi, np. `Linear`, `Sigmoid`\n",
    "- `torch.optim` - wszystko związane z optymalizacją, głównie spadkiem wzdłuż gradientu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FwuIt8S-LAss"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfCiUFXULAss",
    "outputId": "83f6231d-ecc4-461a-b758-fdc4bc2a88a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.6027, 1.5283, 1.3901, 1.9576, 1.7786, 1.5628, 1.2439, 1.4743, 1.4467,\n",
      "        1.5090])\n",
      "tensor([0.6027, 0.5283, 0.3901, 0.9576, 0.7786, 0.5628, 0.2439, 0.4743, 0.4467,\n",
      "        0.5090])\n",
      "tensor(5.4941)\n"
     ]
    }
   ],
   "source": [
    "ones = torch.ones(10)\n",
    "noise = torch.ones(10) * torch.rand(10)\n",
    "\n",
    "# elementwise sum\n",
    "print(ones + noise)\n",
    "\n",
    "# elementwise multiplication\n",
    "print(ones * noise)\n",
    "\n",
    "# dot product\n",
    "print(ones @ noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ynNd_kD0LAst"
   },
   "outputs": [],
   "source": [
    "# beware - shares memory with original Numpy array!\n",
    "# very fast, but modifications are visible to original variable\n",
    "x = torch.from_numpy(x)\n",
    "y = torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9kkxczELAsu"
   },
   "source": [
    "Jeżeli dla stworzonych przez nas tensorów chcemy śledzić operacje i obliczać gradient, to musimy oznaczyć `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8HtZL-KfLAsu",
    "outputId": "47c6d930-5678-452a-95bc-227935138b40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.7178], requires_grad=True), tensor([0.4949], requires_grad=True))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(1, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nl1guWZ_LAsv"
   },
   "source": [
    "PyTorch zawiera większość powszechnie używanych funkcji kosztu, np. MSE. Mogą być one używane na 2 sposoby, z czego pierwszy jest popularniejszy:\n",
    "- jako klasy wywoływalne z modułu `torch.nn`\n",
    "- jako funkcje z modułu `torch.nn.functional`\n",
    "\n",
    "Po wykonaniu poniższego kodu widzimy, że zwraca on nam tensor z dodatkowymi atrybutami. Co ważne, jest to skalar (0-wymiarowy tensor), bo potrzebujemy zwyczajnej liczby do obliczania propagacji wstecznych (pochodnych czątkowych)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1365, dtype=torch.float64, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = nn.MSELoss()\n",
    "mse(y, a * x + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vS35r49nLAsw"
   },
   "source": [
    "Atrybutu `grad_fn` nie używamy wprost, bo korzysta z niego w środku PyTorch, ale widać, że tensor jest \"świadomy\", że liczy się na nim pochodną. Możemy natomiast skorzystać z atrybutu `grad`, który zawiera faktyczny gradient. Zanim go jednak dostaniemy, to trzeba powiedzieć PyTorchowi, żeby policzył gradient. Służy do tego metoda `.backward()`, wywoływana na obiekcie zwracanym przez funkcję kosztu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Qb7l6Xg1LAsx"
   },
   "outputs": [],
   "source": [
    "loss = mse(y, a * x + b)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6LfQbLVoLAsx",
    "outputId": "d5b87fb7-d284-423c-f467-b677384b2f67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3049])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kdf1iweELAsy"
   },
   "source": [
    "Ważne jest, że PyTorch nie liczy za każdym razem nowego gradientu, tylko dodaje go do istniejącego, czyli go akumuluje. Jest to przydatne w niektórych sieciach neuronowych, ale zazwyczaj trzeba go zerować. Jeżeli tego nie zrobimy, to dostaniemy coraz większe gradienty.\n",
    "\n",
    "Do zerowania służy metoda `.zero_()`. W PyTorchu wszystkie metody modyfikujące tensor w miejscu mają `_` na końcu nazwy. Jest to dość niskopoziomowa operacja dla pojedynczych tensorów - zobaczymy za chwilę, jak to robić łatwiej dla całej sieci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DiCQZKJsLAsy",
    "outputId": "2f779622-480d-43fc-b9d0-a0e36ff4b28b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6098])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse(y, a * x + b)\n",
    "loss.backward()\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNC3Ag8uLAsz"
   },
   "source": [
    "Zobaczmy, jak wyglądałaby regresja liniowa, ale napisana w PyTorchu. Jest to oczywiście bardzo niskopoziomowa implementacja - za chwilę zobaczymy, jak to wygląda w praktyce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AKnxyeboLAsz",
    "outputId": "2f939474-901a-4773-9704-686a40ae6e8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss:  tensor(0.1365, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "step 100 loss:  tensor(0.0109, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "step 200 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "step 300 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "step 400 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "step 500 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "step 600 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "step 700 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "step 800 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "step 900 loss:  tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "final loss: tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "for i in range(1000):\n",
    "    loss = mse(y, a * x + b)\n",
    "\n",
    "    # compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    a.data -= learning_rate * a.grad\n",
    "    b.data -= learning_rate * b.grad\n",
    "\n",
    "    # zero gradients\n",
    "    a.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"step {i} loss: \", loss)\n",
    "\n",
    "print(\"final loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DXNVhshmmI-"
   },
   "source": [
    "Trening modeli w PyTorchu jest dosyć schematyczny i najczęściej rozdziela się go na kilka bloków, dających razem **pętlę uczącą (training loop)**, powtarzaną w każdej epoce:\n",
    "1. Forward pass - obliczenie predykcji sieci\n",
    "2. Loss calculation\n",
    "3. Backpropagation - obliczenie pochodnych oraz zerowanie gradientów\n",
    "4. Optimalization - aktualizacja wag\n",
    "5. Other - ewaluacja na zbiorze walidacyjnym, logging etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2etpw7TNLAs0",
    "outputId": "8ac35c12-6c70-41ec-bf57-414456fc3c96",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss: 0.0900\n",
      "step 100 loss: 0.0137\n",
      "step 200 loss: 0.0103\n",
      "step 300 loss: 0.0101\n",
      "step 400 loss: 0.0101\n",
      "step 500 loss: 0.0101\n",
      "step 600 loss: 0.0101\n",
      "step 700 loss: 0.0101\n",
      "step 800 loss: 0.0101\n",
      "step 900 loss: 0.0101\n",
      "final loss: tensor(0.0101, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "learning_rate = 0.1\n",
    "a = torch.rand(1, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "optimizer = torch.optim.SGD([a, b], lr=learning_rate)\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "# training loop in each epoch\n",
    "for i in range(1000):\n",
    "    # forward pass\n",
    "    y_hat = a * x + b\n",
    "\n",
    "    # loss calculation\n",
    "    loss = mse(y, y_hat)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # optimization\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()  # zeroes all gradients - very convenient!\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        if loss < best_loss:\n",
    "            best_model = (a.clone(), b.clone())\n",
    "            best_loss = loss\n",
    "        print(f\"step {i} loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"final loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przejdziemy teraz do budowy sieci neuronowej do klasyfikacji. Typowo implementuje się ją po prostu jako sieć dla regresji, ale zwracającą tyle wyników, ile mamy klas, a potem aplikuje się na tym funkcję sigmoidalną (2 klasy) lub softmax (>2 klasy). W przypadku klasyfikacji binarnej zwraca się czasem tylko 1 wartość, przepuszczaną przez sigmoidę - wtedy wyjście z sieci to prawdopodobieństwo klasy pozytywnej.\n",
    "\n",
    "Funkcją kosztu zwykle jest **entropia krzyżowa (cross-entropy)**, stosowana też w klasycznej regresji logistycznej. Co ważne, sieci neuronowe, nawet tak proste, uczą się szybciej i stabilniej, gdy dane na wejściu (a przynajmniej zmienne numeryczne) są **ustandaryzowane (standardized)**. Operacja ta polega na odjęciu średniej i podzieleniu przez odchylenie standardowe (tzw. *Z-score transformation*).\n",
    "\n",
    "**Uwaga - PyTorch wymaga tensora klas będącego liczbami zmiennoprzecinkowymi!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zbiór danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na tym laboratorium wykorzystamy zbiór [Adult Census](https://archive.ics.uci.edu/ml/datasets/adult). Dotyczy on przewidywania na podstawie danych demograficznych, czy dany człowiek zarabia powyżej 50 tysięcy dolarów rocznie, czy też mniej. Jest to cenna informacja np. przy planowaniu kampanii marketingowych. Jak możesz się domyślić, zbiór pochodzi z czasów, kiedy inflacja była dużo niższa :)\n",
    "\n",
    "Poniżej znajduje się kod do ściągnięcia i preprocessingu zbioru. Nie musisz go dokładnie analizować."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DNsaZAnLAs0",
    "outputId": "70822008-530d-4173-deb9-8149a9fe5b41",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' >50K'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education-num\",\n",
    "    \"marital-status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"capital-gain\",\n",
    "    \"capital-loss\",\n",
    "    \"hours-per-week\",\n",
    "    \"native-country\",\n",
    "    \"wage\"\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "age: continuous.\n",
    "workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "fnlwgt: continuous.\n",
    "education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "education-num: continuous.\n",
    "marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "sex: Female, Male.\n",
    "capital-gain: continuous.\n",
    "capital-loss: continuous.\n",
    "hours-per-week: continuous.\n",
    "native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv(\"adult.data\", header=None, names=columns)\n",
    "df.wage.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attribution: https://www.kaggle.com/code/royshih23/topic7-classification-in-python\n",
    "df['education'].replace('Preschool', 'dropout',inplace=True)\n",
    "df['education'].replace('10th', 'dropout',inplace=True)\n",
    "df['education'].replace('11th', 'dropout',inplace=True)\n",
    "df['education'].replace('12th', 'dropout',inplace=True)\n",
    "df['education'].replace('1st-4th', 'dropout',inplace=True)\n",
    "df['education'].replace('5th-6th', 'dropout',inplace=True)\n",
    "df['education'].replace('7th-8th', 'dropout',inplace=True)\n",
    "df['education'].replace('9th', 'dropout',inplace=True)\n",
    "df['education'].replace('HS-Grad', 'HighGrad',inplace=True)\n",
    "df['education'].replace('HS-grad', 'HighGrad',inplace=True)\n",
    "df['education'].replace('Some-college', 'CommunityCollege',inplace=True)\n",
    "df['education'].replace('Assoc-acdm', 'CommunityCollege',inplace=True)\n",
    "df['education'].replace('Assoc-voc', 'CommunityCollege',inplace=True)\n",
    "df['education'].replace('Bachelors', 'Bachelors',inplace=True)\n",
    "df['education'].replace('Masters', 'Masters',inplace=True)\n",
    "df['education'].replace('Prof-school', 'Masters',inplace=True)\n",
    "df['education'].replace('Doctorate', 'Doctorate',inplace=True)\n",
    "\n",
    "df['marital-status'].replace('Never-married', 'NotMarried',inplace=True)\n",
    "df['marital-status'].replace(['Married-AF-spouse'], 'Married',inplace=True)\n",
    "df['marital-status'].replace(['Married-civ-spouse'], 'Married',inplace=True)\n",
    "df['marital-status'].replace(['Married-spouse-absent'], 'NotMarried',inplace=True)\n",
    "df['marital-status'].replace(['Separated'], 'Separated',inplace=True)\n",
    "df['marital-status'].replace(['Divorced'], 'Separated',inplace=True)\n",
    "df['marital-status'].replace(['Widowed'], 'Widowed',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LiOxs_6mLAs1",
    "outputId": "c95418cf-2632-41d0-de0a-9caf109de113",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Władek\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((20838, 108), (20838,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "X = df.copy()\n",
    "y = (X.pop(\"wage\") == ' >50K').astype(int).values\n",
    "\n",
    "train_valid_size = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=train_valid_size, \n",
    "    random_state=0, \n",
    "    shuffle=True, \n",
    "    stratify=y\n",
    ")\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, \n",
    "    test_size=train_valid_size, \n",
    "    random_state=0, \n",
    "    shuffle=True, \n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "continuous_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "continuous_X_train = X_train[continuous_cols]\n",
    "categorical_X_train = X_train.loc[:, ~X_train.columns.isin(continuous_cols)]\n",
    "\n",
    "continuous_X_valid = X_valid[continuous_cols]\n",
    "categorical_X_valid = X_valid.loc[:, ~X_valid.columns.isin(continuous_cols)]\n",
    "\n",
    "continuous_X_test = X_test[continuous_cols]\n",
    "categorical_X_test = X_test.loc[:, ~X_test.columns.isin(continuous_cols)]\n",
    "\n",
    "categorical_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "continuous_scaler = StandardScaler() #MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "categorical_encoder.fit(categorical_X_train)\n",
    "continuous_scaler.fit(continuous_X_train)\n",
    "\n",
    "continuous_X_train = continuous_scaler.transform(continuous_X_train)\n",
    "continuous_X_valid = continuous_scaler.transform(continuous_X_valid)\n",
    "continuous_X_test = continuous_scaler.transform(continuous_X_test)\n",
    "\n",
    "categorical_X_train = categorical_encoder.transform(categorical_X_train)\n",
    "categorical_X_valid = categorical_encoder.transform(categorical_X_valid)\n",
    "categorical_X_test = categorical_encoder.transform(categorical_X_test)\n",
    "\n",
    "X_train = np.concatenate([continuous_X_train, categorical_X_train], axis=1)\n",
    "X_valid = np.concatenate([continuous_X_valid, categorical_X_valid], axis=1)\n",
    "X_test = np.concatenate([continuous_X_test, categorical_X_test], axis=1)\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uwaga co do typów - PyTorchu wszystko w sieci neuronowej musi być typu `float32`. W szczególności trzeba uważać na konwersje z Numpy'a, który używa domyślnie typu `float64`. Może ci się przydać metoda `.float()`.\n",
    "\n",
    "Uwaga co do kształtów wyjścia - wejścia do `nn.BCELoss` muszą być tego samego kształtu. Może ci się przydać metoda `.squeeze()` lub `.unsqueeze()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "qfRA3xEoLAs1"
   },
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float().unsqueeze(-1)\n",
    "\n",
    "X_valid = torch.from_numpy(X_valid).float()\n",
    "y_valid = torch.from_numpy(y_valid).float().unsqueeze(-1)\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float().unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podobnie jak w laboratorium 2, mamy tu do czynienia z klasyfikacją niezbalansowaną:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pos_perc = 100 * y_train.sum().item() / len(y_train)\n",
    "y_neg_perc = 100 - y_pos_perc\n",
    "\n",
    "plt.title(\"Class percentages\")\n",
    "plt.bar([\"<50k\", \">=50k\"], [y_neg_perc, y_pos_perc])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W związku z powyższym będziemy używać odpowiednich metryk, czyli AUROC, precyzji i czułości."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLexWff-LAs0"
   },
   "source": [
    "#### Zadanie 3 (1.0 punkt)\n",
    "\n",
    "Zaimplementuj regresję logistyczną dla tego zbioru danych, używając PyTorcha. Dane wejściowe zostały dla ciebie przygotowane w komórkach poniżej.\n",
    "\n",
    "Sama sieć składa się z 2 elementów:\n",
    "- warstwa liniowa `nn.Linear`, przekształcająca wektor wejściowy na 1 wyjście - logit\n",
    "- aktywacja sigmoidalna `nn.Sigmoid`, przekształcająca logit na prawdopodobieństwo klasy pozytywnej\n",
    "\n",
    "Użyj binarnej entropii krzyżowej `nn.BCELoss` jako funkcji kosztu. Użyj optymalizatora SGD ze stałą uczącą `1e-3`. Trenuj przez 3000 epok. Pamiętaj, aby przekazać do optymalizatora `torch.optim.SGD` parametry sieci (metoda `.parameters()`). Dopisz logowanie kosztu raz na 100 epok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20838, 108])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NbABKz5-LAs2",
    "outputId": "086dc0f3-0184-4072-9fd3-275b60dee2e4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss: 0.7186\n",
      "step 100 loss: 0.7163\n",
      "step 200 loss: 0.7141\n",
      "step 300 loss: 0.7118\n",
      "step 400 loss: 0.7096\n",
      "step 500 loss: 0.7074\n",
      "step 600 loss: 0.7052\n",
      "step 700 loss: 0.7031\n",
      "step 800 loss: 0.7009\n",
      "step 900 loss: 0.6988\n",
      "step 1000 loss: 0.6968\n",
      "step 1100 loss: 0.6947\n",
      "step 1200 loss: 0.6927\n",
      "step 1300 loss: 0.6908\n",
      "step 1400 loss: 0.6888\n",
      "step 1500 loss: 0.6869\n",
      "step 1600 loss: 0.6851\n",
      "step 1700 loss: 0.6833\n",
      "step 1800 loss: 0.6815\n",
      "step 1900 loss: 0.6798\n",
      "step 2000 loss: 0.6781\n",
      "step 2100 loss: 0.6765\n",
      "step 2200 loss: 0.6749\n",
      "step 2300 loss: 0.6733\n",
      "step 2400 loss: 0.6718\n",
      "step 2500 loss: 0.6703\n",
      "step 2600 loss: 0.6689\n",
      "step 2700 loss: 0.6675\n",
      "step 2800 loss: 0.6661\n",
      "step 2900 loss: 0.6648\n",
      "final loss: tensor(0.6635, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "model = torch.nn.Linear(X_train.shape[1], 1)\n",
    "activation = torch.nn.Sigmoid()\n",
    "loss_fn=nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "best_loss = float(\"inf\")\n",
    "optimizer.zero_grad()\n",
    "model.train()\n",
    "for i in range(3000):\n",
    "    # forward pass\n",
    "    y_hat = activation(model(X_train))\n",
    "\n",
    "    # loss calculation\n",
    "    loss = loss_fn(activation(y_train), y_hat)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # optimization\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()  # zeroes all gradients - very convenient!\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        if loss < best_loss:\n",
    "            best_model = (a.clone(), b.clone())\n",
    "            best_loss = loss\n",
    "        print(f\"step {i} loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"final loss:\", loss)\n",
    "# implement me!\n",
    "# your_code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz trzeba sprawdzić, jak poszło naszej sieci. W PyTorchu sieć pracuje zawsze w jednym z dwóch trybów: treningowym lub ewaluacyjnym (predykcyjnym). Ten drugi wyłącza niektóre mechanizmy, które są używane tylko podczas treningu, w szczególności regularyzację dropout. Do przełączania służą metody modelu `.train()` i `.eval()`.\n",
    "\n",
    "Dodatkowo podczas liczenia predykcji dobrze jest wyłączyć liczenie gradientów, bo nie będą potrzebne, a oszczędza to czas i pamięć. Używa się do tego menadżera kontekstu `with torch.no_grad():`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zH37zDX4LAs2",
    "outputId": "b1f93309-6f04-4ffc-b0ca-08d0a32120a0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 80.42%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_score = activation(model(X_test))\n",
    "\n",
    "auroc = roc_auc_score(y_test, y_score)\n",
    "print(f\"AUROC: {100 * auroc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jest to całkiem dobry wynik, a może być jeszcze lepszy. Sprawdźmy dla pewności jeszcze inne metryki: precyzję, recall oraz F1-score. Dodatkowo narysujemy krzywą precision-recall, czyli jak zmieniają się te metryki w zależności od przyjętego progu (threshold) prawdopodobieństwa, powyżej którego przyjmujemy klasę pozytywną. Taką krzywą należy rysować na zbiorze walidacyjnym, bo później chcemy wykorzystać tę informację do doboru progu, a nie chcemy mieć wycieku danych testowych (data leakage).\n",
    "\n",
    "Poniżej zaimplementowano także funkcję `get_optimal_threshold()`, która sprawdza, dla którego progu uzyskujemy maksymalny F1-score, i zwraca indeks oraz wartość optymalnego progu. Przyda ci się ona w dalszej części laboratorium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "\n",
    "def get_optimal_threshold(\n",
    "    precisions: np.array, \n",
    "    recalls: np.array, \n",
    "    thresholds: np.array\n",
    ") -> Tuple[int, float]:\n",
    "    \n",
    "    numerator = 2 * precisions * recalls\n",
    "    denominator = precisions + recalls\n",
    "    f1_scores = np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator != 0)\n",
    "    \n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    return optimal_idx, optimal_threshold\n",
    "\n",
    "\n",
    "def plot_precision_recall_curve(y_true, y_pred_score) -> None:\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred_score)\n",
    "    optimal_idx, optimal_threshold = get_optimal_threshold(precisions, recalls, thresholds)\n",
    "\n",
    "    disp = PrecisionRecallDisplay(precisions, recalls)\n",
    "    disp.plot()\n",
    "    plt.title(f\"Precision-recall curve (opt. thresh.: {optimal_threshold:.4f})\")\n",
    "    plt.axvline(recalls[optimal_idx], color=\"green\", linestyle=\"-.\")\n",
    "    plt.axhline(precisions[optimal_idx], color=\"green\", linestyle=\"-.\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_valid_score = activation(model(X_valid))\n",
    "\n",
    "plot_precision_recall_curve(y_valid, y_pred_valid_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfQPIUQ_LAs2"
   },
   "source": [
    "Jak widać, chociaż AUROC jest wysokie, to dla optymalnego F1-score recall nie jest zbyt wysoki, a precyzja jest już dość niska. Być może wynik uda się poprawić, używając modelu o większej pojemności - pełnej, głębokiej sieci neuronowej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sieci neuronowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YP298w6Cq7T6"
   },
   "source": [
    "Wszystko zaczęło się od inspirowanych biologią [sztucznych neuronów](https://en.wikipedia.org/wiki/Artificial_neuron), których próbowano użyć do symulacji mózgu. Naukowcy szybko odeszli od tego podejścia (sam problem modelowania okazał się też znacznie trudniejszy, niż sądzono), zamiast tego używając neuronów jako jednostek reprezentującą dowolną funkcję parametryczną $f(x, \\Theta)$. Każdy neuron jest zatem bardzo elastyczny, bo jedyne wymagania to funkcja różniczkowalna, a mamy do tego wektor parametrów $\\Theta$.\n",
    "\n",
    "W praktyce najczęściej można spotkać się z kilkoma rodzinami sieci neuronowych:\n",
    "1. Perceptrony wielowarstwowe (*MultiLayer Perceptron*, MLP) - najbardziej podobne do powyższego opisu, niezbędne do klasyfikacji i regresji\n",
    "2. Konwolucyjne (*Convolutional Neural Networks*, CNNs) - do przetwarzania danych z zależnościami przestrzennymi, np. obrazów czy dźwięku\n",
    "3. Rekurencyjne (*Recurrent Neural Networks*, RNNs) - do przetwarzania danych z zależnościami sekwencyjnymi, np. szeregi czasowe, oraz kiedyś do języka naturalnego\n",
    "4. Transformacyjne (*Transformers*), oparte o mechanizm atencji (*attention*) - do przetwarzania języka naturalnego (NLP), z którego wyparły RNNs, a coraz częściej także do wszelkich innych danych, np. obrazów, dźwięku\n",
    "5. Grafowe (*Graph Neural Networks*, GNNS) - do przetwarzania grafów\n",
    "\n",
    "Na tym laboratorium skupimy się na najprostszej architekturze, czyli MLP. Jest ona powszechnie łączona z wszelkimi innymi architekturami, bo pozwala dokonywać klasyfikacji i regresji. Przykładowo, klasyfikacja obrazów to zwykle CNN + MLP, klasyfikacja tekstów to transformer + MLP, a regresja na grafach to GNN + MLP.\n",
    "\n",
    "Dodatkowo, pomimo prostoty MLP są bardzo potężne - udowodniono, że perceptrony (ich powszechna nazwa) są [uniwersalnym aproksymatorem](https://www.sciencedirect.com/science/article/abs/pii/0893608089900208), będącym w stanie przybliżyć dowolną funkcję z odpowiednio małym błędem, zakładając wystarczającą wielkość warstw sieci. Szczególne ich wersje potrafią nawet [reprezentować drzewa decyzyjne](https://www.youtube.com/watch?v=_okxGdHM5b8).\n",
    "\n",
    "Dla zainteresowanych polecamy [doskonałą książkę \"Dive into Deep Learning\", z implementacjami w PyTorchu](https://d2l.ai/chapter_multilayer-perceptrons/index.html), [klasyczną książkę \"Deep Learning Book\"](https://www.deeplearningbook.org/contents/mlp.html), oraz [ten filmik](https://www.youtube.com/watch?v=BFHrIxKcLjA), jeśli zastanawiałeś/-aś się, czemu używamy deep learning, a nie naprzykład (wide?) learning. (aka. czemu staramy się budować głębokie sieci, a nie płytkie za to szerokie)"
   ]
  },
  {
   "attachments": {
    "1_x-3NGQv0pRIab8xDT-f_Hg.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "id": "S_ZjoGBU5upj"
   },
   "source": [
    "### Sieci MLP\n",
    "\n",
    "Dla przypomnienia, na wejściu mamy punkty ze zbioru treningowego, czyli $d$-wymiarowe wektory. W klasyfikacji chcemy znaleźć granicę decyzyjną, czyli krzywą, która oddzieli od siebie klasy. W wejściowej przestrzeni może być to trudne, bo chmury punktów z poszczególnych klas mogą być ze sobą dość pomieszane. Pamiętajmy też, że regresja logistyczna jest klasyfikatorem liniowym, czyli w danej przestrzeni potrafi oddzielić punkty tylko linią prostą.\n",
    "\n",
    "Sieć MLP składa się z warstw. Każda z nich dokonuje nieliniowego przekształcenia przestrzeni (można o tym myśleć jak o składaniu przestrzeni jakąś prostą/łamaną), tak, aby w finalnej przestrzeni nasze punkty były możliwie liniowo separowalne. Wtedy ostatnia warstwa z sigmoidą będzie potrafiła je rozdzielić od siebie.\n",
    "\n",
    "![1_x-3NGQv0pRIab8xDT-f_Hg.png](attachment:1_x-3NGQv0pRIab8xDT-f_Hg.png)\n",
    "\n",
    "Poszczególne neurony składają się z iloczynu skalarnego wejść z wagami neuronu, oraz nieliniowej funkcji aktywacji. W PyTorchu są to osobne obiekty - `nn.Linear` oraz np. `nn.Sigmoid`. Funkcja aktywacji przyjmuje wynik iloczynu skalarnego i przekształca go, aby sprawdzić, jak mocno reaguje neuron na dane wejście. Musi być nieliniowa z dwóch powodów. Po pierwsze, tylko nieliniowe przekształcenia są na tyle potężne, żeby umożliwić liniową separację danych w ostatniej warstwie. Po drugie, liniowe przekształcenia zwyczajnie nie działają. Aby zrozumieć czemu, trzeba zobaczyć, co matematycznie oznacza sieć MLP.\n",
    "\n",
    "![perceptron](https://www.saedsayad.com/images/Perceptron_bkp_1.png)\n",
    "\n",
    "Zapisane matematycznie MLP to:\n",
    "\n",
    "$\\large\n",
    "h_1 = f_1(x) \\\\\n",
    "h_2 = f_2(h_1) \\\\\n",
    "h_3 = f_3(h_2) \\\\\n",
    "... \\\\\n",
    "h_n = f_n(h_{n-1})\n",
    "$\n",
    "\n",
    "gdzie $x$ to wejście $f_i$ to funkcja aktywacji $i$-tej warstwy, a $h_i$ to wyjście $i$-tej warstwy, nazywane **ukrytą reprezentacją (hidden representation)**, lub *latent representation*. Nazwa bierze się z tego, że w środku sieci wyciągamy cechy i wzorce w danych, które nie są widoczne na pierwszy rzut oka na wejściu.\n",
    "\n",
    "Załóżmy, że uczymy się na danych $x$ o jednym wymiarze (dla uproszczenia wzorów) oraz nie mamy funkcji aktywacji, czyli wykorzystujemy tak naprawdę aktywację liniową $f(x) = x$. Zobaczmy jak będą wyglądać dane przechodząc przez kolejne warstwy:\n",
    "\n",
    "$\\large\n",
    "h_1 = f_1(xw_1) = xw_1 \\\\\n",
    "h_2 = f_2(h_1w_2) = xw_1w_2 \\\\\n",
    "... \\\\\n",
    "h_n = f_n(h_{n-1}w_n) = xw_1w_2...w_n\n",
    "$\n",
    "\n",
    "gdzie $w_i$ to jest parametr $i$-tej warstwy sieci, $x$ to są dane (w naszym przypadku jedna liczba) wejściowa, a $h_i$ to wyjście $i$-tej warstwy.\n",
    "\n",
    "Jak widać, taka sieć o $n$ warstwach jest równoważna sieci o jednej warstwie z parametrem $w = w_1w_2...w_n$. Wynika to z tego, że złożenie funkcji liniowych jest także funkcją liniową - patrz notatki z algebry :)\n",
    "\n",
    "Jeżeli natomiast użyjemy nieliniowej funkcji aktywacji, często oznaczanej jako $\\sigma$, to wszystko będzie działać. Co ważne, ostatnia warstwa, dająca wyjście sieci, ma zwykle inną aktywację od warstw wewnątrz sieci, bo też ma inne zadanie - zwrócić wartość dla klasyfikacji lub regresji. Na wyjściu korzysta się z funkcji liniowej (regresja), sigmoidalnej (klasyfikacja binarna) lub softmax (klasyfikacja wieloklasowa).\n",
    "\n",
    "Wewnątrz sieci używano kiedyś sigmoidy oraz tangensa hiperbolicznego `tanh`, ale okazało się to nieefektywne przy uczeniu głębokich sieci o wielu warstwach. Nowoczesne sieci korzystają zwykle z funkcji ReLU (*rectified linear unit*), która jest zaskakująco prosta: $ReLU(x) = \\max(0, x)$. Okazało się, że bardzo dobrze nadaje się do treningu nawet bardzo głębokich sieci neuronowych. Nowsze funkcje aktywacji są głównie modyfikacjami ReLU.\n",
    "\n",
    "![relu](https://www.nomidl.com/wp-content/uploads/2022/04/image-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP w PyTorchu\n",
    "\n",
    "Warstwę neuronów w MLP nazywa się warstwą gęstą (*dense layer*) lub warstwą w pełni połączoną (*fully-connected layer*), i taki opis oznacza zwykle same neurony oraz funkcję aktywacji. PyTorch, jak już widzieliśmy, definiuje osobno transformację liniową oraz aktywację, a więc jedna warstwa składa się de facto z 2 obiektów, wywoływanych jeden po drugim. Inne frameworki, szczególnie wysokopoziomowe (np. Keras) łączą to często w jeden obiekt.\n",
    "\n",
    "MLP składa się zatem z sekwencji obiektów, które potem wywołuje się jeden po drugim, gdzie wyjście poprzedniego to wejście kolejnego. Ale nie można tutaj używać Pythonowych list! Z perspektywy PyTorcha to wtedy niezależne obiekty i nie zostanie wtedy przekazany między nimi gradient. Trzeba tutaj skorzystać z `nn.Sequential`, aby tworzyć taki pipeline.\n",
    "\n",
    "Rozmiary wejścia i wyjścia dla każdej warstwy trzeba w PyTorchu podawać explicite. Jest to po pierwsze edukacyjne, a po drugie często ułatwia wnioskowanie o działaniu sieci oraz jej debugowanie - mamy jasno podane, czego oczekujemy. Niektóre frameworki (np. Keras) obliczają to automatycznie.\n",
    "\n",
    "Co ważne, ostatnia warstwa zwykle nie ma funkcji aktywacji. Wynika to z tego, że obliczanie wielu funkcji kosztu (np. entropii krzyżowej) na aktywacjach jest często niestabilne numerycznie. Z tego powodu PyTorch oferuje funkcje kosztu zawierające w środku aktywację dla ostatniej warstwy, a ich implementacje są stabilne numerycznie. Przykładowo, `nn.BCELoss` przyjmuje wejście z zaaplikowanymi już aktywacjami, ale może skutkować under/overflow, natomiast `nn.BCEWithLogitsLoss` przyjmuje wejście bez aktywacji, a w środku ma specjalną implementację łączącą binarną entropię krzyżową z aktywacją sigmoidalną. Oczywiście w związku z tym aby dokonać potem predykcji w praktyce, trzeba pamiętać o użyciu funkcji aktywacji. Często korzysta się przy tym z funkcji z modułu `torch.nn.functional`, które są w tym wypadku nieco wygodniejsze od klas wywoływalnych z `torch.nn`.\n",
    "\n",
    "Całe sieci w PyTorchu tworzy się jako klasy dziedziczące po `nn.Module`. Co ważne, obiekty, z których tworzymy sieć, np. `nn.Linear`, także dziedziczą po tej klasie. Pozwala to na bardzo modułową budowę kodu, zgodną z zasadami OOP. W konstruktorze najpierw trzeba zawsze wywołać konstruktor rodzica - `super().__init__()`, a później tworzy się potrzebne obiekty i zapisuje jako atrybuty. Każdy atrybut dziedziczący po `nn.Module` lub `nn.Parameter` jest uważany za taki, który zawiera parametry sieci, a więc przy wywołaniu metody `parameters()` - parametry z tych atrybutów pojawią się w liście wszystkich parametrów. Musimy też zdefiniować metodę `forward()`, która przyjmuje tensor `x` i zwraca wynik. Typowo ta metoda po prostu używa obiektów zdefiniowanych w konstruktorze.\n",
    "\n",
    "\n",
    "**UWAGA: nigdy w normalnych warunkach się nie woła metody `forward` ręcznie**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8niDgExAMDO"
   },
   "source": [
    "#### Zadanie 4 (0.5 punktu)\n",
    "\n",
    "Uzupełnij implementację 3-warstwowej sieci MLP. Użyj rozmiarów:\n",
    "* pierwsza warstwa: input_size x 256\n",
    "* druga warstwa: 256 x 128\n",
    "* trzecia warstwa: 128 x 1\n",
    "\n",
    "Użyj funkcji aktywacji ReLU.\n",
    "\n",
    "Przydatne klasy:\n",
    "- `nn.Sequential`\n",
    "- `nn.Linear`\n",
    "- `nn.ReLU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import sigmoid\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size: int):\n",
    "        super().__init__()\n",
    "        activation = torch.nn.ReLU()\n",
    "        L1 = torch.nn.Linear(input_size, 256)\n",
    "        L2 = torch.nn.Linear(256, 128)\n",
    "        L3 = torch.nn.Linear(128, 1)\n",
    "        self.model = nn.Sequential(\n",
    "            L1,\n",
    "            activation,\n",
    "            L2,\n",
    "            activation,\n",
    "            L3\n",
    "        )\n",
    "        self.model.train()\n",
    "        # implement me!\n",
    "        # your_code\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        # implement me!\n",
    "        # your_code\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        return sigmoid(self(x))\n",
    "    \n",
    "    def predict(self, x, threshold: float = 0.5):\n",
    "        y_pred_score = self.predict_proba(x)\n",
    "        return (y_pred_score > threshold).to(torch.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 0.7161\n",
      "Epoch 200 train loss: 0.6873\n",
      "Epoch 400 train loss: 0.6632\n",
      "Epoch 600 train loss: 0.6424\n",
      "Epoch 800 train loss: 0.6243\n",
      "Epoch 1000 train loss: 0.6083\n",
      "Epoch 1200 train loss: 0.5942\n",
      "Epoch 1400 train loss: 0.5816\n",
      "Epoch 1600 train loss: 0.5705\n",
      "Epoch 1800 train loss: 0.5606\n",
      "final loss: 0.5518\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "model = MLP(input_size=X_train.shape[1])\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# note that we are using loss function with sigmoid built in\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "num_epochs = 2000\n",
    "evaluation_steps = 200\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % evaluation_steps == 0:\n",
    "        print(f\"Epoch {i} train loss: {loss.item():.4f}\")\n",
    "\n",
    "print(f\"final loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LP5GSup24dXU",
    "outputId": "05f332c4-5d94-41f6-f85b-17793d3c4b49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 77.76%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # positive class probabilities\n",
    "    y_pred_valid_score = model.predict_proba(X_valid)\n",
    "    y_pred_test_score = model.predict_proba(X_test)\n",
    "\n",
    "auroc = roc_auc_score(y_test, y_pred_test_score)\n",
    "print(f\"AUROC: {100 * auroc:.2f}%\")\n",
    "\n",
    "plot_precision_recall_curve(y_valid, y_pred_valid_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUROC jest podobne, a precision i recall spadły - wypadamy wręcz gorzej od regresji liniowej! Skoro dodaliśmy więcej warstw, to może pojemność modelu jest teraz za duża i trzeba by go zregularyzować?\n",
    "\n",
    "Sieci neuronowe bardzo łatwo przeuczają, bo są bardzo elastycznymi i pojemnymi modelami. Dlatego mają wiele różnych rodzajów regularyzacji, których używa się razem. Co ciekawe, udowodniono eksperymentalnie, że zbyt duże sieci z mocną regularyzacją działają lepiej niż mniejsze sieci, odpowiedniego rozmiaru, za to ze słabszą regularyzacją.\n",
    "\n",
    "Pierwszy rodzaj regularyzacji to znana nam już **regularyzacja L2**, czyli penalizacja zbyt dużych wag. W kontekście sieci neuronowych nazywa się też ją czasem *weight decay*. W PyTorchu dodaje się ją jako argument do optymalizatora.\n",
    "\n",
    "Regularyzacja specyficzna dla sieci neuronowych to **dropout**. Polega on na losowym wyłączaniu zadanego procenta neuronów podczas treningu. Pomimo prostoty okazała się niesamowicie skuteczna, szczególnie w treningu bardzo głębokich sieci. Co ważne, jest to mechanizm używany tylko podczas treningu - w trakcie predykcji za pomocą sieci wyłącza się ten mechanizm i dokonuje normalnie predykcji całą siecią. Podejście to można potraktować jak ensemble learning, podobny do lasów losowych - wyłączając losowe części sieci, w każdej iteracji trenujemy nieco inną sieć, co odpowiada uśrednianiu predykcji różnych algorytmów. Typowo stosuje się dość mocny dropout, rzędu 25-50%. W PyTorchu implementuje go warstwa `nn.Dropout`, aplikowana zazwyczaj po funkcji aktywacji.\n",
    "\n",
    "Ostatni, a być może najważniejszy rodzaj regularyzacji to **wczesny stop (early stopping)**. W każdym kroku mocniej dostosowujemy terenową sieć do zbioru treningowego, a więc zbyt długi trening będzie skutkował przeuczeniem. W metodzie wczesnego stopu używamy wydzielonego zbioru walidacyjnego (pojedynczego, metoda holdout), sprawdzając co określoną liczbę epok wynik na tym zbiorze. Jeżeli nie uzyskamy wyniku lepszego od najlepszego dotychczas uzyskanego przez określoną liczbę epok, to przerywamy trening. Okres, przez który czekamy na uzyskanie lepszego wyniku, to cierpliwość (*patience*). Im mniejsze, tym mocniejszy jest ten rodzaj regularyzacji, ale trzeba z tym uważać, bo łatwo jest przesadzić i zbyt szybko przerywać trening. Niektóre implementacje uwzględniają tzw. *grace period*, czyli gwarantowaną minimalną liczbę epok, przez którą będziemy trenować sieć, niezależnie od wybranej cierpliwości.\n",
    "\n",
    "Dodatkowo ryzyko przeuczenia można zmniejszyć, używając mniejszej stałej uczącej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie 5 (1.5 punktu)\n",
    "\n",
    "Zaimplementuj funkcję `evaluate_model()`, obliczającą metryki na zbiorze testowym:\n",
    "- wartość funkcji kosztu (loss)\n",
    "- AUROC\n",
    "- optymalny próg\n",
    "- F1-score przy optymalnym progu\n",
    "- precyzję oraz recall dla optymalnego progu\n",
    "\n",
    "Jeżeli podana jest wartość argumentu `threshold`, to użyj jej do zamiany prawdopodobieństw na twarde predykcje. W przeciwnym razie użyj funkcji `get_optimal_threshold` i oblicz optymalną wartość progu.\n",
    "\n",
    "Pamiętaj o przełączeniu modelu w tryb ewaluacji oraz o wyłączeniu obliczania gradientów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch import sigmoid\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: nn.Module, \n",
    "    X: torch.Tensor, \n",
    "    y: torch.Tensor, \n",
    "    loss_fn: nn.Module,\n",
    "    threshold: Optional[float]= None\n",
    ") -> Dict[str, float]:\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        y_pred_proba = model(X)\n",
    "        loss = loss_fn(y_pred_proba, y)\n",
    "        if threshold is not None:\n",
    "            y_pred = np.where(y_pred_proba>threshold,1,0)\n",
    "        else:\n",
    "            precisions, recalls, thresholds = precision_recall_curve(y, y_pred_proba)\n",
    "            optimal_idx, threshold = get_optimal_threshold(precisions, recalls, thresholds)\n",
    "            print(f\"optimal threshold: {threshold:.4f}\")\n",
    "            y_pred = np.where(y_pred_proba>threshold,1,0)\n",
    "        auroc = roc_auc_score(y, y_pred_proba)\n",
    "        precission = precision_score(y, y_pred)\n",
    "        recall = recall_score(y, y_pred)\n",
    "        f1 = f1_score(y, y_pred)\n",
    "        return {\n",
    "            'loss':loss,\n",
    "            'AUROC':auroc,\n",
    "            'F1-score':f1,\n",
    "            'precision':precission,\n",
    "            'recall':recall,\n",
    "            'threshold':threshold,\n",
    "        }\n",
    "    \n",
    "    \n",
    "    # implement me!\n",
    "    # your_code\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie 6 (0.5 punktu)\n",
    "\n",
    "Zaimplementuj 3-warstwową sieć MLP z dropout (50%). Rozmiary warstw ukrytych mają wynosić 256 i 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedMLP(nn.Module):\n",
    "    def __init__(self, input_size: int, dropout_p: float = 0.5):\n",
    "        super().__init__()\n",
    "        activation = torch.nn.ReLU()\n",
    "        dropout = torch.nn.Dropout(dropout_p, inplace=False)\n",
    "        L1 = torch.nn.Linear(input_size, 256)\n",
    "        L2 = torch.nn.Linear(256, 128)\n",
    "        L3 = torch.nn.Linear(128, 1)\n",
    "        self.model = nn.Sequential(\n",
    "            L1,\n",
    "            activation,\n",
    "            dropout,\n",
    "            L2,\n",
    "            activation,\n",
    "            dropout,\n",
    "            L3\n",
    "        )\n",
    "        self.model.train()\n",
    "        # implement me!\n",
    "        # your_code\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        return sigmoid(self(x))\n",
    "    \n",
    "    def predict(self, x, threshold: float = 0.5):\n",
    "        y_pred_score = self.predict_proba(x)\n",
    "        return (y_pred_score > threshold).to(torch.int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEk9azaULAsz"
   },
   "source": [
    "Opisaliśmy wcześniej podstawowy optymalizator w sieciach neuronowych - spadek wzdłuż gradientu. Jednak wymaga on użycia całego zbioru danych, aby obliczyć gradient, co jest często niewykonalne przez rozmiar zbioru. Dlatego wymyślono **stochastyczny spadek wzdłuż gradientu (stochastic gradient descent, SGD)**, w którym używamy 1 przykładu naraz, liczymy gradient tylko po nim i aktualizujemy parametry. Jest to oczywiście dość grube przybliżenie gradientu, ale pozwala robić szybko dużo małych kroków. Kompromisem, którego używa się w praktyce, jest **minibatch gradient descent**, czyli używanie batchy np. 32, 64 czy 128 przykładów.\n",
    "\n",
    "Rzadko wspominanym, a ważnym faktem jest także to, że stochastyczność metody optymalizacji jest sama w sobie też [metodą regularyzacji](https://arxiv.org/abs/2101.12176), a więc `batch_size` to także hiperparametr.\n",
    "\n",
    "Obecnie najpopularniejszą odmianą SGD jest [Adam](https://arxiv.org/abs/1412.6980), gdyż uczy on szybko sieć oraz daje bardzo dobre wyniki nawet przy niekoniecznie idealnie dobranych hiperparametrach. W PyTorchu najlepiej korzystać z jego implementacji `AdamW`, która jest nieco lepsza niż implementacja `Adam`. Jest to zasadniczo zawsze wybór domyślny przy treningu współczesnych sieci neuronowych.\n",
    "\n",
    "Na razie użyjemy jednak minibatch SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej znajduje się implementacja prostej klasy dziedziczącej po `Dataset` - tak w PyTorchu implementuje się własne zbiory danych. Użycie takich klas umożliwia użycie klas ładujących dane (`DataLoader`), które z kolei pozwalają łatwo ładować batche danych. Trzeba w takiej klasie zaimplementować metody:\n",
    "- `__len__` - zwraca ilość punktów w zbiorze\n",
    "- `__getitem__` - zwraca przykład ze zbioru pod danym indeksem oraz jego klasę\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, y):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data = data\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie 7 (1.5 punktu)\n",
    "\n",
    "Zaimplementuj pętlę treningowo-walidacyjną dla sieci neuronowej. Wykorzystaj podane wartości hiperparametrów do treningu (stała ucząca, prawdopodobieństwo dropoutu, regularyzacja L2, rozmiar batcha, maksymalna liczba epok). Użyj optymalizatora SGD.\n",
    "\n",
    "Dodatkowo zaimplementuj regularyzację przez early stopping. Sprawdzaj co epokę wynik na zbiorze walidacyjnym. Użyj podanej wartości patience, a jako metryki po prostu wartości funkcji kosztu. Może się tutaj przydać zaimplementowana funkcja `evaluate_model()`.\n",
    "\n",
    "Pamiętaj o tym, aby przechowywać najlepszy dotychczasowy wynik walidacyjny oraz najlepszy dotychczasowy model. Zapamiętaj też optymalny próg do klasyfikacji dla najlepszego modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "learning_rate = 1e-3\n",
    "dropout_p = 0.5\n",
    "l2_reg = 1e-4\n",
    "batch_size = 128\n",
    "max_epochs = 300\n",
    "\n",
    "early_stopping_patience = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.0800\n",
      "Epoch 0 train loss: 0.6793, eval loss 0.6859622597694397\n",
      "optimal threshold: -0.1204\n",
      "Epoch 1 train loss: 0.6679, eval loss 0.669268012046814\n",
      "optimal threshold: -0.1825\n",
      "Epoch 2 train loss: 0.6557, eval loss 0.6543168425559998\n",
      "optimal threshold: -0.2462\n",
      "Epoch 3 train loss: 0.6331, eval loss 0.6408593654632568\n",
      "optimal threshold: -0.3067\n",
      "Epoch 4 train loss: 0.6294, eval loss 0.6286293864250183\n",
      "optimal threshold: -0.3663\n",
      "Epoch 5 train loss: 0.6158, eval loss 0.6175408959388733\n",
      "optimal threshold: -0.4151\n",
      "Epoch 6 train loss: 0.5991, eval loss 0.6074447631835938\n",
      "optimal threshold: -0.4661\n",
      "Epoch 7 train loss: 0.5877, eval loss 0.5981877446174622\n",
      "optimal threshold: -0.5187\n",
      "Epoch 8 train loss: 0.5799, eval loss 0.5897964239120483\n",
      "optimal threshold: -0.5487\n",
      "Epoch 9 train loss: 0.5793, eval loss 0.5821008086204529\n",
      "optimal threshold: -0.5895\n",
      "Epoch 10 train loss: 0.5761, eval loss 0.5750584006309509\n",
      "optimal threshold: -0.6330\n",
      "Epoch 11 train loss: 0.5615, eval loss 0.5685989856719971\n",
      "optimal threshold: -0.6675\n",
      "Epoch 12 train loss: 0.5511, eval loss 0.5627154111862183\n",
      "optimal threshold: -0.7217\n",
      "Epoch 13 train loss: 0.5517, eval loss 0.5573018193244934\n",
      "optimal threshold: -0.7394\n",
      "Epoch 14 train loss: 0.5560, eval loss 0.5522387623786926\n",
      "optimal threshold: -0.7702\n",
      "Epoch 15 train loss: 0.5329, eval loss 0.5475764274597168\n",
      "optimal threshold: -0.8039\n",
      "Epoch 16 train loss: 0.5369, eval loss 0.5432103872299194\n",
      "optimal threshold: -0.8336\n",
      "Epoch 17 train loss: 0.5321, eval loss 0.5391169786453247\n",
      "optimal threshold: -0.8482\n",
      "Epoch 18 train loss: 0.5111, eval loss 0.5352430939674377\n",
      "optimal threshold: -0.8998\n",
      "Epoch 19 train loss: 0.5271, eval loss 0.5315554141998291\n",
      "optimal threshold: -0.9231\n",
      "Epoch 20 train loss: 0.5303, eval loss 0.5280116200447083\n",
      "optimal threshold: -0.9477\n",
      "Epoch 21 train loss: 0.5145, eval loss 0.5245906114578247\n",
      "optimal threshold: -0.9735\n",
      "Epoch 22 train loss: 0.5285, eval loss 0.5212357640266418\n",
      "optimal threshold: -0.9868\n",
      "Epoch 23 train loss: 0.5215, eval loss 0.5179729461669922\n",
      "optimal threshold: -0.9970\n",
      "Epoch 24 train loss: 0.5058, eval loss 0.5147712826728821\n",
      "optimal threshold: -1.0258\n",
      "Epoch 25 train loss: 0.5132, eval loss 0.5116081237792969\n",
      "optimal threshold: -1.0118\n",
      "Epoch 26 train loss: 0.5240, eval loss 0.50849449634552\n",
      "optimal threshold: -1.0432\n",
      "Epoch 27 train loss: 0.5079, eval loss 0.5053714513778687\n",
      "optimal threshold: -1.0194\n",
      "Epoch 28 train loss: 0.5092, eval loss 0.5022823810577393\n",
      "optimal threshold: -1.0329\n",
      "Epoch 29 train loss: 0.4968, eval loss 0.4991897940635681\n",
      "optimal threshold: -1.0430\n",
      "Epoch 30 train loss: 0.4993, eval loss 0.4961077570915222\n",
      "optimal threshold: -1.0496\n",
      "Epoch 31 train loss: 0.4877, eval loss 0.4930155575275421\n",
      "optimal threshold: -1.0475\n",
      "Epoch 32 train loss: 0.4868, eval loss 0.4899079501628876\n",
      "optimal threshold: -1.0626\n",
      "Epoch 33 train loss: 0.4945, eval loss 0.4867975115776062\n",
      "optimal threshold: -1.0520\n",
      "Epoch 34 train loss: 0.4821, eval loss 0.4836955666542053\n",
      "optimal threshold: -1.0683\n",
      "Epoch 35 train loss: 0.4825, eval loss 0.48056963086128235\n",
      "optimal threshold: -1.0698\n",
      "Epoch 36 train loss: 0.4759, eval loss 0.477483868598938\n",
      "optimal threshold: -1.0751\n",
      "Epoch 37 train loss: 0.4741, eval loss 0.47436580061912537\n",
      "optimal threshold: -1.0421\n",
      "Epoch 38 train loss: 0.4726, eval loss 0.4712837338447571\n",
      "optimal threshold: -1.0456\n",
      "Epoch 39 train loss: 0.4725, eval loss 0.468194842338562\n",
      "optimal threshold: -1.0473\n",
      "Epoch 40 train loss: 0.4633, eval loss 0.46512383222579956\n",
      "optimal threshold: -1.0489\n",
      "Epoch 41 train loss: 0.4796, eval loss 0.4621002972126007\n",
      "optimal threshold: -1.0519\n",
      "Epoch 42 train loss: 0.4675, eval loss 0.45908573269844055\n",
      "optimal threshold: -1.0779\n",
      "Epoch 43 train loss: 0.4607, eval loss 0.4560745060443878\n",
      "optimal threshold: -1.0734\n",
      "Epoch 44 train loss: 0.4448, eval loss 0.45312395691871643\n",
      "optimal threshold: -1.0663\n",
      "Epoch 45 train loss: 0.4596, eval loss 0.45017582178115845\n",
      "optimal threshold: -1.0703\n",
      "Epoch 46 train loss: 0.4467, eval loss 0.4472654461860657\n",
      "optimal threshold: -1.0703\n",
      "Epoch 47 train loss: 0.4605, eval loss 0.4443807005882263\n",
      "optimal threshold: -1.0591\n",
      "Epoch 48 train loss: 0.4590, eval loss 0.44155171513557434\n",
      "optimal threshold: -1.0557\n",
      "Epoch 49 train loss: 0.4492, eval loss 0.43874698877334595\n",
      "optimal threshold: -1.0585\n",
      "Epoch 50 train loss: 0.4560, eval loss 0.4360046982765198\n",
      "optimal threshold: -1.0559\n",
      "Epoch 51 train loss: 0.4379, eval loss 0.43325430154800415\n",
      "optimal threshold: -1.0498\n",
      "Epoch 52 train loss: 0.4393, eval loss 0.4305651783943176\n",
      "optimal threshold: -1.0367\n",
      "Epoch 53 train loss: 0.4405, eval loss 0.42791905999183655\n",
      "optimal threshold: -0.9964\n",
      "Epoch 54 train loss: 0.4270, eval loss 0.425333172082901\n",
      "optimal threshold: -1.0236\n",
      "Epoch 55 train loss: 0.4369, eval loss 0.4227810800075531\n",
      "optimal threshold: -1.0092\n",
      "Epoch 56 train loss: 0.4284, eval loss 0.4203076660633087\n",
      "optimal threshold: -1.0109\n",
      "Epoch 57 train loss: 0.4276, eval loss 0.41784989833831787\n",
      "optimal threshold: -0.9918\n",
      "Epoch 58 train loss: 0.4319, eval loss 0.41544029116630554\n",
      "optimal threshold: -0.9265\n",
      "Epoch 59 train loss: 0.4298, eval loss 0.41310274600982666\n",
      "optimal threshold: -0.9191\n",
      "Epoch 60 train loss: 0.4326, eval loss 0.4107823371887207\n",
      "optimal threshold: -0.9627\n",
      "Epoch 61 train loss: 0.4196, eval loss 0.40850892663002014\n",
      "optimal threshold: -0.9481\n",
      "Epoch 62 train loss: 0.4181, eval loss 0.40632590651512146\n",
      "optimal threshold: -0.9404\n",
      "Epoch 63 train loss: 0.4286, eval loss 0.4042038023471832\n",
      "optimal threshold: -0.9423\n",
      "Epoch 64 train loss: 0.4216, eval loss 0.4021143317222595\n",
      "optimal threshold: -0.9016\n",
      "Epoch 65 train loss: 0.4333, eval loss 0.400048166513443\n",
      "optimal threshold: -0.8837\n",
      "Epoch 66 train loss: 0.4277, eval loss 0.3980486989021301\n",
      "optimal threshold: -0.8653\n",
      "Epoch 67 train loss: 0.3999, eval loss 0.39613357186317444\n",
      "optimal threshold: -0.8551\n",
      "Epoch 68 train loss: 0.4373, eval loss 0.3942626416683197\n",
      "optimal threshold: -0.8791\n",
      "Epoch 69 train loss: 0.4238, eval loss 0.39243003726005554\n",
      "optimal threshold: -0.8696\n",
      "Epoch 70 train loss: 0.3925, eval loss 0.3906751275062561\n",
      "optimal threshold: -0.8222\n",
      "Epoch 71 train loss: 0.4081, eval loss 0.3889619708061218\n",
      "optimal threshold: -0.8113\n",
      "Epoch 72 train loss: 0.4053, eval loss 0.38732919096946716\n",
      "optimal threshold: -0.8017\n",
      "Epoch 73 train loss: 0.4299, eval loss 0.38574203848838806\n",
      "optimal threshold: -0.8161\n",
      "Epoch 74 train loss: 0.4068, eval loss 0.3841688930988312\n",
      "optimal threshold: -0.8379\n",
      "Epoch 75 train loss: 0.4160, eval loss 0.38266459107398987\n",
      "optimal threshold: -0.8275\n",
      "Epoch 76 train loss: 0.4078, eval loss 0.3812224864959717\n",
      "optimal threshold: -0.8199\n",
      "Epoch 77 train loss: 0.3946, eval loss 0.3798317015171051\n",
      "optimal threshold: -0.8058\n",
      "Epoch 78 train loss: 0.4116, eval loss 0.37848109006881714\n",
      "optimal threshold: -0.7985\n",
      "Epoch 79 train loss: 0.3876, eval loss 0.37721413373947144\n",
      "optimal threshold: -0.7841\n",
      "Epoch 80 train loss: 0.4248, eval loss 0.37598690390586853\n",
      "optimal threshold: -0.7743\n",
      "Epoch 81 train loss: 0.4139, eval loss 0.37480485439300537\n",
      "optimal threshold: -0.7638\n",
      "Epoch 82 train loss: 0.4014, eval loss 0.3736801743507385\n",
      "optimal threshold: -0.7579\n",
      "Epoch 83 train loss: 0.4328, eval loss 0.3725913166999817\n",
      "optimal threshold: -0.7616\n",
      "Epoch 84 train loss: 0.3991, eval loss 0.3715346157550812\n",
      "optimal threshold: -0.7095\n",
      "Epoch 85 train loss: 0.4245, eval loss 0.3705325126647949\n",
      "optimal threshold: -0.7041\n",
      "Epoch 86 train loss: 0.3876, eval loss 0.3695712983608246\n",
      "optimal threshold: -0.7052\n",
      "Epoch 87 train loss: 0.4037, eval loss 0.3686618208885193\n",
      "optimal threshold: -0.6976\n",
      "Epoch 88 train loss: 0.4102, eval loss 0.3677958846092224\n",
      "optimal threshold: -0.6901\n",
      "Epoch 89 train loss: 0.4243, eval loss 0.3669433295726776\n",
      "optimal threshold: -0.6830\n",
      "Epoch 90 train loss: 0.4082, eval loss 0.3661418557167053\n",
      "optimal threshold: -0.6778\n",
      "Epoch 91 train loss: 0.4092, eval loss 0.36533528566360474\n",
      "optimal threshold: -0.6714\n",
      "Epoch 92 train loss: 0.3975, eval loss 0.3645699620246887\n",
      "optimal threshold: -0.6439\n",
      "Epoch 93 train loss: 0.3962, eval loss 0.3638547956943512\n",
      "optimal threshold: -0.6376\n",
      "Epoch 94 train loss: 0.4055, eval loss 0.36317557096481323\n",
      "optimal threshold: -0.6310\n",
      "Epoch 95 train loss: 0.3667, eval loss 0.36251381039619446\n",
      "optimal threshold: -0.6279\n",
      "Epoch 96 train loss: 0.3983, eval loss 0.36187729239463806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6385\n",
      "Epoch 97 train loss: 0.4119, eval loss 0.3612672984600067\n",
      "optimal threshold: -0.6321\n",
      "Epoch 98 train loss: 0.4080, eval loss 0.36068516969680786\n",
      "optimal threshold: -0.6304\n",
      "Epoch 99 train loss: 0.3927, eval loss 0.3601219058036804\n",
      "optimal threshold: -0.6251\n",
      "Epoch 100 train loss: 0.3902, eval loss 0.3595942258834839\n",
      "optimal threshold: -0.9187\n",
      "Epoch 101 train loss: 0.4052, eval loss 0.35907283425331116\n",
      "optimal threshold: -0.9134\n",
      "Epoch 102 train loss: 0.3954, eval loss 0.3585621118545532\n",
      "optimal threshold: -0.9064\n",
      "Epoch 103 train loss: 0.4141, eval loss 0.35808947682380676\n",
      "optimal threshold: -0.9005\n",
      "Epoch 104 train loss: 0.4005, eval loss 0.35762152075767517\n",
      "optimal threshold: -0.8983\n",
      "Epoch 105 train loss: 0.3991, eval loss 0.35718420147895813\n",
      "optimal threshold: -0.8986\n",
      "Epoch 106 train loss: 0.3985, eval loss 0.35675135254859924\n",
      "optimal threshold: -0.8976\n",
      "Epoch 107 train loss: 0.4298, eval loss 0.3563348352909088\n",
      "optimal threshold: -0.5654\n",
      "Epoch 108 train loss: 0.3969, eval loss 0.35591742396354675\n",
      "optimal threshold: -0.8986\n",
      "Epoch 109 train loss: 0.3879, eval loss 0.35551562905311584\n",
      "optimal threshold: -0.8860\n",
      "Epoch 110 train loss: 0.3984, eval loss 0.35513579845428467\n",
      "optimal threshold: -0.8803\n",
      "Epoch 111 train loss: 0.4138, eval loss 0.35476821660995483\n",
      "optimal threshold: -0.8731\n",
      "Epoch 112 train loss: 0.3931, eval loss 0.3544160723686218\n",
      "optimal threshold: -0.8768\n",
      "Epoch 113 train loss: 0.3697, eval loss 0.35406917333602905\n",
      "optimal threshold: -0.8609\n",
      "Epoch 114 train loss: 0.3982, eval loss 0.3537234365940094\n",
      "optimal threshold: -0.8599\n",
      "Epoch 115 train loss: 0.3826, eval loss 0.353402316570282\n",
      "optimal threshold: -0.8803\n",
      "Epoch 116 train loss: 0.3945, eval loss 0.3530864119529724\n",
      "optimal threshold: -0.8825\n",
      "Epoch 117 train loss: 0.4143, eval loss 0.3527798354625702\n",
      "optimal threshold: -0.8754\n",
      "Epoch 118 train loss: 0.4061, eval loss 0.35247862339019775\n",
      "optimal threshold: -0.8712\n",
      "Epoch 119 train loss: 0.4044, eval loss 0.3521870970726013\n",
      "optimal threshold: -0.8698\n",
      "Epoch 120 train loss: 0.4060, eval loss 0.35190463066101074\n",
      "optimal threshold: -0.8778\n",
      "Epoch 121 train loss: 0.4158, eval loss 0.3516032099723816\n",
      "optimal threshold: -0.8912\n",
      "Epoch 122 train loss: 0.3829, eval loss 0.35132646560668945\n",
      "optimal threshold: -0.8870\n",
      "Epoch 123 train loss: 0.3958, eval loss 0.35106077790260315\n",
      "optimal threshold: -0.8861\n",
      "Epoch 124 train loss: 0.3732, eval loss 0.3507857620716095\n",
      "optimal threshold: -0.8847\n",
      "Epoch 125 train loss: 0.3860, eval loss 0.3505218029022217\n",
      "optimal threshold: -0.8852\n",
      "Epoch 126 train loss: 0.4170, eval loss 0.35026317834854126\n",
      "optimal threshold: -0.8767\n",
      "Epoch 127 train loss: 0.4252, eval loss 0.35000309348106384\n",
      "optimal threshold: -0.8734\n",
      "Epoch 128 train loss: 0.3996, eval loss 0.3497643768787384\n",
      "optimal threshold: -0.8738\n",
      "Epoch 129 train loss: 0.3719, eval loss 0.3495246171951294\n",
      "optimal threshold: -0.8750\n",
      "Epoch 130 train loss: 0.3763, eval loss 0.3492790460586548\n",
      "optimal threshold: -0.8725\n",
      "Epoch 131 train loss: 0.3878, eval loss 0.34903791546821594\n",
      "optimal threshold: -0.8709\n",
      "Epoch 132 train loss: 0.4162, eval loss 0.34880736470222473\n",
      "optimal threshold: -0.8757\n",
      "Epoch 133 train loss: 0.4106, eval loss 0.3485819697380066\n",
      "optimal threshold: -0.8802\n",
      "Epoch 134 train loss: 0.3824, eval loss 0.34834930300712585\n",
      "optimal threshold: -0.8837\n",
      "Epoch 135 train loss: 0.3725, eval loss 0.3481304943561554\n",
      "optimal threshold: -0.8909\n",
      "Epoch 136 train loss: 0.4124, eval loss 0.3479134142398834\n",
      "optimal threshold: -0.8912\n",
      "Epoch 137 train loss: 0.3979, eval loss 0.3476990759372711\n",
      "optimal threshold: -0.8851\n",
      "Epoch 138 train loss: 0.3944, eval loss 0.34748855233192444\n",
      "optimal threshold: -0.8878\n",
      "Epoch 139 train loss: 0.3837, eval loss 0.3472716808319092\n",
      "optimal threshold: -0.8844\n",
      "Epoch 140 train loss: 0.3791, eval loss 0.347064346075058\n",
      "optimal threshold: -0.8390\n",
      "Epoch 141 train loss: 0.4142, eval loss 0.34686538577079773\n",
      "optimal threshold: -0.8407\n",
      "Epoch 142 train loss: 0.3745, eval loss 0.346666544675827\n",
      "optimal threshold: -0.8459\n",
      "Epoch 143 train loss: 0.4162, eval loss 0.34646549820899963\n",
      "optimal threshold: -0.8522\n",
      "Epoch 144 train loss: 0.3718, eval loss 0.3462737798690796\n",
      "optimal threshold: -0.8466\n",
      "Epoch 145 train loss: 0.3871, eval loss 0.34608012437820435\n",
      "optimal threshold: -0.8495\n",
      "Epoch 146 train loss: 0.3751, eval loss 0.3458848297595978\n",
      "optimal threshold: -0.8436\n",
      "Epoch 147 train loss: 0.3908, eval loss 0.3456985056400299\n",
      "optimal threshold: -0.8465\n",
      "Epoch 148 train loss: 0.3963, eval loss 0.34551697969436646\n",
      "optimal threshold: -0.8509\n",
      "Epoch 149 train loss: 0.3622, eval loss 0.3453168272972107\n",
      "optimal threshold: -0.8584\n",
      "Epoch 150 train loss: 0.4285, eval loss 0.34513336420059204\n",
      "optimal threshold: -0.8586\n",
      "Epoch 151 train loss: 0.4317, eval loss 0.34495922923088074\n",
      "optimal threshold: -0.4981\n",
      "Epoch 152 train loss: 0.4035, eval loss 0.3447759747505188\n",
      "optimal threshold: -0.7557\n",
      "Epoch 153 train loss: 0.3605, eval loss 0.34459108114242554\n",
      "optimal threshold: -0.7563\n",
      "Epoch 154 train loss: 0.3791, eval loss 0.3444201350212097\n",
      "optimal threshold: -0.7517\n",
      "Epoch 155 train loss: 0.3764, eval loss 0.3442360460758209\n",
      "optimal threshold: -0.7512\n",
      "Epoch 156 train loss: 0.4015, eval loss 0.34406065940856934\n",
      "optimal threshold: -0.9504\n",
      "Epoch 157 train loss: 0.4072, eval loss 0.34388214349746704\n",
      "optimal threshold: -0.9526\n",
      "Epoch 158 train loss: 0.3833, eval loss 0.3437078297138214\n",
      "optimal threshold: -0.9337\n",
      "Epoch 159 train loss: 0.3549, eval loss 0.3435322046279907\n",
      "optimal threshold: -0.9292\n",
      "Epoch 160 train loss: 0.3910, eval loss 0.3433660566806793\n",
      "optimal threshold: -0.7578\n",
      "Epoch 161 train loss: 0.3797, eval loss 0.34319889545440674\n",
      "optimal threshold: -0.7612\n",
      "Epoch 162 train loss: 0.4276, eval loss 0.34303563833236694\n",
      "optimal threshold: -0.7639\n",
      "Epoch 163 train loss: 0.3724, eval loss 0.3428659439086914\n",
      "optimal threshold: -0.8325\n",
      "Epoch 164 train loss: 0.3846, eval loss 0.3427022099494934\n",
      "optimal threshold: -0.7644\n",
      "Epoch 165 train loss: 0.4070, eval loss 0.3425428867340088\n",
      "optimal threshold: -0.9318\n",
      "Epoch 166 train loss: 0.3669, eval loss 0.34237048029899597\n",
      "optimal threshold: -0.9314\n",
      "Epoch 167 train loss: 0.3818, eval loss 0.3422125577926636\n",
      "optimal threshold: -0.9020\n",
      "Epoch 168 train loss: 0.3884, eval loss 0.34206727147102356\n",
      "optimal threshold: -0.8945\n",
      "Epoch 169 train loss: 0.4000, eval loss 0.3419048488140106\n",
      "optimal threshold: -0.8921\n",
      "Epoch 170 train loss: 0.3985, eval loss 0.3417527675628662\n",
      "optimal threshold: -0.8947\n",
      "Epoch 171 train loss: 0.3845, eval loss 0.34160247445106506\n",
      "optimal threshold: -0.8982\n",
      "Epoch 172 train loss: 0.3982, eval loss 0.3414532542228699\n",
      "optimal threshold: -0.8275\n",
      "Epoch 173 train loss: 0.3560, eval loss 0.3412942588329315\n",
      "optimal threshold: -0.8287\n",
      "Epoch 174 train loss: 0.3905, eval loss 0.3411368429660797\n",
      "optimal threshold: -0.8320\n",
      "Epoch 175 train loss: 0.3779, eval loss 0.34099170565605164\n",
      "optimal threshold: -0.8287\n",
      "Epoch 176 train loss: 0.4066, eval loss 0.3408406972885132\n",
      "optimal threshold: -0.8271\n",
      "Epoch 177 train loss: 0.3807, eval loss 0.34068921208381653\n",
      "optimal threshold: -0.8288\n",
      "Epoch 178 train loss: 0.3860, eval loss 0.34054937958717346\n",
      "optimal threshold: -0.8306\n",
      "Epoch 179 train loss: 0.3739, eval loss 0.340403288602829\n",
      "optimal threshold: -0.8328\n",
      "Epoch 180 train loss: 0.3799, eval loss 0.3402561545372009\n",
      "optimal threshold: -0.8298\n",
      "Epoch 181 train loss: 0.3926, eval loss 0.3401016294956207\n",
      "optimal threshold: -0.8348\n",
      "Epoch 182 train loss: 0.3902, eval loss 0.3399581015110016\n",
      "optimal threshold: -0.8247\n",
      "Epoch 183 train loss: 0.4035, eval loss 0.3398042917251587\n",
      "optimal threshold: -0.8245\n",
      "Epoch 184 train loss: 0.3838, eval loss 0.33967041969299316\n",
      "optimal threshold: -0.8282\n",
      "Epoch 185 train loss: 0.4338, eval loss 0.3395397365093231\n",
      "optimal threshold: -0.8265\n",
      "Epoch 186 train loss: 0.3686, eval loss 0.33940309286117554\n",
      "optimal threshold: -0.8255\n",
      "Epoch 187 train loss: 0.3854, eval loss 0.3392568826675415\n",
      "optimal threshold: -0.8277\n",
      "Epoch 188 train loss: 0.3849, eval loss 0.33912280201911926\n",
      "optimal threshold: -0.8304\n",
      "Epoch 189 train loss: 0.3750, eval loss 0.3389941453933716\n",
      "optimal threshold: -0.8175\n",
      "Epoch 190 train loss: 0.3541, eval loss 0.3388454020023346\n",
      "optimal threshold: -0.8180\n",
      "Epoch 191 train loss: 0.3694, eval loss 0.3387117385864258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.8191\n",
      "Epoch 192 train loss: 0.3972, eval loss 0.33858320116996765\n",
      "optimal threshold: -0.8229\n",
      "Epoch 193 train loss: 0.4081, eval loss 0.3384525179862976\n",
      "optimal threshold: -0.8104\n",
      "Epoch 194 train loss: 0.3807, eval loss 0.33832812309265137\n",
      "optimal threshold: -0.8043\n",
      "Epoch 195 train loss: 0.3745, eval loss 0.3381955325603485\n",
      "optimal threshold: -0.8158\n",
      "Epoch 196 train loss: 0.3748, eval loss 0.33806726336479187\n",
      "optimal threshold: -0.8153\n",
      "Epoch 197 train loss: 0.4058, eval loss 0.3379388451576233\n",
      "optimal threshold: -0.8210\n",
      "Epoch 198 train loss: 0.3860, eval loss 0.3378155529499054\n",
      "optimal threshold: -0.8184\n",
      "Epoch 199 train loss: 0.3822, eval loss 0.33768704533576965\n",
      "optimal threshold: -0.8241\n",
      "Epoch 200 train loss: 0.3853, eval loss 0.33756428956985474\n",
      "optimal threshold: -0.8240\n",
      "Epoch 201 train loss: 0.3898, eval loss 0.337445467710495\n",
      "optimal threshold: -0.8228\n",
      "Epoch 202 train loss: 0.3916, eval loss 0.33731144666671753\n",
      "optimal threshold: -0.8230\n",
      "Epoch 203 train loss: 0.4113, eval loss 0.3371961712837219\n",
      "optimal threshold: -0.8275\n",
      "Epoch 204 train loss: 0.3693, eval loss 0.33707934617996216\n",
      "optimal threshold: -0.8365\n",
      "Epoch 205 train loss: 0.3712, eval loss 0.33696383237838745\n",
      "optimal threshold: -0.7762\n",
      "Epoch 206 train loss: 0.3862, eval loss 0.3368397057056427\n",
      "optimal threshold: -0.7798\n",
      "Epoch 207 train loss: 0.3809, eval loss 0.3367150127887726\n",
      "optimal threshold: -0.7770\n",
      "Epoch 208 train loss: 0.3604, eval loss 0.3365897536277771\n",
      "optimal threshold: -0.6404\n",
      "Epoch 209 train loss: 0.3585, eval loss 0.3364672064781189\n",
      "optimal threshold: -0.7786\n",
      "Epoch 210 train loss: 0.3977, eval loss 0.33635133504867554\n",
      "optimal threshold: -0.7780\n",
      "Epoch 211 train loss: 0.3960, eval loss 0.3362368047237396\n",
      "optimal threshold: -0.7772\n",
      "Epoch 212 train loss: 0.3938, eval loss 0.3361385762691498\n",
      "optimal threshold: -0.7722\n",
      "Epoch 213 train loss: 0.3983, eval loss 0.33600956201553345\n",
      "optimal threshold: -0.7847\n",
      "Epoch 214 train loss: 0.3939, eval loss 0.33588331937789917\n",
      "optimal threshold: -0.7768\n",
      "Epoch 215 train loss: 0.3875, eval loss 0.3357657790184021\n",
      "optimal threshold: -0.7508\n",
      "Epoch 216 train loss: 0.3804, eval loss 0.3356468677520752\n",
      "optimal threshold: -0.7766\n",
      "Epoch 217 train loss: 0.3801, eval loss 0.3355501592159271\n",
      "optimal threshold: -0.7592\n",
      "Epoch 218 train loss: 0.3848, eval loss 0.33544039726257324\n",
      "optimal threshold: -0.7615\n",
      "Epoch 219 train loss: 0.3533, eval loss 0.33532410860061646\n",
      "optimal threshold: -0.7600\n",
      "Epoch 220 train loss: 0.3700, eval loss 0.335214227437973\n",
      "optimal threshold: -0.7655\n",
      "Epoch 221 train loss: 0.3534, eval loss 0.33511224389076233\n",
      "optimal threshold: -0.7769\n",
      "Epoch 222 train loss: 0.3728, eval loss 0.3350115418434143\n",
      "optimal threshold: -0.7845\n",
      "Epoch 223 train loss: 0.3537, eval loss 0.334902286529541\n",
      "optimal threshold: -0.7828\n",
      "Epoch 224 train loss: 0.3628, eval loss 0.33479800820350647\n",
      "optimal threshold: -0.7891\n",
      "Epoch 225 train loss: 0.3862, eval loss 0.33468079566955566\n",
      "optimal threshold: -0.7854\n",
      "Epoch 226 train loss: 0.3695, eval loss 0.3345814049243927\n",
      "optimal threshold: -0.7833\n",
      "Epoch 227 train loss: 0.3972, eval loss 0.3344815671443939\n",
      "optimal threshold: -0.7856\n",
      "Epoch 228 train loss: 0.3861, eval loss 0.3343925476074219\n",
      "optimal threshold: -0.7903\n",
      "Epoch 229 train loss: 0.3159, eval loss 0.33429574966430664\n",
      "optimal threshold: -0.7854\n",
      "Epoch 230 train loss: 0.3592, eval loss 0.3341848850250244\n",
      "optimal threshold: -0.7840\n",
      "Epoch 231 train loss: 0.3435, eval loss 0.3340694308280945\n",
      "optimal threshold: -0.7825\n",
      "Epoch 232 train loss: 0.3431, eval loss 0.3339671790599823\n",
      "optimal threshold: -0.7580\n",
      "Epoch 233 train loss: 0.3624, eval loss 0.33386749029159546\n",
      "optimal threshold: -0.7531\n",
      "Epoch 234 train loss: 0.3984, eval loss 0.33376064896583557\n",
      "optimal threshold: -0.7525\n",
      "Epoch 235 train loss: 0.3666, eval loss 0.333666056394577\n",
      "optimal threshold: -0.7459\n",
      "Epoch 236 train loss: 0.3672, eval loss 0.33355462551116943\n",
      "optimal threshold: -0.7501\n",
      "Epoch 237 train loss: 0.3642, eval loss 0.3334580957889557\n",
      "optimal threshold: -0.7497\n",
      "Epoch 238 train loss: 0.3734, eval loss 0.3333597183227539\n",
      "optimal threshold: -0.7511\n",
      "Epoch 239 train loss: 0.3601, eval loss 0.3332679569721222\n",
      "optimal threshold: -0.7438\n",
      "Epoch 240 train loss: 0.4044, eval loss 0.3331596255302429\n",
      "optimal threshold: -0.7460\n",
      "Epoch 241 train loss: 0.3924, eval loss 0.33306699991226196\n",
      "optimal threshold: -0.7469\n",
      "Epoch 242 train loss: 0.3918, eval loss 0.3329799175262451\n",
      "optimal threshold: -0.7445\n",
      "Epoch 243 train loss: 0.3581, eval loss 0.3328835964202881\n",
      "optimal threshold: -0.7434\n",
      "Epoch 244 train loss: 0.4273, eval loss 0.3327864110469818\n",
      "optimal threshold: -0.7365\n",
      "Epoch 245 train loss: 0.3926, eval loss 0.3326796889305115\n",
      "optimal threshold: -0.7326\n",
      "Epoch 246 train loss: 0.3675, eval loss 0.3325939178466797\n",
      "optimal threshold: -0.7337\n",
      "Epoch 247 train loss: 0.3682, eval loss 0.3325096070766449\n",
      "optimal threshold: -0.7344\n",
      "Epoch 248 train loss: 0.3610, eval loss 0.33242014050483704\n",
      "optimal threshold: -0.7353\n",
      "Epoch 249 train loss: 0.4264, eval loss 0.3323320746421814\n",
      "optimal threshold: -0.6139\n",
      "Epoch 250 train loss: 0.4200, eval loss 0.33224231004714966\n",
      "optimal threshold: -0.6372\n",
      "Epoch 251 train loss: 0.3534, eval loss 0.3321520984172821\n",
      "optimal threshold: -0.6397\n",
      "Epoch 252 train loss: 0.3563, eval loss 0.33206823468208313\n",
      "optimal threshold: -0.7414\n",
      "Epoch 253 train loss: 0.3861, eval loss 0.33198902010917664\n",
      "optimal threshold: -0.6444\n",
      "Epoch 254 train loss: 0.3635, eval loss 0.331892192363739\n",
      "optimal threshold: -0.6396\n",
      "Epoch 255 train loss: 0.3804, eval loss 0.33180439472198486\n",
      "optimal threshold: -0.6428\n",
      "Epoch 256 train loss: 0.3644, eval loss 0.33172371983528137\n",
      "optimal threshold: -0.6420\n",
      "Epoch 257 train loss: 0.3870, eval loss 0.33163708448410034\n",
      "optimal threshold: -0.6295\n",
      "Epoch 258 train loss: 0.3866, eval loss 0.3315514624118805\n",
      "optimal threshold: -0.6245\n",
      "Epoch 259 train loss: 0.3609, eval loss 0.331464558839798\n",
      "optimal threshold: -0.6514\n",
      "Epoch 260 train loss: 0.3639, eval loss 0.331387996673584\n",
      "optimal threshold: -0.6488\n",
      "Epoch 261 train loss: 0.3777, eval loss 0.3313003182411194\n",
      "optimal threshold: -0.6439\n",
      "Epoch 262 train loss: 0.3737, eval loss 0.33121535181999207\n",
      "optimal threshold: -0.6405\n",
      "Epoch 263 train loss: 0.3620, eval loss 0.33113542199134827\n",
      "optimal threshold: -0.6451\n",
      "Epoch 264 train loss: 0.3702, eval loss 0.33106115460395813\n",
      "optimal threshold: -0.6458\n",
      "Epoch 265 train loss: 0.3680, eval loss 0.3309887647628784\n",
      "optimal threshold: -0.6467\n",
      "Epoch 266 train loss: 0.3840, eval loss 0.3309001922607422\n",
      "optimal threshold: -0.6468\n",
      "Epoch 267 train loss: 0.3698, eval loss 0.330807089805603\n",
      "optimal threshold: -0.6490\n",
      "Epoch 268 train loss: 0.3725, eval loss 0.33072683215141296\n",
      "optimal threshold: -0.6488\n",
      "Epoch 269 train loss: 0.3546, eval loss 0.330651193857193\n",
      "optimal threshold: -0.6345\n",
      "Epoch 270 train loss: 0.4035, eval loss 0.33058401942253113\n",
      "optimal threshold: -0.6550\n",
      "Epoch 271 train loss: 0.3709, eval loss 0.3305070102214813\n",
      "optimal threshold: -0.6361\n",
      "Epoch 272 train loss: 0.3774, eval loss 0.330421507358551\n",
      "optimal threshold: -0.6338\n",
      "Epoch 273 train loss: 0.3685, eval loss 0.3303404152393341\n",
      "optimal threshold: -0.6355\n",
      "Epoch 274 train loss: 0.3600, eval loss 0.3302662968635559\n",
      "optimal threshold: -0.6397\n",
      "Epoch 275 train loss: 0.3706, eval loss 0.33019423484802246\n",
      "optimal threshold: -0.6370\n",
      "Epoch 276 train loss: 0.3548, eval loss 0.33011800050735474\n",
      "optimal threshold: -0.6345\n",
      "Epoch 277 train loss: 0.3341, eval loss 0.3300466239452362\n",
      "optimal threshold: -0.6302\n",
      "Epoch 278 train loss: 0.3823, eval loss 0.3299676477909088\n",
      "optimal threshold: -0.6359\n",
      "Epoch 279 train loss: 0.3540, eval loss 0.32989537715911865\n",
      "optimal threshold: -0.6311\n",
      "Epoch 280 train loss: 0.3792, eval loss 0.32981622219085693\n",
      "optimal threshold: -0.6315\n",
      "Epoch 281 train loss: 0.3464, eval loss 0.3297460377216339\n",
      "optimal threshold: -0.6320\n",
      "Epoch 282 train loss: 0.3788, eval loss 0.3296748101711273\n",
      "optimal threshold: -0.6316\n",
      "Epoch 283 train loss: 0.3734, eval loss 0.3295973539352417\n",
      "optimal threshold: -0.5968\n",
      "Epoch 284 train loss: 0.3740, eval loss 0.3295271098613739\n",
      "optimal threshold: -0.6001\n",
      "Epoch 285 train loss: 0.3750, eval loss 0.3294622302055359\n",
      "optimal threshold: -0.7066\n",
      "Epoch 286 train loss: 0.3621, eval loss 0.3293745815753937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5888\n",
      "Epoch 287 train loss: 0.3498, eval loss 0.3293163478374481\n",
      "optimal threshold: -0.5971\n",
      "Epoch 288 train loss: 0.3670, eval loss 0.3292429447174072\n",
      "optimal threshold: -0.5922\n",
      "Epoch 289 train loss: 0.3770, eval loss 0.3291637599468231\n",
      "optimal threshold: -0.5927\n",
      "Epoch 290 train loss: 0.3752, eval loss 0.32909032702445984\n",
      "optimal threshold: -0.5948\n",
      "Epoch 291 train loss: 0.3655, eval loss 0.3290310502052307\n",
      "optimal threshold: -0.5923\n",
      "Epoch 292 train loss: 0.4151, eval loss 0.32895585894584656\n",
      "optimal threshold: -0.5973\n",
      "Epoch 293 train loss: 0.4077, eval loss 0.32889053225517273\n",
      "optimal threshold: -0.5924\n",
      "Epoch 294 train loss: 0.3952, eval loss 0.3288157284259796\n",
      "optimal threshold: -0.5932\n",
      "Epoch 295 train loss: 0.3386, eval loss 0.328757643699646\n",
      "optimal threshold: -0.5446\n",
      "Epoch 296 train loss: 0.3627, eval loss 0.32867535948753357\n",
      "optimal threshold: -0.5461\n",
      "Epoch 297 train loss: 0.3393, eval loss 0.32859909534454346\n",
      "optimal threshold: -0.5464\n",
      "Epoch 298 train loss: 0.3473, eval loss 0.3285346031188965\n",
      "optimal threshold: -0.5844\n",
      "Epoch 299 train loss: 0.3728, eval loss 0.3284570872783661\n"
     ]
    }
   ],
   "source": [
    "model = RegularizedMLP(\n",
    "    input_size=X_train.shape[1], \n",
    "    dropout_p=dropout_p\n",
    ")\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), \n",
    "    lr=learning_rate, \n",
    "    weight_decay=l2_reg\n",
    ")\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_dataset = MyDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "steps_without_improvement = 0\n",
    "\n",
    "best_val_loss = np.inf\n",
    "best_model = None\n",
    "best_threshold = None\n",
    "\n",
    "for epoch_num in range(max_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # note that we are using DataLoader to get batches\n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # model training\n",
    "        # implement me!\n",
    "        # your_code\n",
    "    \n",
    "    # model evaluation, early stopping\n",
    "    valid_metrics=evaluate_model(\n",
    "    model, \n",
    "    X_valid, \n",
    "    y_valid, \n",
    "    loss_fn)\n",
    "    val_loss = valid_metrics['loss']\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        steps_without_improvement = 0\n",
    "        best_model = deepcopy(model)\n",
    "        best_threshold = valid_metrics['threshold']\n",
    "    else:\n",
    "        steps_without_improvement += 1\n",
    "    if early_stopping_patience == steps_without_improvement:\n",
    "        break\n",
    "    # implement me!\n",
    "    # your_code\n",
    "    \n",
    "    \n",
    "    print(f\"Epoch {epoch_num} train loss: {loss.item():.4f}, eval loss {valid_metrics['loss']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 90.10%\n",
      "F1: 68.19%\n",
      "Precision: 62.24%\n",
      "Recall: 75.38%\n"
     ]
    }
   ],
   "source": [
    "test_metrics = evaluate_model(best_model, X_test, y_test, loss_fn, best_threshold)\n",
    "\n",
    "print(f\"AUROC: {100 * test_metrics['AUROC']:.2f}%\")\n",
    "print(f\"F1: {100 * test_metrics['F1-score']:.2f}%\")\n",
    "print(f\"Precision: {100 * test_metrics['precision']:.2f}%\")\n",
    "print(f\"Recall: {100 * test_metrics['recall']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyniki wyglądają już dużo lepiej.\n",
    "\n",
    "Na koniec laboratorium dołożymy do naszego modelu jeszcze 3 powrzechnie używane techniki, które są bardzo proste, a pozwalają często ulepszyć wynik modelu.\n",
    "\n",
    "Pierwszą z nich są **warstwy normalizacji (normalization layers)**. Powstały one początkowo z założeniem, że przez przekształcenia przestrzeni dokonywane przez sieć zmienia się rozkład prawdopodobieństw pomiędzy warstwami, czyli tzw. *internal covariate shift*. Później okazało się, że zastosowanie takiej normalizacji wygładza powierzchnię funkcji kosztu, co ułatwia i przyspiesza optymalizację. Najpowszechniej używaną normalizacją jest **batch normalization (batch norm)**.\n",
    "\n",
    "Drugim ulepszeniem jest dodanie **wag klas (class weights)**. Mamy do czynienia z problemem klasyfikacji niezbalansowanej, więc klasa mniejszościowa, ważniejsza dla nas, powinna dostać większą wagę. Implementuje się to trywialnie prosto - po prostu mnożymy wartość funkcji kosztu dla danego przykładu przez wagę dla prawdziwej klasy tego przykładu. Praktycznie każdy klasyfikator operujący na jakiejś ważonej funkcji może działać w ten sposób, nie tylko sieci neuronowe.\n",
    "\n",
    "Ostatnim ulepszeniem jest zamiana SGD na optymalizator Adam, a konkretnie na optymalizator `AdamW`. Jest to przykład **optymalizatora adaptacyjnego (adaptive optimizer)**, który potrafi zaadaptować stałą uczącą dla każdego parametru z osobna w trakcie treningu. Wykorzystuje do tego gradienty - w uproszczeniu, im większa wariancja gradientu, tym mniejsze kroki w tym kierunku robimy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zadanie 8 (0.5 punktu)\n",
    "\n",
    "Zaimplementuj model `NormalizingMLP`, o takiej samej strukturze jak `RegularizedMLP`, ale dodatkowo z warstwami `BatchNorm1d` pomiędzy warstwami `Linear` oraz `ReLU`.\n",
    "\n",
    "Za pomocą funkcji `compute_class_weight()` oblicz wagi dla poszczególnych klas. Użyj opcji `\"balanced\"`. Przekaż do funkcji kosztu wagę klasy pozytywnej (pamiętaj, aby zamienić ją na tensor).\n",
    "\n",
    "Zamień używany optymalizator na `AdamW`.\n",
    "\n",
    "Na koniec skopiuj resztę kodu do treningu z poprzedniego zadania, wytrenuj sieć i oblicz wyniki na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizingMLP(nn.Module):\n",
    "    def __init__(self, input_size: int, dropout_p: float = 0.5):\n",
    "        super().__init__()\n",
    "        activation = torch.nn.ReLU()\n",
    "        bn1 = torch.nn.BatchNorm1d(256)\n",
    "        bn2 = torch.nn.BatchNorm1d(128)\n",
    "        dropout = torch.nn.Dropout(dropout_p, inplace=False)\n",
    "        L1 = torch.nn.Linear(input_size, 256)\n",
    "        L2 = torch.nn.Linear(256, 128)\n",
    "        L3 = torch.nn.Linear(128, 1)\n",
    "        self.model = nn.Sequential(\n",
    "            L1,\n",
    "            bn1,\n",
    "            activation,\n",
    "            dropout,\n",
    "            L2,\n",
    "            bn2,\n",
    "            activation,\n",
    "            dropout,\n",
    "            L3\n",
    "        )\n",
    "        self.model.train()\n",
    "        # implement me!\n",
    "        # your_code\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        return sigmoid(self(x))\n",
    "    \n",
    "    def predict(self, x, threshold: float = 0.5):\n",
    "        y_pred_score = self.predict_proba(x)\n",
    "        return (y_pred_score > threshold).to(torch.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all the hyperparameters\n",
    "# your_code\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "learning_rate = 1e-3\n",
    "dropout_p = 0.5\n",
    "l2_reg = 1e-4\n",
    "batch_size = 128\n",
    "max_epochs = 300\n",
    "\n",
    "early_stopping_patience = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.3469\n",
      "Epoch 0 train loss: 0.7742, eval loss 0.6821264028549194\n",
      "optimal threshold: -0.3957\n",
      "Epoch 1 train loss: 0.7580, eval loss 0.6668983101844788\n",
      "optimal threshold: -0.5437\n",
      "Epoch 2 train loss: 0.7714, eval loss 0.6644926071166992\n",
      "optimal threshold: -0.5498\n",
      "Epoch 3 train loss: 0.7970, eval loss 0.6645700931549072\n",
      "optimal threshold: -0.4917\n",
      "Epoch 4 train loss: 0.8056, eval loss 0.6616824865341187\n",
      "optimal threshold: -0.4794\n",
      "Epoch 5 train loss: 0.7907, eval loss 0.6648877859115601\n",
      "optimal threshold: -0.3404\n",
      "Epoch 6 train loss: 0.6735, eval loss 0.6626525521278381\n",
      "optimal threshold: -0.4647\n",
      "Epoch 7 train loss: 0.7679, eval loss 0.6618438959121704\n",
      "optimal threshold: -0.6242\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "# your_code\n",
    "model = NormalizingMLP(\n",
    "    input_size=X_train.shape[1], \n",
    "    dropout_p=dropout_p\n",
    ")\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=learning_rate, \n",
    "    weight_decay=l2_reg\n",
    ")\n",
    "positive_weight = torch.tensor(compute_class_weight(class_weight='balanced',classes=np.unique(y_train),y=np.ravel(y_train))[1:])\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(weight = positive_weight)\n",
    "\n",
    "train_dataset = MyDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "steps_without_improvement = 0\n",
    "\n",
    "best_val_loss = np.inf\n",
    "best_model = None\n",
    "best_threshold = None\n",
    "\n",
    "for epoch_num in range(max_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # note that we are using DataLoader to get batches\n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # model training\n",
    "        # implement me!\n",
    "        # your_code\n",
    "    \n",
    "    # model evaluation, early stopping\n",
    "    valid_metrics=evaluate_model(\n",
    "    model, \n",
    "    X_valid, \n",
    "    y_valid, \n",
    "    loss_fn)\n",
    "    val_loss = valid_metrics['loss']\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        steps_without_improvement = 0\n",
    "        best_model = deepcopy(model)\n",
    "        best_threshold = valid_metrics['threshold']\n",
    "    else:\n",
    "        steps_without_improvement += 1\n",
    "    if early_stopping_patience == steps_without_improvement:\n",
    "        break\n",
    "    # implement me!\n",
    "    # your_code\n",
    "    \n",
    "    \n",
    "    print(f\"Epoch {epoch_num} train loss: {loss.item():.4f}, eval loss {valid_metrics['loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 90.75%\n",
      "F1: 69.10%\n",
      "Precision: 64.53%\n",
      "Recall: 74.36%\n"
     ]
    }
   ],
   "source": [
    "test_metrics = evaluate_model(best_model, X_test, y_test, loss_fn, best_threshold)\n",
    "\n",
    "print(f\"AUROC: {100 * test_metrics['AUROC']:.2f}%\")\n",
    "print(f\"F1: {100 * test_metrics['F1-score']:.2f}%\")\n",
    "print(f\"Precision: {100 * test_metrics['precision']:.2f}%\")\n",
    "print(f\"Recall: {100 * test_metrics['recall']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyoRnHT4GFR9"
   },
   "source": [
    "## Akceleracja sprzętowa (dla zainteresowanych)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak wcześniej wspominaliśmy, użycie akceleracji sprzętowej, czyli po prostu GPU do obliczeń, jest bardzo efektywne w przypadku sieci neuronowych. Karty graficzne bardzo efektywnie mnożą macierze, a sieci neuronowe to, jak można było się przekonać, dużo mnożenia macierzy.\n",
    "\n",
    "W PyTorchu jest to dosyć łatwe, ale trzeba robić to explicite. Służy do tego metoda `.to()`, która przenosi tensory między CPU i GPU. Poniżej przykład, jak to się robi (oczywiście trzeba mieć skonfigurowane GPU, żeby działało):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport time \\n\\n\\nclass CudaMLP(nn.Module):\\n    def __init__(self, input_size: int, dropout_p: float = 0.5):\\n        super().__init__()\\n\\n        self.mlp = nn.Sequential(\\n            nn.Linear(input_size, 512),\\n            nn.BatchNorm1d(512),\\n            nn.ReLU(),\\n            nn.Dropout(dropout_p),\\n            nn.Linear(512, 256),\\n            nn.BatchNorm1d(256),\\n            nn.ReLU(),\\n            nn.Dropout(dropout_p),\\n            nn.Linear(256, 256),\\n            nn.BatchNorm1d(256),\\n            nn.ReLU(),\\n            nn.Dropout(dropout_p),\\n            nn.Linear(256, 128),\\n            nn.BatchNorm1d(128),\\n            nn.ReLU(),\\n            nn.Dropout(dropout_p),\\n            nn.Linear(128, 1),\\n        )\\n    \\n    def forward(self, x):\\n        return self.mlp(x)\\n\\n    def predict_proba(self, x):\\n        return sigmoid(self(x))\\n    \\n    def predict(self, x, threshold: float = 0.5):\\n        y_pred_score = self.predict_proba(x)\\n        return (y_pred_score > threshold).to(torch.int32)\\n\\n\\nmodel = CudaMLP(X_train.shape[1]).to(\\'cuda\\')\\n\\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\\n\\n# note that we are using loss function with sigmoid built in\\nloss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=torch.from_numpy(weights)[1].to(\\'cuda\\'))\\n\\nstep_counter = 0\\ntime_from_eval = time.time()\\nfor epoch_id in range(30):\\n    for batch_x, batch_y in train_dataloader:\\n        batch_x = batch_x.to(\\'cuda\\')\\n        batch_y = batch_y.to(\\'cuda\\')\\n        \\n        loss = loss_fn(model(batch_x), batch_y)\\n        loss.backward()\\n\\n        optimizer.step()\\n        optimizer.zero_grad()\\n        \\n        if step_counter % evaluation_steps == 0:\\n            print(f\"Epoch {epoch_id} train loss: {loss.item():.4f}, time: {time.time() - time_from_eval}\")\\n            time_from_eval = time.time()\\n\\n        step_counter += 1\\n\\ntest_res = evaluate_model(model.to(\\'cpu\\'), X_test, y_test, loss_fn.to(\\'cpu\\'), threshold=0.5)\\n\\nprint(f\"AUROC: {100 * test_res[\\'AUROC\\']:.2f}%\")\\nprint(f\"F1: {100 * test_res[\\'F1-score\\']:.2f}%\")\\nprint(test_res)'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import time \n",
    "\n",
    "\n",
    "class CudaMLP(nn.Module):\n",
    "    def __init__(self, input_size: int, dropout_p: float = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        return sigmoid(self(x))\n",
    "    \n",
    "    def predict(self, x, threshold: float = 0.5):\n",
    "        y_pred_score = self.predict_proba(x)\n",
    "        return (y_pred_score > threshold).to(torch.int32)\n",
    "\n",
    "\n",
    "model = CudaMLP(X_train.shape[1]).to('cuda')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# note that we are using loss function with sigmoid built in\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=torch.from_numpy(weights)[1].to('cuda'))\n",
    "\n",
    "step_counter = 0\n",
    "time_from_eval = time.time()\n",
    "for epoch_id in range(30):\n",
    "    for batch_x, batch_y in train_dataloader:\n",
    "        batch_x = batch_x.to('cuda')\n",
    "        batch_y = batch_y.to('cuda')\n",
    "        \n",
    "        loss = loss_fn(model(batch_x), batch_y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if step_counter % evaluation_steps == 0:\n",
    "            print(f\"Epoch {epoch_id} train loss: {loss.item():.4f}, time: {time.time() - time_from_eval}\")\n",
    "            time_from_eval = time.time()\n",
    "\n",
    "        step_counter += 1\n",
    "\n",
    "test_res = evaluate_model(model.to('cpu'), X_test, y_test, loss_fn.to('cpu'), threshold=0.5)\n",
    "\n",
    "print(f\"AUROC: {100 * test_res['AUROC']:.2f}%\")\n",
    "print(f\"F1: {100 * test_res['F1-score']:.2f}%\")\n",
    "print(test_res)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co prawda ten model nie będzie tak dobry jak ten z laboratorium, ale zwróć uwagę, o ile jest większy, a przy tym szybszy.\n",
    "\n",
    "Dla zainteresowanych polecamy [tę serie artykułów](https://medium.com/@adi.fu7/ai-accelerators-part-i-intro-822c2cdb4ca4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie dla chętnych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widzieliśmy, sieci neuronowe mają bardzo dużo hiperparametrów. Przeszukiwanie ich grid search'em jest więc niewykonalne, a chociaż random search by działał, to potrzebowałby wielu iteracji, co też jest kosztowne obliczeniowo.\n",
    "\n",
    "Zaimplementuj inteligentne przeszukiwanie przestrzeni hiperparametrów za pomocą biblioteki [Optuna](https://optuna.org/). Implementuje ona między innymi algorytm Tree Parzen Estimator (TPE), należący do grupy algorytmów typu Bayesian search. Typowo osiągają one bardzo dobre wyniki, a właściwie zawsze lepsze od przeszukiwania losowego. Do tego wystarcza im często niewielka liczba kroków.\n",
    "\n",
    "Zaimplementuj 3-warstwową sieć MLP, gdzie pierwsza warstwa ma rozmiar ukryty N, a druga N // 2. Ucz ją optymalizatorem Adam przez maksymalnie 300 epok z cierpliwością 10.\n",
    "\n",
    "Przeszukaj wybrane zakresy dla hiperparametrów:\n",
    "- rozmiar warstw ukrytych (N)\n",
    "- stała ucząca\n",
    "- batch size\n",
    "- siła regularyzacji L2\n",
    "- prawdopodobieństwo dropoutu\n",
    "\n",
    "Wykorzystaj przynajmniej 30 iteracji. Następnie przełącz algorytm na losowy (Optuna także jego implementuje), wykonaj 30 iteracji i porównaj jakość wyników.\n",
    "\n",
    "Przydatne materiały:\n",
    "- [Optuna code examples - PyTorch](https://optuna.org/#code_examples)\n",
    "- [Auto-Tuning Hyperparameters with Optuna and PyTorch](https://www.youtube.com/watch?v=P6NwZVl8ttc)\n",
    "- [Hyperparameter Tuning of Neural Networks with Optuna and PyTorch](https://towardsdatascience.com/hyperparameter-tuning-of-neural-networks-with-optuna-and-pytorch-22e179efc837)\n",
    "- [Using Optuna to Optimize PyTorch Hyperparameters](https://medium.com/pytorch/using-optuna-to-optimize-pytorch-hyperparameters-990607385e36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size: int, N: int, dropout_p: float):\n",
    "        super().__init__()\n",
    "        activation = torch.nn.ReLU()\n",
    "        L1 = torch.nn.Linear(input_size, N)\n",
    "        L2 = torch.nn.Linear(N, N//2)\n",
    "        L3 = torch.nn.Linear(N//2, 1)\n",
    "        dropout = torch.nn.Dropout(dropout_p, inplace=False)\n",
    "        self.model = nn.Sequential(\n",
    "            L1,\n",
    "            activation,\n",
    "            dropout,\n",
    "            L2,\n",
    "            activation,\n",
    "            dropout,\n",
    "            L3\n",
    "        )\n",
    "        self.model.train()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        return sigmoid(self(x))\n",
    "    \n",
    "    def predict(self, x, threshold: float = 0.5):\n",
    "        y_pred_score = self.predict_proba(x)\n",
    "        return (y_pred_score > threshold).to(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testMLP(learning_rate: float,dropout_p: float, l2_reg: float, batch_size: int, N: int):\n",
    "    max_epochs = 300\n",
    "    early_stopping_patience = 10\n",
    "    model = MLP(\n",
    "        input_size=X_train.shape[1], \n",
    "        dropout_p=dropout_p,\n",
    "        N=N\n",
    "    )\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=learning_rate, \n",
    "        weight_decay=l2_reg\n",
    "    )\n",
    "    positive_weight = torch.tensor(compute_class_weight(class_weight='balanced',classes=np.unique(y_train),y=np.ravel(y_train))[1:])\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(weight = positive_weight)\n",
    "\n",
    "    train_dataset = MyDataset(X_train, y_train)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "    steps_without_improvement = 0\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    best_model = None\n",
    "    best_threshold = None\n",
    "\n",
    "    for epoch_num in range(max_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for X_batch, y_batch in train_dataloader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # model training\n",
    "\n",
    "        # model evaluation, early stopping\n",
    "        valid_metrics=evaluate_model(\n",
    "        model, \n",
    "        X_valid, \n",
    "        y_valid, \n",
    "        loss_fn)\n",
    "        val_loss = valid_metrics['loss']\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            steps_without_improvement = 0\n",
    "            best_model = deepcopy(model)\n",
    "            best_threshold = valid_metrics['threshold']\n",
    "        else:\n",
    "            steps_without_improvement += 1\n",
    "        if early_stopping_patience == steps_without_improvement:\n",
    "            break\n",
    "        print(f\"Epoch {epoch_num} train loss: {loss.item():.4f}, eval loss {valid_metrics['loss']}\")\n",
    "    return loss.item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "def objective(trial):\n",
    "    learning_rate_exp = trial.suggest_float('learning_rate_exp', -6, -2)\n",
    "    dropout_p         = trial.suggest_float('dropout_p', 0.05, 0.60)\n",
    "    l2_reg_exp        = trial.suggest_float('l2_reg_exp', -7, -2)\n",
    "    batch_size        = trial.suggest_int('batch_size', 8, 512)\n",
    "    N                 = trial.suggest_int('N', 32, 512)\n",
    "    \n",
    "    return testMLP(10**learning_rate_exp, dropout_p, 10**l2_reg_exp, batch_size, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 11:51:48,496] A new study created in memory with name: no-name-10ccdd07-6f1b-4711-bcbb-eae17faaf6b8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6672\n",
      "Epoch 0 train loss: 1.1053, eval loss 1.1148873567581177\n",
      "optimal threshold: -0.9788\n",
      "Epoch 1 train loss: 0.9116, eval loss 0.8965790271759033\n",
      "optimal threshold: -0.8712\n",
      "Epoch 2 train loss: 0.8137, eval loss 0.7711989283561707\n",
      "optimal threshold: -0.9012\n",
      "Epoch 3 train loss: 0.7951, eval loss 0.7299638986587524\n",
      "optimal threshold: -0.8454\n",
      "Epoch 4 train loss: 0.7816, eval loss 0.713834285736084\n",
      "optimal threshold: -0.8269\n",
      "Epoch 5 train loss: 0.7524, eval loss 0.7035317420959473\n",
      "optimal threshold: -0.8515\n",
      "Epoch 6 train loss: 0.7934, eval loss 0.6957297325134277\n",
      "optimal threshold: -0.6329\n",
      "Epoch 7 train loss: 0.7660, eval loss 0.6895248889923096\n",
      "optimal threshold: -0.5876\n",
      "Epoch 8 train loss: 0.7417, eval loss 0.6845573782920837\n",
      "optimal threshold: -0.6427\n",
      "Epoch 9 train loss: 0.7225, eval loss 0.6806442141532898\n",
      "optimal threshold: -0.5974\n",
      "Epoch 10 train loss: 0.7340, eval loss 0.6775942444801331\n",
      "optimal threshold: -0.5669\n",
      "Epoch 11 train loss: 0.8049, eval loss 0.6751189827919006\n",
      "optimal threshold: -0.6090\n",
      "Epoch 12 train loss: 0.7310, eval loss 0.6731095910072327\n",
      "optimal threshold: -0.6629\n",
      "Epoch 13 train loss: 0.7069, eval loss 0.6712561845779419\n",
      "optimal threshold: -0.5726\n",
      "Epoch 14 train loss: 0.6984, eval loss 0.6698211431503296\n",
      "optimal threshold: -0.5460\n",
      "Epoch 15 train loss: 0.6727, eval loss 0.668587327003479\n",
      "optimal threshold: -0.5297\n",
      "Epoch 16 train loss: 0.7372, eval loss 0.6674391627311707\n",
      "optimal threshold: -0.6395\n",
      "Epoch 17 train loss: 0.7170, eval loss 0.6661313772201538\n",
      "optimal threshold: -0.4750\n",
      "Epoch 18 train loss: 0.7255, eval loss 0.6652008891105652\n",
      "optimal threshold: -0.4768\n",
      "Epoch 19 train loss: 0.7354, eval loss 0.6645629405975342\n",
      "optimal threshold: -0.4552\n",
      "Epoch 20 train loss: 0.6803, eval loss 0.6639176607131958\n",
      "optimal threshold: -0.8479\n",
      "Epoch 21 train loss: 0.7193, eval loss 0.6632099747657776\n",
      "optimal threshold: -0.4495\n",
      "Epoch 22 train loss: 0.6923, eval loss 0.6627182364463806\n",
      "optimal threshold: -0.4144\n",
      "Epoch 23 train loss: 0.7302, eval loss 0.6624143123626709\n",
      "optimal threshold: -0.4275\n",
      "Epoch 24 train loss: 0.6843, eval loss 0.6618574857711792\n",
      "optimal threshold: -0.7756\n",
      "Epoch 25 train loss: 0.6533, eval loss 0.661373496055603\n",
      "optimal threshold: -0.8765\n",
      "Epoch 26 train loss: 0.7153, eval loss 0.6610347032546997\n",
      "optimal threshold: -0.7670\n",
      "Epoch 27 train loss: 0.7153, eval loss 0.6606309413909912\n",
      "optimal threshold: -0.7595\n",
      "Epoch 28 train loss: 0.6725, eval loss 0.6600287556648254\n",
      "optimal threshold: -0.7206\n",
      "Epoch 29 train loss: 0.7143, eval loss 0.6596904993057251\n",
      "optimal threshold: -0.7680\n",
      "Epoch 30 train loss: 0.6632, eval loss 0.659500777721405\n",
      "optimal threshold: -0.7698\n",
      "Epoch 31 train loss: 0.7135, eval loss 0.6594107747077942\n",
      "optimal threshold: -0.7085\n",
      "Epoch 32 train loss: 0.6886, eval loss 0.6591053009033203\n",
      "optimal threshold: -0.7442\n",
      "Epoch 33 train loss: 0.6912, eval loss 0.6588709950447083\n",
      "optimal threshold: -0.7745\n",
      "Epoch 34 train loss: 0.6309, eval loss 0.6588230133056641\n",
      "optimal threshold: -0.7540\n",
      "Epoch 35 train loss: 0.6230, eval loss 0.6583791971206665\n",
      "optimal threshold: -0.7506\n",
      "Epoch 36 train loss: 0.7363, eval loss 0.6583591103553772\n",
      "optimal threshold: -0.7451\n",
      "Epoch 37 train loss: 0.6945, eval loss 0.657662570476532\n",
      "optimal threshold: -0.7594\n",
      "Epoch 38 train loss: 0.7024, eval loss 0.657457172870636\n",
      "optimal threshold: -0.4061\n",
      "Epoch 39 train loss: 0.6641, eval loss 0.6574261784553528\n",
      "optimal threshold: -0.7891\n",
      "Epoch 40 train loss: 0.6247, eval loss 0.6575234532356262\n",
      "optimal threshold: -0.3892\n",
      "Epoch 41 train loss: 0.6955, eval loss 0.6573694944381714\n",
      "optimal threshold: -0.3875\n",
      "Epoch 42 train loss: 0.6931, eval loss 0.6571395397186279\n",
      "optimal threshold: -0.3895\n",
      "Epoch 43 train loss: 0.6988, eval loss 0.6572258472442627\n",
      "optimal threshold: -0.4027\n",
      "Epoch 44 train loss: 0.6552, eval loss 0.6572011709213257\n",
      "optimal threshold: -0.3792\n",
      "Epoch 45 train loss: 0.6775, eval loss 0.6568994522094727\n",
      "optimal threshold: -0.3532\n",
      "Epoch 46 train loss: 0.6722, eval loss 0.6567458510398865\n",
      "optimal threshold: -0.3614\n",
      "Epoch 47 train loss: 0.6962, eval loss 0.6567935943603516\n",
      "optimal threshold: -0.3793\n",
      "Epoch 48 train loss: 0.7105, eval loss 0.6568443775177002\n",
      "optimal threshold: -0.3570\n",
      "Epoch 49 train loss: 0.7142, eval loss 0.656629741191864\n",
      "optimal threshold: -0.3790\n",
      "Epoch 50 train loss: 0.6587, eval loss 0.6566656231880188\n",
      "optimal threshold: -0.4331\n",
      "Epoch 51 train loss: 0.6565, eval loss 0.6564504504203796\n",
      "optimal threshold: -0.4432\n",
      "Epoch 52 train loss: 0.6489, eval loss 0.65618497133255\n",
      "optimal threshold: -0.4423\n",
      "Epoch 53 train loss: 0.6295, eval loss 0.6561036109924316\n",
      "optimal threshold: -0.4344\n",
      "Epoch 54 train loss: 0.6559, eval loss 0.6561486721038818\n",
      "optimal threshold: -0.4656\n",
      "Epoch 55 train loss: 0.7029, eval loss 0.6563537120819092\n",
      "optimal threshold: -0.5798\n",
      "Epoch 56 train loss: 0.6707, eval loss 0.6562632322311401\n",
      "optimal threshold: -0.5875\n",
      "Epoch 57 train loss: 0.6950, eval loss 0.6563563942909241\n",
      "optimal threshold: -0.6152\n",
      "Epoch 58 train loss: 0.6803, eval loss 0.6563563346862793\n",
      "optimal threshold: -0.6699\n",
      "Epoch 59 train loss: 0.6786, eval loss 0.6565189361572266\n",
      "optimal threshold: -0.6773\n",
      "Epoch 60 train loss: 0.6696, eval loss 0.6564657688140869\n",
      "optimal threshold: -0.6382\n",
      "Epoch 61 train loss: 0.6524, eval loss 0.6563642024993896\n",
      "optimal threshold: -0.6556\n",
      "Epoch 62 train loss: 0.6892, eval loss 0.655939519405365\n",
      "optimal threshold: -0.5827\n",
      "Epoch 63 train loss: 0.6292, eval loss 0.6563838124275208\n",
      "optimal threshold: -0.6157\n",
      "Epoch 64 train loss: 0.6283, eval loss 0.6561306715011597\n",
      "optimal threshold: -0.5689\n",
      "Epoch 65 train loss: 0.6453, eval loss 0.656240701675415\n",
      "optimal threshold: -0.5761\n",
      "Epoch 66 train loss: 0.6478, eval loss 0.6562376022338867\n",
      "optimal threshold: -0.5731\n",
      "Epoch 67 train loss: 0.6388, eval loss 0.6560438275337219\n",
      "optimal threshold: -0.6382\n",
      "Epoch 68 train loss: 0.6326, eval loss 0.6564677953720093\n",
      "optimal threshold: -0.6542\n",
      "Epoch 69 train loss: 0.7314, eval loss 0.6565216779708862\n",
      "optimal threshold: -0.6238\n",
      "Epoch 70 train loss: 0.6773, eval loss 0.6562417149543762\n",
      "optimal threshold: -0.5535\n",
      "Epoch 71 train loss: 0.6440, eval loss 0.6563662886619568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 11:52:48,088] Trial 0 finished with value: 0.6650635600090027 and parameters: {'learning_rate_exp': -4.325400043246469, 'dropout_p': 0.41213419345976443, 'l2_reg_exp': -5.190794291932059, 'batch_size': 223, 'N': 474}. Best is trial 0 with value: 0.6650635600090027.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5819\n",
      "optimal threshold: -0.4012\n",
      "Epoch 0 train loss: 0.6362, eval loss 0.6688444018363953\n",
      "optimal threshold: -0.4311\n",
      "Epoch 1 train loss: 0.6727, eval loss 0.6632043719291687\n",
      "optimal threshold: -0.4159\n",
      "Epoch 2 train loss: 0.6391, eval loss 0.6588773727416992\n",
      "optimal threshold: -0.6049\n",
      "Epoch 3 train loss: 0.6453, eval loss 0.6681647896766663\n",
      "optimal threshold: -0.4914\n",
      "Epoch 4 train loss: 0.6598, eval loss 0.6651393175125122\n",
      "optimal threshold: -0.6858\n",
      "Epoch 5 train loss: 0.6195, eval loss 0.662865400314331\n",
      "optimal threshold: -0.3498\n",
      "Epoch 6 train loss: 0.6243, eval loss 0.6657825112342834\n",
      "optimal threshold: -0.4431\n",
      "Epoch 7 train loss: 0.6059, eval loss 0.6714516878128052\n",
      "optimal threshold: -0.4195\n",
      "Epoch 8 train loss: 0.5839, eval loss 0.6801894903182983\n",
      "optimal threshold: -0.3608\n",
      "Epoch 9 train loss: 0.6154, eval loss 0.6789467334747314\n",
      "optimal threshold: -0.4462\n",
      "Epoch 10 train loss: 0.6333, eval loss 0.6774603724479675\n",
      "optimal threshold: -0.3454\n",
      "Epoch 11 train loss: 0.6065, eval loss 0.6820998787879944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 11:52:55,977] Trial 1 finished with value: 0.6184212565422058 and parameters: {'learning_rate_exp': -2.057529186947169, 'dropout_p': 0.46250897377241595, 'l2_reg_exp': -4.019426181981968, 'batch_size': 268, 'N': 253}. Best is trial 1 with value: 0.6184212565422058.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.3931\n",
      "optimal threshold: -0.8073\n",
      "Epoch 0 train loss: 0.8398, eval loss 0.7244613170623779\n",
      "optimal threshold: -0.6444\n",
      "Epoch 1 train loss: 0.7366, eval loss 0.6859903335571289\n",
      "optimal threshold: -0.6057\n",
      "Epoch 2 train loss: 0.7042, eval loss 0.6718201637268066\n",
      "optimal threshold: -0.5522\n",
      "Epoch 3 train loss: 0.8067, eval loss 0.6661244034767151\n",
      "optimal threshold: -0.3748\n",
      "Epoch 4 train loss: 0.7690, eval loss 0.6619346737861633\n",
      "optimal threshold: -0.4356\n",
      "Epoch 5 train loss: 0.6618, eval loss 0.6603356003761292\n",
      "optimal threshold: -0.5081\n",
      "Epoch 6 train loss: 0.6537, eval loss 0.6596104502677917\n",
      "optimal threshold: -0.4658\n",
      "Epoch 7 train loss: 0.7426, eval loss 0.6582477688789368\n",
      "optimal threshold: -0.4518\n",
      "Epoch 8 train loss: 0.7502, eval loss 0.6572262048721313\n",
      "optimal threshold: -0.4088\n",
      "Epoch 9 train loss: 0.6433, eval loss 0.6572208404541016\n",
      "optimal threshold: -0.4412\n",
      "Epoch 10 train loss: 0.7192, eval loss 0.656642496585846\n",
      "optimal threshold: -0.5508\n",
      "Epoch 11 train loss: 0.7161, eval loss 0.6560396552085876\n",
      "optimal threshold: -0.5605\n",
      "Epoch 12 train loss: 0.6617, eval loss 0.6562705039978027\n",
      "optimal threshold: -0.4693\n",
      "Epoch 13 train loss: 0.7110, eval loss 0.6564810872077942\n",
      "optimal threshold: -0.4505\n",
      "Epoch 14 train loss: 0.6597, eval loss 0.6564234495162964\n",
      "optimal threshold: -0.4735\n",
      "Epoch 15 train loss: 0.6599, eval loss 0.6561444997787476\n",
      "optimal threshold: -0.4979\n",
      "Epoch 16 train loss: 0.6747, eval loss 0.6563061475753784\n",
      "optimal threshold: -0.5981\n",
      "Epoch 17 train loss: 0.6201, eval loss 0.6563507318496704\n",
      "optimal threshold: -0.5138\n",
      "Epoch 18 train loss: 0.6302, eval loss 0.6560149788856506\n",
      "optimal threshold: -0.4945\n",
      "Epoch 19 train loss: 0.6167, eval loss 0.6570137143135071\n",
      "optimal threshold: -0.5862\n",
      "Epoch 20 train loss: 0.6428, eval loss 0.6567237377166748\n",
      "optimal threshold: -0.5076\n",
      "Epoch 21 train loss: 0.6337, eval loss 0.6567391157150269\n",
      "optimal threshold: -0.5724\n",
      "Epoch 22 train loss: 0.5903, eval loss 0.6568964719772339\n",
      "optimal threshold: -0.5545\n",
      "Epoch 23 train loss: 0.6552, eval loss 0.6575040221214294\n",
      "optimal threshold: -0.5011\n",
      "Epoch 24 train loss: 0.6259, eval loss 0.6576535701751709\n",
      "optimal threshold: -0.4923\n",
      "Epoch 25 train loss: 0.5267, eval loss 0.657177209854126\n",
      "optimal threshold: -0.6630\n",
      "Epoch 26 train loss: 0.6388, eval loss 0.657199501991272\n",
      "optimal threshold: -0.4716\n",
      "Epoch 27 train loss: 0.5925, eval loss 0.65861576795578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 11:53:33,665] Trial 2 finished with value: 0.6015430688858032 and parameters: {'learning_rate_exp': -3.9569800171636866, 'dropout_p': 0.25796465138506997, 'l2_reg_exp': -4.6601853792225825, 'batch_size': 101, 'N': 426}. Best is trial 2 with value: 0.6015430688858032.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5522\n",
      "optimal threshold: -0.9500\n",
      "Epoch 0 train loss: 0.6611, eval loss 0.6955122351646423\n",
      "optimal threshold: -0.5267\n",
      "Epoch 1 train loss: 0.6189, eval loss 0.6689026951789856\n",
      "optimal threshold: -0.7159\n",
      "Epoch 2 train loss: 0.6038, eval loss 0.6643853187561035\n",
      "optimal threshold: -0.8605\n",
      "Epoch 3 train loss: 0.5953, eval loss 0.6663855910301208\n",
      "optimal threshold: -0.6836\n",
      "Epoch 4 train loss: 0.5708, eval loss 0.6651358604431152\n",
      "optimal threshold: -0.9287\n",
      "Epoch 5 train loss: 0.5699, eval loss 0.667400598526001\n",
      "optimal threshold: -0.8399\n",
      "Epoch 6 train loss: 0.5793, eval loss 0.6673734188079834\n",
      "optimal threshold: -0.7677\n",
      "Epoch 7 train loss: 0.5554, eval loss 0.6687062978744507\n",
      "optimal threshold: -0.8535\n",
      "Epoch 8 train loss: 0.5362, eval loss 0.6723506450653076\n",
      "optimal threshold: -0.7783\n",
      "Epoch 9 train loss: 0.5515, eval loss 0.6730241179466248\n",
      "optimal threshold: -0.8136\n",
      "Epoch 10 train loss: 0.5298, eval loss 0.6740732789039612\n",
      "optimal threshold: -0.7776\n",
      "Epoch 11 train loss: 0.5190, eval loss 0.6789501309394836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 11:53:41,351] Trial 3 finished with value: 0.5098819732666016 and parameters: {'learning_rate_exp': -2.9084857338999783, 'dropout_p': 0.17302937077484754, 'l2_reg_exp': -5.652282872676569, 'batch_size': 448, 'N': 345}. Best is trial 3 with value: 0.5098819732666016.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.8020\n",
      "optimal threshold: -0.3752\n",
      "Epoch 0 train loss: 1.2669, eval loss 1.2181028127670288\n",
      "optimal threshold: -0.7623\n",
      "Epoch 1 train loss: 1.0337, eval loss 0.9978921413421631\n",
      "optimal threshold: -0.8580\n",
      "Epoch 2 train loss: 0.8185, eval loss 0.8577830791473389\n",
      "optimal threshold: -0.8409\n",
      "Epoch 3 train loss: 0.6922, eval loss 0.7814964652061462\n",
      "optimal threshold: -0.9371\n",
      "Epoch 4 train loss: 0.6120, eval loss 0.7458717823028564\n",
      "optimal threshold: -1.0087\n",
      "Epoch 5 train loss: 0.5673, eval loss 0.7281809449195862\n",
      "optimal threshold: -1.0614\n",
      "Epoch 6 train loss: 0.5798, eval loss 0.7175149321556091\n",
      "optimal threshold: -0.7237\n",
      "Epoch 7 train loss: 0.6173, eval loss 0.7096944451332092\n",
      "optimal threshold: -0.6391\n",
      "Epoch 8 train loss: 0.5515, eval loss 0.7033977508544922\n",
      "optimal threshold: -0.6176\n",
      "Epoch 9 train loss: 0.5796, eval loss 0.6980535984039307\n",
      "optimal threshold: -0.5049\n",
      "Epoch 10 train loss: 0.5139, eval loss 0.6934439539909363\n",
      "optimal threshold: -0.6516\n",
      "Epoch 11 train loss: 0.6006, eval loss 0.6894277930259705\n",
      "optimal threshold: -0.6431\n",
      "Epoch 12 train loss: 0.4739, eval loss 0.6859521269798279\n",
      "optimal threshold: -0.6539\n",
      "Epoch 13 train loss: 0.5595, eval loss 0.6829456090927124\n",
      "optimal threshold: -0.5301\n",
      "Epoch 14 train loss: 0.5105, eval loss 0.6803255677223206\n",
      "optimal threshold: -0.5174\n",
      "Epoch 15 train loss: 0.4617, eval loss 0.6780250072479248\n",
      "optimal threshold: -0.6213\n",
      "Epoch 16 train loss: 0.5001, eval loss 0.6760472059249878\n",
      "optimal threshold: -0.5557\n",
      "Epoch 17 train loss: 0.6073, eval loss 0.6743272542953491\n",
      "optimal threshold: -0.4995\n",
      "Epoch 18 train loss: 0.4546, eval loss 0.6727854609489441\n",
      "optimal threshold: -0.5741\n",
      "Epoch 19 train loss: 0.5205, eval loss 0.6714466214179993\n",
      "optimal threshold: -0.4199\n",
      "Epoch 20 train loss: 0.4849, eval loss 0.6702998280525208\n",
      "optimal threshold: -0.3978\n",
      "Epoch 21 train loss: 0.4923, eval loss 0.6692185401916504\n",
      "optimal threshold: -0.5057\n",
      "Epoch 22 train loss: 0.5032, eval loss 0.6683251857757568\n",
      "optimal threshold: -0.3724\n",
      "Epoch 23 train loss: 0.4686, eval loss 0.6675043702125549\n",
      "optimal threshold: -0.4022\n",
      "Epoch 24 train loss: 0.5106, eval loss 0.6667353510856628\n",
      "optimal threshold: -0.4196\n",
      "Epoch 25 train loss: 0.4649, eval loss 0.6661350131034851\n",
      "optimal threshold: -0.4127\n",
      "Epoch 26 train loss: 0.5144, eval loss 0.6655280590057373\n",
      "optimal threshold: -0.4110\n",
      "Epoch 27 train loss: 0.5037, eval loss 0.665043830871582\n",
      "optimal threshold: -0.4144\n",
      "Epoch 28 train loss: 0.4945, eval loss 0.6645094752311707\n",
      "optimal threshold: -0.5371\n",
      "Epoch 29 train loss: 0.4869, eval loss 0.6640757322311401\n",
      "optimal threshold: -0.5373\n",
      "Epoch 30 train loss: 0.5193, eval loss 0.6636402606964111\n",
      "optimal threshold: -0.5423\n",
      "Epoch 31 train loss: 0.5063, eval loss 0.6632375717163086\n",
      "optimal threshold: -0.5393\n",
      "Epoch 32 train loss: 0.5182, eval loss 0.6629106998443604\n",
      "optimal threshold: -0.5305\n",
      "Epoch 33 train loss: 0.4988, eval loss 0.6625651121139526\n",
      "optimal threshold: -0.4035\n",
      "Epoch 34 train loss: 0.4693, eval loss 0.6621989011764526\n",
      "optimal threshold: -0.4166\n",
      "Epoch 35 train loss: 0.4977, eval loss 0.6619395613670349\n",
      "optimal threshold: -0.4010\n",
      "Epoch 36 train loss: 0.4753, eval loss 0.6617216467857361\n",
      "optimal threshold: -0.5153\n",
      "Epoch 37 train loss: 0.4583, eval loss 0.6614747047424316\n",
      "optimal threshold: -0.4654\n",
      "Epoch 38 train loss: 0.4835, eval loss 0.6612633466720581\n",
      "optimal threshold: -0.4956\n",
      "Epoch 39 train loss: 0.4889, eval loss 0.6609882712364197\n",
      "optimal threshold: -0.5015\n",
      "Epoch 40 train loss: 0.4691, eval loss 0.6607878804206848\n",
      "optimal threshold: -0.5043\n",
      "Epoch 41 train loss: 0.4709, eval loss 0.6605690121650696\n",
      "optimal threshold: -0.4963\n",
      "Epoch 42 train loss: 0.4745, eval loss 0.6603007316589355\n",
      "optimal threshold: -0.5017\n",
      "Epoch 43 train loss: 0.5036, eval loss 0.6601536870002747\n",
      "optimal threshold: -0.5028\n",
      "Epoch 44 train loss: 0.4509, eval loss 0.6599317193031311\n",
      "optimal threshold: -0.4933\n",
      "Epoch 45 train loss: 0.4543, eval loss 0.6597500443458557\n",
      "optimal threshold: -0.4144\n",
      "Epoch 46 train loss: 0.4750, eval loss 0.6596552729606628\n",
      "optimal threshold: -0.4107\n",
      "Epoch 47 train loss: 0.4565, eval loss 0.6594114303588867\n",
      "optimal threshold: -0.4092\n",
      "Epoch 48 train loss: 0.4504, eval loss 0.6592896580696106\n",
      "optimal threshold: -0.4121\n",
      "Epoch 49 train loss: 0.4849, eval loss 0.659126877784729\n",
      "optimal threshold: -0.4065\n",
      "Epoch 50 train loss: 0.4741, eval loss 0.6590399742126465\n",
      "optimal threshold: -0.4045\n",
      "Epoch 51 train loss: 0.4690, eval loss 0.658890962600708\n",
      "optimal threshold: -0.3912\n",
      "Epoch 52 train loss: 0.5399, eval loss 0.6586884260177612\n",
      "optimal threshold: -0.3809\n",
      "Epoch 53 train loss: 0.4641, eval loss 0.658667266368866\n",
      "optimal threshold: -0.3839\n",
      "Epoch 54 train loss: 0.5329, eval loss 0.6585144996643066\n",
      "optimal threshold: -0.3778\n",
      "Epoch 55 train loss: 0.4684, eval loss 0.6584561467170715\n",
      "optimal threshold: -0.4030\n",
      "Epoch 56 train loss: 0.4290, eval loss 0.6583185195922852\n",
      "optimal threshold: -0.3999\n",
      "Epoch 57 train loss: 0.4800, eval loss 0.6581763625144958\n",
      "optimal threshold: -0.3973\n",
      "Epoch 58 train loss: 0.4966, eval loss 0.6580091118812561\n",
      "optimal threshold: -0.4009\n",
      "Epoch 59 train loss: 0.5060, eval loss 0.6579280495643616\n",
      "optimal threshold: -0.4108\n",
      "Epoch 60 train loss: 0.4690, eval loss 0.6578918695449829\n",
      "optimal threshold: -0.4024\n",
      "Epoch 61 train loss: 0.4651, eval loss 0.6578398942947388\n",
      "optimal threshold: -0.3711\n",
      "Epoch 62 train loss: 0.4931, eval loss 0.657727062702179\n",
      "optimal threshold: -0.3766\n",
      "Epoch 63 train loss: 0.5125, eval loss 0.6576148867607117\n",
      "optimal threshold: -0.3758\n",
      "Epoch 64 train loss: 0.4408, eval loss 0.6574984192848206\n",
      "optimal threshold: -0.3938\n",
      "Epoch 65 train loss: 0.4818, eval loss 0.6573888063430786\n",
      "optimal threshold: -0.3939\n",
      "Epoch 66 train loss: 0.4447, eval loss 0.6573166251182556\n",
      "optimal threshold: -0.3997\n",
      "Epoch 67 train loss: 0.4880, eval loss 0.6572443842887878\n",
      "optimal threshold: -0.3856\n",
      "Epoch 68 train loss: 0.4952, eval loss 0.65725177526474\n",
      "optimal threshold: -0.3970\n",
      "Epoch 69 train loss: 0.4612, eval loss 0.6572121977806091\n",
      "optimal threshold: -0.3811\n",
      "Epoch 70 train loss: 0.4345, eval loss 0.6571568250656128\n",
      "optimal threshold: -0.3793\n",
      "Epoch 71 train loss: 0.4790, eval loss 0.6572093367576599\n",
      "optimal threshold: -0.3840\n",
      "Epoch 72 train loss: 0.4699, eval loss 0.6571337580680847\n",
      "optimal threshold: -0.3899\n",
      "Epoch 73 train loss: 0.4471, eval loss 0.6571094989776611\n",
      "optimal threshold: -0.3897\n",
      "Epoch 74 train loss: 0.4935, eval loss 0.6569358706474304\n",
      "optimal threshold: -0.3920\n",
      "Epoch 75 train loss: 0.4491, eval loss 0.6568769216537476\n",
      "optimal threshold: -0.3736\n",
      "Epoch 76 train loss: 0.4678, eval loss 0.6567375659942627\n",
      "optimal threshold: -0.3740\n",
      "Epoch 77 train loss: 0.4760, eval loss 0.6567630171775818\n",
      "optimal threshold: -0.3783\n",
      "Epoch 78 train loss: 0.5276, eval loss 0.6567848324775696\n",
      "optimal threshold: -0.3781\n",
      "Epoch 79 train loss: 0.5246, eval loss 0.6567288041114807\n",
      "optimal threshold: -0.3582\n",
      "Epoch 80 train loss: 0.4460, eval loss 0.6566726565361023\n",
      "optimal threshold: -0.3675\n",
      "Epoch 81 train loss: 0.4472, eval loss 0.6566135287284851\n",
      "optimal threshold: -0.3692\n",
      "Epoch 82 train loss: 0.4993, eval loss 0.6566593050956726\n",
      "optimal threshold: -0.3714\n",
      "Epoch 83 train loss: 0.4816, eval loss 0.656571090221405\n",
      "optimal threshold: -0.3697\n",
      "Epoch 84 train loss: 0.4838, eval loss 0.6564184427261353\n",
      "optimal threshold: -0.3842\n",
      "Epoch 85 train loss: 0.5046, eval loss 0.6563766598701477\n",
      "optimal threshold: -0.3893\n",
      "Epoch 86 train loss: 0.4744, eval loss 0.656448483467102\n",
      "optimal threshold: -0.3810\n",
      "Epoch 87 train loss: 0.4722, eval loss 0.6564043164253235\n",
      "optimal threshold: -0.3738\n",
      "Epoch 88 train loss: 0.5302, eval loss 0.6563949584960938\n",
      "optimal threshold: -0.4173\n",
      "Epoch 89 train loss: 0.4747, eval loss 0.6563852429389954\n",
      "optimal threshold: -0.4456\n",
      "Epoch 90 train loss: 0.4902, eval loss 0.6563872694969177\n",
      "optimal threshold: -0.4079\n",
      "Epoch 91 train loss: 0.5251, eval loss 0.6562744975090027\n",
      "optimal threshold: -0.4292\n",
      "Epoch 92 train loss: 0.4991, eval loss 0.6562332510948181\n",
      "optimal threshold: -0.4236\n",
      "Epoch 93 train loss: 0.4876, eval loss 0.6562647223472595\n",
      "optimal threshold: -0.4207\n",
      "Epoch 94 train loss: 0.4744, eval loss 0.6562125086784363\n",
      "optimal threshold: -0.4208\n",
      "Epoch 95 train loss: 0.5418, eval loss 0.656212568283081\n",
      "optimal threshold: -0.4557\n",
      "Epoch 96 train loss: 0.5102, eval loss 0.6562672257423401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.3430\n",
      "Epoch 97 train loss: 0.5157, eval loss 0.6561251878738403\n",
      "optimal threshold: -0.4249\n",
      "Epoch 98 train loss: 0.4262, eval loss 0.6561793088912964\n",
      "optimal threshold: -0.3919\n",
      "Epoch 99 train loss: 0.4298, eval loss 0.6561481952667236\n",
      "optimal threshold: -0.4058\n",
      "Epoch 100 train loss: 0.5308, eval loss 0.6561641693115234\n",
      "optimal threshold: -0.4076\n",
      "Epoch 101 train loss: 0.4883, eval loss 0.6561391353607178\n",
      "optimal threshold: -0.4046\n",
      "Epoch 102 train loss: 0.4849, eval loss 0.6562215089797974\n",
      "optimal threshold: -0.4083\n",
      "Epoch 103 train loss: 0.5006, eval loss 0.656210720539093\n",
      "optimal threshold: -0.4067\n",
      "Epoch 104 train loss: 0.4534, eval loss 0.6561474204063416\n",
      "optimal threshold: -0.6391\n",
      "Epoch 105 train loss: 0.5254, eval loss 0.6562028527259827\n",
      "optimal threshold: -0.4055\n",
      "Epoch 106 train loss: 0.5126, eval loss 0.6561944484710693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 11:57:55,899] Trial 4 finished with value: 0.49366649985313416 and parameters: {'learning_rate_exp': -5.067860789808828, 'dropout_p': 0.1328949272994101, 'l2_reg_exp': -3.4006271421715777, 'batch_size': 38, 'N': 407}. Best is trial 4 with value: 0.49366649985313416.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4120\n",
      "optimal threshold: 0.0954\n",
      "Epoch 0 train loss: 1.5050, eval loss 1.506309986114502\n",
      "optimal threshold: 0.0845\n",
      "Epoch 1 train loss: 1.4915, eval loss 1.4947603940963745\n",
      "optimal threshold: 0.0734\n",
      "Epoch 2 train loss: 1.4857, eval loss 1.4833202362060547\n",
      "optimal threshold: 0.0567\n",
      "Epoch 3 train loss: 1.4765, eval loss 1.4718199968338013\n",
      "optimal threshold: 0.0474\n",
      "Epoch 4 train loss: 1.4601, eval loss 1.4601020812988281\n",
      "optimal threshold: 0.0295\n",
      "Epoch 5 train loss: 1.4513, eval loss 1.4480262994766235\n",
      "optimal threshold: 0.0119\n",
      "Epoch 6 train loss: 1.4339, eval loss 1.4355144500732422\n",
      "optimal threshold: -0.0082\n",
      "Epoch 7 train loss: 1.4191, eval loss 1.422413945198059\n",
      "optimal threshold: -0.0301\n",
      "Epoch 8 train loss: 1.4042, eval loss 1.408727765083313\n",
      "optimal threshold: -0.0483\n",
      "Epoch 9 train loss: 1.3964, eval loss 1.3944200277328491\n",
      "optimal threshold: -0.0704\n",
      "Epoch 10 train loss: 1.3802, eval loss 1.3794806003570557\n",
      "optimal threshold: -0.0949\n",
      "Epoch 11 train loss: 1.3692, eval loss 1.3639925718307495\n",
      "optimal threshold: -0.1229\n",
      "Epoch 12 train loss: 1.3484, eval loss 1.3480408191680908\n",
      "optimal threshold: -0.1496\n",
      "Epoch 13 train loss: 1.3325, eval loss 1.3316617012023926\n",
      "optimal threshold: -0.1778\n",
      "Epoch 14 train loss: 1.3203, eval loss 1.3150075674057007\n",
      "optimal threshold: -0.2041\n",
      "Epoch 15 train loss: 1.3009, eval loss 1.2980849742889404\n",
      "optimal threshold: -0.2367\n",
      "Epoch 16 train loss: 1.2857, eval loss 1.2809772491455078\n",
      "optimal threshold: -0.2645\n",
      "Epoch 17 train loss: 1.2684, eval loss 1.2637962102890015\n",
      "optimal threshold: -0.2983\n",
      "Epoch 18 train loss: 1.2418, eval loss 1.2465624809265137\n",
      "optimal threshold: -0.3260\n",
      "Epoch 19 train loss: 1.2364, eval loss 1.2293421030044556\n",
      "optimal threshold: -0.3577\n",
      "Epoch 20 train loss: 1.2166, eval loss 1.212165355682373\n",
      "optimal threshold: -0.3878\n",
      "Epoch 21 train loss: 1.2024, eval loss 1.1950881481170654\n",
      "optimal threshold: -0.4161\n",
      "Epoch 22 train loss: 1.1828, eval loss 1.1781234741210938\n",
      "optimal threshold: -0.4457\n",
      "Epoch 23 train loss: 1.1608, eval loss 1.1613157987594604\n",
      "optimal threshold: -0.4750\n",
      "Epoch 24 train loss: 1.1456, eval loss 1.1447017192840576\n",
      "optimal threshold: -0.5113\n",
      "Epoch 25 train loss: 1.1330, eval loss 1.1283197402954102\n",
      "optimal threshold: -0.5385\n",
      "Epoch 26 train loss: 1.1100, eval loss 1.1122006177902222\n",
      "optimal threshold: -0.5544\n",
      "Epoch 27 train loss: 1.0918, eval loss 1.0963668823242188\n",
      "optimal threshold: -0.5712\n",
      "Epoch 28 train loss: 1.0832, eval loss 1.0808169841766357\n",
      "optimal threshold: -0.6157\n",
      "Epoch 29 train loss: 1.0713, eval loss 1.0655977725982666\n",
      "optimal threshold: -0.6252\n",
      "Epoch 30 train loss: 1.0492, eval loss 1.050750732421875\n",
      "optimal threshold: -0.6655\n",
      "Epoch 31 train loss: 1.0384, eval loss 1.0362434387207031\n",
      "optimal threshold: -0.6956\n",
      "Epoch 32 train loss: 1.0340, eval loss 1.0220707654953003\n",
      "optimal threshold: -0.7189\n",
      "Epoch 33 train loss: 1.0003, eval loss 1.008268117904663\n",
      "optimal threshold: -0.7394\n",
      "Epoch 34 train loss: 0.9931, eval loss 0.9947908520698547\n",
      "optimal threshold: -0.7381\n",
      "Epoch 35 train loss: 0.9793, eval loss 0.981670618057251\n",
      "optimal threshold: -0.7562\n",
      "Epoch 36 train loss: 0.9623, eval loss 0.9689523577690125\n",
      "optimal threshold: -0.8036\n",
      "Epoch 37 train loss: 0.9481, eval loss 0.9565752148628235\n",
      "optimal threshold: -0.8220\n",
      "Epoch 38 train loss: 0.9484, eval loss 0.9445844888687134\n",
      "optimal threshold: -0.8331\n",
      "Epoch 39 train loss: 0.9090, eval loss 0.932948887348175\n",
      "optimal threshold: -0.8428\n",
      "Epoch 40 train loss: 0.9180, eval loss 0.9216772317886353\n",
      "optimal threshold: -0.8587\n",
      "Epoch 41 train loss: 0.9070, eval loss 0.9107778668403625\n",
      "optimal threshold: -0.8878\n",
      "Epoch 42 train loss: 0.8855, eval loss 0.9002848267555237\n",
      "optimal threshold: -0.8946\n",
      "Epoch 43 train loss: 0.8860, eval loss 0.8901241421699524\n",
      "optimal threshold: -0.8951\n",
      "Epoch 44 train loss: 0.8831, eval loss 0.8803091645240784\n",
      "optimal threshold: -0.9041\n",
      "Epoch 45 train loss: 0.8785, eval loss 0.8707917928695679\n",
      "optimal threshold: -0.9185\n",
      "Epoch 46 train loss: 0.8599, eval loss 0.8615940809249878\n",
      "optimal threshold: -0.9293\n",
      "Epoch 47 train loss: 0.8518, eval loss 0.8526108264923096\n",
      "optimal threshold: -0.9413\n",
      "Epoch 48 train loss: 0.8362, eval loss 0.8438213467597961\n",
      "optimal threshold: -0.9386\n",
      "Epoch 49 train loss: 0.8293, eval loss 0.8353583216667175\n",
      "optimal threshold: -0.9689\n",
      "Epoch 50 train loss: 0.8302, eval loss 0.827334463596344\n",
      "optimal threshold: -0.9748\n",
      "Epoch 51 train loss: 0.8192, eval loss 0.8197116851806641\n",
      "optimal threshold: -0.9745\n",
      "Epoch 52 train loss: 0.8113, eval loss 0.8124869465827942\n",
      "optimal threshold: -0.9867\n",
      "Epoch 53 train loss: 0.8028, eval loss 0.8056447505950928\n",
      "optimal threshold: -0.9882\n",
      "Epoch 54 train loss: 0.7825, eval loss 0.7991867661476135\n",
      "optimal threshold: -0.9909\n",
      "Epoch 55 train loss: 0.7818, eval loss 0.7930655479431152\n",
      "optimal threshold: -1.0033\n",
      "Epoch 56 train loss: 0.7727, eval loss 0.787264883518219\n",
      "optimal threshold: -0.9912\n",
      "Epoch 57 train loss: 0.7787, eval loss 0.7818315029144287\n",
      "optimal threshold: -0.9934\n",
      "Epoch 58 train loss: 0.7794, eval loss 0.7767452001571655\n",
      "optimal threshold: -0.9977\n",
      "Epoch 59 train loss: 0.7535, eval loss 0.7719365954399109\n",
      "optimal threshold: -1.0020\n",
      "Epoch 60 train loss: 0.7615, eval loss 0.7674029469490051\n",
      "optimal threshold: -0.9971\n",
      "Epoch 61 train loss: 0.7598, eval loss 0.7631638050079346\n",
      "optimal threshold: -0.9756\n",
      "Epoch 62 train loss: 0.7563, eval loss 0.7591750025749207\n",
      "optimal threshold: -0.9762\n",
      "Epoch 63 train loss: 0.7624, eval loss 0.7554175853729248\n",
      "optimal threshold: -1.0002\n",
      "Epoch 64 train loss: 0.7629, eval loss 0.7518740892410278\n",
      "optimal threshold: -1.0019\n",
      "Epoch 65 train loss: 0.7535, eval loss 0.7485508322715759\n",
      "optimal threshold: -0.9394\n",
      "Epoch 66 train loss: 0.7292, eval loss 0.7454259991645813\n",
      "optimal threshold: -0.9511\n",
      "Epoch 67 train loss: 0.7236, eval loss 0.7424920797348022\n",
      "optimal threshold: -0.9951\n",
      "Epoch 68 train loss: 0.7286, eval loss 0.739726185798645\n",
      "optimal threshold: -0.9867\n",
      "Epoch 69 train loss: 0.7255, eval loss 0.7371305227279663\n",
      "optimal threshold: -0.9802\n",
      "Epoch 70 train loss: 0.7183, eval loss 0.734665036201477\n",
      "optimal threshold: -0.9770\n",
      "Epoch 71 train loss: 0.7095, eval loss 0.732311487197876\n",
      "optimal threshold: -0.9675\n",
      "Epoch 72 train loss: 0.7193, eval loss 0.7301658987998962\n",
      "optimal threshold: -0.9679\n",
      "Epoch 73 train loss: 0.7169, eval loss 0.7281149625778198\n",
      "optimal threshold: -0.9665\n",
      "Epoch 74 train loss: 0.7308, eval loss 0.7261716723442078\n",
      "optimal threshold: -1.0195\n",
      "Epoch 75 train loss: 0.7071, eval loss 0.7243197560310364\n",
      "optimal threshold: -0.9598\n",
      "Epoch 76 train loss: 0.7129, eval loss 0.722606897354126\n",
      "optimal threshold: -1.0118\n",
      "Epoch 77 train loss: 0.7178, eval loss 0.7209637761116028\n",
      "optimal threshold: -1.0139\n",
      "Epoch 78 train loss: 0.7149, eval loss 0.7193878293037415\n",
      "optimal threshold: -0.9241\n",
      "Epoch 79 train loss: 0.7155, eval loss 0.7178773880004883\n",
      "optimal threshold: -0.9180\n",
      "Epoch 80 train loss: 0.6865, eval loss 0.7164715528488159\n",
      "optimal threshold: -0.8035\n",
      "Epoch 81 train loss: 0.7192, eval loss 0.7150959968566895\n",
      "optimal threshold: -0.7996\n",
      "Epoch 82 train loss: 0.7112, eval loss 0.7138000130653381\n",
      "optimal threshold: -0.8004\n",
      "Epoch 83 train loss: 0.7016, eval loss 0.7125867605209351\n",
      "optimal threshold: -0.8009\n",
      "Epoch 84 train loss: 0.6954, eval loss 0.7113864421844482\n",
      "optimal threshold: -0.7926\n",
      "Epoch 85 train loss: 0.7032, eval loss 0.7102389931678772\n",
      "optimal threshold: -0.7926\n",
      "Epoch 86 train loss: 0.6926, eval loss 0.7091346979141235\n",
      "optimal threshold: -0.7721\n",
      "Epoch 87 train loss: 0.6852, eval loss 0.7081126570701599\n",
      "optimal threshold: -0.7722\n",
      "Epoch 88 train loss: 0.6889, eval loss 0.7070846557617188\n",
      "optimal threshold: -0.7743\n",
      "Epoch 89 train loss: 0.6946, eval loss 0.7060948610305786\n",
      "optimal threshold: -0.7728\n",
      "Epoch 90 train loss: 0.6819, eval loss 0.7051477432250977\n",
      "optimal threshold: -0.7728\n",
      "Epoch 91 train loss: 0.6873, eval loss 0.7042468190193176\n",
      "optimal threshold: -0.7410\n",
      "Epoch 92 train loss: 0.7059, eval loss 0.7033793926239014\n",
      "optimal threshold: -0.7374\n",
      "Epoch 93 train loss: 0.7039, eval loss 0.7025041580200195\n",
      "optimal threshold: -0.7257\n",
      "Epoch 94 train loss: 0.6921, eval loss 0.7016850709915161\n",
      "optimal threshold: -0.7229\n",
      "Epoch 95 train loss: 0.6996, eval loss 0.7008643746376038\n",
      "optimal threshold: -0.7259\n",
      "Epoch 96 train loss: 0.7006, eval loss 0.7000871300697327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7208\n",
      "Epoch 97 train loss: 0.6912, eval loss 0.6993467807769775\n",
      "optimal threshold: -0.7086\n",
      "Epoch 98 train loss: 0.6877, eval loss 0.6985940337181091\n",
      "optimal threshold: -0.7034\n",
      "Epoch 99 train loss: 0.6988, eval loss 0.6978541016578674\n",
      "optimal threshold: -0.7004\n",
      "Epoch 100 train loss: 0.6863, eval loss 0.6971719264984131\n",
      "optimal threshold: -0.6960\n",
      "Epoch 101 train loss: 0.6774, eval loss 0.6964813470840454\n",
      "optimal threshold: -0.6972\n",
      "Epoch 102 train loss: 0.6786, eval loss 0.695848286151886\n",
      "optimal threshold: -0.7836\n",
      "Epoch 103 train loss: 0.6737, eval loss 0.6952265501022339\n",
      "optimal threshold: -0.8012\n",
      "Epoch 104 train loss: 0.6682, eval loss 0.6945928931236267\n",
      "optimal threshold: -0.7709\n",
      "Epoch 105 train loss: 0.6787, eval loss 0.6939719915390015\n",
      "optimal threshold: -0.7777\n",
      "Epoch 106 train loss: 0.6696, eval loss 0.693382978439331\n",
      "optimal threshold: -0.7743\n",
      "Epoch 107 train loss: 0.6644, eval loss 0.6928004026412964\n",
      "optimal threshold: -0.7703\n",
      "Epoch 108 train loss: 0.6494, eval loss 0.6922280788421631\n",
      "optimal threshold: -0.7674\n",
      "Epoch 109 train loss: 0.6693, eval loss 0.6916823387145996\n",
      "optimal threshold: -0.7648\n",
      "Epoch 110 train loss: 0.6607, eval loss 0.6911174654960632\n",
      "optimal threshold: -0.8746\n",
      "Epoch 111 train loss: 0.6613, eval loss 0.6905709505081177\n",
      "optimal threshold: -0.7763\n",
      "Epoch 112 train loss: 0.6800, eval loss 0.6900545954704285\n",
      "optimal threshold: -0.7701\n",
      "Epoch 113 train loss: 0.6658, eval loss 0.6895361542701721\n",
      "optimal threshold: -0.9422\n",
      "Epoch 114 train loss: 0.6598, eval loss 0.6890512108802795\n",
      "optimal threshold: -0.9561\n",
      "Epoch 115 train loss: 0.6600, eval loss 0.6886202096939087\n",
      "optimal threshold: -0.9551\n",
      "Epoch 116 train loss: 0.6783, eval loss 0.6881054043769836\n",
      "optimal threshold: -0.9631\n",
      "Epoch 117 train loss: 0.6733, eval loss 0.6876280903816223\n",
      "optimal threshold: -0.9578\n",
      "Epoch 118 train loss: 0.6884, eval loss 0.6871541738510132\n",
      "optimal threshold: -0.9334\n",
      "Epoch 119 train loss: 0.6464, eval loss 0.686696469783783\n",
      "optimal threshold: -0.9306\n",
      "Epoch 120 train loss: 0.6486, eval loss 0.686259925365448\n",
      "optimal threshold: -0.9327\n",
      "Epoch 121 train loss: 0.6687, eval loss 0.6858170628547668\n",
      "optimal threshold: -0.9237\n",
      "Epoch 122 train loss: 0.6469, eval loss 0.6853814721107483\n",
      "optimal threshold: -0.9893\n",
      "Epoch 123 train loss: 0.6633, eval loss 0.6849927306175232\n",
      "optimal threshold: -0.9917\n",
      "Epoch 124 train loss: 0.6563, eval loss 0.6845932006835938\n",
      "optimal threshold: -0.9976\n",
      "Epoch 125 train loss: 0.6579, eval loss 0.6841654181480408\n",
      "optimal threshold: -0.7580\n",
      "Epoch 126 train loss: 0.6393, eval loss 0.6837977170944214\n",
      "optimal threshold: -0.7622\n",
      "Epoch 127 train loss: 0.6510, eval loss 0.6834123730659485\n",
      "optimal threshold: -0.7549\n",
      "Epoch 128 train loss: 0.6574, eval loss 0.6830273866653442\n",
      "optimal threshold: -0.7580\n",
      "Epoch 129 train loss: 0.6557, eval loss 0.6826353669166565\n",
      "optimal threshold: -0.7555\n",
      "Epoch 130 train loss: 0.6609, eval loss 0.6822690367698669\n",
      "optimal threshold: -0.7529\n",
      "Epoch 131 train loss: 0.6509, eval loss 0.6819210648536682\n",
      "optimal threshold: -0.7530\n",
      "Epoch 132 train loss: 0.6432, eval loss 0.6815991401672363\n",
      "optimal threshold: -0.7470\n",
      "Epoch 133 train loss: 0.6620, eval loss 0.681179940700531\n",
      "optimal threshold: -0.7473\n",
      "Epoch 134 train loss: 0.6634, eval loss 0.6808900237083435\n",
      "optimal threshold: -0.7475\n",
      "Epoch 135 train loss: 0.6393, eval loss 0.6805595755577087\n",
      "optimal threshold: -0.7480\n",
      "Epoch 136 train loss: 0.6594, eval loss 0.6802269816398621\n",
      "optimal threshold: -0.7508\n",
      "Epoch 137 train loss: 0.6585, eval loss 0.6799201369285583\n",
      "optimal threshold: -0.7461\n",
      "Epoch 138 train loss: 0.6616, eval loss 0.6796185970306396\n",
      "optimal threshold: -0.7483\n",
      "Epoch 139 train loss: 0.6606, eval loss 0.6793115139007568\n",
      "optimal threshold: -0.7453\n",
      "Epoch 140 train loss: 0.6443, eval loss 0.6789790987968445\n",
      "optimal threshold: -0.7467\n",
      "Epoch 141 train loss: 0.6443, eval loss 0.6786932945251465\n",
      "optimal threshold: -0.6815\n",
      "Epoch 142 train loss: 0.6651, eval loss 0.6783763766288757\n",
      "optimal threshold: -0.6830\n",
      "Epoch 143 train loss: 0.6535, eval loss 0.6780787110328674\n",
      "optimal threshold: -0.6766\n",
      "Epoch 144 train loss: 0.6489, eval loss 0.6777842044830322\n",
      "optimal threshold: -0.6883\n",
      "Epoch 145 train loss: 0.6527, eval loss 0.6775258183479309\n",
      "optimal threshold: -0.6593\n",
      "Epoch 146 train loss: 0.6584, eval loss 0.6772418022155762\n",
      "optimal threshold: -0.6557\n",
      "Epoch 147 train loss: 0.6387, eval loss 0.6769206523895264\n",
      "optimal threshold: -0.6578\n",
      "Epoch 148 train loss: 0.6503, eval loss 0.6766948103904724\n",
      "optimal threshold: -0.6671\n",
      "Epoch 149 train loss: 0.6702, eval loss 0.6764370799064636\n",
      "optimal threshold: -0.6722\n",
      "Epoch 150 train loss: 0.6502, eval loss 0.6761792302131653\n",
      "optimal threshold: -0.6696\n",
      "Epoch 151 train loss: 0.6295, eval loss 0.6759411692619324\n",
      "optimal threshold: -0.6654\n",
      "Epoch 152 train loss: 0.6444, eval loss 0.675677478313446\n",
      "optimal threshold: -0.6656\n",
      "Epoch 153 train loss: 0.6615, eval loss 0.6754524111747742\n",
      "optimal threshold: -0.6652\n",
      "Epoch 154 train loss: 0.6433, eval loss 0.6752146482467651\n",
      "optimal threshold: -0.6650\n",
      "Epoch 155 train loss: 0.6337, eval loss 0.6749861240386963\n",
      "optimal threshold: -0.6653\n",
      "Epoch 156 train loss: 0.6549, eval loss 0.6747745275497437\n",
      "optimal threshold: -0.6667\n",
      "Epoch 157 train loss: 0.6185, eval loss 0.6745603680610657\n",
      "optimal threshold: -0.6698\n",
      "Epoch 158 train loss: 0.6696, eval loss 0.6743512749671936\n",
      "optimal threshold: -0.6746\n",
      "Epoch 159 train loss: 0.6383, eval loss 0.6741291284561157\n",
      "optimal threshold: -0.6747\n",
      "Epoch 160 train loss: 0.6368, eval loss 0.6738865375518799\n",
      "optimal threshold: -0.6810\n",
      "Epoch 161 train loss: 0.6161, eval loss 0.6736656427383423\n",
      "optimal threshold: -0.6780\n",
      "Epoch 162 train loss: 0.6500, eval loss 0.6734251379966736\n",
      "optimal threshold: -0.6748\n",
      "Epoch 163 train loss: 0.6403, eval loss 0.6732062101364136\n",
      "optimal threshold: -0.6784\n",
      "Epoch 164 train loss: 0.6310, eval loss 0.6730136275291443\n",
      "optimal threshold: -0.6832\n",
      "Epoch 165 train loss: 0.6488, eval loss 0.6728325486183167\n",
      "optimal threshold: -0.6699\n",
      "Epoch 166 train loss: 0.6277, eval loss 0.6725993156433105\n",
      "optimal threshold: -0.6708\n",
      "Epoch 167 train loss: 0.6447, eval loss 0.6724444627761841\n",
      "optimal threshold: -0.6989\n",
      "Epoch 168 train loss: 0.6587, eval loss 0.6722362637519836\n",
      "optimal threshold: -0.7372\n",
      "Epoch 169 train loss: 0.6466, eval loss 0.6720988750457764\n",
      "optimal threshold: -0.7401\n",
      "Epoch 170 train loss: 0.6159, eval loss 0.6718989014625549\n",
      "optimal threshold: -0.7386\n",
      "Epoch 171 train loss: 0.6219, eval loss 0.6717388033866882\n",
      "optimal threshold: -0.7155\n",
      "Epoch 172 train loss: 0.6221, eval loss 0.671547532081604\n",
      "optimal threshold: -0.7334\n",
      "Epoch 173 train loss: 0.6460, eval loss 0.6713741421699524\n",
      "optimal threshold: -0.7327\n",
      "Epoch 174 train loss: 0.6397, eval loss 0.6711932420730591\n",
      "optimal threshold: -0.7306\n",
      "Epoch 175 train loss: 0.6430, eval loss 0.6710126996040344\n",
      "optimal threshold: -0.7299\n",
      "Epoch 176 train loss: 0.6393, eval loss 0.6708201169967651\n",
      "optimal threshold: -0.7314\n",
      "Epoch 177 train loss: 0.6565, eval loss 0.6706726551055908\n",
      "optimal threshold: -0.7296\n",
      "Epoch 178 train loss: 0.6406, eval loss 0.6705073118209839\n",
      "optimal threshold: -0.7362\n",
      "Epoch 179 train loss: 0.6173, eval loss 0.6703400015830994\n",
      "optimal threshold: -0.7362\n",
      "Epoch 180 train loss: 0.6250, eval loss 0.6701747179031372\n",
      "optimal threshold: -0.7365\n",
      "Epoch 181 train loss: 0.6317, eval loss 0.6700164675712585\n",
      "optimal threshold: -0.7243\n",
      "Epoch 182 train loss: 0.6283, eval loss 0.6698551177978516\n",
      "optimal threshold: -0.7232\n",
      "Epoch 183 train loss: 0.6468, eval loss 0.66971355676651\n",
      "optimal threshold: -0.7245\n",
      "Epoch 184 train loss: 0.6196, eval loss 0.6695542335510254\n",
      "optimal threshold: -0.7295\n",
      "Epoch 185 train loss: 0.6474, eval loss 0.6694681644439697\n",
      "optimal threshold: -0.7777\n",
      "Epoch 186 train loss: 0.6498, eval loss 0.6693185567855835\n",
      "optimal threshold: -0.7731\n",
      "Epoch 187 train loss: 0.6414, eval loss 0.6691823601722717\n",
      "optimal threshold: -0.7768\n",
      "Epoch 188 train loss: 0.6424, eval loss 0.669033408164978\n",
      "optimal threshold: -0.7718\n",
      "Epoch 189 train loss: 0.6226, eval loss 0.668915867805481\n",
      "optimal threshold: -0.7755\n",
      "Epoch 190 train loss: 0.6245, eval loss 0.668755829334259\n",
      "optimal threshold: -0.7757\n",
      "Epoch 191 train loss: 0.6273, eval loss 0.6686218976974487\n",
      "optimal threshold: -0.7762\n",
      "Epoch 192 train loss: 0.6259, eval loss 0.6685113906860352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7783\n",
      "Epoch 193 train loss: 0.6381, eval loss 0.6683839559555054\n",
      "optimal threshold: -0.7821\n",
      "Epoch 194 train loss: 0.6385, eval loss 0.6682500839233398\n",
      "optimal threshold: -0.7833\n",
      "Epoch 195 train loss: 0.6224, eval loss 0.6681005954742432\n",
      "optimal threshold: -0.7880\n",
      "Epoch 196 train loss: 0.6229, eval loss 0.6680009961128235\n",
      "optimal threshold: -0.7878\n",
      "Epoch 197 train loss: 0.6241, eval loss 0.6678532958030701\n",
      "optimal threshold: -0.7051\n",
      "Epoch 198 train loss: 0.6207, eval loss 0.6677554249763489\n",
      "optimal threshold: -0.7094\n",
      "Epoch 199 train loss: 0.6039, eval loss 0.6676397323608398\n",
      "optimal threshold: -0.7078\n",
      "Epoch 200 train loss: 0.6306, eval loss 0.6674954891204834\n",
      "optimal threshold: -0.7072\n",
      "Epoch 201 train loss: 0.6210, eval loss 0.667418897151947\n",
      "optimal threshold: -0.7042\n",
      "Epoch 202 train loss: 0.6083, eval loss 0.6672953367233276\n",
      "optimal threshold: -0.6999\n",
      "Epoch 203 train loss: 0.6026, eval loss 0.6671432256698608\n",
      "optimal threshold: -0.7057\n",
      "Epoch 204 train loss: 0.6098, eval loss 0.6670566201210022\n",
      "optimal threshold: -0.7036\n",
      "Epoch 205 train loss: 0.6297, eval loss 0.6669539213180542\n",
      "optimal threshold: -0.7035\n",
      "Epoch 206 train loss: 0.6314, eval loss 0.6668160557746887\n",
      "optimal threshold: -0.7058\n",
      "Epoch 207 train loss: 0.6184, eval loss 0.6667340397834778\n",
      "optimal threshold: -0.7035\n",
      "Epoch 208 train loss: 0.6405, eval loss 0.6665830612182617\n",
      "optimal threshold: -0.7043\n",
      "Epoch 209 train loss: 0.6384, eval loss 0.6664594411849976\n",
      "optimal threshold: -0.6960\n",
      "Epoch 210 train loss: 0.6167, eval loss 0.6663810610771179\n",
      "optimal threshold: -0.7055\n",
      "Epoch 211 train loss: 0.6409, eval loss 0.666273295879364\n",
      "optimal threshold: -0.7055\n",
      "Epoch 212 train loss: 0.6147, eval loss 0.6661770343780518\n",
      "optimal threshold: -0.7040\n",
      "Epoch 213 train loss: 0.6295, eval loss 0.6660569906234741\n",
      "optimal threshold: -0.6898\n",
      "Epoch 214 train loss: 0.6173, eval loss 0.6659412384033203\n",
      "optimal threshold: -0.7049\n",
      "Epoch 215 train loss: 0.6058, eval loss 0.6658852696418762\n",
      "optimal threshold: -0.7090\n",
      "Epoch 216 train loss: 0.6347, eval loss 0.6657947301864624\n",
      "optimal threshold: -0.7997\n",
      "Epoch 217 train loss: 0.6171, eval loss 0.665683925151825\n",
      "optimal threshold: -0.8007\n",
      "Epoch 218 train loss: 0.6492, eval loss 0.6655799746513367\n",
      "optimal threshold: -0.7364\n",
      "Epoch 219 train loss: 0.6104, eval loss 0.6655116677284241\n",
      "optimal threshold: -0.7373\n",
      "Epoch 220 train loss: 0.6181, eval loss 0.6654404401779175\n",
      "optimal threshold: -0.8047\n",
      "Epoch 221 train loss: 0.6063, eval loss 0.6653144359588623\n",
      "optimal threshold: -0.7338\n",
      "Epoch 222 train loss: 0.6354, eval loss 0.6652061343193054\n",
      "optimal threshold: -0.8173\n",
      "Epoch 223 train loss: 0.6347, eval loss 0.6651325225830078\n",
      "optimal threshold: -0.8182\n",
      "Epoch 224 train loss: 0.6422, eval loss 0.6650663018226624\n",
      "optimal threshold: -0.6790\n",
      "Epoch 225 train loss: 0.6167, eval loss 0.6649424433708191\n",
      "optimal threshold: -0.8158\n",
      "Epoch 226 train loss: 0.6283, eval loss 0.6649011969566345\n",
      "optimal threshold: -0.6794\n",
      "Epoch 227 train loss: 0.6340, eval loss 0.6647859811782837\n",
      "optimal threshold: -0.6765\n",
      "Epoch 228 train loss: 0.6227, eval loss 0.664682149887085\n",
      "optimal threshold: -0.6771\n",
      "Epoch 229 train loss: 0.6103, eval loss 0.664594292640686\n",
      "optimal threshold: -0.8121\n",
      "Epoch 230 train loss: 0.6050, eval loss 0.6645305156707764\n",
      "optimal threshold: -0.8113\n",
      "Epoch 231 train loss: 0.6058, eval loss 0.664423942565918\n",
      "optimal threshold: -0.6781\n",
      "Epoch 232 train loss: 0.6399, eval loss 0.6643548011779785\n",
      "optimal threshold: -0.8074\n",
      "Epoch 233 train loss: 0.6122, eval loss 0.6642714738845825\n",
      "optimal threshold: -0.6787\n",
      "Epoch 234 train loss: 0.6153, eval loss 0.6642568111419678\n",
      "optimal threshold: -0.6784\n",
      "Epoch 235 train loss: 0.6303, eval loss 0.6641841530799866\n",
      "optimal threshold: -0.6757\n",
      "Epoch 236 train loss: 0.6344, eval loss 0.6640658378601074\n",
      "optimal threshold: -0.8031\n",
      "Epoch 237 train loss: 0.6315, eval loss 0.66395103931427\n",
      "optimal threshold: -0.8057\n",
      "Epoch 238 train loss: 0.6250, eval loss 0.6639519929885864\n",
      "optimal threshold: -0.7402\n",
      "Epoch 239 train loss: 0.6162, eval loss 0.6638422012329102\n",
      "optimal threshold: -0.8005\n",
      "Epoch 240 train loss: 0.6212, eval loss 0.6637473106384277\n",
      "optimal threshold: -0.8016\n",
      "Epoch 241 train loss: 0.6148, eval loss 0.6637088656425476\n",
      "optimal threshold: -0.8438\n",
      "Epoch 242 train loss: 0.6127, eval loss 0.6637105345726013\n",
      "optimal threshold: -0.8411\n",
      "Epoch 243 train loss: 0.6083, eval loss 0.6636361479759216\n",
      "optimal threshold: -0.8397\n",
      "Epoch 244 train loss: 0.6351, eval loss 0.6635481715202332\n",
      "optimal threshold: -0.8366\n",
      "Epoch 245 train loss: 0.6250, eval loss 0.6634509563446045\n",
      "optimal threshold: -0.8391\n",
      "Epoch 246 train loss: 0.6159, eval loss 0.6633715629577637\n",
      "optimal threshold: -0.8382\n",
      "Epoch 247 train loss: 0.6400, eval loss 0.6632717251777649\n",
      "optimal threshold: -0.8317\n",
      "Epoch 248 train loss: 0.6374, eval loss 0.663212776184082\n",
      "optimal threshold: -0.8375\n",
      "Epoch 249 train loss: 0.6261, eval loss 0.6631679534912109\n",
      "optimal threshold: -0.8377\n",
      "Epoch 250 train loss: 0.6537, eval loss 0.6631175875663757\n",
      "optimal threshold: -0.8440\n",
      "Epoch 251 train loss: 0.6421, eval loss 0.6630904674530029\n",
      "optimal threshold: -0.8391\n",
      "Epoch 252 train loss: 0.6001, eval loss 0.6629931330680847\n",
      "optimal threshold: -0.8390\n",
      "Epoch 253 train loss: 0.6429, eval loss 0.6629380583763123\n",
      "optimal threshold: -0.8452\n",
      "Epoch 254 train loss: 0.6220, eval loss 0.6629216074943542\n",
      "optimal threshold: -0.8470\n",
      "Epoch 255 train loss: 0.6036, eval loss 0.6628907918930054\n",
      "optimal threshold: -0.8326\n",
      "Epoch 256 train loss: 0.6252, eval loss 0.6628407835960388\n",
      "optimal threshold: -0.8275\n",
      "Epoch 257 train loss: 0.6152, eval loss 0.662725031375885\n",
      "optimal threshold: -0.8247\n",
      "Epoch 258 train loss: 0.6245, eval loss 0.6626720428466797\n",
      "optimal threshold: -0.8305\n",
      "Epoch 259 train loss: 0.6285, eval loss 0.6626530885696411\n",
      "optimal threshold: -0.8294\n",
      "Epoch 260 train loss: 0.6083, eval loss 0.662599503993988\n",
      "optimal threshold: -0.8186\n",
      "Epoch 261 train loss: 0.6185, eval loss 0.6625478863716125\n",
      "optimal threshold: -0.8118\n",
      "Epoch 262 train loss: 0.6322, eval loss 0.6624610424041748\n",
      "optimal threshold: -0.8100\n",
      "Epoch 263 train loss: 0.6346, eval loss 0.6624054312705994\n",
      "optimal threshold: -0.8405\n",
      "Epoch 264 train loss: 0.6254, eval loss 0.6623337268829346\n",
      "optimal threshold: -0.8102\n",
      "Epoch 265 train loss: 0.6199, eval loss 0.6622669100761414\n",
      "optimal threshold: -0.8497\n",
      "Epoch 266 train loss: 0.6223, eval loss 0.6622636318206787\n",
      "optimal threshold: -0.7675\n",
      "Epoch 267 train loss: 0.6146, eval loss 0.6622433066368103\n",
      "optimal threshold: -0.7627\n",
      "Epoch 268 train loss: 0.6082, eval loss 0.6621671319007874\n",
      "optimal threshold: -0.7643\n",
      "Epoch 269 train loss: 0.6158, eval loss 0.6620996594429016\n",
      "optimal threshold: -0.7630\n",
      "Epoch 270 train loss: 0.6144, eval loss 0.6620365977287292\n",
      "optimal threshold: -0.7641\n",
      "Epoch 271 train loss: 0.6253, eval loss 0.6619911193847656\n",
      "optimal threshold: -0.7590\n",
      "Epoch 272 train loss: 0.6399, eval loss 0.6618751287460327\n",
      "optimal threshold: -0.7581\n",
      "Epoch 273 train loss: 0.6248, eval loss 0.661827564239502\n",
      "optimal threshold: -0.8143\n",
      "Epoch 274 train loss: 0.6063, eval loss 0.6618425250053406\n",
      "optimal threshold: -0.8158\n",
      "Epoch 275 train loss: 0.5858, eval loss 0.6618218421936035\n",
      "optimal threshold: -0.8143\n",
      "Epoch 276 train loss: 0.6232, eval loss 0.6617584228515625\n",
      "optimal threshold: -0.7684\n",
      "Epoch 277 train loss: 0.6043, eval loss 0.6617035865783691\n",
      "optimal threshold: -0.7661\n",
      "Epoch 278 train loss: 0.6049, eval loss 0.6616449356079102\n",
      "optimal threshold: -0.7684\n",
      "Epoch 279 train loss: 0.6299, eval loss 0.6616131663322449\n",
      "optimal threshold: -0.7635\n",
      "Epoch 280 train loss: 0.6137, eval loss 0.6616214513778687\n",
      "optimal threshold: -0.7636\n",
      "Epoch 281 train loss: 0.6463, eval loss 0.6615703105926514\n",
      "optimal threshold: -0.7624\n",
      "Epoch 282 train loss: 0.6049, eval loss 0.6615339517593384\n",
      "optimal threshold: -0.7617\n",
      "Epoch 283 train loss: 0.6175, eval loss 0.6614930033683777\n",
      "optimal threshold: -0.7611\n",
      "Epoch 284 train loss: 0.6332, eval loss 0.6614557504653931\n",
      "optimal threshold: -0.7606\n",
      "Epoch 285 train loss: 0.6084, eval loss 0.661406934261322\n",
      "optimal threshold: -0.7594\n",
      "Epoch 286 train loss: 0.6294, eval loss 0.6613520383834839\n",
      "optimal threshold: -0.7611\n",
      "Epoch 287 train loss: 0.6065, eval loss 0.6613619923591614\n",
      "optimal threshold: -0.7581\n",
      "Epoch 288 train loss: 0.6085, eval loss 0.6612944006919861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7563\n",
      "Epoch 289 train loss: 0.6229, eval loss 0.6612169146537781\n",
      "optimal threshold: -0.7540\n",
      "Epoch 290 train loss: 0.6234, eval loss 0.661151647567749\n",
      "optimal threshold: -0.7535\n",
      "Epoch 291 train loss: 0.5809, eval loss 0.661103367805481\n",
      "optimal threshold: -0.7517\n",
      "Epoch 292 train loss: 0.5969, eval loss 0.6610389947891235\n",
      "optimal threshold: -0.7505\n",
      "Epoch 293 train loss: 0.6292, eval loss 0.6609859466552734\n",
      "optimal threshold: -0.7504\n",
      "Epoch 294 train loss: 0.6079, eval loss 0.6609523892402649\n",
      "optimal threshold: -0.7539\n",
      "Epoch 295 train loss: 0.6086, eval loss 0.660956621170044\n",
      "optimal threshold: -0.5030\n",
      "Epoch 296 train loss: 0.5966, eval loss 0.6609418392181396\n",
      "optimal threshold: -0.7790\n",
      "Epoch 297 train loss: 0.6236, eval loss 0.6609231233596802\n",
      "optimal threshold: -0.7785\n",
      "Epoch 298 train loss: 0.6233, eval loss 0.6608924269676208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 12:00:02,275] Trial 5 finished with value: 0.6138169765472412 and parameters: {'learning_rate_exp': -4.957583736831479, 'dropout_p': 0.15801842088511503, 'l2_reg_exp': -3.5760169713966645, 'batch_size': 491, 'N': 146}. Best is trial 4 with value: 0.49366649985313416.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4988\n",
      "Epoch 299 train loss: 0.6138, eval loss 0.6608469486236572\n",
      "optimal threshold: -0.8712\n",
      "Epoch 0 train loss: 0.2599, eval loss 0.6749852299690247\n",
      "optimal threshold: -0.7206\n",
      "Epoch 1 train loss: 0.7529, eval loss 0.6803928017616272\n",
      "optimal threshold: -0.7871\n",
      "Epoch 2 train loss: 0.4005, eval loss 0.6917563676834106\n",
      "optimal threshold: -0.6756\n",
      "Epoch 3 train loss: 0.3725, eval loss 0.6948298215866089\n",
      "optimal threshold: -0.4270\n",
      "Epoch 4 train loss: 0.3283, eval loss 0.6862303018569946\n",
      "optimal threshold: -0.6104\n",
      "Epoch 5 train loss: 0.3965, eval loss 0.7040987610816956\n",
      "optimal threshold: -0.4521\n",
      "Epoch 6 train loss: 0.3941, eval loss 0.7177454829216003\n",
      "optimal threshold: -0.4853\n",
      "Epoch 7 train loss: 0.2400, eval loss 0.7009866237640381\n",
      "optimal threshold: -0.5295\n",
      "Epoch 8 train loss: 0.2368, eval loss 0.6904971599578857\n",
      "optimal threshold: -0.5090\n",
      "Epoch 9 train loss: 0.5598, eval loss 0.7126233577728271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 12:00:43,650] Trial 6 finished with value: 0.22441545128822327 and parameters: {'learning_rate_exp': -2.183218474058122, 'dropout_p': 0.3922070242036134, 'l2_reg_exp': -5.172311747879126, 'batch_size': 22, 'N': 231}. Best is trial 6 with value: 0.22441545128822327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5615\n",
      "optimal threshold: -0.7818\n",
      "Epoch 0 train loss: 1.0636, eval loss 1.073926329612732\n",
      "optimal threshold: -0.8942\n",
      "Epoch 1 train loss: 0.8492, eval loss 0.8291903138160706\n",
      "optimal threshold: -0.7063\n",
      "Epoch 2 train loss: 0.7796, eval loss 0.7334268093109131\n",
      "optimal threshold: -0.9671\n",
      "Epoch 3 train loss: 0.7309, eval loss 0.7097318768501282\n",
      "optimal threshold: -0.9196\n",
      "Epoch 4 train loss: 0.6691, eval loss 0.6971383690834045\n",
      "optimal threshold: -0.7867\n",
      "Epoch 5 train loss: 0.6730, eval loss 0.6879879832267761\n",
      "optimal threshold: -0.9420\n",
      "Epoch 6 train loss: 0.7133, eval loss 0.6816247701644897\n",
      "optimal threshold: -0.7151\n",
      "Epoch 7 train loss: 0.6892, eval loss 0.6763940453529358\n",
      "optimal threshold: -0.6457\n",
      "Epoch 8 train loss: 0.6435, eval loss 0.6728527545928955\n",
      "optimal threshold: -0.4840\n",
      "Epoch 9 train loss: 0.6842, eval loss 0.670073926448822\n",
      "optimal threshold: -0.6165\n",
      "Epoch 10 train loss: 0.6207, eval loss 0.6679393649101257\n",
      "optimal threshold: -0.4632\n",
      "Epoch 11 train loss: 0.6742, eval loss 0.6663253903388977\n",
      "optimal threshold: -0.6364\n",
      "Epoch 12 train loss: 0.6202, eval loss 0.6646913886070251\n",
      "optimal threshold: -0.6223\n",
      "Epoch 13 train loss: 0.6355, eval loss 0.663620114326477\n",
      "optimal threshold: -0.6128\n",
      "Epoch 14 train loss: 0.6470, eval loss 0.6628852486610413\n",
      "optimal threshold: -0.6155\n",
      "Epoch 15 train loss: 0.6565, eval loss 0.6621889472007751\n",
      "optimal threshold: -0.6102\n",
      "Epoch 16 train loss: 0.6601, eval loss 0.6613409519195557\n",
      "optimal threshold: -0.6301\n",
      "Epoch 17 train loss: 0.6513, eval loss 0.6606183052062988\n",
      "optimal threshold: -0.6481\n",
      "Epoch 18 train loss: 0.6829, eval loss 0.6603412628173828\n",
      "optimal threshold: -0.6471\n",
      "Epoch 19 train loss: 0.6165, eval loss 0.6596980094909668\n",
      "optimal threshold: -0.6258\n",
      "Epoch 20 train loss: 0.6420, eval loss 0.6591019630432129\n",
      "optimal threshold: -0.6922\n",
      "Epoch 21 train loss: 0.6491, eval loss 0.6589348316192627\n",
      "optimal threshold: -0.6784\n",
      "Epoch 22 train loss: 0.6455, eval loss 0.6587848663330078\n",
      "optimal threshold: -0.5441\n",
      "Epoch 23 train loss: 0.6725, eval loss 0.658229410648346\n",
      "optimal threshold: -0.6905\n",
      "Epoch 24 train loss: 0.6334, eval loss 0.6576943397521973\n",
      "optimal threshold: -0.7088\n",
      "Epoch 25 train loss: 0.6388, eval loss 0.6574857831001282\n",
      "optimal threshold: -0.7522\n",
      "Epoch 26 train loss: 0.6472, eval loss 0.6573136448860168\n",
      "optimal threshold: -0.6797\n",
      "Epoch 27 train loss: 0.6533, eval loss 0.6572807431221008\n",
      "optimal threshold: -0.7076\n",
      "Epoch 28 train loss: 0.6229, eval loss 0.6567350625991821\n",
      "optimal threshold: -0.5693\n",
      "Epoch 29 train loss: 0.6653, eval loss 0.6566241979598999\n",
      "optimal threshold: -0.6747\n",
      "Epoch 30 train loss: 0.6360, eval loss 0.6567567586898804\n",
      "optimal threshold: -0.6730\n",
      "Epoch 31 train loss: 0.6231, eval loss 0.6563156247138977\n",
      "optimal threshold: -0.6254\n",
      "Epoch 32 train loss: 0.6287, eval loss 0.6562700271606445\n",
      "optimal threshold: -0.5997\n",
      "Epoch 33 train loss: 0.6041, eval loss 0.6561716794967651\n",
      "optimal threshold: -0.6077\n",
      "Epoch 34 train loss: 0.6697, eval loss 0.6560866236686707\n",
      "optimal threshold: -0.5963\n",
      "Epoch 35 train loss: 0.6525, eval loss 0.6561054587364197\n",
      "optimal threshold: -0.5966\n",
      "Epoch 36 train loss: 0.6508, eval loss 0.6557946801185608\n",
      "optimal threshold: -0.6127\n",
      "Epoch 37 train loss: 0.6319, eval loss 0.6561294198036194\n",
      "optimal threshold: -0.6118\n",
      "Epoch 38 train loss: 0.6261, eval loss 0.6559153199195862\n",
      "optimal threshold: -0.6016\n",
      "Epoch 39 train loss: 0.6209, eval loss 0.6559413075447083\n",
      "optimal threshold: -0.6004\n",
      "Epoch 40 train loss: 0.6270, eval loss 0.6562036871910095\n",
      "optimal threshold: -0.4478\n",
      "Epoch 41 train loss: 0.6359, eval loss 0.6561072468757629\n",
      "optimal threshold: -0.6147\n",
      "Epoch 42 train loss: 0.6503, eval loss 0.656119167804718\n",
      "optimal threshold: -0.6218\n",
      "Epoch 43 train loss: 0.6084, eval loss 0.6561181545257568\n",
      "optimal threshold: -0.4420\n",
      "Epoch 44 train loss: 0.6163, eval loss 0.6563472151756287\n",
      "optimal threshold: -0.4473\n",
      "Epoch 45 train loss: 0.6102, eval loss 0.6561111807823181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 12:01:21,344] Trial 7 finished with value: 0.5717821717262268 and parameters: {'learning_rate_exp': -4.142543232222796, 'dropout_p': 0.4918919409993212, 'l2_reg_exp': -6.092427259270391, 'batch_size': 249, 'N': 490}. Best is trial 6 with value: 0.22441545128822327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4572\n",
      "optimal threshold: -0.0878\n",
      "Epoch 0 train loss: 1.3936, eval loss 1.4033992290496826\n",
      "optimal threshold: -0.0991\n",
      "Epoch 1 train loss: 1.3861, eval loss 1.3959910869598389\n",
      "optimal threshold: -0.1081\n",
      "Epoch 2 train loss: 1.3767, eval loss 1.388567328453064\n",
      "optimal threshold: -0.1062\n",
      "Epoch 3 train loss: 1.3674, eval loss 1.3811172246932983\n",
      "optimal threshold: -0.1149\n",
      "Epoch 4 train loss: 1.3554, eval loss 1.3735630512237549\n",
      "optimal threshold: -0.1308\n",
      "Epoch 5 train loss: 1.3529, eval loss 1.3658303022384644\n",
      "optimal threshold: -0.1402\n",
      "Epoch 6 train loss: 1.3410, eval loss 1.357945203781128\n",
      "optimal threshold: -0.1542\n",
      "Epoch 7 train loss: 1.3317, eval loss 1.3498576879501343\n",
      "optimal threshold: -0.1675\n",
      "Epoch 8 train loss: 1.2939, eval loss 1.3415919542312622\n",
      "optimal threshold: -0.1805\n",
      "Epoch 9 train loss: 1.3131, eval loss 1.3331133127212524\n",
      "optimal threshold: -0.1941\n",
      "Epoch 10 train loss: 1.3145, eval loss 1.324444055557251\n",
      "optimal threshold: -0.2120\n",
      "Epoch 11 train loss: 1.2786, eval loss 1.3155423402786255\n",
      "optimal threshold: -0.2274\n",
      "Epoch 12 train loss: 1.2842, eval loss 1.3064309358596802\n",
      "optimal threshold: -0.2451\n",
      "Epoch 13 train loss: 1.2842, eval loss 1.2971481084823608\n",
      "optimal threshold: -0.2574\n",
      "Epoch 14 train loss: 1.2572, eval loss 1.2876982688903809\n",
      "optimal threshold: -0.2723\n",
      "Epoch 15 train loss: 1.2409, eval loss 1.2781342267990112\n",
      "optimal threshold: -0.2869\n",
      "Epoch 16 train loss: 1.2679, eval loss 1.268389344215393\n",
      "optimal threshold: -0.3020\n",
      "Epoch 17 train loss: 1.2485, eval loss 1.2585341930389404\n",
      "optimal threshold: -0.3219\n",
      "Epoch 18 train loss: 1.2508, eval loss 1.248569369316101\n",
      "optimal threshold: -0.3399\n",
      "Epoch 19 train loss: 1.2191, eval loss 1.238555908203125\n",
      "optimal threshold: -0.3570\n",
      "Epoch 20 train loss: 1.1998, eval loss 1.2285469770431519\n",
      "optimal threshold: -0.3761\n",
      "Epoch 21 train loss: 1.2014, eval loss 1.2184737920761108\n",
      "optimal threshold: -0.3853\n",
      "Epoch 22 train loss: 1.1674, eval loss 1.2084239721298218\n",
      "optimal threshold: -0.4174\n",
      "Epoch 23 train loss: 1.1653, eval loss 1.1984102725982666\n",
      "optimal threshold: -0.4271\n",
      "Epoch 24 train loss: 1.1924, eval loss 1.1884280443191528\n",
      "optimal threshold: -0.4522\n",
      "Epoch 25 train loss: 1.1278, eval loss 1.178519368171692\n",
      "optimal threshold: -0.4720\n",
      "Epoch 26 train loss: 1.1299, eval loss 1.1686816215515137\n",
      "optimal threshold: -0.4840\n",
      "Epoch 27 train loss: 1.0795, eval loss 1.1589146852493286\n",
      "optimal threshold: -0.5093\n",
      "Epoch 28 train loss: 1.0930, eval loss 1.149347186088562\n",
      "optimal threshold: -0.5287\n",
      "Epoch 29 train loss: 1.1178, eval loss 1.1398671865463257\n",
      "optimal threshold: -0.5384\n",
      "Epoch 30 train loss: 1.1037, eval loss 1.1305568218231201\n",
      "optimal threshold: -0.5544\n",
      "Epoch 31 train loss: 1.0751, eval loss 1.1214016675949097\n",
      "optimal threshold: -0.5793\n",
      "Epoch 32 train loss: 1.0920, eval loss 1.11240816116333\n",
      "optimal threshold: -0.5964\n",
      "Epoch 33 train loss: 1.0428, eval loss 1.1035860776901245\n",
      "optimal threshold: -0.6343\n",
      "Epoch 34 train loss: 1.0712, eval loss 1.0949223041534424\n",
      "optimal threshold: -0.6514\n",
      "Epoch 35 train loss: 1.0292, eval loss 1.0864382982254028\n",
      "optimal threshold: -0.6685\n",
      "Epoch 36 train loss: 1.0517, eval loss 1.0781413316726685\n",
      "optimal threshold: -0.6956\n",
      "Epoch 37 train loss: 1.0297, eval loss 1.0700229406356812\n",
      "optimal threshold: -0.7022\n",
      "Epoch 38 train loss: 1.0470, eval loss 1.0620182752609253\n",
      "optimal threshold: -0.7191\n",
      "Epoch 39 train loss: 1.0169, eval loss 1.0542192459106445\n",
      "optimal threshold: -0.7512\n",
      "Epoch 40 train loss: 0.9915, eval loss 1.0466192960739136\n",
      "optimal threshold: -0.7494\n",
      "Epoch 41 train loss: 0.9808, eval loss 1.0391618013381958\n",
      "optimal threshold: -0.7827\n",
      "Epoch 42 train loss: 0.9854, eval loss 1.031859040260315\n",
      "optimal threshold: -0.8004\n",
      "Epoch 43 train loss: 0.9817, eval loss 1.0247000455856323\n",
      "optimal threshold: -0.7965\n",
      "Epoch 44 train loss: 0.9576, eval loss 1.017686128616333\n",
      "optimal threshold: -0.8374\n",
      "Epoch 45 train loss: 0.9673, eval loss 1.0108470916748047\n",
      "optimal threshold: -0.8424\n",
      "Epoch 46 train loss: 0.9858, eval loss 1.0041310787200928\n",
      "optimal threshold: -0.8552\n",
      "Epoch 47 train loss: 0.9882, eval loss 0.9975613355636597\n",
      "optimal threshold: -0.8714\n",
      "Epoch 48 train loss: 0.9890, eval loss 0.9911361336708069\n",
      "optimal threshold: -0.8842\n",
      "Epoch 49 train loss: 0.9674, eval loss 0.9847908616065979\n",
      "optimal threshold: -0.8974\n",
      "Epoch 50 train loss: 0.9301, eval loss 0.9785884618759155\n",
      "optimal threshold: -0.9106\n",
      "Epoch 51 train loss: 0.9369, eval loss 0.9725012183189392\n",
      "optimal threshold: -0.9231\n",
      "Epoch 52 train loss: 0.8859, eval loss 0.9665147662162781\n",
      "optimal threshold: -0.9325\n",
      "Epoch 53 train loss: 0.9501, eval loss 0.9606239795684814\n",
      "optimal threshold: -0.9417\n",
      "Epoch 54 train loss: 0.8647, eval loss 0.9548131823539734\n",
      "optimal threshold: -0.9533\n",
      "Epoch 55 train loss: 0.9757, eval loss 0.9490960836410522\n",
      "optimal threshold: -0.9563\n",
      "Epoch 56 train loss: 0.8911, eval loss 0.9435052871704102\n",
      "optimal threshold: -0.9701\n",
      "Epoch 57 train loss: 0.9476, eval loss 0.9379953742027283\n",
      "optimal threshold: -0.9698\n",
      "Epoch 58 train loss: 0.9112, eval loss 0.9325708746910095\n",
      "optimal threshold: -0.9857\n",
      "Epoch 59 train loss: 0.9005, eval loss 0.9272150993347168\n",
      "optimal threshold: -1.0119\n",
      "Epoch 60 train loss: 0.9131, eval loss 0.9219367504119873\n",
      "optimal threshold: -1.0366\n",
      "Epoch 61 train loss: 0.8451, eval loss 0.9167410135269165\n",
      "optimal threshold: -1.0410\n",
      "Epoch 62 train loss: 0.8979, eval loss 0.9116320610046387\n",
      "optimal threshold: -1.0447\n",
      "Epoch 63 train loss: 0.8153, eval loss 0.9065631031990051\n",
      "optimal threshold: -1.0527\n",
      "Epoch 64 train loss: 0.8895, eval loss 0.9015849828720093\n",
      "optimal threshold: -1.0606\n",
      "Epoch 65 train loss: 0.8873, eval loss 0.8967005610466003\n",
      "optimal threshold: -1.0689\n",
      "Epoch 66 train loss: 0.8597, eval loss 0.8918928503990173\n",
      "optimal threshold: -1.0959\n",
      "Epoch 67 train loss: 0.7986, eval loss 0.8871251344680786\n",
      "optimal threshold: -1.0992\n",
      "Epoch 68 train loss: 0.8723, eval loss 0.8824432492256165\n",
      "optimal threshold: -1.0220\n",
      "Epoch 69 train loss: 0.8240, eval loss 0.8778144121170044\n",
      "optimal threshold: -1.1198\n",
      "Epoch 70 train loss: 0.8686, eval loss 0.8732903599739075\n",
      "optimal threshold: -1.1301\n",
      "Epoch 71 train loss: 0.8553, eval loss 0.8688108921051025\n",
      "optimal threshold: -1.1328\n",
      "Epoch 72 train loss: 0.8172, eval loss 0.8644135594367981\n",
      "optimal threshold: -1.0410\n",
      "Epoch 73 train loss: 0.7510, eval loss 0.8600597381591797\n",
      "optimal threshold: -1.1352\n",
      "Epoch 74 train loss: 0.8085, eval loss 0.8557944297790527\n",
      "optimal threshold: -1.0670\n",
      "Epoch 75 train loss: 0.8062, eval loss 0.8515840768814087\n",
      "optimal threshold: -1.0673\n",
      "Epoch 76 train loss: 0.8294, eval loss 0.8474451899528503\n",
      "optimal threshold: -1.0634\n",
      "Epoch 77 train loss: 0.7745, eval loss 0.8434036374092102\n",
      "optimal threshold: -1.0706\n",
      "Epoch 78 train loss: 0.7922, eval loss 0.8394178748130798\n",
      "optimal threshold: -0.9677\n",
      "Epoch 79 train loss: 0.8264, eval loss 0.8355473279953003\n",
      "optimal threshold: -0.9656\n",
      "Epoch 80 train loss: 0.8346, eval loss 0.8317311406135559\n",
      "optimal threshold: -1.0076\n",
      "Epoch 81 train loss: 0.7972, eval loss 0.8279543519020081\n",
      "optimal threshold: -1.0061\n",
      "Epoch 82 train loss: 0.7988, eval loss 0.8242828249931335\n",
      "optimal threshold: -1.0049\n",
      "Epoch 83 train loss: 0.7821, eval loss 0.8206952810287476\n",
      "optimal threshold: -1.0069\n",
      "Epoch 84 train loss: 0.7368, eval loss 0.8171414136886597\n",
      "optimal threshold: -1.0095\n",
      "Epoch 85 train loss: 0.8025, eval loss 0.8136752247810364\n",
      "optimal threshold: -1.0040\n",
      "Epoch 86 train loss: 0.7777, eval loss 0.8102850317955017\n",
      "optimal threshold: -0.9998\n",
      "Epoch 87 train loss: 0.7671, eval loss 0.8069738745689392\n",
      "optimal threshold: -1.0082\n",
      "Epoch 88 train loss: 0.7726, eval loss 0.8037230968475342\n",
      "optimal threshold: -0.9987\n",
      "Epoch 89 train loss: 0.7716, eval loss 0.8005784153938293\n",
      "optimal threshold: -0.9942\n",
      "Epoch 90 train loss: 0.8180, eval loss 0.797510027885437\n",
      "optimal threshold: -0.9830\n",
      "Epoch 91 train loss: 0.7838, eval loss 0.7945281863212585\n",
      "optimal threshold: -1.0030\n",
      "Epoch 92 train loss: 0.7860, eval loss 0.7916273474693298\n",
      "optimal threshold: -0.9666\n",
      "Epoch 93 train loss: 0.7537, eval loss 0.7887980341911316\n",
      "optimal threshold: -0.9627\n",
      "Epoch 94 train loss: 0.8069, eval loss 0.7860304117202759\n",
      "optimal threshold: -0.9591\n",
      "Epoch 95 train loss: 0.7848, eval loss 0.7833576202392578\n",
      "optimal threshold: -0.9827\n",
      "Epoch 96 train loss: 0.7212, eval loss 0.7807201743125916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.9827\n",
      "Epoch 97 train loss: 0.7327, eval loss 0.7782083749771118\n",
      "optimal threshold: -0.9762\n",
      "Epoch 98 train loss: 0.6762, eval loss 0.7757479548454285\n",
      "optimal threshold: -0.9737\n",
      "Epoch 99 train loss: 0.7041, eval loss 0.7733497023582458\n",
      "optimal threshold: -0.9708\n",
      "Epoch 100 train loss: 0.7852, eval loss 0.7710238695144653\n",
      "optimal threshold: -0.9742\n",
      "Epoch 101 train loss: 0.7615, eval loss 0.7687830328941345\n",
      "optimal threshold: -0.9704\n",
      "Epoch 102 train loss: 0.7278, eval loss 0.7665653228759766\n",
      "optimal threshold: -0.9129\n",
      "Epoch 103 train loss: 0.7528, eval loss 0.7644357085227966\n",
      "optimal threshold: -0.9077\n",
      "Epoch 104 train loss: 0.7982, eval loss 0.7623559236526489\n",
      "optimal threshold: -0.9066\n",
      "Epoch 105 train loss: 0.6964, eval loss 0.760375440120697\n",
      "optimal threshold: -0.8987\n",
      "Epoch 106 train loss: 0.7985, eval loss 0.7584255933761597\n",
      "optimal threshold: -0.9537\n",
      "Epoch 107 train loss: 0.7197, eval loss 0.756573498249054\n",
      "optimal threshold: -0.9515\n",
      "Epoch 108 train loss: 0.7541, eval loss 0.7547642588615417\n",
      "optimal threshold: -0.8835\n",
      "Epoch 109 train loss: 0.7429, eval loss 0.7530196905136108\n",
      "optimal threshold: -0.8733\n",
      "Epoch 110 train loss: 0.6952, eval loss 0.7513049840927124\n",
      "optimal threshold: -0.8647\n",
      "Epoch 111 train loss: 0.7996, eval loss 0.7496407628059387\n",
      "optimal threshold: -0.8623\n",
      "Epoch 112 train loss: 0.7546, eval loss 0.7480480074882507\n",
      "optimal threshold: -0.9648\n",
      "Epoch 113 train loss: 0.7305, eval loss 0.7464996576309204\n",
      "optimal threshold: -0.9241\n",
      "Epoch 114 train loss: 0.7477, eval loss 0.744995653629303\n",
      "optimal threshold: -0.9751\n",
      "Epoch 115 train loss: 0.7716, eval loss 0.743562638759613\n",
      "optimal threshold: -0.9721\n",
      "Epoch 116 train loss: 0.6858, eval loss 0.7421524524688721\n",
      "optimal threshold: -0.8912\n",
      "Epoch 117 train loss: 0.7783, eval loss 0.7407662272453308\n",
      "optimal threshold: -0.9541\n",
      "Epoch 118 train loss: 0.7537, eval loss 0.739438533782959\n",
      "optimal threshold: -0.9772\n",
      "Epoch 119 train loss: 0.7237, eval loss 0.7381811141967773\n",
      "optimal threshold: -0.9453\n",
      "Epoch 120 train loss: 0.7635, eval loss 0.7369437217712402\n",
      "optimal threshold: -0.9421\n",
      "Epoch 121 train loss: 0.7223, eval loss 0.7357891201972961\n",
      "optimal threshold: -0.9428\n",
      "Epoch 122 train loss: 0.6726, eval loss 0.7346540689468384\n",
      "optimal threshold: -0.9394\n",
      "Epoch 123 train loss: 0.7221, eval loss 0.7335318922996521\n",
      "optimal threshold: -0.9363\n",
      "Epoch 124 train loss: 0.7332, eval loss 0.7324861288070679\n",
      "optimal threshold: -0.9340\n",
      "Epoch 125 train loss: 0.7968, eval loss 0.7314634323120117\n",
      "optimal threshold: -0.9522\n",
      "Epoch 126 train loss: 0.6768, eval loss 0.7304881811141968\n",
      "optimal threshold: -0.9279\n",
      "Epoch 127 train loss: 0.7361, eval loss 0.7295353412628174\n",
      "optimal threshold: -0.9357\n",
      "Epoch 128 train loss: 0.7647, eval loss 0.7285902500152588\n",
      "optimal threshold: -0.9310\n",
      "Epoch 129 train loss: 0.7487, eval loss 0.7276897430419922\n",
      "optimal threshold: -0.9364\n",
      "Epoch 130 train loss: 0.7971, eval loss 0.7268247008323669\n",
      "optimal threshold: -0.9359\n",
      "Epoch 131 train loss: 0.7218, eval loss 0.7259912490844727\n",
      "optimal threshold: -0.9357\n",
      "Epoch 132 train loss: 0.7783, eval loss 0.7251538634300232\n",
      "optimal threshold: -0.9369\n",
      "Epoch 133 train loss: 0.7050, eval loss 0.7243815660476685\n",
      "optimal threshold: -0.9382\n",
      "Epoch 134 train loss: 0.7851, eval loss 0.7236409783363342\n",
      "optimal threshold: -0.9423\n",
      "Epoch 135 train loss: 0.8230, eval loss 0.7228745222091675\n",
      "optimal threshold: -0.9324\n",
      "Epoch 136 train loss: 0.7228, eval loss 0.7221394181251526\n",
      "optimal threshold: -0.9138\n",
      "Epoch 137 train loss: 0.8089, eval loss 0.7214409112930298\n",
      "optimal threshold: -0.9158\n",
      "Epoch 138 train loss: 0.7595, eval loss 0.7207936644554138\n",
      "optimal threshold: -0.9084\n",
      "Epoch 139 train loss: 0.7410, eval loss 0.720126748085022\n",
      "optimal threshold: -0.8985\n",
      "Epoch 140 train loss: 0.7391, eval loss 0.7194818258285522\n",
      "optimal threshold: -0.9103\n",
      "Epoch 141 train loss: 0.7513, eval loss 0.7188495397567749\n",
      "optimal threshold: -0.8860\n",
      "Epoch 142 train loss: 0.7391, eval loss 0.7182385921478271\n",
      "optimal threshold: -0.8872\n",
      "Epoch 143 train loss: 0.7751, eval loss 0.7176595330238342\n",
      "optimal threshold: -0.8855\n",
      "Epoch 144 train loss: 0.8003, eval loss 0.7170952558517456\n",
      "optimal threshold: -0.8891\n",
      "Epoch 145 train loss: 0.7894, eval loss 0.7165313959121704\n",
      "optimal threshold: -0.8907\n",
      "Epoch 146 train loss: 0.8348, eval loss 0.7160018682479858\n",
      "optimal threshold: -0.8621\n",
      "Epoch 147 train loss: 0.8203, eval loss 0.7154613733291626\n",
      "optimal threshold: -0.8605\n",
      "Epoch 148 train loss: 0.6996, eval loss 0.7149285078048706\n",
      "optimal threshold: -0.8610\n",
      "Epoch 149 train loss: 0.7029, eval loss 0.714419960975647\n",
      "optimal threshold: -0.8588\n",
      "Epoch 150 train loss: 0.8000, eval loss 0.713948667049408\n",
      "optimal threshold: -0.8556\n",
      "Epoch 151 train loss: 0.6679, eval loss 0.7134689688682556\n",
      "optimal threshold: -0.8559\n",
      "Epoch 152 train loss: 0.7891, eval loss 0.7130113244056702\n",
      "optimal threshold: -0.8551\n",
      "Epoch 153 train loss: 0.8248, eval loss 0.7125638127326965\n",
      "optimal threshold: -0.8551\n",
      "Epoch 154 train loss: 0.7987, eval loss 0.7121298909187317\n",
      "optimal threshold: -0.8554\n",
      "Epoch 155 train loss: 0.8704, eval loss 0.711705207824707\n",
      "optimal threshold: -0.8548\n",
      "Epoch 156 train loss: 0.7500, eval loss 0.7112765908241272\n",
      "optimal threshold: -0.8546\n",
      "Epoch 157 train loss: 0.7968, eval loss 0.7108676433563232\n",
      "optimal threshold: -0.8545\n",
      "Epoch 158 train loss: 0.8090, eval loss 0.7104547619819641\n",
      "optimal threshold: -0.8554\n",
      "Epoch 159 train loss: 0.7759, eval loss 0.7100599408149719\n",
      "optimal threshold: -0.8584\n",
      "Epoch 160 train loss: 0.7818, eval loss 0.7096388936042786\n",
      "optimal threshold: -0.8595\n",
      "Epoch 161 train loss: 0.7711, eval loss 0.7092687487602234\n",
      "optimal threshold: -0.8559\n",
      "Epoch 162 train loss: 0.7380, eval loss 0.7088770866394043\n",
      "optimal threshold: -0.8561\n",
      "Epoch 163 train loss: 0.7611, eval loss 0.7085162401199341\n",
      "optimal threshold: -0.7947\n",
      "Epoch 164 train loss: 0.8595, eval loss 0.7081546783447266\n",
      "optimal threshold: -0.7929\n",
      "Epoch 165 train loss: 0.7701, eval loss 0.7077881097793579\n",
      "optimal threshold: -0.8004\n",
      "Epoch 166 train loss: 0.7732, eval loss 0.7074280977249146\n",
      "optimal threshold: -0.8005\n",
      "Epoch 167 train loss: 0.7373, eval loss 0.7070826292037964\n",
      "optimal threshold: -0.6026\n",
      "Epoch 168 train loss: 0.6956, eval loss 0.7067525386810303\n",
      "optimal threshold: -0.6048\n",
      "Epoch 169 train loss: 0.7612, eval loss 0.7064248919487\n",
      "optimal threshold: -0.6023\n",
      "Epoch 170 train loss: 0.6899, eval loss 0.7061227560043335\n",
      "optimal threshold: -0.6061\n",
      "Epoch 171 train loss: 0.7485, eval loss 0.7058077454566956\n",
      "optimal threshold: -0.5991\n",
      "Epoch 172 train loss: 0.7948, eval loss 0.7054954171180725\n",
      "optimal threshold: -0.6166\n",
      "Epoch 173 train loss: 0.7800, eval loss 0.7051675319671631\n",
      "optimal threshold: -0.6091\n",
      "Epoch 174 train loss: 0.7938, eval loss 0.7048457860946655\n",
      "optimal threshold: -0.6018\n",
      "Epoch 175 train loss: 0.8330, eval loss 0.7044941782951355\n",
      "optimal threshold: -0.6121\n",
      "Epoch 176 train loss: 0.7033, eval loss 0.704194188117981\n",
      "optimal threshold: -0.5788\n",
      "Epoch 177 train loss: 0.8212, eval loss 0.7038974165916443\n",
      "optimal threshold: -0.5760\n",
      "Epoch 178 train loss: 0.6684, eval loss 0.7035881876945496\n",
      "optimal threshold: -0.5961\n",
      "Epoch 179 train loss: 0.7023, eval loss 0.7033162117004395\n",
      "optimal threshold: -0.5728\n",
      "Epoch 180 train loss: 0.6648, eval loss 0.7030156850814819\n",
      "optimal threshold: -0.5715\n",
      "Epoch 181 train loss: 0.8270, eval loss 0.7027477622032166\n",
      "optimal threshold: -0.5757\n",
      "Epoch 182 train loss: 0.7669, eval loss 0.7024849653244019\n",
      "optimal threshold: -0.5771\n",
      "Epoch 183 train loss: 0.6252, eval loss 0.7022008299827576\n",
      "optimal threshold: -0.5928\n",
      "Epoch 184 train loss: 0.7048, eval loss 0.7019479274749756\n",
      "optimal threshold: -0.5887\n",
      "Epoch 185 train loss: 0.7684, eval loss 0.7016822099685669\n",
      "optimal threshold: -0.5883\n",
      "Epoch 186 train loss: 0.6900, eval loss 0.7014214992523193\n",
      "optimal threshold: -0.6015\n",
      "Epoch 187 train loss: 0.7594, eval loss 0.7011396884918213\n",
      "optimal threshold: -0.6025\n",
      "Epoch 188 train loss: 0.7815, eval loss 0.7008809447288513\n",
      "optimal threshold: -0.5848\n",
      "Epoch 189 train loss: 0.7841, eval loss 0.7006138563156128\n",
      "optimal threshold: -0.5858\n",
      "Epoch 190 train loss: 0.7513, eval loss 0.7003540992736816\n",
      "optimal threshold: -0.6030\n",
      "Epoch 191 train loss: 0.8058, eval loss 0.700083315372467\n",
      "optimal threshold: -0.6042\n",
      "Epoch 192 train loss: 0.7389, eval loss 0.6998283267021179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5946\n",
      "Epoch 193 train loss: 0.6610, eval loss 0.699581503868103\n",
      "optimal threshold: -0.5905\n",
      "Epoch 194 train loss: 0.6886, eval loss 0.6993163228034973\n",
      "optimal threshold: -0.5922\n",
      "Epoch 195 train loss: 0.6561, eval loss 0.6990721225738525\n",
      "optimal threshold: -0.5942\n",
      "Epoch 196 train loss: 0.6587, eval loss 0.6988347768783569\n",
      "optimal threshold: -0.5773\n",
      "Epoch 197 train loss: 0.6989, eval loss 0.6985937356948853\n",
      "optimal threshold: -0.5731\n",
      "Epoch 198 train loss: 0.7672, eval loss 0.6983526349067688\n",
      "optimal threshold: -0.5708\n",
      "Epoch 199 train loss: 0.8348, eval loss 0.6981271505355835\n",
      "optimal threshold: -0.5688\n",
      "Epoch 200 train loss: 0.8538, eval loss 0.6978574395179749\n",
      "optimal threshold: -0.5700\n",
      "Epoch 201 train loss: 0.7615, eval loss 0.6976361274719238\n",
      "optimal threshold: -0.5703\n",
      "Epoch 202 train loss: 0.6800, eval loss 0.6974055767059326\n",
      "optimal threshold: -0.5711\n",
      "Epoch 203 train loss: 0.7008, eval loss 0.6971760392189026\n",
      "optimal threshold: -0.5719\n",
      "Epoch 204 train loss: 0.6745, eval loss 0.6969630718231201\n",
      "optimal threshold: -0.5732\n",
      "Epoch 205 train loss: 0.8343, eval loss 0.696739673614502\n",
      "optimal threshold: -0.5728\n",
      "Epoch 206 train loss: 0.8070, eval loss 0.6965063810348511\n",
      "optimal threshold: -0.5771\n",
      "Epoch 207 train loss: 0.7187, eval loss 0.6963090300559998\n",
      "optimal threshold: -0.5709\n",
      "Epoch 208 train loss: 0.8258, eval loss 0.6961010098457336\n",
      "optimal threshold: -0.5763\n",
      "Epoch 209 train loss: 0.8199, eval loss 0.6958752870559692\n",
      "optimal threshold: -0.5755\n",
      "Epoch 210 train loss: 0.7914, eval loss 0.6956737637519836\n",
      "optimal threshold: -0.5723\n",
      "Epoch 211 train loss: 0.7364, eval loss 0.6954581141471863\n",
      "optimal threshold: -0.5570\n",
      "Epoch 212 train loss: 0.6768, eval loss 0.6952358484268188\n",
      "optimal threshold: -0.5735\n",
      "Epoch 213 train loss: 0.7564, eval loss 0.6950141191482544\n",
      "optimal threshold: -0.7868\n",
      "Epoch 214 train loss: 0.8704, eval loss 0.6948085427284241\n",
      "optimal threshold: -0.7838\n",
      "Epoch 215 train loss: 0.7424, eval loss 0.694606602191925\n",
      "optimal threshold: -0.7845\n",
      "Epoch 216 train loss: 0.6987, eval loss 0.6943712830543518\n",
      "optimal threshold: -0.7845\n",
      "Epoch 217 train loss: 0.7154, eval loss 0.6941746473312378\n",
      "optimal threshold: -0.7836\n",
      "Epoch 218 train loss: 0.7705, eval loss 0.6939573287963867\n",
      "optimal threshold: -0.7836\n",
      "Epoch 219 train loss: 0.7443, eval loss 0.6937659978866577\n",
      "optimal threshold: -0.5569\n",
      "Epoch 220 train loss: 0.7064, eval loss 0.6935814023017883\n",
      "optimal threshold: -0.5288\n",
      "Epoch 221 train loss: 0.7382, eval loss 0.6933655142784119\n",
      "optimal threshold: -0.7471\n",
      "Epoch 222 train loss: 0.5961, eval loss 0.6931515336036682\n",
      "optimal threshold: -0.5282\n",
      "Epoch 223 train loss: 0.7945, eval loss 0.6929455399513245\n",
      "optimal threshold: -0.7456\n",
      "Epoch 224 train loss: 0.6587, eval loss 0.6927526593208313\n",
      "optimal threshold: -0.7463\n",
      "Epoch 225 train loss: 0.7923, eval loss 0.6925678849220276\n",
      "optimal threshold: -0.7493\n",
      "Epoch 226 train loss: 0.7335, eval loss 0.6923691630363464\n",
      "optimal threshold: -0.7480\n",
      "Epoch 227 train loss: 0.7207, eval loss 0.6921709179878235\n",
      "optimal threshold: -0.7497\n",
      "Epoch 228 train loss: 0.8136, eval loss 0.691978931427002\n",
      "optimal threshold: -0.7502\n",
      "Epoch 229 train loss: 0.7273, eval loss 0.691772997379303\n",
      "optimal threshold: -0.7430\n",
      "Epoch 230 train loss: 0.7674, eval loss 0.6915867328643799\n",
      "optimal threshold: -0.7357\n",
      "Epoch 231 train loss: 0.6957, eval loss 0.6914260983467102\n",
      "optimal threshold: -0.7352\n",
      "Epoch 232 train loss: 0.7867, eval loss 0.6912431716918945\n",
      "optimal threshold: -0.9458\n",
      "Epoch 233 train loss: 0.7483, eval loss 0.6910771131515503\n",
      "optimal threshold: -0.9461\n",
      "Epoch 234 train loss: 0.7070, eval loss 0.6909195184707642\n",
      "optimal threshold: -0.9462\n",
      "Epoch 235 train loss: 0.7552, eval loss 0.6907227635383606\n",
      "optimal threshold: -0.9445\n",
      "Epoch 236 train loss: 0.8037, eval loss 0.6905568838119507\n",
      "optimal threshold: -0.9297\n",
      "Epoch 237 train loss: 0.7695, eval loss 0.6903811693191528\n",
      "optimal threshold: -0.9353\n",
      "Epoch 238 train loss: 0.8802, eval loss 0.6902171969413757\n",
      "optimal threshold: -0.9297\n",
      "Epoch 239 train loss: 0.8128, eval loss 0.690028727054596\n",
      "optimal threshold: -0.9287\n",
      "Epoch 240 train loss: 0.8650, eval loss 0.6898611187934875\n",
      "optimal threshold: -0.9277\n",
      "Epoch 241 train loss: 0.7222, eval loss 0.689662516117096\n",
      "optimal threshold: -0.9275\n",
      "Epoch 242 train loss: 0.8114, eval loss 0.6894729733467102\n",
      "optimal threshold: -0.9277\n",
      "Epoch 243 train loss: 0.7279, eval loss 0.6892786622047424\n",
      "optimal threshold: -0.9341\n",
      "Epoch 244 train loss: 0.8022, eval loss 0.6891025900840759\n",
      "optimal threshold: -0.9384\n",
      "Epoch 245 train loss: 0.7446, eval loss 0.6889487504959106\n",
      "optimal threshold: -0.9530\n",
      "Epoch 246 train loss: 0.7675, eval loss 0.6887908577919006\n",
      "optimal threshold: -0.9198\n",
      "Epoch 247 train loss: 0.8012, eval loss 0.6886352300643921\n",
      "optimal threshold: -0.9191\n",
      "Epoch 248 train loss: 0.7034, eval loss 0.6884944438934326\n",
      "optimal threshold: -0.9207\n",
      "Epoch 249 train loss: 0.7712, eval loss 0.6883476972579956\n",
      "optimal threshold: -0.9197\n",
      "Epoch 250 train loss: 0.7285, eval loss 0.6881770491600037\n",
      "optimal threshold: -0.9203\n",
      "Epoch 251 train loss: 0.7174, eval loss 0.688007652759552\n",
      "optimal threshold: -0.9205\n",
      "Epoch 252 train loss: 0.7614, eval loss 0.6878325939178467\n",
      "optimal threshold: -0.9222\n",
      "Epoch 253 train loss: 0.8383, eval loss 0.6876786351203918\n",
      "optimal threshold: -0.9436\n",
      "Epoch 254 train loss: 0.7853, eval loss 0.6875035762786865\n",
      "optimal threshold: -0.9398\n",
      "Epoch 255 train loss: 0.7991, eval loss 0.6873591542243958\n",
      "optimal threshold: -0.9366\n",
      "Epoch 256 train loss: 0.8204, eval loss 0.6872174143791199\n",
      "optimal threshold: -0.9405\n",
      "Epoch 257 train loss: 0.7345, eval loss 0.6870467662811279\n",
      "optimal threshold: -0.9377\n",
      "Epoch 258 train loss: 0.7759, eval loss 0.6868807673454285\n",
      "optimal threshold: -0.9295\n",
      "Epoch 259 train loss: 0.8366, eval loss 0.6867236495018005\n",
      "optimal threshold: -0.9228\n",
      "Epoch 260 train loss: 0.7368, eval loss 0.6865746378898621\n",
      "optimal threshold: -0.9214\n",
      "Epoch 261 train loss: 0.7763, eval loss 0.6864323616027832\n",
      "optimal threshold: -0.9215\n",
      "Epoch 262 train loss: 0.7501, eval loss 0.6862853169441223\n",
      "optimal threshold: -0.9391\n",
      "Epoch 263 train loss: 0.7565, eval loss 0.6861212253570557\n",
      "optimal threshold: -0.9399\n",
      "Epoch 264 train loss: 0.9051, eval loss 0.6859865188598633\n",
      "optimal threshold: -0.9403\n",
      "Epoch 265 train loss: 0.8649, eval loss 0.6858364939689636\n",
      "optimal threshold: -0.9411\n",
      "Epoch 266 train loss: 0.7960, eval loss 0.6856988668441772\n",
      "optimal threshold: -0.9418\n",
      "Epoch 267 train loss: 0.8332, eval loss 0.685566782951355\n",
      "optimal threshold: -0.9379\n",
      "Epoch 268 train loss: 0.7501, eval loss 0.6854365468025208\n",
      "optimal threshold: -0.9282\n",
      "Epoch 269 train loss: 0.8417, eval loss 0.6852871775627136\n",
      "optimal threshold: -0.9375\n",
      "Epoch 270 train loss: 0.7978, eval loss 0.6851310729980469\n",
      "optimal threshold: -0.9369\n",
      "Epoch 271 train loss: 0.8239, eval loss 0.684987485408783\n",
      "optimal threshold: -0.9352\n",
      "Epoch 272 train loss: 0.6984, eval loss 0.6848208904266357\n",
      "optimal threshold: -0.9499\n",
      "Epoch 273 train loss: 0.6971, eval loss 0.6846873164176941\n",
      "optimal threshold: -0.9492\n",
      "Epoch 274 train loss: 0.7275, eval loss 0.6845406889915466\n",
      "optimal threshold: -0.9497\n",
      "Epoch 275 train loss: 0.8113, eval loss 0.6844159364700317\n",
      "optimal threshold: -0.9485\n",
      "Epoch 276 train loss: 0.6898, eval loss 0.6842700839042664\n",
      "optimal threshold: -0.9485\n",
      "Epoch 277 train loss: 0.7782, eval loss 0.6841384768486023\n",
      "optimal threshold: -0.9481\n",
      "Epoch 278 train loss: 0.8499, eval loss 0.6839966773986816\n",
      "optimal threshold: -0.9484\n",
      "Epoch 279 train loss: 0.7310, eval loss 0.6838619112968445\n",
      "optimal threshold: -0.9494\n",
      "Epoch 280 train loss: 0.7611, eval loss 0.6837266087532043\n",
      "optimal threshold: -0.9485\n",
      "Epoch 281 train loss: 0.7583, eval loss 0.6835625767707825\n",
      "optimal threshold: -0.9515\n",
      "Epoch 282 train loss: 0.6513, eval loss 0.6834586262702942\n",
      "optimal threshold: -0.9526\n",
      "Epoch 283 train loss: 0.6783, eval loss 0.6833300590515137\n",
      "optimal threshold: -0.9477\n",
      "Epoch 284 train loss: 0.7946, eval loss 0.6831905841827393\n",
      "optimal threshold: -0.9484\n",
      "Epoch 285 train loss: 0.7387, eval loss 0.6830796599388123\n",
      "optimal threshold: -0.9491\n",
      "Epoch 286 train loss: 0.7932, eval loss 0.682967483997345\n",
      "optimal threshold: -0.9474\n",
      "Epoch 287 train loss: 0.7032, eval loss 0.682819128036499\n",
      "optimal threshold: -0.9492\n",
      "Epoch 288 train loss: 0.7131, eval loss 0.6827191710472107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.9477\n",
      "Epoch 289 train loss: 0.8757, eval loss 0.6825724840164185\n",
      "optimal threshold: -0.9485\n",
      "Epoch 290 train loss: 0.8083, eval loss 0.6824672818183899\n",
      "optimal threshold: -0.9499\n",
      "Epoch 291 train loss: 0.8245, eval loss 0.6823625564575195\n",
      "optimal threshold: -0.9501\n",
      "Epoch 292 train loss: 0.7829, eval loss 0.6822460293769836\n",
      "optimal threshold: -0.9535\n",
      "Epoch 293 train loss: 0.7646, eval loss 0.6821317672729492\n",
      "optimal threshold: -0.9501\n",
      "Epoch 294 train loss: 0.7765, eval loss 0.6819832921028137\n",
      "optimal threshold: -0.9509\n",
      "Epoch 295 train loss: 0.7242, eval loss 0.6818708181381226\n",
      "optimal threshold: -0.9520\n",
      "Epoch 296 train loss: 0.6235, eval loss 0.6817551851272583\n",
      "optimal threshold: -0.9518\n",
      "Epoch 297 train loss: 0.7710, eval loss 0.6816142797470093\n",
      "optimal threshold: -0.9543\n",
      "Epoch 298 train loss: 0.7191, eval loss 0.6815318465232849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 12:04:43,048] Trial 8 finished with value: 0.7589697241783142 and parameters: {'learning_rate_exp': -5.622491310549236, 'dropout_p': 0.4920067176869001, 'l2_reg_exp': -4.6161842687026535, 'batch_size': 204, 'N': 259}. Best is trial 6 with value: 0.22441545128822327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.9803\n",
      "Epoch 299 train loss: 0.7590, eval loss 0.6814150214195251\n",
      "optimal threshold: -0.5215\n",
      "Epoch 0 train loss: 0.7621, eval loss 0.679905891418457\n",
      "optimal threshold: -0.3497\n",
      "Epoch 1 train loss: 0.7266, eval loss 0.6673070788383484\n",
      "optimal threshold: -0.3267\n",
      "Epoch 2 train loss: 0.7357, eval loss 0.667455792427063\n",
      "optimal threshold: -0.4346\n",
      "Epoch 3 train loss: 0.6719, eval loss 0.6662411093711853\n",
      "optimal threshold: -0.5475\n",
      "Epoch 4 train loss: 0.7051, eval loss 0.6679524183273315\n",
      "optimal threshold: -0.4315\n",
      "Epoch 5 train loss: 0.7336, eval loss 0.6650242805480957\n",
      "optimal threshold: -0.3477\n",
      "Epoch 6 train loss: 0.6394, eval loss 0.6641123294830322\n",
      "optimal threshold: -0.6916\n",
      "Epoch 7 train loss: 0.6649, eval loss 0.6634733080863953\n",
      "optimal threshold: -0.5768\n",
      "Epoch 8 train loss: 0.6998, eval loss 0.6654040217399597\n",
      "optimal threshold: -0.6752\n",
      "Epoch 9 train loss: 0.6924, eval loss 0.6691297292709351\n",
      "optimal threshold: -0.4318\n",
      "Epoch 10 train loss: 0.7089, eval loss 0.6689521074295044\n",
      "optimal threshold: -0.3839\n",
      "Epoch 11 train loss: 0.6773, eval loss 0.6694186329841614\n",
      "optimal threshold: -0.5960\n",
      "Epoch 12 train loss: 0.6758, eval loss 0.6651049852371216\n",
      "optimal threshold: -0.5200\n",
      "Epoch 13 train loss: 0.6642, eval loss 0.6719840168952942\n",
      "optimal threshold: -0.6099\n",
      "Epoch 14 train loss: 0.6456, eval loss 0.6692955493927002\n",
      "optimal threshold: -0.6971\n",
      "Epoch 15 train loss: 0.5812, eval loss 0.6782914996147156\n",
      "optimal threshold: -0.6256\n",
      "Epoch 16 train loss: 0.6777, eval loss 0.6781103610992432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 12:04:50,917] Trial 9 finished with value: 0.6104923486709595 and parameters: {'learning_rate_exp': -2.15118360464723, 'dropout_p': 0.5020571753705337, 'l2_reg_exp': -2.051991639673732, 'batch_size': 292, 'N': 69}. Best is trial 6 with value: 0.22441545128822327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6538\n",
      "optimal threshold: -0.4812\n",
      "Epoch 0 train loss: 0.6549, eval loss 0.665800929069519\n",
      "optimal threshold: -0.4083\n",
      "Epoch 1 train loss: 0.4954, eval loss 0.6611120104789734\n",
      "optimal threshold: -0.5169\n",
      "Epoch 2 train loss: 0.5988, eval loss 0.6589739918708801\n",
      "optimal threshold: -0.4868\n",
      "Epoch 3 train loss: 0.5424, eval loss 0.658159613609314\n",
      "optimal threshold: -0.3523\n",
      "Epoch 4 train loss: 0.6257, eval loss 0.6621463298797607\n",
      "optimal threshold: -0.6636\n",
      "Epoch 5 train loss: 0.5388, eval loss 0.6594688892364502\n",
      "optimal threshold: -0.4108\n",
      "Epoch 6 train loss: 0.5842, eval loss 0.6588241457939148\n",
      "optimal threshold: -0.4100\n",
      "Epoch 7 train loss: 0.6243, eval loss 0.6566323637962341\n",
      "optimal threshold: -0.3791\n",
      "Epoch 8 train loss: 0.6199, eval loss 0.6553756594657898\n",
      "optimal threshold: -0.5170\n",
      "Epoch 9 train loss: 0.5306, eval loss 0.6590044498443604\n",
      "optimal threshold: -0.4869\n",
      "Epoch 10 train loss: 0.5152, eval loss 0.6591609120368958\n",
      "optimal threshold: -0.4452\n",
      "Epoch 11 train loss: 0.5293, eval loss 0.6565547585487366\n",
      "optimal threshold: -0.4809\n",
      "Epoch 12 train loss: 0.6617, eval loss 0.6618695259094238\n",
      "optimal threshold: -0.5612\n",
      "Epoch 13 train loss: 0.5607, eval loss 0.6612607836723328\n",
      "optimal threshold: -0.5954\n",
      "Epoch 14 train loss: 0.6081, eval loss 0.6614652872085571\n",
      "optimal threshold: -0.6465\n",
      "Epoch 15 train loss: 0.7865, eval loss 0.6592512130737305\n",
      "optimal threshold: -0.6646\n",
      "Epoch 16 train loss: 0.5710, eval loss 0.6604924201965332\n",
      "optimal threshold: -0.4687\n",
      "Epoch 17 train loss: 0.5901, eval loss 0.6650100946426392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 12:05:35,101] Trial 10 finished with value: 0.5695439577102661 and parameters: {'learning_rate_exp': -2.998878186547972, 'dropout_p': 0.5908760090415164, 'l2_reg_exp': -6.7605296417244265, 'batch_size': 25, 'N': 166}. Best is trial 6 with value: 0.22441545128822327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6433\n",
      "optimal threshold: -0.1932\n",
      "Epoch 0 train loss: 1.4538, eval loss 1.3284361362457275\n",
      "optimal threshold: -0.3487\n",
      "Epoch 1 train loss: 1.4990, eval loss 1.2471792697906494\n",
      "optimal threshold: -0.5108\n",
      "Epoch 2 train loss: 1.4472, eval loss 1.1722153425216675\n",
      "optimal threshold: -0.6426\n",
      "Epoch 3 train loss: 1.5024, eval loss 1.1074550151824951\n",
      "optimal threshold: -0.7777\n",
      "Epoch 4 train loss: 1.3600, eval loss 1.053760290145874\n",
      "optimal threshold: -0.8767\n",
      "Epoch 5 train loss: 1.4262, eval loss 1.0089256763458252\n",
      "optimal threshold: -0.9570\n",
      "Epoch 6 train loss: 1.3125, eval loss 0.9706423282623291\n",
      "optimal threshold: -0.9719\n",
      "Epoch 7 train loss: 1.2474, eval loss 0.9366575479507446\n",
      "optimal threshold: -0.9972\n",
      "Epoch 8 train loss: 1.2147, eval loss 0.9059115648269653\n",
      "optimal threshold: -1.0117\n",
      "Epoch 9 train loss: 1.0872, eval loss 0.8780496716499329\n",
      "optimal threshold: -0.9761\n",
      "Epoch 10 train loss: 1.1389, eval loss 0.8532033562660217\n",
      "optimal threshold: -0.9832\n",
      "Epoch 11 train loss: 1.0134, eval loss 0.8312500715255737\n",
      "optimal threshold: -0.9730\n",
      "Epoch 12 train loss: 0.9219, eval loss 0.8122683167457581\n",
      "optimal threshold: -0.9606\n",
      "Epoch 13 train loss: 0.8567, eval loss 0.7960768938064575\n",
      "optimal threshold: -0.9517\n",
      "Epoch 14 train loss: 0.7755, eval loss 0.7823973298072815\n",
      "optimal threshold: -0.9280\n",
      "Epoch 15 train loss: 0.6949, eval loss 0.7709522843360901\n",
      "optimal threshold: -0.9065\n",
      "Epoch 16 train loss: 0.7310, eval loss 0.7613337635993958\n",
      "optimal threshold: -0.8832\n",
      "Epoch 17 train loss: 0.7628, eval loss 0.7532103061676025\n",
      "optimal threshold: -0.8734\n",
      "Epoch 18 train loss: 0.5911, eval loss 0.7464550733566284\n",
      "optimal threshold: -0.8632\n",
      "Epoch 19 train loss: 0.6418, eval loss 0.740683376789093\n",
      "optimal threshold: -0.8503\n",
      "Epoch 20 train loss: 0.5391, eval loss 0.7357255816459656\n",
      "optimal threshold: -0.8305\n",
      "Epoch 21 train loss: 0.5681, eval loss 0.731502115726471\n",
      "optimal threshold: -0.8293\n",
      "Epoch 22 train loss: 0.5226, eval loss 0.7278591394424438\n",
      "optimal threshold: -0.8269\n",
      "Epoch 23 train loss: 0.6270, eval loss 0.7245970368385315\n",
      "optimal threshold: -0.8409\n",
      "Epoch 24 train loss: 0.5125, eval loss 0.7217989563941956\n",
      "optimal threshold: -0.8130\n",
      "Epoch 25 train loss: 0.5576, eval loss 0.7191963195800781\n",
      "optimal threshold: -0.8805\n",
      "Epoch 26 train loss: 0.4665, eval loss 0.716858983039856\n",
      "optimal threshold: -0.9477\n",
      "Epoch 27 train loss: 0.5154, eval loss 0.7147246599197388\n",
      "optimal threshold: -0.8904\n",
      "Epoch 28 train loss: 0.6021, eval loss 0.712748110294342\n",
      "optimal threshold: -0.8927\n",
      "Epoch 29 train loss: 0.5210, eval loss 0.7109314799308777\n",
      "optimal threshold: -0.8837\n",
      "Epoch 30 train loss: 0.4790, eval loss 0.7092577815055847\n",
      "optimal threshold: -0.9053\n",
      "Epoch 31 train loss: 0.5255, eval loss 0.7076868414878845\n",
      "optimal threshold: -0.8359\n",
      "Epoch 32 train loss: 0.4747, eval loss 0.7061630487442017\n",
      "optimal threshold: -0.8292\n",
      "Epoch 33 train loss: 0.5041, eval loss 0.7047443389892578\n",
      "optimal threshold: -0.8194\n",
      "Epoch 34 train loss: 0.5379, eval loss 0.7033789157867432\n",
      "optimal threshold: -0.8242\n",
      "Epoch 35 train loss: 0.4994, eval loss 0.7020795345306396\n",
      "optimal threshold: -0.8252\n",
      "Epoch 36 train loss: 0.4729, eval loss 0.7008088231086731\n",
      "optimal threshold: -0.9534\n",
      "Epoch 37 train loss: 0.4923, eval loss 0.6996292471885681\n",
      "optimal threshold: -0.8836\n",
      "Epoch 38 train loss: 0.4382, eval loss 0.6984680891036987\n",
      "optimal threshold: -0.8665\n",
      "Epoch 39 train loss: 0.4398, eval loss 0.6973294019699097\n",
      "optimal threshold: -0.8675\n",
      "Epoch 40 train loss: 0.5023, eval loss 0.696287214756012\n",
      "optimal threshold: -0.8830\n",
      "Epoch 41 train loss: 0.4606, eval loss 0.695268452167511\n",
      "optimal threshold: -0.8828\n",
      "Epoch 42 train loss: 0.4554, eval loss 0.6942720413208008\n",
      "optimal threshold: -0.9024\n",
      "Epoch 43 train loss: 0.4266, eval loss 0.6932814717292786\n",
      "optimal threshold: -0.8965\n",
      "Epoch 44 train loss: 0.4065, eval loss 0.6923201084136963\n",
      "optimal threshold: -0.9190\n",
      "Epoch 45 train loss: 0.4013, eval loss 0.6914650201797485\n",
      "optimal threshold: -0.9077\n",
      "Epoch 46 train loss: 0.3415, eval loss 0.6905759572982788\n",
      "optimal threshold: -0.9313\n",
      "Epoch 47 train loss: 0.5571, eval loss 0.6897859573364258\n",
      "optimal threshold: -0.9311\n",
      "Epoch 48 train loss: 0.4092, eval loss 0.688971757888794\n",
      "optimal threshold: -0.9338\n",
      "Epoch 49 train loss: 0.3494, eval loss 0.6881874799728394\n",
      "optimal threshold: -0.6455\n",
      "Epoch 50 train loss: 0.3794, eval loss 0.6874474883079529\n",
      "optimal threshold: -0.6514\n",
      "Epoch 51 train loss: 0.4089, eval loss 0.6867192387580872\n",
      "optimal threshold: -0.6518\n",
      "Epoch 52 train loss: 0.4767, eval loss 0.6859747767448425\n",
      "optimal threshold: -0.6531\n",
      "Epoch 53 train loss: 0.4523, eval loss 0.6852759122848511\n",
      "optimal threshold: -0.6866\n",
      "Epoch 54 train loss: 0.4462, eval loss 0.6846622824668884\n",
      "optimal threshold: -0.6873\n",
      "Epoch 55 train loss: 0.4150, eval loss 0.6840125918388367\n",
      "optimal threshold: -0.9829\n",
      "Epoch 56 train loss: 0.3647, eval loss 0.683375895023346\n",
      "optimal threshold: -0.9981\n",
      "Epoch 57 train loss: 0.5592, eval loss 0.6828320622444153\n",
      "optimal threshold: -0.9971\n",
      "Epoch 58 train loss: 0.4507, eval loss 0.6822675466537476\n",
      "optimal threshold: -0.9993\n",
      "Epoch 59 train loss: 0.4035, eval loss 0.681699812412262\n",
      "optimal threshold: -0.9705\n",
      "Epoch 60 train loss: 0.4126, eval loss 0.6811274886131287\n",
      "optimal threshold: -0.9579\n",
      "Epoch 61 train loss: 0.4353, eval loss 0.6806121468544006\n",
      "optimal threshold: -0.7791\n",
      "Epoch 62 train loss: 0.4562, eval loss 0.6801333427429199\n",
      "optimal threshold: -0.7944\n",
      "Epoch 63 train loss: 0.3873, eval loss 0.6796374917030334\n",
      "optimal threshold: -0.7830\n",
      "Epoch 64 train loss: 0.4025, eval loss 0.6791685223579407\n",
      "optimal threshold: -0.8040\n",
      "Epoch 65 train loss: 0.4332, eval loss 0.6787526607513428\n",
      "optimal threshold: -0.7971\n",
      "Epoch 66 train loss: 0.4282, eval loss 0.678309977054596\n",
      "optimal threshold: -0.7892\n",
      "Epoch 67 train loss: 0.3467, eval loss 0.677794337272644\n",
      "optimal threshold: -1.0004\n",
      "Epoch 68 train loss: 0.3643, eval loss 0.6774141788482666\n",
      "optimal threshold: -0.9996\n",
      "Epoch 69 train loss: 0.4373, eval loss 0.6769737005233765\n",
      "optimal threshold: -0.4867\n",
      "Epoch 70 train loss: 0.3825, eval loss 0.6766053438186646\n",
      "optimal threshold: -0.4841\n",
      "Epoch 71 train loss: 0.4868, eval loss 0.6761994957923889\n",
      "optimal threshold: -0.4898\n",
      "Epoch 72 train loss: 0.4419, eval loss 0.6758908033370972\n",
      "optimal threshold: -0.4971\n",
      "Epoch 73 train loss: 0.6247, eval loss 0.6755828261375427\n",
      "optimal threshold: -0.5010\n",
      "Epoch 74 train loss: 0.5369, eval loss 0.6752544641494751\n",
      "optimal threshold: -0.8307\n",
      "Epoch 75 train loss: 0.5045, eval loss 0.6748287677764893\n",
      "optimal threshold: -0.8263\n",
      "Epoch 76 train loss: 0.3624, eval loss 0.6745100021362305\n",
      "optimal threshold: -0.7742\n",
      "Epoch 77 train loss: 0.5057, eval loss 0.6741766929626465\n",
      "optimal threshold: -0.7768\n",
      "Epoch 78 train loss: 0.5165, eval loss 0.6739257574081421\n",
      "optimal threshold: -0.7742\n",
      "Epoch 79 train loss: 0.5640, eval loss 0.6736226677894592\n",
      "optimal threshold: -0.7797\n",
      "Epoch 80 train loss: 0.3404, eval loss 0.6733506917953491\n",
      "optimal threshold: -0.4924\n",
      "Epoch 81 train loss: 0.4163, eval loss 0.6730894446372986\n",
      "optimal threshold: -1.0554\n",
      "Epoch 82 train loss: 0.5853, eval loss 0.6728314161300659\n",
      "optimal threshold: -0.3601\n",
      "Epoch 83 train loss: 0.4075, eval loss 0.6726022958755493\n",
      "optimal threshold: -0.3749\n",
      "Epoch 84 train loss: 0.4543, eval loss 0.6723906993865967\n",
      "optimal threshold: -0.3731\n",
      "Epoch 85 train loss: 0.4443, eval loss 0.6721270680427551\n",
      "optimal threshold: -0.3702\n",
      "Epoch 86 train loss: 0.3191, eval loss 0.6718984246253967\n",
      "optimal threshold: -0.3646\n",
      "Epoch 87 train loss: 0.4504, eval loss 0.6716046929359436\n",
      "optimal threshold: -0.5178\n",
      "Epoch 88 train loss: 0.5656, eval loss 0.671386182308197\n",
      "optimal threshold: -0.5111\n",
      "Epoch 89 train loss: 0.3571, eval loss 0.6711964011192322\n",
      "optimal threshold: -0.5138\n",
      "Epoch 90 train loss: 0.5044, eval loss 0.6709845662117004\n",
      "optimal threshold: -0.5066\n",
      "Epoch 91 train loss: 0.4587, eval loss 0.6707468032836914\n",
      "optimal threshold: -0.5009\n",
      "Epoch 92 train loss: 0.4986, eval loss 0.6705604791641235\n",
      "optimal threshold: -0.5056\n",
      "Epoch 93 train loss: 0.4565, eval loss 0.6703715324401855\n",
      "optimal threshold: -0.5029\n",
      "Epoch 94 train loss: 0.3173, eval loss 0.6701619625091553\n",
      "optimal threshold: -0.5033\n",
      "Epoch 95 train loss: 0.4652, eval loss 0.6699810028076172\n",
      "optimal threshold: -0.5029\n",
      "Epoch 96 train loss: 0.3429, eval loss 0.6698064804077148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.3657\n",
      "Epoch 97 train loss: 0.4890, eval loss 0.6696261763572693\n",
      "optimal threshold: -0.3637\n",
      "Epoch 98 train loss: 0.5414, eval loss 0.6694664359092712\n",
      "optimal threshold: -0.3600\n",
      "Epoch 99 train loss: 0.3071, eval loss 0.6692376732826233\n",
      "optimal threshold: -0.3584\n",
      "Epoch 100 train loss: 0.3776, eval loss 0.6691231727600098\n",
      "optimal threshold: -0.6888\n",
      "Epoch 101 train loss: 0.3811, eval loss 0.6689718961715698\n",
      "optimal threshold: -0.3481\n",
      "Epoch 102 train loss: 0.4264, eval loss 0.6687666177749634\n",
      "optimal threshold: -0.6859\n",
      "Epoch 103 train loss: 0.4505, eval loss 0.6686504483222961\n",
      "optimal threshold: -0.6834\n",
      "Epoch 104 train loss: 0.3427, eval loss 0.6685117483139038\n",
      "optimal threshold: -0.6795\n",
      "Epoch 105 train loss: 0.6830, eval loss 0.6683938503265381\n",
      "optimal threshold: -0.6805\n",
      "Epoch 106 train loss: 0.4917, eval loss 0.668235182762146\n",
      "optimal threshold: -0.6787\n",
      "Epoch 107 train loss: 0.3055, eval loss 0.6680692434310913\n",
      "optimal threshold: -0.6808\n",
      "Epoch 108 train loss: 0.6153, eval loss 0.6679803133010864\n",
      "optimal threshold: -0.6771\n",
      "Epoch 109 train loss: 0.2838, eval loss 0.6678003072738647\n",
      "optimal threshold: -0.6794\n",
      "Epoch 110 train loss: 0.4016, eval loss 0.667672336101532\n",
      "optimal threshold: -0.6766\n",
      "Epoch 111 train loss: 0.3531, eval loss 0.6675299406051636\n",
      "optimal threshold: -0.6845\n",
      "Epoch 112 train loss: 0.4944, eval loss 0.6674418449401855\n",
      "optimal threshold: -0.6824\n",
      "Epoch 113 train loss: 0.3972, eval loss 0.66729336977005\n",
      "optimal threshold: -0.6487\n",
      "Epoch 114 train loss: 0.3742, eval loss 0.6671529412269592\n",
      "optimal threshold: -0.6344\n",
      "Epoch 115 train loss: 0.4079, eval loss 0.6670180559158325\n",
      "optimal threshold: -0.4480\n",
      "Epoch 116 train loss: 0.2783, eval loss 0.6668376922607422\n",
      "optimal threshold: -0.7358\n",
      "Epoch 117 train loss: 0.4215, eval loss 0.6667464375495911\n",
      "optimal threshold: -0.7378\n",
      "Epoch 118 train loss: 0.5409, eval loss 0.6666644215583801\n",
      "optimal threshold: -0.7354\n",
      "Epoch 119 train loss: 0.3942, eval loss 0.6665570139884949\n",
      "optimal threshold: -0.7150\n",
      "Epoch 120 train loss: 0.3211, eval loss 0.6664440035820007\n",
      "optimal threshold: -0.7252\n",
      "Epoch 121 train loss: 0.4601, eval loss 0.6663320064544678\n",
      "optimal threshold: -0.7228\n",
      "Epoch 122 train loss: 0.3653, eval loss 0.6662567853927612\n",
      "optimal threshold: -0.5812\n",
      "Epoch 123 train loss: 0.4596, eval loss 0.6661694049835205\n",
      "optimal threshold: -0.7342\n",
      "Epoch 124 train loss: 0.4849, eval loss 0.6660374402999878\n",
      "optimal threshold: -0.5822\n",
      "Epoch 125 train loss: 0.3546, eval loss 0.6659594774246216\n",
      "optimal threshold: -0.4374\n",
      "Epoch 126 train loss: 0.3078, eval loss 0.6658188700675964\n",
      "optimal threshold: -0.4353\n",
      "Epoch 127 train loss: 0.3526, eval loss 0.6657121777534485\n",
      "optimal threshold: -0.5701\n",
      "Epoch 128 train loss: 0.4010, eval loss 0.6655904650688171\n",
      "optimal threshold: -0.5723\n",
      "Epoch 129 train loss: 0.3786, eval loss 0.6655314564704895\n",
      "optimal threshold: -0.5727\n",
      "Epoch 130 train loss: 0.3622, eval loss 0.6654564142227173\n",
      "optimal threshold: -0.5703\n",
      "Epoch 131 train loss: 0.4441, eval loss 0.6653230786323547\n",
      "optimal threshold: -0.5700\n",
      "Epoch 132 train loss: 0.3779, eval loss 0.6652635931968689\n",
      "optimal threshold: -0.5741\n",
      "Epoch 133 train loss: 0.5015, eval loss 0.6652200222015381\n",
      "optimal threshold: -0.5729\n",
      "Epoch 134 train loss: 0.4160, eval loss 0.6650970578193665\n",
      "optimal threshold: -0.5736\n",
      "Epoch 135 train loss: 0.4317, eval loss 0.6650089025497437\n",
      "optimal threshold: -0.5756\n",
      "Epoch 136 train loss: 0.4107, eval loss 0.6649126410484314\n",
      "optimal threshold: -0.5773\n",
      "Epoch 137 train loss: 0.3901, eval loss 0.6647621393203735\n",
      "optimal threshold: -0.5754\n",
      "Epoch 138 train loss: 0.3563, eval loss 0.6646760106086731\n",
      "optimal threshold: -0.5816\n",
      "Epoch 139 train loss: 0.4015, eval loss 0.6646831035614014\n",
      "optimal threshold: -0.5729\n",
      "Epoch 140 train loss: 0.4241, eval loss 0.6645838022232056\n",
      "optimal threshold: -0.5826\n",
      "Epoch 141 train loss: 0.3216, eval loss 0.6645127534866333\n",
      "optimal threshold: -0.5683\n",
      "Epoch 142 train loss: 0.3296, eval loss 0.6643917560577393\n",
      "optimal threshold: -0.5739\n",
      "Epoch 143 train loss: 0.3890, eval loss 0.6643664836883545\n",
      "optimal threshold: -0.5835\n",
      "Epoch 144 train loss: 0.3067, eval loss 0.6642581224441528\n",
      "optimal threshold: -0.5956\n",
      "Epoch 145 train loss: 0.3973, eval loss 0.6641843318939209\n",
      "optimal threshold: -0.5945\n",
      "Epoch 146 train loss: 0.4075, eval loss 0.6640762686729431\n",
      "optimal threshold: -0.4652\n",
      "Epoch 147 train loss: 0.4030, eval loss 0.6640292406082153\n",
      "optimal threshold: -0.4728\n",
      "Epoch 148 train loss: 0.3509, eval loss 0.6640377044677734\n",
      "optimal threshold: -0.4739\n",
      "Epoch 149 train loss: 0.4447, eval loss 0.6639676690101624\n",
      "optimal threshold: -0.6122\n",
      "Epoch 150 train loss: 0.3394, eval loss 0.6638673543930054\n",
      "optimal threshold: -0.4718\n",
      "Epoch 151 train loss: 0.4717, eval loss 0.6638120412826538\n",
      "optimal threshold: -0.6011\n",
      "Epoch 152 train loss: 0.2487, eval loss 0.6637520790100098\n",
      "optimal threshold: -0.6059\n",
      "Epoch 153 train loss: 0.4654, eval loss 0.6636326313018799\n",
      "optimal threshold: -0.6048\n",
      "Epoch 154 train loss: 0.3548, eval loss 0.6635568141937256\n",
      "optimal threshold: -0.4668\n",
      "Epoch 155 train loss: 0.3148, eval loss 0.6634751558303833\n",
      "optimal threshold: -0.5957\n",
      "Epoch 156 train loss: 0.3261, eval loss 0.6634311079978943\n",
      "optimal threshold: -0.5930\n",
      "Epoch 157 train loss: 0.3402, eval loss 0.6633727550506592\n",
      "optimal threshold: -0.4911\n",
      "Epoch 158 train loss: 0.3611, eval loss 0.6634073853492737\n",
      "optimal threshold: -0.4830\n",
      "Epoch 159 train loss: 0.3704, eval loss 0.6633156538009644\n",
      "optimal threshold: -0.4861\n",
      "Epoch 160 train loss: 0.3160, eval loss 0.6633080244064331\n",
      "optimal threshold: -0.5114\n",
      "Epoch 161 train loss: 0.4758, eval loss 0.6632097959518433\n",
      "optimal threshold: -0.4883\n",
      "Epoch 162 train loss: 0.5402, eval loss 0.6631542444229126\n",
      "optimal threshold: -0.6247\n",
      "Epoch 163 train loss: 0.3270, eval loss 0.663090169429779\n",
      "optimal threshold: -0.6280\n",
      "Epoch 164 train loss: 0.3179, eval loss 0.663062572479248\n",
      "optimal threshold: -0.6226\n",
      "Epoch 165 train loss: 0.4063, eval loss 0.6629547476768494\n",
      "optimal threshold: -0.4556\n",
      "Epoch 166 train loss: 0.3207, eval loss 0.6629197001457214\n",
      "optimal threshold: -0.4518\n",
      "Epoch 167 train loss: 0.3102, eval loss 0.6628067493438721\n",
      "optimal threshold: -0.6299\n",
      "Epoch 168 train loss: 0.3455, eval loss 0.662771463394165\n",
      "optimal threshold: -0.4597\n",
      "Epoch 169 train loss: 0.3238, eval loss 0.6627475023269653\n",
      "optimal threshold: -0.4755\n",
      "Epoch 170 train loss: 0.3375, eval loss 0.6626218557357788\n",
      "optimal threshold: -0.4772\n",
      "Epoch 171 train loss: 0.3960, eval loss 0.6625945568084717\n",
      "optimal threshold: -0.4813\n",
      "Epoch 172 train loss: 0.3838, eval loss 0.6625075936317444\n",
      "optimal threshold: -0.4792\n",
      "Epoch 173 train loss: 0.4074, eval loss 0.6624470353126526\n",
      "optimal threshold: -0.4782\n",
      "Epoch 174 train loss: 0.3336, eval loss 0.6623489856719971\n",
      "optimal threshold: -0.4770\n",
      "Epoch 175 train loss: 0.3049, eval loss 0.6622713208198547\n",
      "optimal threshold: -0.4786\n",
      "Epoch 176 train loss: 0.3664, eval loss 0.6622425317764282\n",
      "optimal threshold: -0.4731\n",
      "Epoch 177 train loss: 0.5300, eval loss 0.6622307896614075\n",
      "optimal threshold: -0.4732\n",
      "Epoch 178 train loss: 0.3497, eval loss 0.6622125506401062\n",
      "optimal threshold: -0.6195\n",
      "Epoch 179 train loss: 0.3063, eval loss 0.6621399521827698\n",
      "optimal threshold: -0.6128\n",
      "Epoch 180 train loss: 0.3214, eval loss 0.6620974540710449\n",
      "optimal threshold: -0.6212\n",
      "Epoch 181 train loss: 0.3848, eval loss 0.662112295627594\n",
      "optimal threshold: -0.6170\n",
      "Epoch 182 train loss: 0.3086, eval loss 0.6620197296142578\n",
      "optimal threshold: -0.6108\n",
      "Epoch 183 train loss: 0.4197, eval loss 0.6619856357574463\n",
      "optimal threshold: -0.6116\n",
      "Epoch 184 train loss: 0.4134, eval loss 0.6619541049003601\n",
      "optimal threshold: -0.4683\n",
      "Epoch 185 train loss: 0.3040, eval loss 0.6619724631309509\n",
      "optimal threshold: -0.4639\n",
      "Epoch 186 train loss: 0.3226, eval loss 0.6619089841842651\n",
      "optimal threshold: -0.4578\n",
      "Epoch 187 train loss: 0.3346, eval loss 0.6618380546569824\n",
      "optimal threshold: -0.4559\n",
      "Epoch 188 train loss: 0.5289, eval loss 0.6617966890335083\n",
      "optimal threshold: -0.4557\n",
      "Epoch 189 train loss: 0.3978, eval loss 0.6617696285247803\n",
      "optimal threshold: -0.4564\n",
      "Epoch 190 train loss: 0.3329, eval loss 0.6617313027381897\n",
      "optimal threshold: -0.4508\n",
      "Epoch 191 train loss: 0.3650, eval loss 0.6616448760032654\n",
      "optimal threshold: -0.4601\n",
      "Epoch 192 train loss: 0.3204, eval loss 0.6616420745849609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4626\n",
      "Epoch 193 train loss: 0.4488, eval loss 0.661625325679779\n",
      "optimal threshold: -0.4691\n",
      "Epoch 194 train loss: 0.2791, eval loss 0.6616429090499878\n",
      "optimal threshold: -0.4676\n",
      "Epoch 195 train loss: 0.3002, eval loss 0.6616055369377136\n",
      "optimal threshold: -0.4674\n",
      "Epoch 196 train loss: 0.4066, eval loss 0.6615699529647827\n",
      "optimal threshold: -0.4652\n",
      "Epoch 197 train loss: 0.4242, eval loss 0.6615081429481506\n",
      "optimal threshold: -0.4658\n",
      "Epoch 198 train loss: 0.3186, eval loss 0.6614753603935242\n",
      "optimal threshold: -0.4682\n",
      "Epoch 199 train loss: 0.4107, eval loss 0.6614677906036377\n",
      "optimal threshold: -0.4694\n",
      "Epoch 200 train loss: 0.3294, eval loss 0.6613965034484863\n",
      "optimal threshold: -0.4585\n",
      "Epoch 201 train loss: 0.3503, eval loss 0.6613926291465759\n",
      "optimal threshold: -0.4546\n",
      "Epoch 202 train loss: 0.3598, eval loss 0.6612523198127747\n",
      "optimal threshold: -0.6378\n",
      "Epoch 203 train loss: 0.4409, eval loss 0.6611708998680115\n",
      "optimal threshold: -0.6396\n",
      "Epoch 204 train loss: 0.3581, eval loss 0.6611437797546387\n",
      "optimal threshold: -0.6373\n",
      "Epoch 205 train loss: 0.4991, eval loss 0.6610963344573975\n",
      "optimal threshold: -0.4605\n",
      "Epoch 206 train loss: 0.4058, eval loss 0.6611310243606567\n",
      "optimal threshold: -0.6387\n",
      "Epoch 207 train loss: 0.2590, eval loss 0.6610373854637146\n",
      "optimal threshold: -0.4597\n",
      "Epoch 208 train loss: 0.3825, eval loss 0.6610783934593201\n",
      "optimal threshold: -0.4620\n",
      "Epoch 209 train loss: 0.3625, eval loss 0.6610744595527649\n",
      "optimal threshold: -0.6482\n",
      "Epoch 210 train loss: 0.3964, eval loss 0.6610531210899353\n",
      "optimal threshold: -0.4622\n",
      "Epoch 211 train loss: 0.4094, eval loss 0.6610223650932312\n",
      "optimal threshold: -0.4609\n",
      "Epoch 212 train loss: 0.3304, eval loss 0.6609201431274414\n",
      "optimal threshold: -0.6409\n",
      "Epoch 213 train loss: 0.3360, eval loss 0.6609152555465698\n",
      "optimal threshold: -0.4624\n",
      "Epoch 214 train loss: 0.3357, eval loss 0.6608495116233826\n",
      "optimal threshold: -0.4639\n",
      "Epoch 215 train loss: 0.3495, eval loss 0.6608666181564331\n",
      "optimal threshold: -0.6357\n",
      "Epoch 216 train loss: 0.5356, eval loss 0.6607864499092102\n",
      "optimal threshold: -0.6326\n",
      "Epoch 217 train loss: 0.4236, eval loss 0.6607007384300232\n",
      "optimal threshold: -0.4637\n",
      "Epoch 218 train loss: 0.3095, eval loss 0.6607003808021545\n",
      "optimal threshold: -0.4638\n",
      "Epoch 219 train loss: 0.4418, eval loss 0.6606488823890686\n",
      "optimal threshold: -0.4594\n",
      "Epoch 220 train loss: 0.3225, eval loss 0.6606041193008423\n",
      "optimal threshold: -0.4557\n",
      "Epoch 221 train loss: 0.3822, eval loss 0.6605484485626221\n",
      "optimal threshold: -0.4572\n",
      "Epoch 222 train loss: 0.3798, eval loss 0.6605304479598999\n",
      "optimal threshold: -0.4714\n",
      "Epoch 223 train loss: 0.3940, eval loss 0.6604971289634705\n",
      "optimal threshold: -0.4709\n",
      "Epoch 224 train loss: 0.4940, eval loss 0.6604801416397095\n",
      "optimal threshold: -0.4685\n",
      "Epoch 225 train loss: 0.3425, eval loss 0.6604744791984558\n",
      "optimal threshold: -0.4632\n",
      "Epoch 226 train loss: 0.4574, eval loss 0.660407304763794\n",
      "optimal threshold: -0.4619\n",
      "Epoch 227 train loss: 0.3542, eval loss 0.6603767275810242\n",
      "optimal threshold: -0.4635\n",
      "Epoch 228 train loss: 0.3526, eval loss 0.6603789329528809\n",
      "optimal threshold: -0.4651\n",
      "Epoch 229 train loss: 0.4214, eval loss 0.6603856086730957\n",
      "optimal threshold: -0.4639\n",
      "Epoch 230 train loss: 0.3310, eval loss 0.660320520401001\n",
      "optimal threshold: -0.4496\n",
      "Epoch 231 train loss: 0.2791, eval loss 0.660286009311676\n",
      "optimal threshold: -0.4472\n",
      "Epoch 232 train loss: 0.4709, eval loss 0.6601995229721069\n",
      "optimal threshold: -0.4475\n",
      "Epoch 233 train loss: 0.3171, eval loss 0.6601839661598206\n",
      "optimal threshold: -0.4466\n",
      "Epoch 234 train loss: 0.4131, eval loss 0.6601295471191406\n",
      "optimal threshold: -0.4474\n",
      "Epoch 235 train loss: 0.3208, eval loss 0.6600895524024963\n",
      "optimal threshold: -0.4777\n",
      "Epoch 236 train loss: 0.3291, eval loss 0.6600747108459473\n",
      "optimal threshold: -0.4508\n",
      "Epoch 237 train loss: 0.5055, eval loss 0.6600674986839294\n",
      "optimal threshold: -0.4422\n",
      "Epoch 238 train loss: 0.3288, eval loss 0.6600149869918823\n",
      "optimal threshold: -0.4788\n",
      "Epoch 239 train loss: 0.3462, eval loss 0.6600351929664612\n",
      "optimal threshold: -0.4488\n",
      "Epoch 240 train loss: 0.4729, eval loss 0.659989595413208\n",
      "optimal threshold: -0.4815\n",
      "Epoch 241 train loss: 0.3191, eval loss 0.6599864363670349\n",
      "optimal threshold: -0.4755\n",
      "Epoch 242 train loss: 0.3513, eval loss 0.6599662899971008\n",
      "optimal threshold: -0.4731\n",
      "Epoch 243 train loss: 0.4897, eval loss 0.6599173545837402\n",
      "optimal threshold: -0.4737\n",
      "Epoch 244 train loss: 0.2982, eval loss 0.6598280668258667\n",
      "optimal threshold: -0.4699\n",
      "Epoch 245 train loss: 0.3945, eval loss 0.6597883105278015\n",
      "optimal threshold: -0.4771\n",
      "Epoch 246 train loss: 0.3151, eval loss 0.6598232984542847\n",
      "optimal threshold: -0.4704\n",
      "Epoch 247 train loss: 0.3444, eval loss 0.6597604751586914\n",
      "optimal threshold: -0.4724\n",
      "Epoch 248 train loss: 0.3236, eval loss 0.6597318649291992\n",
      "optimal threshold: -0.4732\n",
      "Epoch 249 train loss: 0.3311, eval loss 0.6597290635108948\n",
      "optimal threshold: -0.4710\n",
      "Epoch 250 train loss: 0.2990, eval loss 0.6596997380256653\n",
      "optimal threshold: -0.4763\n",
      "Epoch 251 train loss: 0.3440, eval loss 0.6597127914428711\n",
      "optimal threshold: -0.4772\n",
      "Epoch 252 train loss: 0.3701, eval loss 0.6596775054931641\n",
      "optimal threshold: -0.4725\n",
      "Epoch 253 train loss: 0.3496, eval loss 0.6596032381057739\n",
      "optimal threshold: -0.4740\n",
      "Epoch 254 train loss: 0.3294, eval loss 0.6595770716667175\n",
      "optimal threshold: -0.4767\n",
      "Epoch 255 train loss: 0.2900, eval loss 0.6595919132232666\n",
      "optimal threshold: -0.7420\n",
      "Epoch 256 train loss: 0.3167, eval loss 0.6595755815505981\n",
      "optimal threshold: -0.7467\n",
      "Epoch 257 train loss: 0.3492, eval loss 0.6596004366874695\n",
      "optimal threshold: -0.7686\n",
      "Epoch 258 train loss: 0.2897, eval loss 0.6596232652664185\n",
      "optimal threshold: -0.7676\n",
      "Epoch 259 train loss: 0.3632, eval loss 0.6596235632896423\n",
      "optimal threshold: -0.7619\n",
      "Epoch 260 train loss: 0.4025, eval loss 0.6595451235771179\n",
      "optimal threshold: -0.7612\n",
      "Epoch 261 train loss: 0.3540, eval loss 0.6595196723937988\n",
      "optimal threshold: -0.7424\n",
      "Epoch 262 train loss: 0.2876, eval loss 0.6595168113708496\n",
      "optimal threshold: -0.7296\n",
      "Epoch 263 train loss: 0.3190, eval loss 0.6594439148902893\n",
      "optimal threshold: -0.7271\n",
      "Epoch 264 train loss: 0.4220, eval loss 0.6593785881996155\n",
      "optimal threshold: -0.7696\n",
      "Epoch 265 train loss: 0.3354, eval loss 0.6594032645225525\n",
      "optimal threshold: -0.7304\n",
      "Epoch 266 train loss: 0.3445, eval loss 0.6593773365020752\n",
      "optimal threshold: -0.7303\n",
      "Epoch 267 train loss: 0.3677, eval loss 0.6593806147575378\n",
      "optimal threshold: -0.7288\n",
      "Epoch 268 train loss: 0.2997, eval loss 0.6593474745750427\n",
      "optimal threshold: -0.7329\n",
      "Epoch 269 train loss: 0.3660, eval loss 0.6593204140663147\n",
      "optimal threshold: -0.7341\n",
      "Epoch 270 train loss: 0.2991, eval loss 0.6593500971794128\n",
      "optimal threshold: -0.7616\n",
      "Epoch 271 train loss: 0.3510, eval loss 0.6592805981636047\n",
      "optimal threshold: -0.7630\n",
      "Epoch 272 train loss: 0.3208, eval loss 0.6592874526977539\n",
      "optimal threshold: -0.7613\n",
      "Epoch 273 train loss: 0.2551, eval loss 0.6592758297920227\n",
      "optimal threshold: -0.7617\n",
      "Epoch 274 train loss: 0.3262, eval loss 0.6592149138450623\n",
      "optimal threshold: -0.7597\n",
      "Epoch 275 train loss: 0.3580, eval loss 0.6591778993606567\n",
      "optimal threshold: -0.7610\n",
      "Epoch 276 train loss: 0.3506, eval loss 0.6591910123825073\n",
      "optimal threshold: -0.7610\n",
      "Epoch 277 train loss: 0.2829, eval loss 0.6591752767562866\n",
      "optimal threshold: -0.7627\n",
      "Epoch 278 train loss: 0.3318, eval loss 0.6591750979423523\n",
      "optimal threshold: -0.7588\n",
      "Epoch 279 train loss: 0.3452, eval loss 0.659117579460144\n",
      "optimal threshold: -0.7603\n",
      "Epoch 280 train loss: 0.4290, eval loss 0.6591208577156067\n",
      "optimal threshold: -0.7592\n",
      "Epoch 281 train loss: 0.2914, eval loss 0.6590678691864014\n",
      "optimal threshold: -0.7617\n",
      "Epoch 282 train loss: 0.3292, eval loss 0.6590718626976013\n",
      "optimal threshold: -0.7571\n",
      "Epoch 283 train loss: 0.3857, eval loss 0.6590455770492554\n",
      "optimal threshold: -0.7265\n",
      "Epoch 284 train loss: 0.2934, eval loss 0.658983588218689\n",
      "optimal threshold: -0.7296\n",
      "Epoch 285 train loss: 0.2998, eval loss 0.6590180993080139\n",
      "optimal threshold: -0.7390\n",
      "Epoch 286 train loss: 0.3175, eval loss 0.6589978337287903\n",
      "optimal threshold: -0.7368\n",
      "Epoch 287 train loss: 0.3156, eval loss 0.6589774489402771\n",
      "optimal threshold: -0.7327\n",
      "Epoch 288 train loss: 0.3462, eval loss 0.6589124202728271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7351\n",
      "Epoch 289 train loss: 0.3222, eval loss 0.6588852405548096\n",
      "optimal threshold: -0.7352\n",
      "Epoch 290 train loss: 0.3014, eval loss 0.6589230298995972\n",
      "optimal threshold: -0.7348\n",
      "Epoch 291 train loss: 0.3111, eval loss 0.6588894724845886\n",
      "optimal threshold: -0.7305\n",
      "Epoch 292 train loss: 0.4201, eval loss 0.6589046120643616\n",
      "optimal threshold: -0.7305\n",
      "Epoch 293 train loss: 0.3782, eval loss 0.6588565707206726\n",
      "optimal threshold: -0.7273\n",
      "Epoch 294 train loss: 0.3174, eval loss 0.6588484644889832\n",
      "optimal threshold: -0.7350\n",
      "Epoch 295 train loss: 0.2972, eval loss 0.6588490605354309\n",
      "optimal threshold: -0.7332\n",
      "Epoch 296 train loss: 0.3113, eval loss 0.6588215827941895\n",
      "optimal threshold: -0.7321\n",
      "Epoch 297 train loss: 0.4089, eval loss 0.6587789058685303\n",
      "optimal threshold: -0.7307\n",
      "Epoch 298 train loss: 0.4044, eval loss 0.6587883830070496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 12:40:27,057] Trial 11 finished with value: 0.3034173846244812 and parameters: {'learning_rate_exp': -5.881798747138811, 'dropout_p': 0.31488662630043823, 'l2_reg_exp': -3.252748219580417, 'batch_size': 10, 'N': 350}. Best is trial 6 with value: 0.22441545128822327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7376\n",
      "Epoch 299 train loss: 0.3034, eval loss 0.6588298082351685\n",
      "optimal threshold: -0.2551\n",
      "Epoch 0 train loss: 1.3955, eval loss 1.4008153676986694\n",
      "optimal threshold: -0.3105\n",
      "Epoch 1 train loss: 1.3819, eval loss 1.3837519884109497\n",
      "optimal threshold: -0.1406\n",
      "Epoch 2 train loss: 1.3617, eval loss 1.36697518825531\n",
      "optimal threshold: -0.1700\n",
      "Epoch 3 train loss: 1.3428, eval loss 1.3503262996673584\n",
      "optimal threshold: -0.1983\n",
      "Epoch 4 train loss: 1.3325, eval loss 1.3337137699127197\n",
      "optimal threshold: -0.2237\n",
      "Epoch 5 train loss: 1.3105, eval loss 1.317132592201233\n",
      "optimal threshold: -0.2568\n",
      "Epoch 6 train loss: 1.2882, eval loss 1.300641655921936\n",
      "optimal threshold: -0.2827\n",
      "Epoch 7 train loss: 1.2828, eval loss 1.2842237949371338\n",
      "optimal threshold: -0.3182\n",
      "Epoch 8 train loss: 1.2514, eval loss 1.2679202556610107\n",
      "optimal threshold: -0.3513\n",
      "Epoch 9 train loss: 1.2405, eval loss 1.2518593072891235\n",
      "optimal threshold: -0.3841\n",
      "Epoch 10 train loss: 1.2321, eval loss 1.2361093759536743\n",
      "optimal threshold: -0.4137\n",
      "Epoch 11 train loss: 1.2184, eval loss 1.2206429243087769\n",
      "optimal threshold: -0.4481\n",
      "Epoch 12 train loss: 1.1963, eval loss 1.2055199146270752\n",
      "optimal threshold: -0.4957\n",
      "Epoch 13 train loss: 1.1690, eval loss 1.1908022165298462\n",
      "optimal threshold: -0.5264\n",
      "Epoch 14 train loss: 1.1685, eval loss 1.1763591766357422\n",
      "optimal threshold: -0.5597\n",
      "Epoch 15 train loss: 1.1639, eval loss 1.1622676849365234\n",
      "optimal threshold: -0.5922\n",
      "Epoch 16 train loss: 1.1561, eval loss 1.1485373973846436\n",
      "optimal threshold: -0.6137\n",
      "Epoch 17 train loss: 1.1184, eval loss 1.135108470916748\n",
      "optimal threshold: -0.6450\n",
      "Epoch 18 train loss: 1.1098, eval loss 1.121984601020813\n",
      "optimal threshold: -0.6758\n",
      "Epoch 19 train loss: 1.0899, eval loss 1.109142541885376\n",
      "optimal threshold: -0.6907\n",
      "Epoch 20 train loss: 1.0859, eval loss 1.0965713262557983\n",
      "optimal threshold: -0.6892\n",
      "Epoch 21 train loss: 1.0768, eval loss 1.0842081308364868\n",
      "optimal threshold: -0.7329\n",
      "Epoch 22 train loss: 1.0796, eval loss 1.072067141532898\n",
      "optimal threshold: -0.7580\n",
      "Epoch 23 train loss: 1.0502, eval loss 1.0601369142532349\n",
      "optimal threshold: -0.7768\n",
      "Epoch 24 train loss: 1.0417, eval loss 1.0483567714691162\n",
      "optimal threshold: -0.7958\n",
      "Epoch 25 train loss: 1.0500, eval loss 1.0368088483810425\n",
      "optimal threshold: -0.7880\n",
      "Epoch 26 train loss: 1.0346, eval loss 1.0254273414611816\n",
      "optimal threshold: -0.8144\n",
      "Epoch 27 train loss: 1.0006, eval loss 1.0142340660095215\n",
      "optimal threshold: -0.8253\n",
      "Epoch 28 train loss: 0.9799, eval loss 1.0032199621200562\n",
      "optimal threshold: -0.8386\n",
      "Epoch 29 train loss: 0.9813, eval loss 0.992327868938446\n",
      "optimal threshold: -0.8520\n",
      "Epoch 30 train loss: 0.9816, eval loss 0.9816128015518188\n",
      "optimal threshold: -0.8624\n",
      "Epoch 31 train loss: 0.9434, eval loss 0.9710980653762817\n",
      "optimal threshold: -0.8745\n",
      "Epoch 32 train loss: 0.9489, eval loss 0.9607518315315247\n",
      "optimal threshold: -0.8819\n",
      "Epoch 33 train loss: 0.9401, eval loss 0.9505999684333801\n",
      "optimal threshold: -0.9026\n",
      "Epoch 34 train loss: 0.9273, eval loss 0.9406237006187439\n",
      "optimal threshold: -0.9072\n",
      "Epoch 35 train loss: 0.9303, eval loss 0.9308336973190308\n",
      "optimal threshold: -0.9168\n",
      "Epoch 36 train loss: 0.9197, eval loss 0.9212848544120789\n",
      "optimal threshold: -0.9117\n",
      "Epoch 37 train loss: 0.9052, eval loss 0.9119323492050171\n",
      "optimal threshold: -0.9149\n",
      "Epoch 38 train loss: 0.8981, eval loss 0.9028051495552063\n",
      "optimal threshold: -0.9205\n",
      "Epoch 39 train loss: 0.8740, eval loss 0.8938688635826111\n",
      "optimal threshold: -0.9299\n",
      "Epoch 40 train loss: 0.8891, eval loss 0.8852226138114929\n",
      "optimal threshold: -0.9290\n",
      "Epoch 41 train loss: 0.8751, eval loss 0.8768295049667358\n",
      "optimal threshold: -0.9284\n",
      "Epoch 42 train loss: 0.8548, eval loss 0.8686791658401489\n",
      "optimal threshold: -0.9402\n",
      "Epoch 43 train loss: 0.8534, eval loss 0.8607943654060364\n",
      "optimal threshold: -0.9381\n",
      "Epoch 44 train loss: 0.8345, eval loss 0.8532492518424988\n",
      "optimal threshold: -0.9316\n",
      "Epoch 45 train loss: 0.8624, eval loss 0.8459489345550537\n",
      "optimal threshold: -0.9259\n",
      "Epoch 46 train loss: 0.8652, eval loss 0.8389067649841309\n",
      "optimal threshold: -0.9206\n",
      "Epoch 47 train loss: 0.8281, eval loss 0.8321247696876526\n",
      "optimal threshold: -0.9644\n",
      "Epoch 48 train loss: 0.8491, eval loss 0.8256426453590393\n",
      "optimal threshold: -0.9856\n",
      "Epoch 49 train loss: 0.8341, eval loss 0.8194512128829956\n",
      "optimal threshold: -0.9659\n",
      "Epoch 50 train loss: 0.8178, eval loss 0.8135091662406921\n",
      "optimal threshold: -0.9614\n",
      "Epoch 51 train loss: 0.8084, eval loss 0.8078046441078186\n",
      "optimal threshold: -0.9586\n",
      "Epoch 52 train loss: 0.7921, eval loss 0.8024173378944397\n",
      "optimal threshold: -0.9554\n",
      "Epoch 53 train loss: 0.7814, eval loss 0.7972740530967712\n",
      "optimal threshold: -0.9524\n",
      "Epoch 54 train loss: 0.7914, eval loss 0.7924280762672424\n",
      "optimal threshold: -0.9389\n",
      "Epoch 55 train loss: 0.7883, eval loss 0.7877667546272278\n",
      "optimal threshold: -0.9330\n",
      "Epoch 56 train loss: 0.8123, eval loss 0.7833846807479858\n",
      "optimal threshold: -0.9278\n",
      "Epoch 57 train loss: 0.8182, eval loss 0.7792515158653259\n",
      "optimal threshold: -0.9264\n",
      "Epoch 58 train loss: 0.7839, eval loss 0.775352954864502\n",
      "optimal threshold: -0.9284\n",
      "Epoch 59 train loss: 0.7761, eval loss 0.7716919779777527\n",
      "optimal threshold: -0.9238\n",
      "Epoch 60 train loss: 0.7885, eval loss 0.7681986689567566\n",
      "optimal threshold: -0.9258\n",
      "Epoch 61 train loss: 0.7824, eval loss 0.7649016976356506\n",
      "optimal threshold: -0.9055\n",
      "Epoch 62 train loss: 0.7727, eval loss 0.7618141770362854\n",
      "optimal threshold: -0.9159\n",
      "Epoch 63 train loss: 0.7761, eval loss 0.758862316608429\n",
      "optimal threshold: -0.9243\n",
      "Epoch 64 train loss: 0.7409, eval loss 0.7560771107673645\n",
      "optimal threshold: -0.9267\n",
      "Epoch 65 train loss: 0.7729, eval loss 0.7534705400466919\n",
      "optimal threshold: -0.9219\n",
      "Epoch 66 train loss: 0.7857, eval loss 0.7510300278663635\n",
      "optimal threshold: -0.9182\n",
      "Epoch 67 train loss: 0.7691, eval loss 0.7487010359764099\n",
      "optimal threshold: -0.9163\n",
      "Epoch 68 train loss: 0.7721, eval loss 0.7465237975120544\n",
      "optimal threshold: -0.9243\n",
      "Epoch 69 train loss: 0.7845, eval loss 0.7444882988929749\n",
      "optimal threshold: -0.9292\n",
      "Epoch 70 train loss: 0.7522, eval loss 0.7425667643547058\n",
      "optimal threshold: -0.9135\n",
      "Epoch 71 train loss: 0.7565, eval loss 0.7407698631286621\n",
      "optimal threshold: -0.9033\n",
      "Epoch 72 train loss: 0.7978, eval loss 0.7390300631523132\n",
      "optimal threshold: -0.9102\n",
      "Epoch 73 train loss: 0.7977, eval loss 0.7374227046966553\n",
      "optimal threshold: -0.9130\n",
      "Epoch 74 train loss: 0.7764, eval loss 0.7359034419059753\n",
      "optimal threshold: -0.9297\n",
      "Epoch 75 train loss: 0.7638, eval loss 0.7344391345977783\n",
      "optimal threshold: -0.9371\n",
      "Epoch 76 train loss: 0.7457, eval loss 0.7330335974693298\n",
      "optimal threshold: -0.9403\n",
      "Epoch 77 train loss: 0.7694, eval loss 0.731701135635376\n",
      "optimal threshold: -0.9285\n",
      "Epoch 78 train loss: 0.7767, eval loss 0.7304359674453735\n",
      "optimal threshold: -0.9224\n",
      "Epoch 79 train loss: 0.7939, eval loss 0.72927325963974\n",
      "optimal threshold: -0.9188\n",
      "Epoch 80 train loss: 0.7560, eval loss 0.7281273007392883\n",
      "optimal threshold: -0.9163\n",
      "Epoch 81 train loss: 0.7409, eval loss 0.7270452976226807\n",
      "optimal threshold: -0.9118\n",
      "Epoch 82 train loss: 0.7523, eval loss 0.726006805896759\n",
      "optimal threshold: -0.9119\n",
      "Epoch 83 train loss: 0.7298, eval loss 0.7250173687934875\n",
      "optimal threshold: -0.9187\n",
      "Epoch 84 train loss: 0.7478, eval loss 0.7240676879882812\n",
      "optimal threshold: -0.9122\n",
      "Epoch 85 train loss: 0.7459, eval loss 0.7231577038764954\n",
      "optimal threshold: -0.9121\n",
      "Epoch 86 train loss: 0.7497, eval loss 0.7222924828529358\n",
      "optimal threshold: -0.9134\n",
      "Epoch 87 train loss: 0.7581, eval loss 0.7214515209197998\n",
      "optimal threshold: -0.9108\n",
      "Epoch 88 train loss: 0.7642, eval loss 0.7206346392631531\n",
      "optimal threshold: -0.9140\n",
      "Epoch 89 train loss: 0.7386, eval loss 0.7198365926742554\n",
      "optimal threshold: -0.9152\n",
      "Epoch 90 train loss: 0.7255, eval loss 0.7190667986869812\n",
      "optimal threshold: -0.9190\n",
      "Epoch 91 train loss: 0.7392, eval loss 0.7183277606964111\n",
      "optimal threshold: -0.9087\n",
      "Epoch 92 train loss: 0.7705, eval loss 0.7175893187522888\n",
      "optimal threshold: -0.9084\n",
      "Epoch 93 train loss: 0.7548, eval loss 0.7168826460838318\n",
      "optimal threshold: -0.8935\n",
      "Epoch 94 train loss: 0.7540, eval loss 0.7162120938301086\n",
      "optimal threshold: -0.8945\n",
      "Epoch 95 train loss: 0.7415, eval loss 0.7155478596687317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.8963\n",
      "Epoch 96 train loss: 0.7690, eval loss 0.7149087190628052\n",
      "optimal threshold: -0.8900\n",
      "Epoch 97 train loss: 0.7357, eval loss 0.7142838835716248\n",
      "optimal threshold: -0.8901\n",
      "Epoch 98 train loss: 0.7295, eval loss 0.7136462330818176\n",
      "optimal threshold: -0.9310\n",
      "Epoch 99 train loss: 0.7652, eval loss 0.7130635380744934\n",
      "optimal threshold: -0.9309\n",
      "Epoch 100 train loss: 0.7605, eval loss 0.7124568223953247\n",
      "optimal threshold: -0.8515\n",
      "Epoch 101 train loss: 0.7210, eval loss 0.7118552327156067\n",
      "optimal threshold: -0.9307\n",
      "Epoch 102 train loss: 0.7316, eval loss 0.7112652659416199\n",
      "optimal threshold: -0.9328\n",
      "Epoch 103 train loss: 0.7425, eval loss 0.710720419883728\n",
      "optimal threshold: -0.9285\n",
      "Epoch 104 train loss: 0.7141, eval loss 0.7101730108261108\n",
      "optimal threshold: -0.9266\n",
      "Epoch 105 train loss: 0.7215, eval loss 0.7096011638641357\n",
      "optimal threshold: -0.9381\n",
      "Epoch 106 train loss: 0.7471, eval loss 0.7090686559677124\n",
      "optimal threshold: -0.9437\n",
      "Epoch 107 train loss: 0.7522, eval loss 0.7085394263267517\n",
      "optimal threshold: -0.9330\n",
      "Epoch 108 train loss: 0.7765, eval loss 0.7080173492431641\n",
      "optimal threshold: -0.9318\n",
      "Epoch 109 train loss: 0.7733, eval loss 0.7074984312057495\n",
      "optimal threshold: -0.9388\n",
      "Epoch 110 train loss: 0.7152, eval loss 0.7070199847221375\n",
      "optimal threshold: -0.9559\n",
      "Epoch 111 train loss: 0.8031, eval loss 0.7065020799636841\n",
      "optimal threshold: -0.9527\n",
      "Epoch 112 train loss: 0.7601, eval loss 0.7060418725013733\n",
      "optimal threshold: -0.9505\n",
      "Epoch 113 train loss: 0.7420, eval loss 0.7055391669273376\n",
      "optimal threshold: -0.7137\n",
      "Epoch 114 train loss: 0.7346, eval loss 0.7050538063049316\n",
      "optimal threshold: -0.6963\n",
      "Epoch 115 train loss: 0.7350, eval loss 0.7045923471450806\n",
      "optimal threshold: -0.7134\n",
      "Epoch 116 train loss: 0.7251, eval loss 0.7041187882423401\n",
      "optimal threshold: -0.7062\n",
      "Epoch 117 train loss: 0.7627, eval loss 0.703674852848053\n",
      "optimal threshold: -0.6906\n",
      "Epoch 118 train loss: 0.7365, eval loss 0.7032186985015869\n",
      "optimal threshold: -0.6891\n",
      "Epoch 119 train loss: 0.7795, eval loss 0.702753484249115\n",
      "optimal threshold: -0.7183\n",
      "Epoch 120 train loss: 0.7572, eval loss 0.7023468613624573\n",
      "optimal threshold: -0.7379\n",
      "Epoch 121 train loss: 0.7434, eval loss 0.701924741268158\n",
      "optimal threshold: -0.6113\n",
      "Epoch 122 train loss: 0.7630, eval loss 0.7014665603637695\n",
      "optimal threshold: -0.8359\n",
      "Epoch 123 train loss: 0.7410, eval loss 0.7010340094566345\n",
      "optimal threshold: -0.7274\n",
      "Epoch 124 train loss: 0.7498, eval loss 0.7006141543388367\n",
      "optimal threshold: -0.7620\n",
      "Epoch 125 train loss: 0.7347, eval loss 0.7002140879631042\n",
      "optimal threshold: -0.7621\n",
      "Epoch 126 train loss: 0.7479, eval loss 0.6998181939125061\n",
      "optimal threshold: -0.7774\n",
      "Epoch 127 train loss: 0.7259, eval loss 0.6993899941444397\n",
      "optimal threshold: -0.7728\n",
      "Epoch 128 train loss: 0.7248, eval loss 0.6989724636077881\n",
      "optimal threshold: -0.7769\n",
      "Epoch 129 train loss: 0.7499, eval loss 0.6985931396484375\n",
      "optimal threshold: -0.7852\n",
      "Epoch 130 train loss: 0.7497, eval loss 0.6982147097587585\n",
      "optimal threshold: -0.7838\n",
      "Epoch 131 train loss: 0.7367, eval loss 0.6978265643119812\n",
      "optimal threshold: -0.7947\n",
      "Epoch 132 train loss: 0.7965, eval loss 0.6974710822105408\n",
      "optimal threshold: -0.7905\n",
      "Epoch 133 train loss: 0.7265, eval loss 0.697064220905304\n",
      "optimal threshold: -0.7269\n",
      "Epoch 134 train loss: 0.7584, eval loss 0.6967231035232544\n",
      "optimal threshold: -0.7289\n",
      "Epoch 135 train loss: 0.7440, eval loss 0.6963812112808228\n",
      "optimal threshold: -0.7282\n",
      "Epoch 136 train loss: 0.7318, eval loss 0.6959978938102722\n",
      "optimal threshold: -0.7226\n",
      "Epoch 137 train loss: 0.7615, eval loss 0.6956227421760559\n",
      "optimal threshold: -0.7379\n",
      "Epoch 138 train loss: 0.7159, eval loss 0.695281445980072\n",
      "optimal threshold: -0.7344\n",
      "Epoch 139 train loss: 0.7314, eval loss 0.6949032545089722\n",
      "optimal threshold: -0.7130\n",
      "Epoch 140 train loss: 0.7703, eval loss 0.6945600509643555\n",
      "optimal threshold: -0.7095\n",
      "Epoch 141 train loss: 0.7349, eval loss 0.6941999793052673\n",
      "optimal threshold: -0.7103\n",
      "Epoch 142 train loss: 0.6996, eval loss 0.693880021572113\n",
      "optimal threshold: -0.7101\n",
      "Epoch 143 train loss: 0.7411, eval loss 0.6935549378395081\n",
      "optimal threshold: -0.7227\n",
      "Epoch 144 train loss: 0.7066, eval loss 0.6932207345962524\n",
      "optimal threshold: -0.7110\n",
      "Epoch 145 train loss: 0.7451, eval loss 0.6929130554199219\n",
      "optimal threshold: -0.7279\n",
      "Epoch 146 train loss: 0.7338, eval loss 0.6926165223121643\n",
      "optimal threshold: -0.7220\n",
      "Epoch 147 train loss: 0.7515, eval loss 0.6923025846481323\n",
      "optimal threshold: -0.9093\n",
      "Epoch 148 train loss: 0.7364, eval loss 0.6919816732406616\n",
      "optimal threshold: -0.7246\n",
      "Epoch 149 train loss: 0.7557, eval loss 0.6916961669921875\n",
      "optimal threshold: -0.7220\n",
      "Epoch 150 train loss: 0.7148, eval loss 0.6913754343986511\n",
      "optimal threshold: -0.7203\n",
      "Epoch 151 train loss: 0.7141, eval loss 0.6910605430603027\n",
      "optimal threshold: -0.7272\n",
      "Epoch 152 train loss: 0.7269, eval loss 0.6907543540000916\n",
      "optimal threshold: -0.7254\n",
      "Epoch 153 train loss: 0.7316, eval loss 0.6904651522636414\n",
      "optimal threshold: -0.7180\n",
      "Epoch 154 train loss: 0.7040, eval loss 0.6901645064353943\n",
      "optimal threshold: -0.7135\n",
      "Epoch 155 train loss: 0.7225, eval loss 0.6898711323738098\n",
      "optimal threshold: -0.7084\n",
      "Epoch 156 train loss: 0.7411, eval loss 0.689606785774231\n",
      "optimal threshold: -0.6987\n",
      "Epoch 157 train loss: 0.7177, eval loss 0.6893101334571838\n",
      "optimal threshold: -0.7094\n",
      "Epoch 158 train loss: 0.7352, eval loss 0.6890491247177124\n",
      "optimal threshold: -0.7053\n",
      "Epoch 159 train loss: 0.7090, eval loss 0.688778281211853\n",
      "optimal threshold: -0.7078\n",
      "Epoch 160 train loss: 0.7635, eval loss 0.6885042786598206\n",
      "optimal threshold: -0.7175\n",
      "Epoch 161 train loss: 0.7382, eval loss 0.688251793384552\n",
      "optimal threshold: -0.7015\n",
      "Epoch 162 train loss: 0.7209, eval loss 0.687953531742096\n",
      "optimal threshold: -0.7008\n",
      "Epoch 163 train loss: 0.7154, eval loss 0.6877167224884033\n",
      "optimal threshold: -0.7059\n",
      "Epoch 164 train loss: 0.7004, eval loss 0.6874661445617676\n",
      "optimal threshold: -0.6995\n",
      "Epoch 165 train loss: 0.7215, eval loss 0.6871919631958008\n",
      "optimal threshold: -0.7011\n",
      "Epoch 166 train loss: 0.7322, eval loss 0.6869405508041382\n",
      "optimal threshold: -0.6996\n",
      "Epoch 167 train loss: 0.7412, eval loss 0.6866368651390076\n",
      "optimal threshold: -0.6971\n",
      "Epoch 168 train loss: 0.7316, eval loss 0.686369001865387\n",
      "optimal threshold: -0.6987\n",
      "Epoch 169 train loss: 0.7060, eval loss 0.6861286759376526\n",
      "optimal threshold: -0.6998\n",
      "Epoch 170 train loss: 0.7050, eval loss 0.6858643293380737\n",
      "optimal threshold: -0.7040\n",
      "Epoch 171 train loss: 0.7528, eval loss 0.6856688857078552\n",
      "optimal threshold: -0.7039\n",
      "Epoch 172 train loss: 0.7571, eval loss 0.6854310631752014\n",
      "optimal threshold: -0.7062\n",
      "Epoch 173 train loss: 0.7254, eval loss 0.6852133870124817\n",
      "optimal threshold: -0.7037\n",
      "Epoch 174 train loss: 0.7198, eval loss 0.6849769353866577\n",
      "optimal threshold: -0.7089\n",
      "Epoch 175 train loss: 0.7230, eval loss 0.6847571730613708\n",
      "optimal threshold: -0.7110\n",
      "Epoch 176 train loss: 0.7320, eval loss 0.6845387816429138\n",
      "optimal threshold: -0.7098\n",
      "Epoch 177 train loss: 0.6901, eval loss 0.6843045353889465\n",
      "optimal threshold: -0.7109\n",
      "Epoch 178 train loss: 0.7523, eval loss 0.6840962767601013\n",
      "optimal threshold: -0.7105\n",
      "Epoch 179 train loss: 0.7058, eval loss 0.6838831305503845\n",
      "optimal threshold: -0.7118\n",
      "Epoch 180 train loss: 0.6965, eval loss 0.6836819648742676\n",
      "optimal threshold: -0.7178\n",
      "Epoch 181 train loss: 0.7528, eval loss 0.6834713220596313\n",
      "optimal threshold: -0.7160\n",
      "Epoch 182 train loss: 0.6970, eval loss 0.6832468509674072\n",
      "optimal threshold: -0.7169\n",
      "Epoch 183 train loss: 0.7260, eval loss 0.6830591559410095\n",
      "optimal threshold: -0.7152\n",
      "Epoch 184 train loss: 0.7135, eval loss 0.6828437447547913\n",
      "optimal threshold: -0.7155\n",
      "Epoch 185 train loss: 0.7193, eval loss 0.6826438903808594\n",
      "optimal threshold: -0.7140\n",
      "Epoch 186 train loss: 0.6965, eval loss 0.6824402213096619\n",
      "optimal threshold: -0.7161\n",
      "Epoch 187 train loss: 0.6815, eval loss 0.6822278499603271\n",
      "optimal threshold: -0.7130\n",
      "Epoch 188 train loss: 0.7096, eval loss 0.6820054054260254\n",
      "optimal threshold: -0.7135\n",
      "Epoch 189 train loss: 0.7405, eval loss 0.6818515658378601\n",
      "optimal threshold: -0.6899\n",
      "Epoch 190 train loss: 0.7259, eval loss 0.6816532015800476\n",
      "optimal threshold: -0.6767\n",
      "Epoch 191 train loss: 0.7097, eval loss 0.6814441084861755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7081\n",
      "Epoch 192 train loss: 0.7207, eval loss 0.6812444925308228\n",
      "optimal threshold: -0.7075\n",
      "Epoch 193 train loss: 0.7685, eval loss 0.6810728907585144\n",
      "optimal threshold: -0.7111\n",
      "Epoch 194 train loss: 0.7455, eval loss 0.6808809041976929\n",
      "optimal threshold: -0.7340\n",
      "Epoch 195 train loss: 0.7114, eval loss 0.6807146072387695\n",
      "optimal threshold: -0.7388\n",
      "Epoch 196 train loss: 0.6963, eval loss 0.6805475950241089\n",
      "optimal threshold: -0.7345\n",
      "Epoch 197 train loss: 0.7328, eval loss 0.6803755164146423\n",
      "optimal threshold: -0.7133\n",
      "Epoch 198 train loss: 0.7247, eval loss 0.6802150011062622\n",
      "optimal threshold: -0.7225\n",
      "Epoch 199 train loss: 0.7206, eval loss 0.680049479007721\n",
      "optimal threshold: -0.7150\n",
      "Epoch 200 train loss: 0.7243, eval loss 0.6798782348632812\n",
      "optimal threshold: -0.7093\n",
      "Epoch 201 train loss: 0.7324, eval loss 0.6796960234642029\n",
      "optimal threshold: -0.7080\n",
      "Epoch 202 train loss: 0.7446, eval loss 0.6795200109481812\n",
      "optimal threshold: -0.7318\n",
      "Epoch 203 train loss: 0.7427, eval loss 0.6793603897094727\n",
      "optimal threshold: -0.7002\n",
      "Epoch 204 train loss: 0.7380, eval loss 0.6792129874229431\n",
      "optimal threshold: -0.6999\n",
      "Epoch 205 train loss: 0.7455, eval loss 0.6790615916252136\n",
      "optimal threshold: -0.7014\n",
      "Epoch 206 train loss: 0.7277, eval loss 0.6789318323135376\n",
      "optimal threshold: -0.6840\n",
      "Epoch 207 train loss: 0.7529, eval loss 0.6787797808647156\n",
      "optimal threshold: -0.6821\n",
      "Epoch 208 train loss: 0.7131, eval loss 0.6785920262336731\n",
      "optimal threshold: -0.6859\n",
      "Epoch 209 train loss: 0.6825, eval loss 0.6784758567810059\n",
      "optimal threshold: -0.7008\n",
      "Epoch 210 train loss: 0.7318, eval loss 0.6783400774002075\n",
      "optimal threshold: -0.6998\n",
      "Epoch 211 train loss: 0.7075, eval loss 0.6781939268112183\n",
      "optimal threshold: -0.6976\n",
      "Epoch 212 train loss: 0.7206, eval loss 0.6780279278755188\n",
      "optimal threshold: -0.6958\n",
      "Epoch 213 train loss: 0.6822, eval loss 0.6778919696807861\n",
      "optimal threshold: -0.6949\n",
      "Epoch 214 train loss: 0.6956, eval loss 0.6777586936950684\n",
      "optimal threshold: -0.6934\n",
      "Epoch 215 train loss: 0.6997, eval loss 0.6775961518287659\n",
      "optimal threshold: -0.7229\n",
      "Epoch 216 train loss: 0.7365, eval loss 0.6774725914001465\n",
      "optimal threshold: -0.7238\n",
      "Epoch 217 train loss: 0.7162, eval loss 0.6773220300674438\n",
      "optimal threshold: -0.7224\n",
      "Epoch 218 train loss: 0.6743, eval loss 0.6771922707557678\n",
      "optimal threshold: -0.7167\n",
      "Epoch 219 train loss: 0.7335, eval loss 0.6770575642585754\n",
      "optimal threshold: -0.7176\n",
      "Epoch 220 train loss: 0.6901, eval loss 0.6769540905952454\n",
      "optimal threshold: -0.7153\n",
      "Epoch 221 train loss: 0.7003, eval loss 0.6768149137496948\n",
      "optimal threshold: -0.7262\n",
      "Epoch 222 train loss: 0.7030, eval loss 0.6766940951347351\n",
      "optimal threshold: -0.7274\n",
      "Epoch 223 train loss: 0.6951, eval loss 0.6766074895858765\n",
      "optimal threshold: -0.7156\n",
      "Epoch 224 train loss: 0.7478, eval loss 0.6764771938323975\n",
      "optimal threshold: -0.7151\n",
      "Epoch 225 train loss: 0.6795, eval loss 0.676341712474823\n",
      "optimal threshold: -0.7217\n",
      "Epoch 226 train loss: 0.6833, eval loss 0.6761764883995056\n",
      "optimal threshold: -0.7218\n",
      "Epoch 227 train loss: 0.7373, eval loss 0.6760718822479248\n",
      "optimal threshold: -0.7223\n",
      "Epoch 228 train loss: 0.6927, eval loss 0.6759629845619202\n",
      "optimal threshold: -0.7219\n",
      "Epoch 229 train loss: 0.7112, eval loss 0.675832211971283\n",
      "optimal threshold: -0.7242\n",
      "Epoch 230 train loss: 0.7090, eval loss 0.6756936311721802\n",
      "optimal threshold: -0.7165\n",
      "Epoch 231 train loss: 0.7004, eval loss 0.6755438446998596\n",
      "optimal threshold: -0.7173\n",
      "Epoch 232 train loss: 0.6913, eval loss 0.675440788269043\n",
      "optimal threshold: -0.7166\n",
      "Epoch 233 train loss: 0.6896, eval loss 0.6753278970718384\n",
      "optimal threshold: -0.7177\n",
      "Epoch 234 train loss: 0.6881, eval loss 0.675207793712616\n",
      "optimal threshold: -0.7158\n",
      "Epoch 235 train loss: 0.7168, eval loss 0.6750956773757935\n",
      "optimal threshold: -0.7150\n",
      "Epoch 236 train loss: 0.6850, eval loss 0.674984872341156\n",
      "optimal threshold: -0.7143\n",
      "Epoch 237 train loss: 0.6895, eval loss 0.674853503704071\n",
      "optimal threshold: -0.7125\n",
      "Epoch 238 train loss: 0.7432, eval loss 0.6747312545776367\n",
      "optimal threshold: -0.7145\n",
      "Epoch 239 train loss: 0.7006, eval loss 0.6746429204940796\n",
      "optimal threshold: -0.7165\n",
      "Epoch 240 train loss: 0.7092, eval loss 0.6745367646217346\n",
      "optimal threshold: -0.7186\n",
      "Epoch 241 train loss: 0.6838, eval loss 0.6744197607040405\n",
      "optimal threshold: -0.7255\n",
      "Epoch 242 train loss: 0.7217, eval loss 0.6743348836898804\n",
      "optimal threshold: -0.7257\n",
      "Epoch 243 train loss: 0.7033, eval loss 0.6742213368415833\n",
      "optimal threshold: -0.7273\n",
      "Epoch 244 train loss: 0.7260, eval loss 0.6741247773170471\n",
      "optimal threshold: -0.7308\n",
      "Epoch 245 train loss: 0.7534, eval loss 0.67405104637146\n",
      "optimal threshold: -0.7330\n",
      "Epoch 246 train loss: 0.6697, eval loss 0.6739541888237\n",
      "optimal threshold: -0.7171\n",
      "Epoch 247 train loss: 0.6630, eval loss 0.6738507151603699\n",
      "optimal threshold: -0.7118\n",
      "Epoch 248 train loss: 0.7108, eval loss 0.6737244129180908\n",
      "optimal threshold: -0.7080\n",
      "Epoch 249 train loss: 0.6716, eval loss 0.6736093759536743\n",
      "optimal threshold: -0.7168\n",
      "Epoch 250 train loss: 0.6671, eval loss 0.6735387444496155\n",
      "optimal threshold: -0.7152\n",
      "Epoch 251 train loss: 0.7218, eval loss 0.6734339594841003\n",
      "optimal threshold: -0.7115\n",
      "Epoch 252 train loss: 0.6728, eval loss 0.6733810305595398\n",
      "optimal threshold: -0.7070\n",
      "Epoch 253 train loss: 0.7440, eval loss 0.6732510328292847\n",
      "optimal threshold: -0.7118\n",
      "Epoch 254 train loss: 0.6924, eval loss 0.6731681227684021\n",
      "optimal threshold: -0.7103\n",
      "Epoch 255 train loss: 0.7656, eval loss 0.673079788684845\n",
      "optimal threshold: -0.7082\n",
      "Epoch 256 train loss: 0.7096, eval loss 0.6729692816734314\n",
      "optimal threshold: -0.7026\n",
      "Epoch 257 train loss: 0.6695, eval loss 0.6728530526161194\n",
      "optimal threshold: -0.6935\n",
      "Epoch 258 train loss: 0.7127, eval loss 0.672771692276001\n",
      "optimal threshold: -0.6947\n",
      "Epoch 259 train loss: 0.7353, eval loss 0.672749936580658\n",
      "optimal threshold: -0.6946\n",
      "Epoch 260 train loss: 0.7125, eval loss 0.6726480722427368\n",
      "optimal threshold: -0.6943\n",
      "Epoch 261 train loss: 0.7463, eval loss 0.6725384593009949\n",
      "optimal threshold: -0.7036\n",
      "Epoch 262 train loss: 0.6694, eval loss 0.6724411249160767\n",
      "optimal threshold: -0.7041\n",
      "Epoch 263 train loss: 0.7569, eval loss 0.6723724007606506\n",
      "optimal threshold: -0.6978\n",
      "Epoch 264 train loss: 0.7046, eval loss 0.6723138093948364\n",
      "optimal threshold: -0.6980\n",
      "Epoch 265 train loss: 0.6878, eval loss 0.6722022891044617\n",
      "optimal threshold: -0.6979\n",
      "Epoch 266 train loss: 0.6993, eval loss 0.6721006631851196\n",
      "optimal threshold: -0.7116\n",
      "Epoch 267 train loss: 0.7007, eval loss 0.6720359921455383\n",
      "optimal threshold: -0.7114\n",
      "Epoch 268 train loss: 0.7011, eval loss 0.6719462275505066\n",
      "optimal threshold: -0.7092\n",
      "Epoch 269 train loss: 0.7169, eval loss 0.6718342900276184\n",
      "optimal threshold: -0.7044\n",
      "Epoch 270 train loss: 0.6771, eval loss 0.6717531681060791\n",
      "optimal threshold: -0.7051\n",
      "Epoch 271 train loss: 0.7478, eval loss 0.671701967716217\n",
      "optimal threshold: -0.7218\n",
      "Epoch 272 train loss: 0.7168, eval loss 0.671606183052063\n",
      "optimal threshold: -0.7219\n",
      "Epoch 273 train loss: 0.6997, eval loss 0.6715286374092102\n",
      "optimal threshold: -0.7217\n",
      "Epoch 274 train loss: 0.7376, eval loss 0.6714924573898315\n",
      "optimal threshold: -0.7193\n",
      "Epoch 275 train loss: 0.7073, eval loss 0.6713873744010925\n",
      "optimal threshold: -0.7185\n",
      "Epoch 276 train loss: 0.6957, eval loss 0.6713137030601501\n",
      "optimal threshold: -0.7175\n",
      "Epoch 277 train loss: 0.7462, eval loss 0.6712305545806885\n",
      "optimal threshold: -0.7140\n",
      "Epoch 278 train loss: 0.7422, eval loss 0.6711540818214417\n",
      "optimal threshold: -0.7164\n",
      "Epoch 279 train loss: 0.7085, eval loss 0.6710793972015381\n",
      "optimal threshold: -0.7150\n",
      "Epoch 280 train loss: 0.6828, eval loss 0.6710070967674255\n",
      "optimal threshold: -0.7155\n",
      "Epoch 281 train loss: 0.6841, eval loss 0.6709531545639038\n",
      "optimal threshold: -0.7140\n",
      "Epoch 282 train loss: 0.7160, eval loss 0.6708697080612183\n",
      "optimal threshold: -0.7139\n",
      "Epoch 283 train loss: 0.7071, eval loss 0.6708124279975891\n",
      "optimal threshold: -0.7127\n",
      "Epoch 284 train loss: 0.7130, eval loss 0.6707379817962646\n",
      "optimal threshold: -0.7120\n",
      "Epoch 285 train loss: 0.7139, eval loss 0.6706592440605164\n",
      "optimal threshold: -0.7127\n",
      "Epoch 286 train loss: 0.7184, eval loss 0.6706000566482544\n",
      "optimal threshold: -0.7100\n",
      "Epoch 287 train loss: 0.7072, eval loss 0.6705246567726135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7104\n",
      "Epoch 288 train loss: 0.7474, eval loss 0.6704535484313965\n",
      "optimal threshold: -0.7126\n",
      "Epoch 289 train loss: 0.7105, eval loss 0.6704228520393372\n",
      "optimal threshold: -0.7136\n",
      "Epoch 290 train loss: 0.6944, eval loss 0.6703375577926636\n",
      "optimal threshold: -0.7110\n",
      "Epoch 291 train loss: 0.6758, eval loss 0.6702505946159363\n",
      "optimal threshold: -0.7137\n",
      "Epoch 292 train loss: 0.7221, eval loss 0.6701626777648926\n",
      "optimal threshold: -0.7122\n",
      "Epoch 293 train loss: 0.7277, eval loss 0.6701095700263977\n",
      "optimal threshold: -0.7126\n",
      "Epoch 294 train loss: 0.7668, eval loss 0.6700503826141357\n",
      "optimal threshold: -0.7142\n",
      "Epoch 295 train loss: 0.7686, eval loss 0.6700009107589722\n",
      "optimal threshold: -0.7147\n",
      "Epoch 296 train loss: 0.7112, eval loss 0.6699351668357849\n",
      "optimal threshold: -0.7150\n",
      "Epoch 297 train loss: 0.6882, eval loss 0.6698634028434753\n",
      "optimal threshold: -0.7141\n",
      "Epoch 298 train loss: 0.7245, eval loss 0.6697944402694702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 12:45:12,289] Trial 12 finished with value: 0.7269095778465271 and parameters: {'learning_rate_exp': -5.713697919417162, 'dropout_p': 0.29499216314267795, 'l2_reg_exp': -2.8552598129545133, 'batch_size': 114, 'N': 317}. Best is trial 6 with value: 0.22441545128822327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7136\n",
      "Epoch 299 train loss: 0.7269, eval loss 0.6697381138801575\n",
      "optimal threshold: 0.1086\n",
      "Epoch 0 train loss: 1.5113, eval loss 1.4974318742752075\n",
      "optimal threshold: 0.1036\n",
      "Epoch 1 train loss: 1.4977, eval loss 1.4915943145751953\n",
      "optimal threshold: 0.0961\n",
      "Epoch 2 train loss: 1.4969, eval loss 1.48582124710083\n",
      "optimal threshold: 0.0850\n",
      "Epoch 3 train loss: 1.4860, eval loss 1.480057716369629\n",
      "optimal threshold: 0.0760\n",
      "Epoch 4 train loss: 1.4799, eval loss 1.4742902517318726\n",
      "optimal threshold: 0.0686\n",
      "Epoch 5 train loss: 1.4696, eval loss 1.4685732126235962\n",
      "optimal threshold: 0.0624\n",
      "Epoch 6 train loss: 1.4619, eval loss 1.462801218032837\n",
      "optimal threshold: 0.0506\n",
      "Epoch 7 train loss: 1.4606, eval loss 1.4570327997207642\n",
      "optimal threshold: 0.0429\n",
      "Epoch 8 train loss: 1.4565, eval loss 1.4512125253677368\n",
      "optimal threshold: 0.0304\n",
      "Epoch 9 train loss: 1.4606, eval loss 1.445380687713623\n",
      "optimal threshold: 0.0241\n",
      "Epoch 10 train loss: 1.4436, eval loss 1.4395079612731934\n",
      "optimal threshold: 0.0146\n",
      "Epoch 11 train loss: 1.4401, eval loss 1.4335936307907104\n",
      "optimal threshold: 0.0055\n",
      "Epoch 12 train loss: 1.4298, eval loss 1.4276244640350342\n",
      "optimal threshold: -0.0043\n",
      "Epoch 13 train loss: 1.4197, eval loss 1.4216129779815674\n",
      "optimal threshold: -0.0139\n",
      "Epoch 14 train loss: 1.4145, eval loss 1.4155268669128418\n",
      "optimal threshold: -0.0232\n",
      "Epoch 15 train loss: 1.4244, eval loss 1.4094003438949585\n",
      "optimal threshold: -0.0291\n",
      "Epoch 16 train loss: 1.4049, eval loss 1.403238296508789\n",
      "optimal threshold: -0.0362\n",
      "Epoch 17 train loss: 1.3962, eval loss 1.39704430103302\n",
      "optimal threshold: -0.0462\n",
      "Epoch 18 train loss: 1.3922, eval loss 1.390779972076416\n",
      "optimal threshold: -0.0559\n",
      "Epoch 19 train loss: 1.3866, eval loss 1.3844712972640991\n",
      "optimal threshold: -0.0661\n",
      "Epoch 20 train loss: 1.3740, eval loss 1.378099799156189\n",
      "optimal threshold: -0.0754\n",
      "Epoch 21 train loss: 1.3679, eval loss 1.3717401027679443\n",
      "optimal threshold: -0.0866\n",
      "Epoch 22 train loss: 1.3737, eval loss 1.3653124570846558\n",
      "optimal threshold: -0.0965\n",
      "Epoch 23 train loss: 1.3661, eval loss 1.3588520288467407\n",
      "optimal threshold: -0.1080\n",
      "Epoch 24 train loss: 1.3679, eval loss 1.352357268333435\n",
      "optimal threshold: -0.1172\n",
      "Epoch 25 train loss: 1.3501, eval loss 1.3458317518234253\n",
      "optimal threshold: -0.1360\n",
      "Epoch 26 train loss: 1.3493, eval loss 1.3392783403396606\n",
      "optimal threshold: -0.1489\n",
      "Epoch 27 train loss: 1.3320, eval loss 1.3327150344848633\n",
      "optimal threshold: -0.1607\n",
      "Epoch 28 train loss: 1.3320, eval loss 1.3261427879333496\n",
      "optimal threshold: -0.1469\n",
      "Epoch 29 train loss: 1.3215, eval loss 1.319570779800415\n",
      "optimal threshold: -0.1832\n",
      "Epoch 30 train loss: 1.3206, eval loss 1.312951683998108\n",
      "optimal threshold: -0.1941\n",
      "Epoch 31 train loss: 1.3051, eval loss 1.3063346147537231\n",
      "optimal threshold: -0.2056\n",
      "Epoch 32 train loss: 1.3021, eval loss 1.2997241020202637\n",
      "optimal threshold: -0.2176\n",
      "Epoch 33 train loss: 1.3009, eval loss 1.293110728263855\n",
      "optimal threshold: -0.2317\n",
      "Epoch 34 train loss: 1.2831, eval loss 1.2864919900894165\n",
      "optimal threshold: -0.2433\n",
      "Epoch 35 train loss: 1.2809, eval loss 1.2798638343811035\n",
      "optimal threshold: -0.2548\n",
      "Epoch 36 train loss: 1.2713, eval loss 1.2732553482055664\n",
      "optimal threshold: -0.2678\n",
      "Epoch 37 train loss: 1.2596, eval loss 1.2666542530059814\n",
      "optimal threshold: -0.2777\n",
      "Epoch 38 train loss: 1.2594, eval loss 1.26006281375885\n",
      "optimal threshold: -0.2849\n",
      "Epoch 39 train loss: 1.2467, eval loss 1.2534695863723755\n",
      "optimal threshold: -0.2673\n",
      "Epoch 40 train loss: 1.2638, eval loss 1.2468992471694946\n",
      "optimal threshold: -0.2784\n",
      "Epoch 41 train loss: 1.2407, eval loss 1.240375280380249\n",
      "optimal threshold: -0.2879\n",
      "Epoch 42 train loss: 1.2390, eval loss 1.233835220336914\n",
      "optimal threshold: -0.2975\n",
      "Epoch 43 train loss: 1.2269, eval loss 1.2273427248001099\n",
      "optimal threshold: -0.3444\n",
      "Epoch 44 train loss: 1.2206, eval loss 1.2208479642868042\n",
      "optimal threshold: -0.3576\n",
      "Epoch 45 train loss: 1.2256, eval loss 1.2144018411636353\n",
      "optimal threshold: -0.3698\n",
      "Epoch 46 train loss: 1.2029, eval loss 1.2079646587371826\n",
      "optimal threshold: -0.3855\n",
      "Epoch 47 train loss: 1.1931, eval loss 1.2015653848648071\n",
      "optimal threshold: -0.3978\n",
      "Epoch 48 train loss: 1.1871, eval loss 1.1952078342437744\n",
      "optimal threshold: -0.4114\n",
      "Epoch 49 train loss: 1.2029, eval loss 1.1888694763183594\n",
      "optimal threshold: -0.4019\n",
      "Epoch 50 train loss: 1.1835, eval loss 1.1825453042984009\n",
      "optimal threshold: -0.4123\n",
      "Epoch 51 train loss: 1.1862, eval loss 1.176273226737976\n",
      "optimal threshold: -0.4523\n",
      "Epoch 52 train loss: 1.1839, eval loss 1.1700626611709595\n",
      "optimal threshold: -0.4632\n",
      "Epoch 53 train loss: 1.1719, eval loss 1.1638895273208618\n",
      "optimal threshold: -0.4763\n",
      "Epoch 54 train loss: 1.1721, eval loss 1.157723307609558\n",
      "optimal threshold: -0.4901\n",
      "Epoch 55 train loss: 1.1533, eval loss 1.1515998840332031\n",
      "optimal threshold: -0.5042\n",
      "Epoch 56 train loss: 1.1443, eval loss 1.1455494165420532\n",
      "optimal threshold: -0.5155\n",
      "Epoch 57 train loss: 1.1408, eval loss 1.1395388841629028\n",
      "optimal threshold: -0.4871\n",
      "Epoch 58 train loss: 1.1253, eval loss 1.133557677268982\n",
      "optimal threshold: -0.5308\n",
      "Epoch 59 train loss: 1.1349, eval loss 1.1276308298110962\n",
      "optimal threshold: -0.5544\n",
      "Epoch 60 train loss: 1.1036, eval loss 1.1217567920684814\n",
      "optimal threshold: -0.5672\n",
      "Epoch 61 train loss: 1.1171, eval loss 1.1159155368804932\n",
      "optimal threshold: -0.5724\n",
      "Epoch 62 train loss: 1.1075, eval loss 1.1101458072662354\n",
      "optimal threshold: -0.5554\n",
      "Epoch 63 train loss: 1.1007, eval loss 1.1044254302978516\n",
      "optimal threshold: -0.5599\n",
      "Epoch 64 train loss: 1.0983, eval loss 1.0987542867660522\n",
      "optimal threshold: -0.5716\n",
      "Epoch 65 train loss: 1.0958, eval loss 1.093144178390503\n",
      "optimal threshold: -0.5772\n",
      "Epoch 66 train loss: 1.0930, eval loss 1.0875941514968872\n",
      "optimal threshold: -0.5847\n",
      "Epoch 67 train loss: 1.0743, eval loss 1.082093358039856\n",
      "optimal threshold: -0.6007\n",
      "Epoch 68 train loss: 1.0887, eval loss 1.0766586065292358\n",
      "optimal threshold: -0.6107\n",
      "Epoch 69 train loss: 1.0914, eval loss 1.0712594985961914\n",
      "optimal threshold: -0.6159\n",
      "Epoch 70 train loss: 1.0699, eval loss 1.065934419631958\n",
      "optimal threshold: -0.6273\n",
      "Epoch 71 train loss: 1.0640, eval loss 1.0606651306152344\n",
      "optimal threshold: -0.6383\n",
      "Epoch 72 train loss: 1.0644, eval loss 1.0554566383361816\n",
      "optimal threshold: -0.6481\n",
      "Epoch 73 train loss: 1.0341, eval loss 1.0503051280975342\n",
      "optimal threshold: -0.6974\n",
      "Epoch 74 train loss: 1.0499, eval loss 1.0452125072479248\n",
      "optimal threshold: -0.7088\n",
      "Epoch 75 train loss: 1.0297, eval loss 1.0401698350906372\n",
      "optimal threshold: -0.7228\n",
      "Epoch 76 train loss: 1.0317, eval loss 1.0352036952972412\n",
      "optimal threshold: -0.7340\n",
      "Epoch 77 train loss: 1.0358, eval loss 1.0303078889846802\n",
      "optimal threshold: -0.7413\n",
      "Epoch 78 train loss: 1.0472, eval loss 1.025467038154602\n",
      "optimal threshold: -0.7545\n",
      "Epoch 79 train loss: 1.0041, eval loss 1.0206875801086426\n",
      "optimal threshold: -0.7619\n",
      "Epoch 80 train loss: 1.0060, eval loss 1.0159639120101929\n",
      "optimal threshold: -0.7721\n",
      "Epoch 81 train loss: 1.0140, eval loss 1.0113322734832764\n",
      "optimal threshold: -0.7785\n",
      "Epoch 82 train loss: 1.0016, eval loss 1.0067347288131714\n",
      "optimal threshold: -0.7896\n",
      "Epoch 83 train loss: 1.0043, eval loss 1.0021973848342896\n",
      "optimal threshold: -0.8022\n",
      "Epoch 84 train loss: 0.9849, eval loss 0.9977139830589294\n",
      "optimal threshold: -0.8138\n",
      "Epoch 85 train loss: 0.9753, eval loss 0.993295431137085\n",
      "optimal threshold: -0.8249\n",
      "Epoch 86 train loss: 0.9992, eval loss 0.9889329671859741\n",
      "optimal threshold: -0.8332\n",
      "Epoch 87 train loss: 0.9965, eval loss 0.9846310615539551\n",
      "optimal threshold: -0.8436\n",
      "Epoch 88 train loss: 1.0099, eval loss 0.9803801774978638\n",
      "optimal threshold: -0.8053\n",
      "Epoch 89 train loss: 0.9897, eval loss 0.9761999845504761\n",
      "optimal threshold: -0.8136\n",
      "Epoch 90 train loss: 0.9884, eval loss 0.9720665812492371\n",
      "optimal threshold: -0.8209\n",
      "Epoch 91 train loss: 0.9774, eval loss 0.9679866433143616\n",
      "optimal threshold: -0.8294\n",
      "Epoch 92 train loss: 0.9456, eval loss 0.9639672040939331\n",
      "optimal threshold: -0.8320\n",
      "Epoch 93 train loss: 0.9407, eval loss 0.9599919319152832\n",
      "optimal threshold: -0.8443\n",
      "Epoch 94 train loss: 0.9439, eval loss 0.956061065196991\n",
      "optimal threshold: -0.8517\n",
      "Epoch 95 train loss: 0.9783, eval loss 0.9521694183349609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.8589\n",
      "Epoch 96 train loss: 0.9528, eval loss 0.948354959487915\n",
      "optimal threshold: -0.8642\n",
      "Epoch 97 train loss: 0.9396, eval loss 0.9445764422416687\n",
      "optimal threshold: -0.8718\n",
      "Epoch 98 train loss: 0.9462, eval loss 0.9408377408981323\n",
      "optimal threshold: -0.8945\n",
      "Epoch 99 train loss: 0.9278, eval loss 0.9371533393859863\n",
      "optimal threshold: -0.9047\n",
      "Epoch 100 train loss: 0.9559, eval loss 0.9335228800773621\n",
      "optimal threshold: -0.9060\n",
      "Epoch 101 train loss: 0.9439, eval loss 0.9299275279045105\n",
      "optimal threshold: -0.9112\n",
      "Epoch 102 train loss: 0.9134, eval loss 0.9263730645179749\n",
      "optimal threshold: -0.9163\n",
      "Epoch 103 train loss: 0.9276, eval loss 0.9228743314743042\n",
      "optimal threshold: -0.9215\n",
      "Epoch 104 train loss: 0.8975, eval loss 0.9193954467773438\n",
      "optimal threshold: -0.9281\n",
      "Epoch 105 train loss: 0.8857, eval loss 0.915961742401123\n",
      "optimal threshold: -0.9322\n",
      "Epoch 106 train loss: 0.9023, eval loss 0.9125791192054749\n",
      "optimal threshold: -0.9353\n",
      "Epoch 107 train loss: 0.9063, eval loss 0.9092274904251099\n",
      "optimal threshold: -0.9417\n",
      "Epoch 108 train loss: 0.9166, eval loss 0.905898928642273\n",
      "optimal threshold: -0.9461\n",
      "Epoch 109 train loss: 0.9032, eval loss 0.9026297926902771\n",
      "optimal threshold: -0.9841\n",
      "Epoch 110 train loss: 0.8987, eval loss 0.8993791937828064\n",
      "optimal threshold: -0.9749\n",
      "Epoch 111 train loss: 0.9163, eval loss 0.89618980884552\n",
      "optimal threshold: -0.9782\n",
      "Epoch 112 train loss: 0.9077, eval loss 0.893022358417511\n",
      "optimal threshold: -0.9824\n",
      "Epoch 113 train loss: 0.8897, eval loss 0.889885663986206\n",
      "optimal threshold: -0.9864\n",
      "Epoch 114 train loss: 0.8906, eval loss 0.8867763876914978\n",
      "optimal threshold: -0.9879\n",
      "Epoch 115 train loss: 0.8670, eval loss 0.8836894631385803\n",
      "optimal threshold: -0.9908\n",
      "Epoch 116 train loss: 0.8806, eval loss 0.8806616067886353\n",
      "optimal threshold: -0.9941\n",
      "Epoch 117 train loss: 0.8995, eval loss 0.8776562809944153\n",
      "optimal threshold: -1.0166\n",
      "Epoch 118 train loss: 0.8852, eval loss 0.8746846318244934\n",
      "optimal threshold: -0.9596\n",
      "Epoch 119 train loss: 0.8549, eval loss 0.8717425465583801\n",
      "optimal threshold: -0.9598\n",
      "Epoch 120 train loss: 0.8810, eval loss 0.8688275218009949\n",
      "optimal threshold: -0.9585\n",
      "Epoch 121 train loss: 0.8494, eval loss 0.8659522533416748\n",
      "optimal threshold: -0.9624\n",
      "Epoch 122 train loss: 0.8677, eval loss 0.8631159067153931\n",
      "optimal threshold: -0.9627\n",
      "Epoch 123 train loss: 0.8583, eval loss 0.8603163361549377\n",
      "optimal threshold: -0.9588\n",
      "Epoch 124 train loss: 0.8669, eval loss 0.8575199246406555\n",
      "optimal threshold: -0.9616\n",
      "Epoch 125 train loss: 0.9055, eval loss 0.8547471761703491\n",
      "optimal threshold: -0.9631\n",
      "Epoch 126 train loss: 0.8396, eval loss 0.8520314693450928\n",
      "optimal threshold: -0.9616\n",
      "Epoch 127 train loss: 0.8850, eval loss 0.8493416905403137\n",
      "optimal threshold: -0.9683\n",
      "Epoch 128 train loss: 0.8936, eval loss 0.846686601638794\n",
      "optimal threshold: -0.9595\n",
      "Epoch 129 train loss: 0.8406, eval loss 0.8440866470336914\n",
      "optimal threshold: -0.9586\n",
      "Epoch 130 train loss: 0.8627, eval loss 0.8415082097053528\n",
      "optimal threshold: -0.9775\n",
      "Epoch 131 train loss: 0.8491, eval loss 0.838969349861145\n",
      "optimal threshold: -0.9769\n",
      "Epoch 132 train loss: 0.8509, eval loss 0.836449384689331\n",
      "optimal threshold: -0.9765\n",
      "Epoch 133 train loss: 0.8510, eval loss 0.8339830636978149\n",
      "optimal threshold: -0.9829\n",
      "Epoch 134 train loss: 0.8684, eval loss 0.8315413594245911\n",
      "optimal threshold: -0.9787\n",
      "Epoch 135 train loss: 0.8646, eval loss 0.8291193842887878\n",
      "optimal threshold: -0.9804\n",
      "Epoch 136 train loss: 0.8484, eval loss 0.8267492055892944\n",
      "optimal threshold: -0.9688\n",
      "Epoch 137 train loss: 0.8295, eval loss 0.8244198560714722\n",
      "optimal threshold: -0.9646\n",
      "Epoch 138 train loss: 0.8288, eval loss 0.8220900893211365\n",
      "optimal threshold: -0.9640\n",
      "Epoch 139 train loss: 0.7991, eval loss 0.8198249936103821\n",
      "optimal threshold: -0.9614\n",
      "Epoch 140 train loss: 0.8357, eval loss 0.8175843954086304\n",
      "optimal threshold: -0.9598\n",
      "Epoch 141 train loss: 0.8164, eval loss 0.8153568506240845\n",
      "optimal threshold: -0.9698\n",
      "Epoch 142 train loss: 0.8154, eval loss 0.8131693601608276\n",
      "optimal threshold: -0.9603\n",
      "Epoch 143 train loss: 0.8029, eval loss 0.8110265135765076\n",
      "optimal threshold: -0.9619\n",
      "Epoch 144 train loss: 0.8094, eval loss 0.8089061379432678\n",
      "optimal threshold: -0.9626\n",
      "Epoch 145 train loss: 0.8029, eval loss 0.8068128824234009\n",
      "optimal threshold: -0.9603\n",
      "Epoch 146 train loss: 0.8300, eval loss 0.8047515153884888\n",
      "optimal threshold: -0.9613\n",
      "Epoch 147 train loss: 0.8325, eval loss 0.802742600440979\n",
      "optimal threshold: -0.9609\n",
      "Epoch 148 train loss: 0.8276, eval loss 0.800763726234436\n",
      "optimal threshold: -0.9622\n",
      "Epoch 149 train loss: 0.8407, eval loss 0.798831045627594\n",
      "optimal threshold: -0.9558\n",
      "Epoch 150 train loss: 0.7772, eval loss 0.7969263792037964\n",
      "optimal threshold: -0.9614\n",
      "Epoch 151 train loss: 0.8041, eval loss 0.7950464487075806\n",
      "optimal threshold: -0.9566\n",
      "Epoch 152 train loss: 0.8239, eval loss 0.7931872010231018\n",
      "optimal threshold: -0.9621\n",
      "Epoch 153 train loss: 0.7766, eval loss 0.7913850545883179\n",
      "optimal threshold: -0.9200\n",
      "Epoch 154 train loss: 0.7980, eval loss 0.7895920276641846\n",
      "optimal threshold: -0.9172\n",
      "Epoch 155 train loss: 0.8011, eval loss 0.7878527641296387\n",
      "optimal threshold: -0.9158\n",
      "Epoch 156 train loss: 0.8052, eval loss 0.786159873008728\n",
      "optimal threshold: -0.9002\n",
      "Epoch 157 train loss: 0.8038, eval loss 0.784486711025238\n",
      "optimal threshold: -0.8994\n",
      "Epoch 158 train loss: 0.8119, eval loss 0.7828512787818909\n",
      "optimal threshold: -0.8974\n",
      "Epoch 159 train loss: 0.8046, eval loss 0.781245768070221\n",
      "optimal threshold: -0.9057\n",
      "Epoch 160 train loss: 0.8142, eval loss 0.7796589732170105\n",
      "optimal threshold: -0.9036\n",
      "Epoch 161 train loss: 0.7873, eval loss 0.7781135439872742\n",
      "optimal threshold: -0.9043\n",
      "Epoch 162 train loss: 0.7655, eval loss 0.7765790820121765\n",
      "optimal threshold: -0.9012\n",
      "Epoch 163 train loss: 0.8237, eval loss 0.7750738859176636\n",
      "optimal threshold: -0.8978\n",
      "Epoch 164 train loss: 0.8092, eval loss 0.7735907435417175\n",
      "optimal threshold: -0.8948\n",
      "Epoch 165 train loss: 0.7809, eval loss 0.7721478939056396\n",
      "optimal threshold: -0.9361\n",
      "Epoch 166 train loss: 0.7927, eval loss 0.7707314491271973\n",
      "optimal threshold: -0.9328\n",
      "Epoch 167 train loss: 0.7843, eval loss 0.7693422436714172\n",
      "optimal threshold: -0.9256\n",
      "Epoch 168 train loss: 0.7757, eval loss 0.7679988145828247\n",
      "optimal threshold: -0.9270\n",
      "Epoch 169 train loss: 0.7647, eval loss 0.7666807770729065\n",
      "optimal threshold: -0.9255\n",
      "Epoch 170 train loss: 0.7803, eval loss 0.7653930187225342\n",
      "optimal threshold: -0.9204\n",
      "Epoch 171 train loss: 0.7941, eval loss 0.7641087174415588\n",
      "optimal threshold: -0.9171\n",
      "Epoch 172 train loss: 0.7902, eval loss 0.7628641128540039\n",
      "optimal threshold: -0.9290\n",
      "Epoch 173 train loss: 0.7778, eval loss 0.7616316676139832\n",
      "optimal threshold: -0.9113\n",
      "Epoch 174 train loss: 0.7724, eval loss 0.760455310344696\n",
      "optimal threshold: -0.9088\n",
      "Epoch 175 train loss: 0.7852, eval loss 0.759304404258728\n",
      "optimal threshold: -0.9161\n",
      "Epoch 176 train loss: 0.7511, eval loss 0.7581676244735718\n",
      "optimal threshold: -0.9077\n",
      "Epoch 177 train loss: 0.7991, eval loss 0.7570578455924988\n",
      "optimal threshold: -0.9069\n",
      "Epoch 178 train loss: 0.7881, eval loss 0.7559539079666138\n",
      "optimal threshold: -0.9056\n",
      "Epoch 179 train loss: 0.7573, eval loss 0.754875123500824\n",
      "optimal threshold: -0.8999\n",
      "Epoch 180 train loss: 0.7918, eval loss 0.7538368105888367\n",
      "optimal threshold: -0.8929\n",
      "Epoch 181 train loss: 0.7975, eval loss 0.7528165578842163\n",
      "optimal threshold: -0.8912\n",
      "Epoch 182 train loss: 0.7627, eval loss 0.7518218755722046\n",
      "optimal threshold: -0.8900\n",
      "Epoch 183 train loss: 0.7745, eval loss 0.750843346118927\n",
      "optimal threshold: -0.9082\n",
      "Epoch 184 train loss: 0.7959, eval loss 0.749906599521637\n",
      "optimal threshold: -0.9062\n",
      "Epoch 185 train loss: 0.7697, eval loss 0.7489689588546753\n",
      "optimal threshold: -0.9045\n",
      "Epoch 186 train loss: 0.7721, eval loss 0.748051106929779\n",
      "optimal threshold: -0.9031\n",
      "Epoch 187 train loss: 0.7685, eval loss 0.7471665740013123\n",
      "optimal threshold: -0.9016\n",
      "Epoch 188 train loss: 0.7637, eval loss 0.7462763786315918\n",
      "optimal threshold: -0.8842\n",
      "Epoch 189 train loss: 0.7496, eval loss 0.745414137840271\n",
      "optimal threshold: -0.8830\n",
      "Epoch 190 train loss: 0.7434, eval loss 0.744568943977356\n",
      "optimal threshold: -0.8822\n",
      "Epoch 191 train loss: 0.7898, eval loss 0.7437524795532227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.8558\n",
      "Epoch 192 train loss: 0.7671, eval loss 0.7429451942443848\n",
      "optimal threshold: -0.8619\n",
      "Epoch 193 train loss: 0.7806, eval loss 0.7421558499336243\n",
      "optimal threshold: -0.8631\n",
      "Epoch 194 train loss: 0.7766, eval loss 0.7413815855979919\n",
      "optimal threshold: -0.8583\n",
      "Epoch 195 train loss: 0.7726, eval loss 0.7406319379806519\n",
      "optimal threshold: -0.8591\n",
      "Epoch 196 train loss: 0.7764, eval loss 0.7398799657821655\n",
      "optimal threshold: -0.8532\n",
      "Epoch 197 train loss: 0.7703, eval loss 0.7391459941864014\n",
      "optimal threshold: -0.8465\n",
      "Epoch 198 train loss: 0.7704, eval loss 0.7384375333786011\n",
      "optimal threshold: -0.8753\n",
      "Epoch 199 train loss: 0.7534, eval loss 0.7377405762672424\n",
      "optimal threshold: -0.8749\n",
      "Epoch 200 train loss: 0.7632, eval loss 0.737052321434021\n",
      "optimal threshold: -0.7629\n",
      "Epoch 201 train loss: 0.7653, eval loss 0.7363663911819458\n",
      "optimal threshold: -0.7615\n",
      "Epoch 202 train loss: 0.7649, eval loss 0.7357138991355896\n",
      "optimal threshold: -0.7601\n",
      "Epoch 203 train loss: 0.7975, eval loss 0.7350705862045288\n",
      "optimal threshold: -0.7611\n",
      "Epoch 204 train loss: 0.7838, eval loss 0.7344577312469482\n",
      "optimal threshold: -0.7581\n",
      "Epoch 205 train loss: 0.7333, eval loss 0.7338298559188843\n",
      "optimal threshold: -0.8831\n",
      "Epoch 206 train loss: 0.7856, eval loss 0.7332326173782349\n",
      "optimal threshold: -0.9034\n",
      "Epoch 207 train loss: 0.7741, eval loss 0.732638418674469\n",
      "optimal threshold: -0.9750\n",
      "Epoch 208 train loss: 0.7623, eval loss 0.732068657875061\n",
      "optimal threshold: -0.9732\n",
      "Epoch 209 train loss: 0.7953, eval loss 0.7314955592155457\n",
      "optimal threshold: -0.9096\n",
      "Epoch 210 train loss: 0.7710, eval loss 0.7309385538101196\n",
      "optimal threshold: -0.9665\n",
      "Epoch 211 train loss: 0.7485, eval loss 0.7304022312164307\n",
      "optimal threshold: -0.9666\n",
      "Epoch 212 train loss: 0.7600, eval loss 0.7298789024353027\n",
      "optimal threshold: -0.9664\n",
      "Epoch 213 train loss: 0.7899, eval loss 0.72935551404953\n",
      "optimal threshold: -0.9658\n",
      "Epoch 214 train loss: 0.7602, eval loss 0.7288423776626587\n",
      "optimal threshold: -0.9679\n",
      "Epoch 215 train loss: 0.7622, eval loss 0.7283377647399902\n",
      "optimal threshold: -0.9610\n",
      "Epoch 216 train loss: 0.7421, eval loss 0.7278569340705872\n",
      "optimal threshold: -0.9592\n",
      "Epoch 217 train loss: 0.7735, eval loss 0.7273774147033691\n",
      "optimal threshold: -0.9576\n",
      "Epoch 218 train loss: 0.7325, eval loss 0.7269051671028137\n",
      "optimal threshold: -0.9574\n",
      "Epoch 219 train loss: 0.7464, eval loss 0.7264379262924194\n",
      "optimal threshold: -0.9570\n",
      "Epoch 220 train loss: 0.7618, eval loss 0.7259756326675415\n",
      "optimal threshold: -0.9563\n",
      "Epoch 221 train loss: 0.7455, eval loss 0.7255117297172546\n",
      "optimal threshold: -0.9511\n",
      "Epoch 222 train loss: 0.7627, eval loss 0.7250677347183228\n",
      "optimal threshold: -0.9500\n",
      "Epoch 223 train loss: 0.7466, eval loss 0.7246365547180176\n",
      "optimal threshold: -0.9488\n",
      "Epoch 224 train loss: 0.7698, eval loss 0.7242118120193481\n",
      "optimal threshold: -0.9472\n",
      "Epoch 225 train loss: 0.7701, eval loss 0.7237891554832458\n",
      "optimal threshold: -0.9468\n",
      "Epoch 226 train loss: 0.7124, eval loss 0.7233622670173645\n",
      "optimal threshold: -0.9472\n",
      "Epoch 227 train loss: 0.7798, eval loss 0.7229595184326172\n",
      "optimal threshold: -0.9412\n",
      "Epoch 228 train loss: 0.7264, eval loss 0.7225308418273926\n",
      "optimal threshold: -0.9418\n",
      "Epoch 229 train loss: 0.7904, eval loss 0.7221457362174988\n",
      "optimal threshold: -0.9408\n",
      "Epoch 230 train loss: 0.7732, eval loss 0.7217681407928467\n",
      "optimal threshold: -0.9397\n",
      "Epoch 231 train loss: 0.7505, eval loss 0.7213916182518005\n",
      "optimal threshold: -0.9385\n",
      "Epoch 232 train loss: 0.7544, eval loss 0.7210135459899902\n",
      "optimal threshold: -0.9376\n",
      "Epoch 233 train loss: 0.7226, eval loss 0.7206498384475708\n",
      "optimal threshold: -0.9428\n",
      "Epoch 234 train loss: 0.7670, eval loss 0.7202845811843872\n",
      "optimal threshold: -0.9409\n",
      "Epoch 235 train loss: 0.7873, eval loss 0.7199226021766663\n",
      "optimal threshold: -0.9395\n",
      "Epoch 236 train loss: 0.7751, eval loss 0.7195767760276794\n",
      "optimal threshold: -0.9520\n",
      "Epoch 237 train loss: 0.7274, eval loss 0.7192280888557434\n",
      "optimal threshold: -0.9361\n",
      "Epoch 238 train loss: 0.7727, eval loss 0.718895435333252\n",
      "optimal threshold: -0.9339\n",
      "Epoch 239 train loss: 0.7359, eval loss 0.7185462117195129\n",
      "optimal threshold: -0.9312\n",
      "Epoch 240 train loss: 0.7857, eval loss 0.7181958556175232\n",
      "optimal threshold: -0.9304\n",
      "Epoch 241 train loss: 0.7090, eval loss 0.7178668975830078\n",
      "optimal threshold: -0.9408\n",
      "Epoch 242 train loss: 0.7767, eval loss 0.7175375819206238\n",
      "optimal threshold: -0.9390\n",
      "Epoch 243 train loss: 0.7702, eval loss 0.7172223925590515\n",
      "optimal threshold: -0.9366\n",
      "Epoch 244 train loss: 0.7466, eval loss 0.7169016003608704\n",
      "optimal threshold: -0.9355\n",
      "Epoch 245 train loss: 0.7309, eval loss 0.716574490070343\n",
      "optimal threshold: -0.9371\n",
      "Epoch 246 train loss: 0.7624, eval loss 0.7162569165229797\n",
      "optimal threshold: -0.9386\n",
      "Epoch 247 train loss: 0.7039, eval loss 0.7159609198570251\n",
      "optimal threshold: -0.9449\n",
      "Epoch 248 train loss: 0.7518, eval loss 0.7156651020050049\n",
      "optimal threshold: -0.9446\n",
      "Epoch 249 train loss: 0.7330, eval loss 0.7153612971305847\n",
      "optimal threshold: -0.9428\n",
      "Epoch 250 train loss: 0.7298, eval loss 0.7150461673736572\n",
      "optimal threshold: -0.9425\n",
      "Epoch 251 train loss: 0.7304, eval loss 0.7147678136825562\n",
      "optimal threshold: -0.9408\n",
      "Epoch 252 train loss: 0.7537, eval loss 0.7144712805747986\n",
      "optimal threshold: -0.9417\n",
      "Epoch 253 train loss: 0.7531, eval loss 0.7141881585121155\n",
      "optimal threshold: -0.9396\n",
      "Epoch 254 train loss: 0.7043, eval loss 0.7138896584510803\n",
      "optimal threshold: -0.9366\n",
      "Epoch 255 train loss: 0.7653, eval loss 0.7136075496673584\n",
      "optimal threshold: -0.9398\n",
      "Epoch 256 train loss: 0.7285, eval loss 0.7133292555809021\n",
      "optimal threshold: -0.9340\n",
      "Epoch 257 train loss: 0.7121, eval loss 0.7130511403083801\n",
      "optimal threshold: -0.9215\n",
      "Epoch 258 train loss: 0.7412, eval loss 0.712790846824646\n",
      "optimal threshold: -0.9131\n",
      "Epoch 259 train loss: 0.7262, eval loss 0.7125106453895569\n",
      "optimal threshold: -0.9196\n",
      "Epoch 260 train loss: 0.8020, eval loss 0.7122294306755066\n",
      "optimal threshold: -0.9638\n",
      "Epoch 261 train loss: 0.7606, eval loss 0.7119745016098022\n",
      "optimal threshold: -0.9213\n",
      "Epoch 262 train loss: 0.7516, eval loss 0.7117218375205994\n",
      "optimal threshold: -0.9221\n",
      "Epoch 263 train loss: 0.7030, eval loss 0.7114729881286621\n",
      "optimal threshold: -0.9244\n",
      "Epoch 264 train loss: 0.7920, eval loss 0.7112070322036743\n",
      "optimal threshold: -0.9233\n",
      "Epoch 265 train loss: 0.7083, eval loss 0.7109561562538147\n",
      "optimal threshold: -0.9253\n",
      "Epoch 266 train loss: 0.7176, eval loss 0.7107035517692566\n",
      "optimal threshold: -0.9255\n",
      "Epoch 267 train loss: 0.7747, eval loss 0.710466742515564\n",
      "optimal threshold: -0.9105\n",
      "Epoch 268 train loss: 0.7498, eval loss 0.7102187871932983\n",
      "optimal threshold: -0.9108\n",
      "Epoch 269 train loss: 0.7270, eval loss 0.7099702954292297\n",
      "optimal threshold: -0.8962\n",
      "Epoch 270 train loss: 0.7331, eval loss 0.7097296118736267\n",
      "optimal threshold: -0.8987\n",
      "Epoch 271 train loss: 0.7552, eval loss 0.7094924449920654\n",
      "optimal threshold: -0.8979\n",
      "Epoch 272 train loss: 0.7792, eval loss 0.7092598080635071\n",
      "optimal threshold: -0.8970\n",
      "Epoch 273 train loss: 0.7235, eval loss 0.7090153098106384\n",
      "optimal threshold: -0.8964\n",
      "Epoch 274 train loss: 0.7215, eval loss 0.7087891101837158\n",
      "optimal threshold: -0.8954\n",
      "Epoch 275 train loss: 0.7599, eval loss 0.708549439907074\n",
      "optimal threshold: -0.8960\n",
      "Epoch 276 train loss: 0.7398, eval loss 0.7083364129066467\n",
      "optimal threshold: -0.8949\n",
      "Epoch 277 train loss: 0.7741, eval loss 0.7080875635147095\n",
      "optimal threshold: -0.8820\n",
      "Epoch 278 train loss: 0.7735, eval loss 0.7078677415847778\n",
      "optimal threshold: -0.8804\n",
      "Epoch 279 train loss: 0.7688, eval loss 0.7076336741447449\n",
      "optimal threshold: -0.8791\n",
      "Epoch 280 train loss: 0.7344, eval loss 0.707400381565094\n",
      "optimal threshold: -0.8903\n",
      "Epoch 281 train loss: 0.7483, eval loss 0.7071779370307922\n",
      "optimal threshold: -0.8900\n",
      "Epoch 282 train loss: 0.7349, eval loss 0.7069607973098755\n",
      "optimal threshold: -0.9114\n",
      "Epoch 283 train loss: 0.7435, eval loss 0.7067481875419617\n",
      "optimal threshold: -0.9114\n",
      "Epoch 284 train loss: 0.7376, eval loss 0.7065398097038269\n",
      "optimal threshold: -0.9115\n",
      "Epoch 285 train loss: 0.7578, eval loss 0.7063407897949219\n",
      "optimal threshold: -0.9092\n",
      "Epoch 286 train loss: 0.7491, eval loss 0.706134557723999\n",
      "optimal threshold: -0.9087\n",
      "Epoch 287 train loss: 0.7349, eval loss 0.7059208750724792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.9100\n",
      "Epoch 288 train loss: 0.7520, eval loss 0.7057137489318848\n",
      "optimal threshold: -0.9145\n",
      "Epoch 289 train loss: 0.7148, eval loss 0.7055074572563171\n",
      "optimal threshold: -0.9130\n",
      "Epoch 290 train loss: 0.7358, eval loss 0.7053049802780151\n",
      "optimal threshold: -0.9084\n",
      "Epoch 291 train loss: 0.7559, eval loss 0.7051001191139221\n",
      "optimal threshold: -0.9075\n",
      "Epoch 292 train loss: 0.7382, eval loss 0.7048841714859009\n",
      "optimal threshold: -0.9067\n",
      "Epoch 293 train loss: 0.7275, eval loss 0.7046684622764587\n",
      "optimal threshold: -0.9093\n",
      "Epoch 294 train loss: 0.6906, eval loss 0.7044624090194702\n",
      "optimal threshold: -0.9070\n",
      "Epoch 295 train loss: 0.7475, eval loss 0.7042728066444397\n",
      "optimal threshold: -0.9071\n",
      "Epoch 296 train loss: 0.7195, eval loss 0.7040782570838928\n",
      "optimal threshold: -0.9070\n",
      "Epoch 297 train loss: 0.7359, eval loss 0.7038792371749878\n",
      "optimal threshold: -0.9067\n",
      "Epoch 298 train loss: 0.7428, eval loss 0.7036811709403992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 12:49:18,513] Trial 13 finished with value: 0.7407963275909424 and parameters: {'learning_rate_exp': -5.976512701544697, 'dropout_p': 0.35292356083788784, 'l2_reg_exp': -4.213862882697128, 'batch_size': 122, 'N': 210}. Best is trial 6 with value: 0.22441545128822327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -1.0457\n",
      "Epoch 299 train loss: 0.7408, eval loss 0.7034901976585388\n",
      "optimal threshold: -0.6352\n",
      "Epoch 0 train loss: 0.4369, eval loss 0.662179708480835\n",
      "optimal threshold: -0.5465\n",
      "Epoch 1 train loss: 0.3807, eval loss 0.6572457551956177\n",
      "optimal threshold: -0.5681\n",
      "Epoch 2 train loss: 0.2523, eval loss 0.6565122008323669\n",
      "optimal threshold: -0.7111\n",
      "Epoch 3 train loss: 0.1392, eval loss 0.6567111015319824\n",
      "optimal threshold: -0.5173\n",
      "Epoch 4 train loss: 0.1454, eval loss 0.6564567685127258\n",
      "optimal threshold: -0.4784\n",
      "Epoch 5 train loss: 0.1192, eval loss 0.6573070883750916\n",
      "optimal threshold: -0.3915\n",
      "Epoch 6 train loss: 0.1054, eval loss 0.6587116718292236\n",
      "optimal threshold: -0.5150\n",
      "Epoch 7 train loss: 0.1528, eval loss 0.6582719683647156\n",
      "optimal threshold: -0.4177\n",
      "Epoch 8 train loss: 0.1252, eval loss 0.6616467833518982\n",
      "optimal threshold: -0.3875\n",
      "Epoch 9 train loss: 0.0925, eval loss 0.66314697265625\n",
      "optimal threshold: -0.5746\n",
      "Epoch 10 train loss: 0.0745, eval loss 0.6656355857849121\n",
      "optimal threshold: -0.5163\n",
      "Epoch 11 train loss: 0.0818, eval loss 0.670387864112854\n",
      "optimal threshold: -0.5695\n",
      "Epoch 12 train loss: 0.0696, eval loss 0.677995502948761\n",
      "optimal threshold: -0.4097\n",
      "Epoch 13 train loss: 0.1843, eval loss 0.6806167364120483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 12:51:14,177] Trial 14 finished with value: 0.10548289865255356 and parameters: {'learning_rate_exp': -3.417577960778775, 'dropout_p': 0.3591737999088154, 'l2_reg_exp': -5.100636178875623, 'batch_size': 9, 'N': 336}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4941\n",
      "optimal threshold: -0.6881\n",
      "Epoch 0 train loss: 0.7477, eval loss 0.7335013151168823\n",
      "optimal threshold: -0.6230\n",
      "Epoch 1 train loss: 0.7281, eval loss 0.6834151148796082\n",
      "optimal threshold: -0.7228\n",
      "Epoch 2 train loss: 0.7039, eval loss 0.6714639663696289\n",
      "optimal threshold: -0.5371\n",
      "Epoch 3 train loss: 0.6434, eval loss 0.6664146184921265\n",
      "optimal threshold: -0.5190\n",
      "Epoch 4 train loss: 0.6354, eval loss 0.6631562113761902\n",
      "optimal threshold: -0.4516\n",
      "Epoch 5 train loss: 0.6523, eval loss 0.6604884266853333\n",
      "optimal threshold: -0.4195\n",
      "Epoch 6 train loss: 0.6221, eval loss 0.6605023741722107\n",
      "optimal threshold: -0.5345\n",
      "Epoch 7 train loss: 0.6390, eval loss 0.6598258018493652\n",
      "optimal threshold: -0.3625\n",
      "Epoch 8 train loss: 0.6421, eval loss 0.659255862236023\n",
      "optimal threshold: -0.5877\n",
      "Epoch 9 train loss: 0.6673, eval loss 0.6603338122367859\n",
      "optimal threshold: -0.5895\n",
      "Epoch 10 train loss: 0.6714, eval loss 0.6579711437225342\n",
      "optimal threshold: -0.5786\n",
      "Epoch 11 train loss: 0.6473, eval loss 0.6588870286941528\n",
      "optimal threshold: -0.5887\n",
      "Epoch 12 train loss: 0.6110, eval loss 0.6590748429298401\n",
      "optimal threshold: -0.5635\n",
      "Epoch 13 train loss: 0.6027, eval loss 0.6606918573379517\n",
      "optimal threshold: -0.5425\n",
      "Epoch 14 train loss: 0.6286, eval loss 0.6591112613677979\n",
      "optimal threshold: -0.5508\n",
      "Epoch 15 train loss: 0.6291, eval loss 0.6587653756141663\n",
      "optimal threshold: -0.5681\n",
      "Epoch 16 train loss: 0.6108, eval loss 0.6582438349723816\n",
      "optimal threshold: -0.7452\n",
      "Epoch 17 train loss: 0.6113, eval loss 0.6596827507019043\n",
      "optimal threshold: -0.5448\n",
      "Epoch 18 train loss: 0.6200, eval loss 0.6574422121047974\n",
      "optimal threshold: -0.7241\n",
      "Epoch 19 train loss: 0.6061, eval loss 0.658480703830719\n",
      "optimal threshold: -0.7131\n",
      "Epoch 20 train loss: 0.6297, eval loss 0.6598941087722778\n",
      "optimal threshold: -0.6957\n",
      "Epoch 21 train loss: 0.6262, eval loss 0.6585015654563904\n",
      "optimal threshold: -0.5377\n",
      "Epoch 22 train loss: 0.6289, eval loss 0.6597732901573181\n",
      "optimal threshold: -0.6836\n",
      "Epoch 23 train loss: 0.6030, eval loss 0.6597788333892822\n",
      "optimal threshold: -0.7847\n",
      "Epoch 24 train loss: 0.6263, eval loss 0.6601120829582214\n",
      "optimal threshold: -0.7735\n",
      "Epoch 25 train loss: 0.6344, eval loss 0.6618340015411377\n",
      "optimal threshold: -0.8001\n",
      "Epoch 26 train loss: 0.5701, eval loss 0.6627675890922546\n",
      "optimal threshold: -0.8166\n",
      "Epoch 27 train loss: 0.6260, eval loss 0.6639286279678345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 12:51:25,825] Trial 15 finished with value: 0.6096204519271851 and parameters: {'learning_rate_exp': -2.9265576786902967, 'dropout_p': 0.3841063856289483, 'l2_reg_exp': -5.22846283862266, 'batch_size': 350, 'N': 69}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7976\n",
      "optimal threshold: -0.8243\n",
      "Epoch 0 train loss: 0.7339, eval loss 0.6862437129020691\n",
      "optimal threshold: -0.6267\n",
      "Epoch 1 train loss: 0.7341, eval loss 0.6656258702278137\n",
      "optimal threshold: -0.5501\n",
      "Epoch 2 train loss: 0.6911, eval loss 0.6603233814239502\n",
      "optimal threshold: -0.5409\n",
      "Epoch 3 train loss: 0.7155, eval loss 0.6585482954978943\n",
      "optimal threshold: -0.5234\n",
      "Epoch 4 train loss: 0.7085, eval loss 0.6572805643081665\n",
      "optimal threshold: -0.4745\n",
      "Epoch 5 train loss: 0.6838, eval loss 0.6566048264503479\n",
      "optimal threshold: -0.5342\n",
      "Epoch 6 train loss: 0.6744, eval loss 0.6562337875366211\n",
      "optimal threshold: -0.5419\n",
      "Epoch 7 train loss: 0.6717, eval loss 0.656570553779602\n",
      "optimal threshold: -0.5804\n",
      "Epoch 8 train loss: 0.6701, eval loss 0.6573692560195923\n",
      "optimal threshold: -0.5652\n",
      "Epoch 9 train loss: 0.6636, eval loss 0.6590250134468079\n",
      "optimal threshold: -0.4409\n",
      "Epoch 10 train loss: 0.6422, eval loss 0.6601167917251587\n",
      "optimal threshold: -0.4798\n",
      "Epoch 11 train loss: 0.6542, eval loss 0.6609644293785095\n",
      "optimal threshold: -0.4568\n",
      "Epoch 12 train loss: 0.5994, eval loss 0.6628293991088867\n",
      "optimal threshold: -0.5009\n",
      "Epoch 13 train loss: 0.6407, eval loss 0.6645809412002563\n",
      "optimal threshold: -0.5624\n",
      "Epoch 14 train loss: 0.6226, eval loss 0.6671290993690491\n",
      "optimal threshold: -0.5356\n",
      "Epoch 15 train loss: 0.6328, eval loss 0.6692401170730591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 12:51:38,988] Trial 16 finished with value: 0.639729380607605 and parameters: {'learning_rate_exp': -3.408727151391613, 'dropout_p': 0.06383019939764556, 'l2_reg_exp': -6.027542420680529, 'batch_size': 166, 'N': 305}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7579\n",
      "optimal threshold: -0.4167\n",
      "Epoch 0 train loss: 0.5504, eval loss 0.6680738925933838\n",
      "optimal threshold: -0.7713\n",
      "Epoch 1 train loss: 0.4772, eval loss 0.6639308929443359\n",
      "optimal threshold: -0.5152\n",
      "Epoch 2 train loss: 0.4860, eval loss 0.6626078486442566\n",
      "optimal threshold: -0.4032\n",
      "Epoch 3 train loss: 0.4427, eval loss 0.6628410220146179\n",
      "optimal threshold: -0.7174\n",
      "Epoch 4 train loss: 0.5202, eval loss 0.6591835021972656\n",
      "optimal threshold: -0.4874\n",
      "Epoch 5 train loss: 0.4863, eval loss 0.6667760014533997\n",
      "optimal threshold: -0.4608\n",
      "Epoch 6 train loss: 0.5516, eval loss 0.6662893891334534\n",
      "optimal threshold: -0.4511\n",
      "Epoch 7 train loss: 0.5221, eval loss 0.6714785695075989\n",
      "optimal threshold: -0.6008\n",
      "Epoch 8 train loss: 0.3944, eval loss 0.6751096248626709\n",
      "optimal threshold: -0.4744\n",
      "Epoch 9 train loss: 0.5021, eval loss 0.684392511844635\n",
      "optimal threshold: -0.3759\n",
      "Epoch 10 train loss: 0.6480, eval loss 0.6970189809799194\n",
      "optimal threshold: -0.5574\n",
      "Epoch 11 train loss: 0.4968, eval loss 0.7028986811637878\n",
      "optimal threshold: -0.4683\n",
      "Epoch 12 train loss: 0.4403, eval loss 0.7066487073898315\n",
      "optimal threshold: -0.4333\n",
      "Epoch 13 train loss: 0.4963, eval loss 0.7195662260055542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 12:51:55,444] Trial 17 finished with value: 0.4248727262020111 and parameters: {'learning_rate_exp': -2.461164348391476, 'dropout_p': 0.2655676839944719, 'l2_reg_exp': -4.981754968504764, 'batch_size': 76, 'N': 199}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6347\n",
      "optimal threshold: -0.5212\n",
      "Epoch 0 train loss: 0.7140, eval loss 0.6650883555412292\n",
      "optimal threshold: -0.4370\n",
      "Epoch 1 train loss: 0.6882, eval loss 0.6617240905761719\n",
      "optimal threshold: -0.5187\n",
      "Epoch 2 train loss: 0.6778, eval loss 0.6608136892318726\n",
      "optimal threshold: -0.5369\n",
      "Epoch 3 train loss: 0.7422, eval loss 0.6573802828788757\n",
      "optimal threshold: -0.6338\n",
      "Epoch 4 train loss: 0.6804, eval loss 0.6617662906646729\n",
      "optimal threshold: -0.5262\n",
      "Epoch 5 train loss: 0.6749, eval loss 0.6615827083587646\n",
      "optimal threshold: -0.4559\n",
      "Epoch 6 train loss: 0.6505, eval loss 0.6636000871658325\n",
      "optimal threshold: -0.4851\n",
      "Epoch 7 train loss: 0.6459, eval loss 0.6690309643745422\n",
      "optimal threshold: -0.5086\n",
      "Epoch 8 train loss: 0.6834, eval loss 0.6676310896873474\n",
      "optimal threshold: -0.3910\n",
      "Epoch 9 train loss: 0.6144, eval loss 0.6765687465667725\n",
      "optimal threshold: -0.5267\n",
      "Epoch 10 train loss: 0.6046, eval loss 0.676733672618866\n",
      "optimal threshold: -0.4725\n",
      "Epoch 11 train loss: 0.5659, eval loss 0.6957847476005554\n",
      "optimal threshold: -0.5007\n",
      "Epoch 12 train loss: 0.6675, eval loss 0.6972048282623291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 12:52:07,966] Trial 18 finished with value: 0.5787723660469055 and parameters: {'learning_rate_exp': -2.480032639239334, 'dropout_p': 0.3950383639806452, 'l2_reg_exp': -5.640704656823698, 'batch_size': 163, 'N': 380}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6827\n",
      "optimal threshold: -0.7798\n",
      "Epoch 0 train loss: 0.9695, eval loss 1.0089648962020874\n",
      "optimal threshold: -0.6880\n",
      "Epoch 1 train loss: 0.7181, eval loss 0.745180070400238\n",
      "optimal threshold: -0.7679\n",
      "Epoch 2 train loss: 0.6526, eval loss 0.7026782631874084\n",
      "optimal threshold: -0.6369\n",
      "Epoch 3 train loss: 0.6792, eval loss 0.6865813732147217\n",
      "optimal threshold: -0.6675\n",
      "Epoch 4 train loss: 0.6171, eval loss 0.6773919463157654\n",
      "optimal threshold: -0.6149\n",
      "Epoch 5 train loss: 0.6780, eval loss 0.6709580421447754\n",
      "optimal threshold: -0.6992\n",
      "Epoch 6 train loss: 0.6759, eval loss 0.6673535704612732\n",
      "optimal threshold: -0.6596\n",
      "Epoch 7 train loss: 0.6567, eval loss 0.6643931269645691\n",
      "optimal threshold: -0.7985\n",
      "Epoch 8 train loss: 0.6461, eval loss 0.662754476070404\n",
      "optimal threshold: -0.3464\n",
      "Epoch 9 train loss: 0.6975, eval loss 0.660825252532959\n",
      "optimal threshold: -0.3863\n",
      "Epoch 10 train loss: 0.6410, eval loss 0.6598312258720398\n",
      "optimal threshold: -0.7303\n",
      "Epoch 11 train loss: 0.5944, eval loss 0.6594789624214172\n",
      "optimal threshold: -0.3705\n",
      "Epoch 12 train loss: 0.6134, eval loss 0.6583138108253479\n",
      "optimal threshold: -0.4167\n",
      "Epoch 13 train loss: 0.6089, eval loss 0.6576241850852966\n",
      "optimal threshold: -0.7616\n",
      "Epoch 14 train loss: 0.6010, eval loss 0.6571624875068665\n",
      "optimal threshold: -0.7566\n",
      "Epoch 15 train loss: 0.5710, eval loss 0.6568976640701294\n",
      "optimal threshold: -0.7025\n",
      "Epoch 16 train loss: 0.5068, eval loss 0.6570044159889221\n",
      "optimal threshold: -0.7134\n",
      "Epoch 17 train loss: 0.5983, eval loss 0.6566649675369263\n",
      "optimal threshold: -0.7276\n",
      "Epoch 18 train loss: 0.5719, eval loss 0.6564422845840454\n",
      "optimal threshold: -0.7608\n",
      "Epoch 19 train loss: 0.5209, eval loss 0.6565123796463013\n",
      "optimal threshold: -0.6713\n",
      "Epoch 20 train loss: 0.5434, eval loss 0.6565050482749939\n",
      "optimal threshold: -0.6940\n",
      "Epoch 21 train loss: 0.5517, eval loss 0.6563112139701843\n",
      "optimal threshold: -0.6825\n",
      "Epoch 22 train loss: 0.5214, eval loss 0.6562147736549377\n",
      "optimal threshold: -0.6892\n",
      "Epoch 23 train loss: 0.5443, eval loss 0.6564223766326904\n",
      "optimal threshold: -0.6768\n",
      "Epoch 24 train loss: 0.5023, eval loss 0.6562183499336243\n",
      "optimal threshold: -0.7004\n",
      "Epoch 25 train loss: 0.5553, eval loss 0.6569055914878845\n",
      "optimal threshold: -0.7092\n",
      "Epoch 26 train loss: 0.5936, eval loss 0.6566397547721863\n",
      "optimal threshold: -0.4637\n",
      "Epoch 27 train loss: 0.5499, eval loss 0.6560616493225098\n",
      "optimal threshold: -0.7050\n",
      "Epoch 28 train loss: 0.5455, eval loss 0.6567829251289368\n",
      "optimal threshold: -0.6830\n",
      "Epoch 29 train loss: 0.5228, eval loss 0.6568624973297119\n",
      "optimal threshold: -0.5181\n",
      "Epoch 30 train loss: 0.5505, eval loss 0.6576575040817261\n",
      "optimal threshold: -0.7158\n",
      "Epoch 31 train loss: 0.4436, eval loss 0.6578676104545593\n",
      "optimal threshold: -0.5242\n",
      "Epoch 32 train loss: 0.5805, eval loss 0.6570653915405273\n",
      "optimal threshold: -0.7300\n",
      "Epoch 33 train loss: 0.4914, eval loss 0.6584970951080322\n",
      "optimal threshold: -0.7115\n",
      "Epoch 34 train loss: 0.5051, eval loss 0.6580876111984253\n",
      "optimal threshold: -0.4354\n",
      "Epoch 35 train loss: 0.4285, eval loss 0.6582516431808472\n",
      "optimal threshold: -0.6217\n",
      "Epoch 36 train loss: 0.4825, eval loss 0.6577860713005066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 12:52:24,582] Trial 19 finished with value: 0.489381343126297 and parameters: {'learning_rate_exp': -3.443324293865258, 'dropout_p': 0.3431071449341892, 'l2_reg_exp': -4.280856975532459, 'batch_size': 371, 'N': 116}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5276\n",
      "optimal threshold: -0.3980\n",
      "Epoch 0 train loss: 0.8171, eval loss 0.6663106679916382\n",
      "optimal threshold: -0.3043\n",
      "Epoch 1 train loss: 0.7660, eval loss 0.6766155362129211\n",
      "optimal threshold: -0.4765\n",
      "Epoch 2 train loss: 0.7129, eval loss 0.6710346341133118\n",
      "optimal threshold: -0.5848\n",
      "Epoch 3 train loss: 0.7340, eval loss 0.6764899492263794\n",
      "optimal threshold: -0.3338\n",
      "Epoch 4 train loss: 0.6129, eval loss 0.6821289658546448\n",
      "optimal threshold: -0.5782\n",
      "Epoch 5 train loss: 0.5735, eval loss 0.6864861249923706\n",
      "optimal threshold: -0.5164\n",
      "Epoch 6 train loss: 0.6825, eval loss 0.6997615694999695\n",
      "optimal threshold: -0.5505\n",
      "Epoch 7 train loss: 0.5869, eval loss 0.7035461068153381\n",
      "optimal threshold: -0.5513\n",
      "Epoch 8 train loss: 0.5751, eval loss 0.7003533244132996\n",
      "optimal threshold: -0.4819\n",
      "Epoch 9 train loss: 0.6040, eval loss 0.7209159135818481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 12:52:40,672] Trial 20 finished with value: 0.571910560131073 and parameters: {'learning_rate_exp': -2.0115689830812102, 'dropout_p': 0.23745232950447198, 'l2_reg_exp': -4.925853242448959, 'batch_size': 64, 'N': 228}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5168\n",
      "optimal threshold: -0.5485\n",
      "Epoch 0 train loss: 0.7504, eval loss 0.6696879863739014\n",
      "optimal threshold: -0.6107\n",
      "Epoch 1 train loss: 0.7472, eval loss 0.6613574624061584\n",
      "optimal threshold: -0.6288\n",
      "Epoch 2 train loss: 0.7692, eval loss 0.6590441465377808\n",
      "optimal threshold: -0.4817\n",
      "Epoch 3 train loss: 0.7426, eval loss 0.6577991247177124\n",
      "optimal threshold: -0.4333\n",
      "Epoch 4 train loss: 0.6463, eval loss 0.6570538878440857\n",
      "optimal threshold: -0.4472\n",
      "Epoch 5 train loss: 0.6899, eval loss 0.6576805710792542\n",
      "optimal threshold: -0.3642\n",
      "Epoch 6 train loss: 0.7061, eval loss 0.6572363376617432\n",
      "optimal threshold: -0.4969\n",
      "Epoch 7 train loss: 0.7120, eval loss 0.6574766635894775\n",
      "optimal threshold: -0.3906\n",
      "Epoch 8 train loss: 0.6950, eval loss 0.6577637791633606\n",
      "optimal threshold: -0.4174\n",
      "Epoch 9 train loss: 0.7376, eval loss 0.6599261164665222\n",
      "optimal threshold: -0.5074\n",
      "Epoch 10 train loss: 0.6989, eval loss 0.6607577800750732\n",
      "optimal threshold: -0.5368\n",
      "Epoch 11 train loss: 0.6983, eval loss 0.6629703044891357\n",
      "optimal threshold: -0.5310\n",
      "Epoch 12 train loss: 0.6498, eval loss 0.664473295211792\n",
      "optimal threshold: -0.4461\n",
      "Epoch 13 train loss: 0.7080, eval loss 0.6658086180686951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 12:53:30,489] Trial 21 finished with value: 0.7737215161323547 and parameters: {'learning_rate_exp': -3.705181960124852, 'dropout_p': 0.30352763429513174, 'l2_reg_exp': -3.899690705654699, 'batch_size': 23, 'N': 355}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4307\n",
      "optimal threshold: -0.8103\n",
      "Epoch 0 train loss: 0.5845, eval loss 0.739281415939331\n",
      "optimal threshold: -0.8199\n",
      "Epoch 1 train loss: 0.4654, eval loss 0.6980028748512268\n",
      "optimal threshold: -0.7475\n",
      "Epoch 2 train loss: 0.4283, eval loss 0.6830411553382874\n",
      "optimal threshold: -0.6549\n",
      "Epoch 3 train loss: 0.4038, eval loss 0.674068033695221\n",
      "optimal threshold: -0.6098\n",
      "Epoch 4 train loss: 0.5099, eval loss 0.6688700318336487\n",
      "optimal threshold: -0.5667\n",
      "Epoch 5 train loss: 0.4899, eval loss 0.6658217906951904\n",
      "optimal threshold: -0.5121\n",
      "Epoch 6 train loss: 0.4799, eval loss 0.6638298630714417\n",
      "optimal threshold: -0.4044\n",
      "Epoch 7 train loss: 0.4997, eval loss 0.6621195673942566\n",
      "optimal threshold: -0.4249\n",
      "Epoch 8 train loss: 0.5589, eval loss 0.6611456274986267\n",
      "optimal threshold: -0.4434\n",
      "Epoch 9 train loss: 0.3868, eval loss 0.6601044535636902\n",
      "optimal threshold: -0.7069\n",
      "Epoch 10 train loss: 0.5100, eval loss 0.659014105796814\n",
      "optimal threshold: -0.3442\n",
      "Epoch 11 train loss: 0.2504, eval loss 0.6582337021827698\n",
      "optimal threshold: -0.3556\n",
      "Epoch 12 train loss: 0.3035, eval loss 0.6576778888702393\n",
      "optimal threshold: -0.3790\n",
      "Epoch 13 train loss: 0.4517, eval loss 0.65708988904953\n",
      "optimal threshold: -0.3871\n",
      "Epoch 14 train loss: 0.4120, eval loss 0.6572554111480713\n",
      "optimal threshold: -0.4385\n",
      "Epoch 15 train loss: 0.2823, eval loss 0.6568753719329834\n",
      "optimal threshold: -0.4538\n",
      "Epoch 16 train loss: 0.3193, eval loss 0.6561709046363831\n",
      "optimal threshold: -0.4398\n",
      "Epoch 17 train loss: 0.3664, eval loss 0.6562475562095642\n",
      "optimal threshold: -0.4032\n",
      "Epoch 18 train loss: 0.4235, eval loss 0.6558645367622375\n",
      "optimal threshold: -0.3978\n",
      "Epoch 19 train loss: 0.3914, eval loss 0.6556055545806885\n",
      "optimal threshold: -0.4361\n",
      "Epoch 20 train loss: 0.3647, eval loss 0.6558650135993958\n",
      "optimal threshold: -0.4642\n",
      "Epoch 21 train loss: 0.2817, eval loss 0.6555516123771667\n",
      "optimal threshold: -0.5178\n",
      "Epoch 22 train loss: 0.3643, eval loss 0.6548606157302856\n",
      "optimal threshold: -0.5185\n",
      "Epoch 23 train loss: 0.3631, eval loss 0.6548251509666443\n",
      "optimal threshold: -0.5245\n",
      "Epoch 24 train loss: 0.3043, eval loss 0.6552711725234985\n",
      "optimal threshold: -0.5705\n",
      "Epoch 25 train loss: 0.2823, eval loss 0.6547238826751709\n",
      "optimal threshold: -0.5576\n",
      "Epoch 26 train loss: 0.3138, eval loss 0.6548182368278503\n",
      "optimal threshold: -0.5862\n",
      "Epoch 27 train loss: 0.2115, eval loss 0.655055582523346\n",
      "optimal threshold: -0.5950\n",
      "Epoch 28 train loss: 0.3085, eval loss 0.6555837988853455\n",
      "optimal threshold: -0.6265\n",
      "Epoch 29 train loss: 0.2631, eval loss 0.6554655432701111\n",
      "optimal threshold: -0.6251\n",
      "Epoch 30 train loss: 0.2988, eval loss 0.6557631492614746\n",
      "optimal threshold: -0.6306\n",
      "Epoch 31 train loss: 0.3591, eval loss 0.655358076095581\n",
      "optimal threshold: -0.6009\n",
      "Epoch 32 train loss: 0.2494, eval loss 0.6557091474533081\n",
      "optimal threshold: -0.5688\n",
      "Epoch 33 train loss: 0.2216, eval loss 0.6549631357192993\n",
      "optimal threshold: -0.6066\n",
      "Epoch 34 train loss: 0.2339, eval loss 0.6554291844367981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 12:55:22,826] Trial 22 finished with value: 0.3225397765636444 and parameters: {'learning_rate_exp': -4.352003614161264, 'dropout_p': 0.3385103236497871, 'l2_reg_exp': -4.550337225055875, 'batch_size': 22, 'N': 280}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5305\n",
      "optimal threshold: -0.8818\n",
      "Epoch 0 train loss: 1.2664, eval loss 0.8256893754005432\n",
      "optimal threshold: -0.9236\n",
      "Epoch 1 train loss: 0.4068, eval loss 0.7207532525062561\n",
      "optimal threshold: -0.8945\n",
      "Epoch 2 train loss: 0.4922, eval loss 0.7005393505096436\n",
      "optimal threshold: -0.8721\n",
      "Epoch 3 train loss: 0.6022, eval loss 0.6889994144439697\n",
      "optimal threshold: -0.7436\n",
      "Epoch 4 train loss: 0.7857, eval loss 0.6809565424919128\n",
      "optimal threshold: -0.7349\n",
      "Epoch 5 train loss: 0.4554, eval loss 0.6753742098808289\n",
      "optimal threshold: -0.6986\n",
      "Epoch 6 train loss: 0.4359, eval loss 0.6714231967926025\n",
      "optimal threshold: -0.6199\n",
      "Epoch 7 train loss: 0.3549, eval loss 0.6683828830718994\n",
      "optimal threshold: -0.6120\n",
      "Epoch 8 train loss: 0.2861, eval loss 0.6663508415222168\n",
      "optimal threshold: -0.5795\n",
      "Epoch 9 train loss: 0.2449, eval loss 0.6645256876945496\n",
      "optimal threshold: -0.6206\n",
      "Epoch 10 train loss: 0.4933, eval loss 0.6633467674255371\n",
      "optimal threshold: -0.5890\n",
      "Epoch 11 train loss: 0.4497, eval loss 0.6624903082847595\n",
      "optimal threshold: -0.6308\n",
      "Epoch 12 train loss: 0.4478, eval loss 0.6614296436309814\n",
      "optimal threshold: -0.6201\n",
      "Epoch 13 train loss: 0.3258, eval loss 0.6609827280044556\n",
      "optimal threshold: -0.6197\n",
      "Epoch 14 train loss: 0.4391, eval loss 0.6603368520736694\n",
      "optimal threshold: -0.5963\n",
      "Epoch 15 train loss: 0.2464, eval loss 0.6595479846000671\n",
      "optimal threshold: -0.7911\n",
      "Epoch 16 train loss: 0.1982, eval loss 0.659186840057373\n",
      "optimal threshold: -0.7987\n",
      "Epoch 17 train loss: 0.3057, eval loss 0.6588353514671326\n",
      "optimal threshold: -0.7915\n",
      "Epoch 18 train loss: 0.3815, eval loss 0.6582517027854919\n",
      "optimal threshold: -0.8049\n",
      "Epoch 19 train loss: 0.4058, eval loss 0.6577258110046387\n",
      "optimal threshold: -0.8334\n",
      "Epoch 20 train loss: 0.3749, eval loss 0.657294750213623\n",
      "optimal threshold: -0.7470\n",
      "Epoch 21 train loss: 0.1837, eval loss 0.6571350693702698\n",
      "optimal threshold: -0.7177\n",
      "Epoch 22 train loss: 0.2737, eval loss 0.656727135181427\n",
      "optimal threshold: -0.7622\n",
      "Epoch 23 train loss: 0.2677, eval loss 0.656566321849823\n",
      "optimal threshold: -0.7094\n",
      "Epoch 24 train loss: 0.2289, eval loss 0.6562991738319397\n",
      "optimal threshold: -0.7542\n",
      "Epoch 25 train loss: 0.4829, eval loss 0.6562103033065796\n",
      "optimal threshold: -0.7491\n",
      "Epoch 26 train loss: 0.1926, eval loss 0.6561955213546753\n",
      "optimal threshold: -0.7130\n",
      "Epoch 27 train loss: 0.2374, eval loss 0.6561291217803955\n",
      "optimal threshold: -0.7521\n",
      "Epoch 28 train loss: 0.3661, eval loss 0.6555427312850952\n",
      "optimal threshold: -0.7030\n",
      "Epoch 29 train loss: 0.4357, eval loss 0.6554436683654785\n",
      "optimal threshold: -0.7198\n",
      "Epoch 30 train loss: 0.1651, eval loss 0.6552953124046326\n",
      "optimal threshold: -0.7071\n",
      "Epoch 31 train loss: 0.2386, eval loss 0.655407726764679\n",
      "optimal threshold: -0.6128\n",
      "Epoch 32 train loss: 0.1760, eval loss 0.6551014184951782\n",
      "optimal threshold: -0.6458\n",
      "Epoch 33 train loss: 0.1976, eval loss 0.6550899744033813\n",
      "optimal threshold: -0.7184\n",
      "Epoch 34 train loss: 0.2870, eval loss 0.6551389694213867\n",
      "optimal threshold: -0.7014\n",
      "Epoch 35 train loss: 0.4394, eval loss 0.6549965143203735\n",
      "optimal threshold: -0.5226\n",
      "Epoch 36 train loss: 0.2108, eval loss 0.6550455093383789\n",
      "optimal threshold: -0.5891\n",
      "Epoch 37 train loss: 0.2193, eval loss 0.6552244424819946\n",
      "optimal threshold: -0.6350\n",
      "Epoch 38 train loss: 0.3428, eval loss 0.655221700668335\n",
      "optimal threshold: -0.5270\n",
      "Epoch 39 train loss: 0.2437, eval loss 0.6555255651473999\n",
      "optimal threshold: -0.4690\n",
      "Epoch 40 train loss: 0.2490, eval loss 0.6554924249649048\n",
      "optimal threshold: -0.7068\n",
      "Epoch 41 train loss: 0.1793, eval loss 0.6553693413734436\n",
      "optimal threshold: -0.7197\n",
      "Epoch 42 train loss: 0.1820, eval loss 0.6555817723274231\n",
      "optimal threshold: -0.6901\n",
      "Epoch 43 train loss: 0.3506, eval loss 0.6552008986473083\n",
      "optimal threshold: -0.4278\n",
      "Epoch 44 train loss: 0.1813, eval loss 0.6556091904640198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 13:02:02,342] Trial 23 finished with value: 0.15651525557041168 and parameters: {'learning_rate_exp': -4.838310108542985, 'dropout_p': 0.4175204649579394, 'l2_reg_exp': -3.096557687297072, 'batch_size': 9, 'N': 435}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4119\n",
      "optimal threshold: -0.7127\n",
      "Epoch 0 train loss: 1.0678, eval loss 1.0893937349319458\n",
      "optimal threshold: -0.9700\n",
      "Epoch 1 train loss: 0.8489, eval loss 0.859258234500885\n",
      "optimal threshold: -0.8238\n",
      "Epoch 2 train loss: 0.7850, eval loss 0.7526091933250427\n",
      "optimal threshold: -0.8323\n",
      "Epoch 3 train loss: 0.7700, eval loss 0.721077561378479\n",
      "optimal threshold: -0.8872\n",
      "Epoch 4 train loss: 0.7210, eval loss 0.7078550457954407\n",
      "optimal threshold: -0.9470\n",
      "Epoch 5 train loss: 0.7539, eval loss 0.6991472840309143\n",
      "optimal threshold: -0.7282\n",
      "Epoch 6 train loss: 0.7546, eval loss 0.692487895488739\n",
      "optimal threshold: -0.8668\n",
      "Epoch 7 train loss: 0.6809, eval loss 0.6869840621948242\n",
      "optimal threshold: -0.8964\n",
      "Epoch 8 train loss: 0.7751, eval loss 0.6825258135795593\n",
      "optimal threshold: -0.8895\n",
      "Epoch 9 train loss: 0.7669, eval loss 0.6788337230682373\n",
      "optimal threshold: -0.8972\n",
      "Epoch 10 train loss: 0.6757, eval loss 0.6758830547332764\n",
      "optimal threshold: -0.8676\n",
      "Epoch 11 train loss: 0.7602, eval loss 0.6733564138412476\n",
      "optimal threshold: -0.8961\n",
      "Epoch 12 train loss: 0.7351, eval loss 0.6711661219596863\n",
      "optimal threshold: -0.6387\n",
      "Epoch 13 train loss: 0.6914, eval loss 0.6694591641426086\n",
      "optimal threshold: -0.4634\n",
      "Epoch 14 train loss: 0.6923, eval loss 0.6677902340888977\n",
      "optimal threshold: -0.4609\n",
      "Epoch 15 train loss: 0.7144, eval loss 0.666758120059967\n",
      "optimal threshold: -0.4898\n",
      "Epoch 16 train loss: 0.6810, eval loss 0.6657091975212097\n",
      "optimal threshold: -0.4323\n",
      "Epoch 17 train loss: 0.6859, eval loss 0.6645023822784424\n",
      "optimal threshold: -0.5142\n",
      "Epoch 18 train loss: 0.6950, eval loss 0.6637993454933167\n",
      "optimal threshold: -0.5057\n",
      "Epoch 19 train loss: 0.7231, eval loss 0.6629140377044678\n",
      "optimal threshold: -0.4756\n",
      "Epoch 20 train loss: 0.7083, eval loss 0.6621617078781128\n",
      "optimal threshold: -0.7144\n",
      "Epoch 21 train loss: 0.7014, eval loss 0.6616499423980713\n",
      "optimal threshold: -0.5955\n",
      "Epoch 22 train loss: 0.6965, eval loss 0.6608790755271912\n",
      "optimal threshold: -0.4119\n",
      "Epoch 23 train loss: 0.6897, eval loss 0.6604511141777039\n",
      "optimal threshold: -0.6040\n",
      "Epoch 24 train loss: 0.6795, eval loss 0.6603555083274841\n",
      "optimal threshold: -0.5520\n",
      "Epoch 25 train loss: 0.7304, eval loss 0.6598836779594421\n",
      "optimal threshold: -0.5322\n",
      "Epoch 26 train loss: 0.6857, eval loss 0.6597862839698792\n",
      "optimal threshold: -0.5340\n",
      "Epoch 27 train loss: 0.6820, eval loss 0.6593390107154846\n",
      "optimal threshold: -0.5990\n",
      "Epoch 28 train loss: 0.6610, eval loss 0.6590936779975891\n",
      "optimal threshold: -0.5582\n",
      "Epoch 29 train loss: 0.6488, eval loss 0.6585550904273987\n",
      "optimal threshold: -0.5910\n",
      "Epoch 30 train loss: 0.7093, eval loss 0.658621609210968\n",
      "optimal threshold: -0.5861\n",
      "Epoch 31 train loss: 0.6765, eval loss 0.6582814455032349\n",
      "optimal threshold: -0.6085\n",
      "Epoch 32 train loss: 0.6493, eval loss 0.6581756472587585\n",
      "optimal threshold: -0.5972\n",
      "Epoch 33 train loss: 0.7063, eval loss 0.6579138040542603\n",
      "optimal threshold: -0.5866\n",
      "Epoch 34 train loss: 0.6532, eval loss 0.6578116416931152\n",
      "optimal threshold: -0.5844\n",
      "Epoch 35 train loss: 0.6848, eval loss 0.6575527787208557\n",
      "optimal threshold: -0.5914\n",
      "Epoch 36 train loss: 0.6493, eval loss 0.6572248935699463\n",
      "optimal threshold: -0.4609\n",
      "Epoch 37 train loss: 0.6557, eval loss 0.6572346091270447\n",
      "optimal threshold: -0.4693\n",
      "Epoch 38 train loss: 0.6545, eval loss 0.657096266746521\n",
      "optimal threshold: -0.3998\n",
      "Epoch 39 train loss: 0.6818, eval loss 0.6569752097129822\n",
      "optimal threshold: -0.3950\n",
      "Epoch 40 train loss: 0.6408, eval loss 0.6567018628120422\n",
      "optimal threshold: -0.3848\n",
      "Epoch 41 train loss: 0.6291, eval loss 0.6565098166465759\n",
      "optimal threshold: -0.3798\n",
      "Epoch 42 train loss: 0.6129, eval loss 0.6565582156181335\n",
      "optimal threshold: -0.3759\n",
      "Epoch 43 train loss: 0.6804, eval loss 0.6566406488418579\n",
      "optimal threshold: -0.4015\n",
      "Epoch 44 train loss: 0.6500, eval loss 0.6565942168235779\n",
      "optimal threshold: -0.3697\n",
      "Epoch 45 train loss: 0.6838, eval loss 0.6562482714653015\n",
      "optimal threshold: -0.3974\n",
      "Epoch 46 train loss: 0.6614, eval loss 0.656204342842102\n",
      "optimal threshold: -0.7053\n",
      "Epoch 47 train loss: 0.6119, eval loss 0.6561166644096375\n",
      "optimal threshold: -0.3954\n",
      "Epoch 48 train loss: 0.7770, eval loss 0.656015157699585\n",
      "optimal threshold: -0.3971\n",
      "Epoch 49 train loss: 0.5963, eval loss 0.6557861566543579\n",
      "optimal threshold: -0.4002\n",
      "Epoch 50 train loss: 0.6296, eval loss 0.6560311913490295\n",
      "optimal threshold: -0.3945\n",
      "Epoch 51 train loss: 0.5996, eval loss 0.6561712026596069\n",
      "optimal threshold: -0.3673\n",
      "Epoch 52 train loss: 0.6102, eval loss 0.6560777425765991\n",
      "optimal threshold: -0.3686\n",
      "Epoch 53 train loss: 0.6846, eval loss 0.6557866930961609\n",
      "optimal threshold: -0.3602\n",
      "Epoch 54 train loss: 0.6154, eval loss 0.6557889580726624\n",
      "optimal threshold: -0.3792\n",
      "Epoch 55 train loss: 0.6684, eval loss 0.6559295654296875\n",
      "optimal threshold: -0.3879\n",
      "Epoch 56 train loss: 0.6588, eval loss 0.6558024883270264\n",
      "optimal threshold: -0.6863\n",
      "Epoch 57 train loss: 0.6812, eval loss 0.6557116508483887\n",
      "optimal threshold: -0.6860\n",
      "Epoch 58 train loss: 0.6642, eval loss 0.6555803418159485\n",
      "optimal threshold: -0.5444\n",
      "Epoch 59 train loss: 0.6326, eval loss 0.6556841731071472\n",
      "optimal threshold: -0.5590\n",
      "Epoch 60 train loss: 0.6523, eval loss 0.6557080149650574\n",
      "optimal threshold: -0.5417\n",
      "Epoch 61 train loss: 0.6969, eval loss 0.6554512977600098\n",
      "optimal threshold: -0.5909\n",
      "Epoch 62 train loss: 0.6444, eval loss 0.6555124521255493\n",
      "optimal threshold: -0.7260\n",
      "Epoch 63 train loss: 0.6259, eval loss 0.655841052532196\n",
      "optimal threshold: -0.5845\n",
      "Epoch 64 train loss: 0.6531, eval loss 0.65541011095047\n",
      "optimal threshold: -0.5736\n",
      "Epoch 65 train loss: 0.6600, eval loss 0.6555427312850952\n",
      "optimal threshold: -0.5814\n",
      "Epoch 66 train loss: 0.6569, eval loss 0.6553950309753418\n",
      "optimal threshold: -0.7083\n",
      "Epoch 67 train loss: 0.6490, eval loss 0.6555896997451782\n",
      "optimal threshold: -0.5751\n",
      "Epoch 68 train loss: 0.6699, eval loss 0.6554775834083557\n",
      "optimal threshold: -0.7475\n",
      "Epoch 69 train loss: 0.6310, eval loss 0.6556010246276855\n",
      "optimal threshold: -0.5880\n",
      "Epoch 70 train loss: 0.7079, eval loss 0.6555885076522827\n",
      "optimal threshold: -0.5798\n",
      "Epoch 71 train loss: 0.6865, eval loss 0.6556299924850464\n",
      "optimal threshold: -0.6455\n",
      "Epoch 72 train loss: 0.6237, eval loss 0.6558980941772461\n",
      "optimal threshold: -0.5968\n",
      "Epoch 73 train loss: 0.6381, eval loss 0.6559959053993225\n",
      "optimal threshold: -0.5602\n",
      "Epoch 74 train loss: 0.6059, eval loss 0.6557564735412598\n",
      "optimal threshold: -0.6664\n",
      "Epoch 75 train loss: 0.7144, eval loss 0.655379593372345\n",
      "optimal threshold: -0.7010\n",
      "Epoch 76 train loss: 0.6095, eval loss 0.655709445476532\n",
      "optimal threshold: -0.4764\n",
      "Epoch 77 train loss: 0.6256, eval loss 0.6558385491371155\n",
      "optimal threshold: -0.4860\n",
      "Epoch 78 train loss: 0.6717, eval loss 0.6559237837791443\n",
      "optimal threshold: -0.6985\n",
      "Epoch 79 train loss: 0.6440, eval loss 0.6560157537460327\n",
      "optimal threshold: -0.7113\n",
      "Epoch 80 train loss: 0.6690, eval loss 0.6560983657836914\n",
      "optimal threshold: -0.7074\n",
      "Epoch 81 train loss: 0.6315, eval loss 0.6561894416809082\n",
      "optimal threshold: -0.6970\n",
      "Epoch 82 train loss: 0.6708, eval loss 0.655890166759491\n",
      "optimal threshold: -0.6938\n",
      "Epoch 83 train loss: 0.6271, eval loss 0.6563538908958435\n",
      "optimal threshold: -0.6830\n",
      "Epoch 84 train loss: 0.6550, eval loss 0.656256914138794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 13:04:05,015] Trial 24 finished with value: 0.618192732334137 and parameters: {'learning_rate_exp': -4.5875897799676615, 'dropout_p': 0.4340998461631621, 'l2_reg_exp': -2.814964717710238, 'batch_size': 75, 'N': 448}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6987\n",
      "optimal threshold: -0.8845\n",
      "Epoch 0 train loss: 0.7570, eval loss 0.7110567092895508\n",
      "optimal threshold: -0.7726\n",
      "Epoch 1 train loss: 0.7301, eval loss 0.6781237721443176\n",
      "optimal threshold: -0.5971\n",
      "Epoch 2 train loss: 0.7088, eval loss 0.6670768857002258\n",
      "optimal threshold: -0.6452\n",
      "Epoch 3 train loss: 0.7125, eval loss 0.6628841757774353\n",
      "optimal threshold: -0.6054\n",
      "Epoch 4 train loss: 0.7257, eval loss 0.6614493727684021\n",
      "optimal threshold: -0.6913\n",
      "Epoch 5 train loss: 0.7209, eval loss 0.6599792242050171\n",
      "optimal threshold: -0.6415\n",
      "Epoch 6 train loss: 0.6822, eval loss 0.6592008471488953\n",
      "optimal threshold: -0.7538\n",
      "Epoch 7 train loss: 0.7128, eval loss 0.6595659255981445\n",
      "optimal threshold: -0.6748\n",
      "Epoch 8 train loss: 0.6750, eval loss 0.6582785248756409\n",
      "optimal threshold: -0.6814\n",
      "Epoch 9 train loss: 0.7336, eval loss 0.6582126021385193\n",
      "optimal threshold: -0.6125\n",
      "Epoch 10 train loss: 0.7113, eval loss 0.6583675146102905\n",
      "optimal threshold: -0.6507\n",
      "Epoch 11 train loss: 0.7188, eval loss 0.6591995358467102\n",
      "optimal threshold: -0.5521\n",
      "Epoch 12 train loss: 0.6774, eval loss 0.6579952836036682\n",
      "optimal threshold: -0.5614\n",
      "Epoch 13 train loss: 0.6807, eval loss 0.6582418084144592\n",
      "optimal threshold: -0.5412\n",
      "Epoch 14 train loss: 0.6903, eval loss 0.6578397154808044\n",
      "optimal threshold: -0.5853\n",
      "Epoch 15 train loss: 0.7089, eval loss 0.6580665111541748\n",
      "optimal threshold: -0.5685\n",
      "Epoch 16 train loss: 0.7044, eval loss 0.6589882373809814\n",
      "optimal threshold: -0.5575\n",
      "Epoch 17 train loss: 0.6943, eval loss 0.658301591873169\n",
      "optimal threshold: -0.5555\n",
      "Epoch 18 train loss: 0.6740, eval loss 0.6595743298530579\n",
      "optimal threshold: -0.5545\n",
      "Epoch 19 train loss: 0.6614, eval loss 0.6594832539558411\n",
      "optimal threshold: -0.5823\n",
      "Epoch 20 train loss: 0.6583, eval loss 0.660488486289978\n",
      "optimal threshold: -0.7491\n",
      "Epoch 21 train loss: 0.6394, eval loss 0.6609594821929932\n",
      "optimal threshold: -0.5623\n",
      "Epoch 22 train loss: 0.6686, eval loss 0.6604472994804382\n",
      "optimal threshold: -0.5648\n",
      "Epoch 23 train loss: 0.6528, eval loss 0.6613631248474121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 13:04:30,757] Trial 25 finished with value: 0.6451597809791565 and parameters: {'learning_rate_exp': -3.7442498378771263, 'dropout_p': 0.3660971868097916, 'l2_reg_exp': -3.7743175104371067, 'batch_size': 158, 'N': 510}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7232\n",
      "optimal threshold: -0.5458\n",
      "Epoch 0 train loss: 0.7452, eval loss 0.663164496421814\n",
      "optimal threshold: -0.5152\n",
      "Epoch 1 train loss: 0.6903, eval loss 0.6577030420303345\n",
      "optimal threshold: -0.4644\n",
      "Epoch 2 train loss: 0.7942, eval loss 0.6565644145011902\n",
      "optimal threshold: -0.6925\n",
      "Epoch 3 train loss: 0.6786, eval loss 0.6542542576789856\n",
      "optimal threshold: -0.4511\n",
      "Epoch 4 train loss: 0.6664, eval loss 0.6551401019096375\n",
      "optimal threshold: -0.6402\n",
      "Epoch 5 train loss: 0.7364, eval loss 0.655342161655426\n",
      "optimal threshold: -0.5591\n",
      "Epoch 6 train loss: 0.6175, eval loss 0.6543843150138855\n",
      "optimal threshold: -0.5485\n",
      "Epoch 7 train loss: 0.6851, eval loss 0.6566745638847351\n",
      "optimal threshold: -0.4960\n",
      "Epoch 8 train loss: 0.6414, eval loss 0.655980110168457\n",
      "optimal threshold: -0.5535\n",
      "Epoch 9 train loss: 0.6500, eval loss 0.6569792032241821\n",
      "optimal threshold: -0.4030\n",
      "Epoch 10 train loss: 0.6604, eval loss 0.6599639654159546\n",
      "optimal threshold: -0.5753\n",
      "Epoch 11 train loss: 0.5671, eval loss 0.6598695516586304\n",
      "optimal threshold: -0.6298\n",
      "Epoch 12 train loss: 0.5963, eval loss 0.6614444255828857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 13:04:53,602] Trial 26 finished with value: 0.6244691014289856 and parameters: {'learning_rate_exp': -3.1803047703948293, 'dropout_p': 0.42371362851034533, 'l2_reg_exp': -4.237027274955487, 'batch_size': 64, 'N': 411}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5118\n",
      "optimal threshold: -0.5389\n",
      "Epoch 0 train loss: 0.7616, eval loss 0.6649563312530518\n",
      "optimal threshold: -0.5862\n",
      "Epoch 1 train loss: 0.6725, eval loss 0.6609100103378296\n",
      "optimal threshold: -0.4837\n",
      "Epoch 2 train loss: 0.6480, eval loss 0.6602522134780884\n",
      "optimal threshold: -0.5050\n",
      "Epoch 3 train loss: 0.6444, eval loss 0.6591789722442627\n",
      "optimal threshold: -0.4319\n",
      "Epoch 4 train loss: 0.6668, eval loss 0.6583839058876038\n",
      "optimal threshold: -0.5561\n",
      "Epoch 5 train loss: 0.6406, eval loss 0.6587024927139282\n",
      "optimal threshold: -0.4679\n",
      "Epoch 6 train loss: 0.5996, eval loss 0.6583724021911621\n",
      "optimal threshold: -0.4544\n",
      "Epoch 7 train loss: 0.5947, eval loss 0.6636222004890442\n",
      "optimal threshold: -0.5682\n",
      "Epoch 8 train loss: 0.5977, eval loss 0.6638744473457336\n",
      "optimal threshold: -0.6165\n",
      "Epoch 9 train loss: 0.6233, eval loss 0.6745045781135559\n",
      "optimal threshold: -0.5560\n",
      "Epoch 10 train loss: 0.5323, eval loss 0.675572395324707\n",
      "optimal threshold: -0.5745\n",
      "Epoch 11 train loss: 0.6354, eval loss 0.6777480244636536\n",
      "optimal threshold: -0.6337\n",
      "Epoch 12 train loss: 0.5421, eval loss 0.6897591352462769\n",
      "optimal threshold: -0.5007\n",
      "Epoch 13 train loss: 0.5654, eval loss 0.6894590258598328\n",
      "optimal threshold: -0.5861\n",
      "Epoch 14 train loss: 0.5540, eval loss 0.7108631730079651\n",
      "optimal threshold: -0.5916\n",
      "Epoch 15 train loss: 0.5591, eval loss 0.7143910527229309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 13:05:09,049] Trial 27 finished with value: 0.6396213173866272 and parameters: {'learning_rate_exp': -2.576570750232559, 'dropout_p': 0.38259668926264817, 'l2_reg_exp': -4.80115433617816, 'batch_size': 135, 'N': 299}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4450\n",
      "optimal threshold: -0.7895\n",
      "Epoch 0 train loss: 0.6379, eval loss 0.6646220684051514\n",
      "optimal threshold: -0.5914\n",
      "Epoch 1 train loss: 0.6964, eval loss 0.6627848148345947\n",
      "optimal threshold: -0.7057\n",
      "Epoch 2 train loss: 0.7406, eval loss 0.6615380048751831\n",
      "optimal threshold: -0.5441\n",
      "Epoch 3 train loss: 0.6744, eval loss 0.6594217419624329\n",
      "optimal threshold: -0.6866\n",
      "Epoch 4 train loss: 0.6362, eval loss 0.6577154397964478\n",
      "optimal threshold: -0.4944\n",
      "Epoch 5 train loss: 0.6892, eval loss 0.6587464213371277\n",
      "optimal threshold: -0.4976\n",
      "Epoch 6 train loss: 0.6694, eval loss 0.6609558463096619\n",
      "optimal threshold: -0.5660\n",
      "Epoch 7 train loss: 0.6436, eval loss 0.6619648933410645\n",
      "optimal threshold: -0.6109\n",
      "Epoch 8 train loss: 0.6284, eval loss 0.6674511432647705\n",
      "optimal threshold: -0.4830\n",
      "Epoch 9 train loss: 0.6239, eval loss 0.6661123633384705\n",
      "optimal threshold: -0.5458\n",
      "Epoch 10 train loss: 0.6000, eval loss 0.6749559044837952\n",
      "optimal threshold: -0.7150\n",
      "Epoch 11 train loss: 0.6200, eval loss 0.6807689070701599\n",
      "optimal threshold: -0.6268\n",
      "Epoch 12 train loss: 0.6011, eval loss 0.6818362474441528\n",
      "optimal threshold: -0.5685\n",
      "Epoch 13 train loss: 0.5637, eval loss 0.6915819048881531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 13:05:27,625] Trial 28 finished with value: 0.6130806803703308 and parameters: {'learning_rate_exp': -2.6972201786494376, 'dropout_p': 0.44308569654195207, 'l2_reg_exp': -5.329285387592438, 'batch_size': 92, 'N': 387}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7012\n",
      "optimal threshold: -0.4613\n",
      "Epoch 0 train loss: 0.7082, eval loss 0.6660580635070801\n",
      "optimal threshold: -0.2944\n",
      "Epoch 1 train loss: 0.7098, eval loss 0.6588107943534851\n",
      "optimal threshold: -0.3105\n",
      "Epoch 2 train loss: 0.7168, eval loss 0.6566349267959595\n",
      "optimal threshold: -0.3793\n",
      "Epoch 3 train loss: 0.6667, eval loss 0.6558557748794556\n",
      "optimal threshold: -0.4776\n",
      "Epoch 4 train loss: 0.6969, eval loss 0.6568539142608643\n",
      "optimal threshold: -0.5012\n",
      "Epoch 5 train loss: 0.6564, eval loss 0.6568799614906311\n",
      "optimal threshold: -0.6783\n",
      "Epoch 6 train loss: 0.6216, eval loss 0.6564950346946716\n",
      "optimal threshold: -0.4598\n",
      "Epoch 7 train loss: 0.7050, eval loss 0.6582673788070679\n",
      "optimal threshold: -0.5497\n",
      "Epoch 8 train loss: 0.6483, eval loss 0.6577603816986084\n",
      "optimal threshold: -0.4963\n",
      "Epoch 9 train loss: 0.7140, eval loss 0.6602846384048462\n",
      "optimal threshold: -0.4478\n",
      "Epoch 10 train loss: 0.6623, eval loss 0.6607535481452942\n",
      "optimal threshold: -0.5369\n",
      "Epoch 11 train loss: 0.6325, eval loss 0.6644214987754822\n",
      "optimal threshold: -0.4800\n",
      "Epoch 12 train loss: 0.7037, eval loss 0.667194128036499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 13:05:57,833] Trial 29 finished with value: 0.6428514719009399 and parameters: {'learning_rate_exp': -3.373847504233666, 'dropout_p': 0.39636440590879396, 'l2_reg_exp': -5.118767586045417, 'batch_size': 46, 'N': 462}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4734\n",
      "optimal threshold: -0.4656\n",
      "Epoch 0 train loss: 0.7253, eval loss 0.6651423573493958\n",
      "optimal threshold: -0.4911\n",
      "Epoch 1 train loss: 0.7110, eval loss 0.663091242313385\n",
      "optimal threshold: -0.4172\n",
      "Epoch 2 train loss: 0.6572, eval loss 0.6629841327667236\n",
      "optimal threshold: -0.4617\n",
      "Epoch 3 train loss: 0.6241, eval loss 0.6585317254066467\n",
      "optimal threshold: -0.4399\n",
      "Epoch 4 train loss: 0.6455, eval loss 0.6615182161331177\n",
      "optimal threshold: -0.3228\n",
      "Epoch 5 train loss: 0.6496, eval loss 0.6616131067276001\n",
      "optimal threshold: -0.5716\n",
      "Epoch 6 train loss: 0.5682, eval loss 0.6687262654304504\n",
      "optimal threshold: -0.4866\n",
      "Epoch 7 train loss: 0.5679, eval loss 0.6686830520629883\n",
      "optimal threshold: -0.4613\n",
      "Epoch 8 train loss: 0.6139, eval loss 0.6766736507415771\n",
      "optimal threshold: -0.5871\n",
      "Epoch 9 train loss: 0.6239, eval loss 0.6781579852104187\n",
      "optimal threshold: -0.6356\n",
      "Epoch 10 train loss: 0.5662, eval loss 0.6802002191543579\n",
      "optimal threshold: -0.5736\n",
      "Epoch 11 train loss: 0.5544, eval loss 0.6884114742279053\n",
      "optimal threshold: -0.5990\n",
      "Epoch 12 train loss: 0.4848, eval loss 0.6920766234397888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 13:06:07,799] Trial 30 finished with value: 0.4212764501571655 and parameters: {'learning_rate_exp': -2.264507701962939, 'dropout_p': 0.4161713337184488, 'l2_reg_exp': -4.442959251061943, 'batch_size': 208, 'N': 321}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5100\n",
      "optimal threshold: -0.8246\n",
      "Epoch 0 train loss: 0.6171, eval loss 0.7570088505744934\n",
      "optimal threshold: -0.7136\n",
      "Epoch 1 train loss: 0.3563, eval loss 0.7055625915527344\n",
      "optimal threshold: -0.7068\n",
      "Epoch 2 train loss: 0.3166, eval loss 0.6884469985961914\n",
      "optimal threshold: -0.7055\n",
      "Epoch 3 train loss: 0.4219, eval loss 0.6779523491859436\n",
      "optimal threshold: -0.7455\n",
      "Epoch 4 train loss: 0.2609, eval loss 0.6719071269035339\n",
      "optimal threshold: -0.7093\n",
      "Epoch 5 train loss: 0.4551, eval loss 0.667931079864502\n",
      "optimal threshold: -0.7410\n",
      "Epoch 6 train loss: 0.3825, eval loss 0.6654046177864075\n",
      "optimal threshold: -0.7631\n",
      "Epoch 7 train loss: 0.4286, eval loss 0.6636790633201599\n",
      "optimal threshold: -0.4301\n",
      "Epoch 8 train loss: 0.3356, eval loss 0.6624740958213806\n",
      "optimal threshold: -0.4154\n",
      "Epoch 9 train loss: 0.2705, eval loss 0.6616004705429077\n",
      "optimal threshold: -0.4423\n",
      "Epoch 10 train loss: 0.2239, eval loss 0.6607199311256409\n",
      "optimal threshold: -0.6026\n",
      "Epoch 11 train loss: 0.4043, eval loss 0.6599765419960022\n",
      "optimal threshold: -0.6057\n",
      "Epoch 12 train loss: 0.1975, eval loss 0.6594344973564148\n",
      "optimal threshold: -0.4293\n",
      "Epoch 13 train loss: 0.2678, eval loss 0.6585902571678162\n",
      "optimal threshold: -0.4455\n",
      "Epoch 14 train loss: 0.2729, eval loss 0.6583898067474365\n",
      "optimal threshold: -0.4185\n",
      "Epoch 15 train loss: 0.2907, eval loss 0.6580827832221985\n",
      "optimal threshold: -0.4193\n",
      "Epoch 16 train loss: 0.2048, eval loss 0.6579376459121704\n",
      "optimal threshold: -0.4299\n",
      "Epoch 17 train loss: 0.2525, eval loss 0.6576744318008423\n",
      "optimal threshold: -0.4717\n",
      "Epoch 18 train loss: 0.3316, eval loss 0.6572909951210022\n",
      "optimal threshold: -0.4140\n",
      "Epoch 19 train loss: 0.2146, eval loss 0.6574772596359253\n",
      "optimal threshold: -0.5166\n",
      "Epoch 20 train loss: 0.2189, eval loss 0.6571173071861267\n",
      "optimal threshold: -0.3818\n",
      "Epoch 21 train loss: 0.2059, eval loss 0.6568949222564697\n",
      "optimal threshold: -0.4526\n",
      "Epoch 22 train loss: 0.1947, eval loss 0.6569185256958008\n",
      "optimal threshold: -0.3758\n",
      "Epoch 23 train loss: 0.3128, eval loss 0.6569055914878845\n",
      "optimal threshold: -0.5532\n",
      "Epoch 24 train loss: 0.2128, eval loss 0.6567257642745972\n",
      "optimal threshold: -0.4689\n",
      "Epoch 25 train loss: 0.2464, eval loss 0.6568922400474548\n",
      "optimal threshold: -0.4739\n",
      "Epoch 26 train loss: 0.1852, eval loss 0.6569826006889343\n",
      "optimal threshold: -0.4781\n",
      "Epoch 27 train loss: 0.4285, eval loss 0.6566285490989685\n",
      "optimal threshold: -0.5361\n",
      "Epoch 28 train loss: 0.1990, eval loss 0.6572586297988892\n",
      "optimal threshold: -0.5574\n",
      "Epoch 29 train loss: 0.1667, eval loss 0.657547116279602\n",
      "optimal threshold: -0.5302\n",
      "Epoch 30 train loss: 0.2020, eval loss 0.6573136448860168\n",
      "optimal threshold: -0.5808\n",
      "Epoch 31 train loss: 0.4724, eval loss 0.6570504903793335\n",
      "optimal threshold: -0.5876\n",
      "Epoch 32 train loss: 0.2334, eval loss 0.6568431258201599\n",
      "optimal threshold: -0.5452\n",
      "Epoch 33 train loss: 0.2379, eval loss 0.6571453809738159\n",
      "optimal threshold: -0.5959\n",
      "Epoch 34 train loss: 0.1391, eval loss 0.6572903990745544\n",
      "optimal threshold: -0.6013\n",
      "Epoch 35 train loss: 0.2247, eval loss 0.6576157212257385\n",
      "optimal threshold: -0.6210\n",
      "Epoch 36 train loss: 0.1879, eval loss 0.6578074097633362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 13:09:53,383] Trial 31 finished with value: 0.19569478929042816 and parameters: {'learning_rate_exp': -4.655389461054744, 'dropout_p': 0.3087939377977745, 'l2_reg_exp': -3.2209951485974315, 'batch_size': 12, 'N': 353}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6019\n",
      "optimal threshold: -0.8400\n",
      "Epoch 0 train loss: 0.4804, eval loss 0.7380977869033813\n",
      "optimal threshold: -0.4407\n",
      "Epoch 1 train loss: 0.4109, eval loss 0.7004785537719727\n",
      "optimal threshold: -0.6558\n",
      "Epoch 2 train loss: 0.2707, eval loss 0.6847203373908997\n",
      "optimal threshold: -0.5969\n",
      "Epoch 3 train loss: 0.2392, eval loss 0.6753478646278381\n",
      "optimal threshold: -0.5346\n",
      "Epoch 4 train loss: 0.2857, eval loss 0.6697563529014587\n",
      "optimal threshold: -0.5207\n",
      "Epoch 5 train loss: 0.3350, eval loss 0.6667401790618896\n",
      "optimal threshold: -0.4420\n",
      "Epoch 6 train loss: 0.4305, eval loss 0.6639499068260193\n",
      "optimal threshold: -0.4829\n",
      "Epoch 7 train loss: 0.3935, eval loss 0.6627207398414612\n",
      "optimal threshold: -0.7289\n",
      "Epoch 8 train loss: 0.2233, eval loss 0.6610847115516663\n",
      "optimal threshold: -0.5069\n",
      "Epoch 9 train loss: 0.1788, eval loss 0.6605680584907532\n",
      "optimal threshold: -0.6299\n",
      "Epoch 10 train loss: 0.2650, eval loss 0.6596544981002808\n",
      "optimal threshold: -0.5555\n",
      "Epoch 11 train loss: 0.3408, eval loss 0.6592807173728943\n",
      "optimal threshold: -0.6134\n",
      "Epoch 12 train loss: 0.2187, eval loss 0.6589367389678955\n",
      "optimal threshold: -0.4200\n",
      "Epoch 13 train loss: 0.2078, eval loss 0.6584784984588623\n",
      "optimal threshold: -0.4030\n",
      "Epoch 14 train loss: 0.1944, eval loss 0.6581466794013977\n",
      "optimal threshold: -0.4190\n",
      "Epoch 15 train loss: 0.2476, eval loss 0.6579490303993225\n",
      "optimal threshold: -0.4125\n",
      "Epoch 16 train loss: 0.1880, eval loss 0.6579198241233826\n",
      "optimal threshold: -0.4252\n",
      "Epoch 17 train loss: 0.2308, eval loss 0.6581098437309265\n",
      "optimal threshold: -0.4092\n",
      "Epoch 18 train loss: 0.2377, eval loss 0.6578037142753601\n",
      "optimal threshold: -0.3836\n",
      "Epoch 19 train loss: 0.2273, eval loss 0.6576858758926392\n",
      "optimal threshold: -0.4337\n",
      "Epoch 20 train loss: 0.1596, eval loss 0.6575487852096558\n",
      "optimal threshold: -0.6144\n",
      "Epoch 21 train loss: 0.1977, eval loss 0.6576782464981079\n",
      "optimal threshold: -0.4772\n",
      "Epoch 22 train loss: 0.2171, eval loss 0.6577421426773071\n",
      "optimal threshold: -0.4801\n",
      "Epoch 23 train loss: 0.1585, eval loss 0.6572759747505188\n",
      "optimal threshold: -0.3412\n",
      "Epoch 24 train loss: 0.1856, eval loss 0.6574795246124268\n",
      "optimal threshold: -0.4670\n",
      "Epoch 25 train loss: 0.1840, eval loss 0.6575816869735718\n",
      "optimal threshold: -0.4577\n",
      "Epoch 26 train loss: 0.1949, eval loss 0.6575046181678772\n",
      "optimal threshold: -0.4007\n",
      "Epoch 27 train loss: 0.2592, eval loss 0.6579601168632507\n",
      "optimal threshold: -0.4257\n",
      "Epoch 28 train loss: 0.1751, eval loss 0.6574685573577881\n",
      "optimal threshold: -0.4183\n",
      "Epoch 29 train loss: 0.2685, eval loss 0.6573905944824219\n",
      "optimal threshold: -0.4068\n",
      "Epoch 30 train loss: 0.1963, eval loss 0.658420979976654\n",
      "optimal threshold: -0.3961\n",
      "Epoch 31 train loss: 0.3909, eval loss 0.6582470536231995\n",
      "optimal threshold: -0.4120\n",
      "Epoch 32 train loss: 0.1796, eval loss 0.6585893034934998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 13:12:51,351] Trial 32 finished with value: 0.21864323318004608 and parameters: {'learning_rate_exp': -4.542703521357545, 'dropout_p': 0.3356238248414205, 'l2_reg_exp': -3.863009084183272, 'batch_size': 14, 'N': 372}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4396\n",
      "optimal threshold: -0.8346\n",
      "Epoch 0 train loss: 1.2020, eval loss 1.0347965955734253\n",
      "optimal threshold: -0.9264\n",
      "Epoch 1 train loss: 0.8898, eval loss 0.8176676034927368\n",
      "optimal threshold: -0.7942\n",
      "Epoch 2 train loss: 0.8200, eval loss 0.7378309965133667\n",
      "optimal threshold: -0.7838\n",
      "Epoch 3 train loss: 0.8303, eval loss 0.7152963876724243\n",
      "optimal threshold: -0.7346\n",
      "Epoch 4 train loss: 0.7540, eval loss 0.7038226127624512\n",
      "optimal threshold: -0.7744\n",
      "Epoch 5 train loss: 0.8719, eval loss 0.6958580613136292\n",
      "optimal threshold: -0.7687\n",
      "Epoch 6 train loss: 0.7902, eval loss 0.6897903084754944\n",
      "optimal threshold: -0.7873\n",
      "Epoch 7 train loss: 0.7885, eval loss 0.685071587562561\n",
      "optimal threshold: -0.7987\n",
      "Epoch 8 train loss: 0.8011, eval loss 0.6811916828155518\n",
      "optimal threshold: -0.7787\n",
      "Epoch 9 train loss: 0.8265, eval loss 0.6780279278755188\n",
      "optimal threshold: -0.6103\n",
      "Epoch 10 train loss: 0.8537, eval loss 0.6756648421287537\n",
      "optimal threshold: -0.6671\n",
      "Epoch 11 train loss: 0.7706, eval loss 0.6736200451850891\n",
      "optimal threshold: -0.7812\n",
      "Epoch 12 train loss: 0.7645, eval loss 0.6720032095909119\n",
      "optimal threshold: -0.7547\n",
      "Epoch 13 train loss: 0.8867, eval loss 0.6705814003944397\n",
      "optimal threshold: -0.7482\n",
      "Epoch 14 train loss: 0.9262, eval loss 0.6694591045379639\n",
      "optimal threshold: -0.7058\n",
      "Epoch 15 train loss: 0.7845, eval loss 0.6683545708656311\n",
      "optimal threshold: -0.7417\n",
      "Epoch 16 train loss: 0.6783, eval loss 0.6675363779067993\n",
      "optimal threshold: -0.4183\n",
      "Epoch 17 train loss: 0.7945, eval loss 0.6669979691505432\n",
      "optimal threshold: -0.4274\n",
      "Epoch 18 train loss: 0.8408, eval loss 0.6661874651908875\n",
      "optimal threshold: -0.4243\n",
      "Epoch 19 train loss: 0.7194, eval loss 0.6655828952789307\n",
      "optimal threshold: -0.4493\n",
      "Epoch 20 train loss: 0.8360, eval loss 0.6650546789169312\n",
      "optimal threshold: -0.4217\n",
      "Epoch 21 train loss: 0.8237, eval loss 0.6644310355186462\n",
      "optimal threshold: -0.4196\n",
      "Epoch 22 train loss: 0.7509, eval loss 0.6639783978462219\n",
      "optimal threshold: -0.4162\n",
      "Epoch 23 train loss: 0.7977, eval loss 0.6634841561317444\n",
      "optimal threshold: -0.7729\n",
      "Epoch 24 train loss: 0.8875, eval loss 0.6634377837181091\n",
      "optimal threshold: -0.7556\n",
      "Epoch 25 train loss: 0.7530, eval loss 0.6630505919456482\n",
      "optimal threshold: -0.7486\n",
      "Epoch 26 train loss: 0.9177, eval loss 0.6626246571540833\n",
      "optimal threshold: -0.4250\n",
      "Epoch 27 train loss: 0.8027, eval loss 0.6623362898826599\n",
      "optimal threshold: -0.7400\n",
      "Epoch 28 train loss: 0.7741, eval loss 0.662014365196228\n",
      "optimal threshold: -0.7268\n",
      "Epoch 29 train loss: 0.9537, eval loss 0.6617133021354675\n",
      "optimal threshold: -0.6527\n",
      "Epoch 30 train loss: 0.7694, eval loss 0.6615230441093445\n",
      "optimal threshold: -0.6382\n",
      "Epoch 31 train loss: 0.6972, eval loss 0.6611837148666382\n",
      "optimal threshold: -0.6487\n",
      "Epoch 32 train loss: 0.7146, eval loss 0.6609694957733154\n",
      "optimal threshold: -0.7659\n",
      "Epoch 33 train loss: 0.8134, eval loss 0.6606485843658447\n",
      "optimal threshold: -0.7559\n",
      "Epoch 34 train loss: 0.8008, eval loss 0.6605168581008911\n",
      "optimal threshold: -0.7484\n",
      "Epoch 35 train loss: 0.7175, eval loss 0.6602529883384705\n",
      "optimal threshold: -0.7387\n",
      "Epoch 36 train loss: 0.7431, eval loss 0.6601981520652771\n",
      "optimal threshold: -0.7508\n",
      "Epoch 37 train loss: 0.7716, eval loss 0.660027801990509\n",
      "optimal threshold: -0.7481\n",
      "Epoch 38 train loss: 0.7901, eval loss 0.6597472429275513\n",
      "optimal threshold: -0.7534\n",
      "Epoch 39 train loss: 0.8640, eval loss 0.6593744158744812\n",
      "optimal threshold: -0.7507\n",
      "Epoch 40 train loss: 0.7801, eval loss 0.6594233512878418\n",
      "optimal threshold: -0.7297\n",
      "Epoch 41 train loss: 0.6439, eval loss 0.6592894792556763\n",
      "optimal threshold: -0.3496\n",
      "Epoch 42 train loss: 0.7182, eval loss 0.6589702367782593\n",
      "optimal threshold: -0.3522\n",
      "Epoch 43 train loss: 0.7033, eval loss 0.6587757468223572\n",
      "optimal threshold: -0.3526\n",
      "Epoch 44 train loss: 0.5968, eval loss 0.6588111519813538\n",
      "optimal threshold: -0.3778\n",
      "Epoch 45 train loss: 0.7328, eval loss 0.6588917970657349\n",
      "optimal threshold: -0.3693\n",
      "Epoch 46 train loss: 0.6603, eval loss 0.6587574481964111\n",
      "optimal threshold: -0.3698\n",
      "Epoch 47 train loss: 0.7780, eval loss 0.6585743427276611\n",
      "optimal threshold: -0.3765\n",
      "Epoch 48 train loss: 0.7704, eval loss 0.6585800647735596\n",
      "optimal threshold: -0.3766\n",
      "Epoch 49 train loss: 0.7228, eval loss 0.6584508419036865\n",
      "optimal threshold: -0.3652\n",
      "Epoch 50 train loss: 0.7835, eval loss 0.6584752202033997\n",
      "optimal threshold: -0.6681\n",
      "Epoch 51 train loss: 0.7575, eval loss 0.6583378314971924\n",
      "optimal threshold: -0.3765\n",
      "Epoch 52 train loss: 0.7492, eval loss 0.6583693623542786\n",
      "optimal threshold: -0.3278\n",
      "Epoch 53 train loss: 0.7529, eval loss 0.6583749055862427\n",
      "optimal threshold: -0.4019\n",
      "Epoch 54 train loss: 0.7505, eval loss 0.6583347916603088\n",
      "optimal threshold: -0.3734\n",
      "Epoch 55 train loss: 0.7681, eval loss 0.6582152247428894\n",
      "optimal threshold: -0.3846\n",
      "Epoch 56 train loss: 0.7748, eval loss 0.6581920981407166\n",
      "optimal threshold: -0.3629\n",
      "Epoch 57 train loss: 0.8019, eval loss 0.6580531001091003\n",
      "optimal threshold: -0.3474\n",
      "Epoch 58 train loss: 0.8246, eval loss 0.6582077145576477\n",
      "optimal threshold: -0.3767\n",
      "Epoch 59 train loss: 0.8135, eval loss 0.6580692529678345\n",
      "optimal threshold: -0.3862\n",
      "Epoch 60 train loss: 0.7670, eval loss 0.6580208539962769\n",
      "optimal threshold: -0.3598\n",
      "Epoch 61 train loss: 0.7032, eval loss 0.6580775380134583\n",
      "optimal threshold: -0.3894\n",
      "Epoch 62 train loss: 0.7137, eval loss 0.6581755876541138\n",
      "optimal threshold: -0.3532\n",
      "Epoch 63 train loss: 0.6960, eval loss 0.6581334471702576\n",
      "optimal threshold: -0.5832\n",
      "Epoch 64 train loss: 0.7362, eval loss 0.6580522060394287\n",
      "optimal threshold: -0.3434\n",
      "Epoch 65 train loss: 0.7195, eval loss 0.6580983996391296\n",
      "optimal threshold: -0.3379\n",
      "Epoch 66 train loss: 0.6849, eval loss 0.6580921411514282\n",
      "optimal threshold: -0.6447\n",
      "Epoch 67 train loss: 0.6713, eval loss 0.6579535007476807\n",
      "optimal threshold: -0.3472\n",
      "Epoch 68 train loss: 0.7465, eval loss 0.65788733959198\n",
      "optimal threshold: -0.5779\n",
      "Epoch 69 train loss: 0.7099, eval loss 0.657838761806488\n",
      "optimal threshold: -0.6503\n",
      "Epoch 70 train loss: 0.7892, eval loss 0.6579452753067017\n",
      "optimal threshold: -0.3518\n",
      "Epoch 71 train loss: 0.7367, eval loss 0.6578823924064636\n",
      "optimal threshold: -0.6421\n",
      "Epoch 72 train loss: 0.7253, eval loss 0.6578941345214844\n",
      "optimal threshold: -0.5850\n",
      "Epoch 73 train loss: 0.7316, eval loss 0.6577447652816772\n",
      "optimal threshold: -0.5762\n",
      "Epoch 74 train loss: 0.7669, eval loss 0.6578384637832642\n",
      "optimal threshold: -0.5787\n",
      "Epoch 75 train loss: 0.7450, eval loss 0.6577101349830627\n",
      "optimal threshold: -0.6060\n",
      "Epoch 76 train loss: 0.7319, eval loss 0.6574773192405701\n",
      "optimal threshold: -0.5900\n",
      "Epoch 77 train loss: 0.7195, eval loss 0.6577768921852112\n",
      "optimal threshold: -0.5813\n",
      "Epoch 78 train loss: 0.8797, eval loss 0.6578277945518494\n",
      "optimal threshold: -0.6291\n",
      "Epoch 79 train loss: 0.7727, eval loss 0.6579020619392395\n",
      "optimal threshold: -0.6496\n",
      "Epoch 80 train loss: 0.8302, eval loss 0.6577759981155396\n",
      "optimal threshold: -0.6435\n",
      "Epoch 81 train loss: 0.6895, eval loss 0.6578354835510254\n",
      "optimal threshold: -0.6415\n",
      "Epoch 82 train loss: 0.7378, eval loss 0.6577787399291992\n",
      "optimal threshold: -0.6588\n",
      "Epoch 83 train loss: 0.8184, eval loss 0.6579564809799194\n",
      "optimal threshold: -0.6411\n",
      "Epoch 84 train loss: 0.6716, eval loss 0.6581044793128967\n",
      "optimal threshold: -0.6330\n",
      "Epoch 85 train loss: 0.5718, eval loss 0.658126175403595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 13:15:12,348] Trial 33 finished with value: 0.6972498297691345 and parameters: {'learning_rate_exp': -4.659994756462843, 'dropout_p': 0.3269043019130761, 'l2_reg_exp': -3.7735906464690574, 'batch_size': 58, 'N': 379}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6473\n",
      "optimal threshold: -0.5158\n",
      "Epoch 0 train loss: 0.3495, eval loss 0.7007612586021423\n",
      "optimal threshold: -0.5691\n",
      "Epoch 1 train loss: 0.5499, eval loss 0.6753390431404114\n",
      "optimal threshold: -0.3522\n",
      "Epoch 2 train loss: 0.5378, eval loss 0.6663928627967834\n",
      "optimal threshold: -0.4815\n",
      "Epoch 3 train loss: 0.2853, eval loss 0.661846935749054\n",
      "optimal threshold: -0.3942\n",
      "Epoch 4 train loss: 0.2787, eval loss 0.6595630049705505\n",
      "optimal threshold: -0.3610\n",
      "Epoch 5 train loss: 0.3659, eval loss 0.657748818397522\n",
      "optimal threshold: -0.3977\n",
      "Epoch 6 train loss: 0.5879, eval loss 0.6571070551872253\n",
      "optimal threshold: -0.2895\n",
      "Epoch 7 train loss: 0.3641, eval loss 0.6565168499946594\n",
      "optimal threshold: -0.3198\n",
      "Epoch 8 train loss: 0.2807, eval loss 0.655864417552948\n",
      "optimal threshold: -0.3472\n",
      "Epoch 9 train loss: 0.2287, eval loss 0.6557643413543701\n",
      "optimal threshold: -0.3799\n",
      "Epoch 10 train loss: 0.2653, eval loss 0.6551786661148071\n",
      "optimal threshold: -0.3834\n",
      "Epoch 11 train loss: 0.2195, eval loss 0.6552777290344238\n",
      "optimal threshold: -0.3752\n",
      "Epoch 12 train loss: 0.2407, eval loss 0.6552602648735046\n",
      "optimal threshold: -0.5219\n",
      "Epoch 13 train loss: 0.2310, eval loss 0.6553072929382324\n",
      "optimal threshold: -0.5488\n",
      "Epoch 14 train loss: 0.2070, eval loss 0.655423641204834\n",
      "optimal threshold: -0.5604\n",
      "Epoch 15 train loss: 0.1271, eval loss 0.6550232768058777\n",
      "optimal threshold: -0.5312\n",
      "Epoch 16 train loss: 0.2513, eval loss 0.6554635763168335\n",
      "optimal threshold: -0.5279\n",
      "Epoch 17 train loss: 0.2039, eval loss 0.6554004549980164\n",
      "optimal threshold: -0.6232\n",
      "Epoch 18 train loss: 0.2008, eval loss 0.6562526822090149\n",
      "optimal threshold: -0.5682\n",
      "Epoch 19 train loss: 0.2876, eval loss 0.6567077040672302\n",
      "optimal threshold: -0.5378\n",
      "Epoch 20 train loss: 0.2856, eval loss 0.6561737656593323\n",
      "optimal threshold: -0.6636\n",
      "Epoch 21 train loss: 0.2303, eval loss 0.6567723751068115\n",
      "optimal threshold: -0.5567\n",
      "Epoch 22 train loss: 0.1712, eval loss 0.6568490266799927\n",
      "optimal threshold: -0.5767\n",
      "Epoch 23 train loss: 0.1416, eval loss 0.6571745276451111\n",
      "optimal threshold: -0.6716\n",
      "Epoch 24 train loss: 0.2965, eval loss 0.6575613021850586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 13:18:55,530] Trial 34 finished with value: 0.117633156478405 and parameters: {'learning_rate_exp': -4.4613450989847365, 'dropout_p': 0.28351217879530716, 'l2_reg_exp': -3.1742877632278876, 'batch_size': 9, 'N': 439}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6495\n",
      "optimal threshold: -0.9115\n",
      "Epoch 0 train loss: 0.5439, eval loss 0.7523028254508972\n",
      "optimal threshold: -0.7767\n",
      "Epoch 1 train loss: 0.3118, eval loss 0.6958045959472656\n",
      "optimal threshold: -0.7632\n",
      "Epoch 2 train loss: 0.2477, eval loss 0.6780776977539062\n",
      "optimal threshold: -0.7791\n",
      "Epoch 3 train loss: 0.3235, eval loss 0.6702672243118286\n",
      "optimal threshold: -0.8610\n",
      "Epoch 4 train loss: 0.2662, eval loss 0.6661142706871033\n",
      "optimal threshold: -0.8346\n",
      "Epoch 5 train loss: 0.2384, eval loss 0.6637014150619507\n",
      "optimal threshold: -0.7681\n",
      "Epoch 6 train loss: 0.2306, eval loss 0.6618006825447083\n",
      "optimal threshold: -0.7956\n",
      "Epoch 7 train loss: 0.2271, eval loss 0.6611512899398804\n",
      "optimal threshold: -0.8078\n",
      "Epoch 8 train loss: 0.2274, eval loss 0.659357488155365\n",
      "optimal threshold: -0.7795\n",
      "Epoch 9 train loss: 0.1948, eval loss 0.6592203378677368\n",
      "optimal threshold: -0.7907\n",
      "Epoch 10 train loss: 0.1698, eval loss 0.6586989760398865\n",
      "optimal threshold: -0.6781\n",
      "Epoch 11 train loss: 0.1880, eval loss 0.6582551002502441\n",
      "optimal threshold: -0.6488\n",
      "Epoch 12 train loss: 0.2094, eval loss 0.6584441065788269\n",
      "optimal threshold: -0.6594\n",
      "Epoch 13 train loss: 0.2032, eval loss 0.6580038070678711\n",
      "optimal threshold: -0.6360\n",
      "Epoch 14 train loss: 0.1796, eval loss 0.6585061550140381\n",
      "optimal threshold: -0.5706\n",
      "Epoch 15 train loss: 0.1831, eval loss 0.6576606035232544\n",
      "optimal threshold: -0.6444\n",
      "Epoch 16 train loss: 0.1583, eval loss 0.6579771637916565\n",
      "optimal threshold: -0.5881\n",
      "Epoch 17 train loss: 0.2712, eval loss 0.6578964591026306\n",
      "optimal threshold: -0.5740\n",
      "Epoch 18 train loss: 0.1522, eval loss 0.6582446694374084\n",
      "optimal threshold: -0.5963\n",
      "Epoch 19 train loss: 0.2203, eval loss 0.6581500768661499\n",
      "optimal threshold: -0.5868\n",
      "Epoch 20 train loss: 0.1790, eval loss 0.6582874655723572\n",
      "optimal threshold: -0.6112\n",
      "Epoch 21 train loss: 0.1796, eval loss 0.6588744521141052\n",
      "optimal threshold: -0.6160\n",
      "Epoch 22 train loss: 0.2015, eval loss 0.6585836410522461\n",
      "optimal threshold: -0.5862\n",
      "Epoch 23 train loss: 0.1849, eval loss 0.6592021584510803\n",
      "optimal threshold: -0.5781\n",
      "Epoch 24 train loss: 0.1572, eval loss 0.6587268114089966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 13:19:27,725] Trial 35 finished with value: 0.18976207077503204 and parameters: {'learning_rate_exp': -4.093492105912114, 'dropout_p': 0.28462756361893743, 'l2_reg_exp': -3.1290032113956503, 'batch_size': 96, 'N': 447}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5601\n",
      "optimal threshold: -0.9225\n",
      "Epoch 0 train loss: 0.7419, eval loss 0.7298932075500488\n",
      "optimal threshold: -0.8751\n",
      "Epoch 1 train loss: 0.6742, eval loss 0.6872413158416748\n",
      "optimal threshold: -0.5805\n",
      "Epoch 2 train loss: 0.6644, eval loss 0.6728694438934326\n",
      "optimal threshold: -0.6860\n",
      "Epoch 3 train loss: 0.6912, eval loss 0.6669976115226746\n",
      "optimal threshold: -0.7549\n",
      "Epoch 4 train loss: 0.6208, eval loss 0.663719892501831\n",
      "optimal threshold: -0.4259\n",
      "Epoch 5 train loss: 0.6630, eval loss 0.6613354086875916\n",
      "optimal threshold: -0.4650\n",
      "Epoch 6 train loss: 0.6368, eval loss 0.6607170701026917\n",
      "optimal threshold: -0.4333\n",
      "Epoch 7 train loss: 0.5993, eval loss 0.6594873070716858\n",
      "optimal threshold: -0.4474\n",
      "Epoch 8 train loss: 0.6222, eval loss 0.6592953205108643\n",
      "optimal threshold: -0.3923\n",
      "Epoch 9 train loss: 0.5768, eval loss 0.6583177447319031\n",
      "optimal threshold: -0.3727\n",
      "Epoch 10 train loss: 0.6635, eval loss 0.6579023599624634\n",
      "optimal threshold: -0.3793\n",
      "Epoch 11 train loss: 0.6174, eval loss 0.6574875712394714\n",
      "optimal threshold: -0.3390\n",
      "Epoch 12 train loss: 0.6294, eval loss 0.6570973992347717\n",
      "optimal threshold: -0.3122\n",
      "Epoch 13 train loss: 0.6777, eval loss 0.6574453115463257\n",
      "optimal threshold: -0.6315\n",
      "Epoch 14 train loss: 0.6069, eval loss 0.6572691798210144\n",
      "optimal threshold: -0.3924\n",
      "Epoch 15 train loss: 0.5474, eval loss 0.657543420791626\n",
      "optimal threshold: -0.5310\n",
      "Epoch 16 train loss: 0.6605, eval loss 0.6583130955696106\n",
      "optimal threshold: -0.5351\n",
      "Epoch 17 train loss: 0.6143, eval loss 0.6574144959449768\n",
      "optimal threshold: -0.5215\n",
      "Epoch 18 train loss: 0.6033, eval loss 0.6580483317375183\n",
      "optimal threshold: -0.5410\n",
      "Epoch 19 train loss: 0.5763, eval loss 0.6579915285110474\n",
      "optimal threshold: -0.5687\n",
      "Epoch 20 train loss: 0.5743, eval loss 0.6582714915275574\n",
      "optimal threshold: -0.5702\n",
      "Epoch 21 train loss: 0.5651, eval loss 0.6587555408477783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 13:19:56,382] Trial 36 finished with value: 0.5991584062576294 and parameters: {'learning_rate_exp': -4.017914013230736, 'dropout_p': 0.22654024287703955, 'l2_reg_exp': -2.9185048086643803, 'batch_size': 98, 'N': 439}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5297\n",
      "optimal threshold: -0.9741\n",
      "Epoch 0 train loss: 0.7434, eval loss 0.7401894330978394\n",
      "optimal threshold: -0.8679\n",
      "Epoch 1 train loss: 0.7404, eval loss 0.6938871741294861\n",
      "optimal threshold: -0.8861\n",
      "Epoch 2 train loss: 0.7153, eval loss 0.6779046058654785\n",
      "optimal threshold: -0.5972\n",
      "Epoch 3 train loss: 0.6845, eval loss 0.6704384684562683\n",
      "optimal threshold: -0.4556\n",
      "Epoch 4 train loss: 0.7081, eval loss 0.6663566827774048\n",
      "optimal threshold: -0.4433\n",
      "Epoch 5 train loss: 0.7001, eval loss 0.6640428900718689\n",
      "optimal threshold: -0.8209\n",
      "Epoch 6 train loss: 0.6484, eval loss 0.6623983383178711\n",
      "optimal threshold: -0.5042\n",
      "Epoch 7 train loss: 0.7464, eval loss 0.6608054637908936\n",
      "optimal threshold: -0.6520\n",
      "Epoch 8 train loss: 0.6806, eval loss 0.6597154140472412\n",
      "optimal threshold: -0.6616\n",
      "Epoch 9 train loss: 0.6312, eval loss 0.6589308381080627\n",
      "optimal threshold: -0.6725\n",
      "Epoch 10 train loss: 0.7024, eval loss 0.6582526564598083\n",
      "optimal threshold: -0.6551\n",
      "Epoch 11 train loss: 0.6823, eval loss 0.657900869846344\n",
      "optimal threshold: -0.6628\n",
      "Epoch 12 train loss: 0.7208, eval loss 0.6572887301445007\n",
      "optimal threshold: -0.6582\n",
      "Epoch 13 train loss: 0.6909, eval loss 0.6570046544075012\n",
      "optimal threshold: -0.6308\n",
      "Epoch 14 train loss: 0.6505, eval loss 0.6565695405006409\n",
      "optimal threshold: -0.6362\n",
      "Epoch 15 train loss: 0.7202, eval loss 0.6567251682281494\n",
      "optimal threshold: -0.4791\n",
      "Epoch 16 train loss: 0.6581, eval loss 0.6566446423530579\n",
      "optimal threshold: -0.5083\n",
      "Epoch 17 train loss: 0.6511, eval loss 0.6568114161491394\n",
      "optimal threshold: -0.6513\n",
      "Epoch 18 train loss: 0.7079, eval loss 0.656647264957428\n",
      "optimal threshold: -0.7498\n",
      "Epoch 19 train loss: 0.7062, eval loss 0.6559901237487793\n",
      "optimal threshold: -0.7755\n",
      "Epoch 20 train loss: 0.6712, eval loss 0.65660160779953\n",
      "optimal threshold: -0.4933\n",
      "Epoch 21 train loss: 0.6480, eval loss 0.6567584872245789\n",
      "optimal threshold: -0.5403\n",
      "Epoch 22 train loss: 0.6681, eval loss 0.6569246053695679\n",
      "optimal threshold: -0.5244\n",
      "Epoch 23 train loss: 0.6117, eval loss 0.6572626233100891\n",
      "optimal threshold: -0.6965\n",
      "Epoch 24 train loss: 0.7640, eval loss 0.6575911045074463\n",
      "optimal threshold: -0.5255\n",
      "Epoch 25 train loss: 0.6602, eval loss 0.6572474837303162\n",
      "optimal threshold: -0.6236\n",
      "Epoch 26 train loss: 0.6052, eval loss 0.6576840281486511\n",
      "optimal threshold: -0.4033\n",
      "Epoch 27 train loss: 0.6936, eval loss 0.6571923494338989\n",
      "optimal threshold: -0.5548\n",
      "Epoch 28 train loss: 0.6476, eval loss 0.657336413860321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 13:20:52,526] Trial 37 finished with value: 0.6818049550056458 and parameters: {'learning_rate_exp': -4.267969858340616, 'dropout_p': 0.2827179957635838, 'l2_reg_exp': -2.512199488259718, 'batch_size': 55, 'N': 477}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5444\n",
      "optimal threshold: -0.9295\n",
      "Epoch 0 train loss: 0.7281, eval loss 0.7214438915252686\n",
      "optimal threshold: -0.8368\n",
      "Epoch 1 train loss: 0.7083, eval loss 0.6808614134788513\n",
      "optimal threshold: -0.3920\n",
      "Epoch 2 train loss: 0.6839, eval loss 0.6681728959083557\n",
      "optimal threshold: -0.4355\n",
      "Epoch 3 train loss: 0.6894, eval loss 0.66313236951828\n",
      "optimal threshold: -0.4630\n",
      "Epoch 4 train loss: 0.6792, eval loss 0.6609302163124084\n",
      "optimal threshold: -0.5128\n",
      "Epoch 5 train loss: 0.6798, eval loss 0.6588990688323975\n",
      "optimal threshold: -0.4962\n",
      "Epoch 6 train loss: 0.6866, eval loss 0.6586011052131653\n",
      "optimal threshold: -0.4981\n",
      "Epoch 7 train loss: 0.6575, eval loss 0.6590115427970886\n",
      "optimal threshold: -0.4636\n",
      "Epoch 8 train loss: 0.6894, eval loss 0.6581266522407532\n",
      "optimal threshold: -0.4188\n",
      "Epoch 9 train loss: 0.6814, eval loss 0.6579660773277283\n",
      "optimal threshold: -0.4635\n",
      "Epoch 10 train loss: 0.6556, eval loss 0.6575588583946228\n",
      "optimal threshold: -0.4825\n",
      "Epoch 11 train loss: 0.6406, eval loss 0.6586991548538208\n",
      "optimal threshold: -0.4521\n",
      "Epoch 12 train loss: 0.6420, eval loss 0.6582698822021484\n",
      "optimal threshold: -0.4248\n",
      "Epoch 13 train loss: 0.6631, eval loss 0.6591247916221619\n",
      "optimal threshold: -0.5495\n",
      "Epoch 14 train loss: 0.6737, eval loss 0.6590328812599182\n",
      "optimal threshold: -0.5374\n",
      "Epoch 15 train loss: 0.6672, eval loss 0.6596975326538086\n",
      "optimal threshold: -0.4699\n",
      "Epoch 16 train loss: 0.6423, eval loss 0.6605061888694763\n",
      "optimal threshold: -0.5581\n",
      "Epoch 17 train loss: 0.6521, eval loss 0.6602568030357361\n",
      "optimal threshold: -0.5263\n",
      "Epoch 18 train loss: 0.6719, eval loss 0.6610418558120728\n",
      "optimal threshold: -0.5274\n",
      "Epoch 19 train loss: 0.6372, eval loss 0.6615811586380005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 13:21:13,632] Trial 38 finished with value: 0.6849212646484375 and parameters: {'learning_rate_exp': -3.8120006994895115, 'dropout_p': 0.23352266905176178, 'l2_reg_exp': -3.5653885201550777, 'batch_size': 141, 'N': 420}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4868\n",
      "optimal threshold: -0.2467\n",
      "Epoch 0 train loss: 1.4281, eval loss 1.3014291524887085\n",
      "optimal threshold: -0.5003\n",
      "Epoch 1 train loss: 1.4181, eval loss 1.179258942604065\n",
      "optimal threshold: -0.7343\n",
      "Epoch 2 train loss: 1.3909, eval loss 1.0818334817886353\n",
      "optimal threshold: -0.8498\n",
      "Epoch 3 train loss: 1.3271, eval loss 1.0063138008117676\n",
      "optimal threshold: -0.9310\n",
      "Epoch 4 train loss: 1.1448, eval loss 0.941422164440155\n",
      "optimal threshold: -0.9188\n",
      "Epoch 5 train loss: 1.0430, eval loss 0.8848156929016113\n",
      "optimal threshold: -0.8623\n",
      "Epoch 6 train loss: 0.8743, eval loss 0.8381752371788025\n",
      "optimal threshold: -0.8612\n",
      "Epoch 7 train loss: 0.7839, eval loss 0.8016698956489563\n",
      "optimal threshold: -0.8903\n",
      "Epoch 8 train loss: 0.6621, eval loss 0.7745428085327148\n",
      "optimal threshold: -0.7890\n",
      "Epoch 9 train loss: 0.5602, eval loss 0.7552188634872437\n",
      "optimal threshold: -0.7548\n",
      "Epoch 10 train loss: 0.5658, eval loss 0.7416576743125916\n",
      "optimal threshold: -0.7809\n",
      "Epoch 11 train loss: 0.5622, eval loss 0.7319483757019043\n",
      "optimal threshold: -0.7655\n",
      "Epoch 12 train loss: 0.4541, eval loss 0.7246199250221252\n",
      "optimal threshold: -0.9066\n",
      "Epoch 13 train loss: 0.4137, eval loss 0.7189560532569885\n",
      "optimal threshold: -0.7705\n",
      "Epoch 14 train loss: 0.3316, eval loss 0.71444171667099\n",
      "optimal threshold: -0.8771\n",
      "Epoch 15 train loss: 0.4682, eval loss 0.7105795741081238\n",
      "optimal threshold: -0.9547\n",
      "Epoch 16 train loss: 0.3564, eval loss 0.7072535753250122\n",
      "optimal threshold: -0.9603\n",
      "Epoch 17 train loss: 0.3213, eval loss 0.704190731048584\n",
      "optimal threshold: -0.7079\n",
      "Epoch 18 train loss: 0.3114, eval loss 0.7014926075935364\n",
      "optimal threshold: -0.6965\n",
      "Epoch 19 train loss: 0.3288, eval loss 0.6989624500274658\n",
      "optimal threshold: -0.7035\n",
      "Epoch 20 train loss: 0.3049, eval loss 0.6966257691383362\n",
      "optimal threshold: -0.9466\n",
      "Epoch 21 train loss: 0.3399, eval loss 0.694485068321228\n",
      "optimal threshold: -0.9345\n",
      "Epoch 22 train loss: 0.3553, eval loss 0.6924135088920593\n",
      "optimal threshold: -0.9165\n",
      "Epoch 23 train loss: 0.2281, eval loss 0.6905453205108643\n",
      "optimal threshold: -0.9295\n",
      "Epoch 24 train loss: 0.3427, eval loss 0.6887374520301819\n",
      "optimal threshold: -0.9054\n",
      "Epoch 25 train loss: 0.3232, eval loss 0.687109112739563\n",
      "optimal threshold: -0.9013\n",
      "Epoch 26 train loss: 0.1963, eval loss 0.6855620741844177\n",
      "optimal threshold: -0.9175\n",
      "Epoch 27 train loss: 0.2883, eval loss 0.6840909123420715\n",
      "optimal threshold: -0.8795\n",
      "Epoch 28 train loss: 0.3494, eval loss 0.6828146576881409\n",
      "optimal threshold: -0.8716\n",
      "Epoch 29 train loss: 0.3794, eval loss 0.6815235614776611\n",
      "optimal threshold: -0.8678\n",
      "Epoch 30 train loss: 0.2234, eval loss 0.6803864240646362\n",
      "optimal threshold: -0.9867\n",
      "Epoch 31 train loss: 0.2917, eval loss 0.6792752146720886\n",
      "optimal threshold: -0.7272\n",
      "Epoch 32 train loss: 0.2640, eval loss 0.678246021270752\n",
      "optimal threshold: -0.9558\n",
      "Epoch 33 train loss: 0.3018, eval loss 0.6772764325141907\n",
      "optimal threshold: -0.4579\n",
      "Epoch 34 train loss: 0.2916, eval loss 0.6763778328895569\n",
      "optimal threshold: -0.4510\n",
      "Epoch 35 train loss: 0.2597, eval loss 0.6755083203315735\n",
      "optimal threshold: -0.4344\n",
      "Epoch 36 train loss: 0.2300, eval loss 0.6747085452079773\n",
      "optimal threshold: -0.4212\n",
      "Epoch 37 train loss: 0.2656, eval loss 0.6739702224731445\n",
      "optimal threshold: -0.4298\n",
      "Epoch 38 train loss: 0.3456, eval loss 0.6733466982841492\n",
      "optimal threshold: -0.4306\n",
      "Epoch 39 train loss: 0.2726, eval loss 0.6727415919303894\n",
      "optimal threshold: -0.4320\n",
      "Epoch 40 train loss: 0.2243, eval loss 0.672119677066803\n",
      "optimal threshold: -0.4180\n",
      "Epoch 41 train loss: 0.4138, eval loss 0.6714937686920166\n",
      "optimal threshold: -0.7768\n",
      "Epoch 42 train loss: 0.2825, eval loss 0.6710565686225891\n",
      "optimal threshold: -0.7813\n",
      "Epoch 43 train loss: 0.2488, eval loss 0.6704562902450562\n",
      "optimal threshold: -0.4326\n",
      "Epoch 44 train loss: 0.3459, eval loss 0.6700438857078552\n",
      "optimal threshold: -0.7981\n",
      "Epoch 45 train loss: 0.3088, eval loss 0.6695297956466675\n",
      "optimal threshold: -0.4224\n",
      "Epoch 46 train loss: 0.2896, eval loss 0.6691449284553528\n",
      "optimal threshold: -0.5951\n",
      "Epoch 47 train loss: 0.1972, eval loss 0.66873699426651\n",
      "optimal threshold: -0.7807\n",
      "Epoch 48 train loss: 0.2968, eval loss 0.6683017611503601\n",
      "optimal threshold: -0.7812\n",
      "Epoch 49 train loss: 0.2511, eval loss 0.6679627299308777\n",
      "optimal threshold: -0.6144\n",
      "Epoch 50 train loss: 0.2286, eval loss 0.6675613522529602\n",
      "optimal threshold: -0.6093\n",
      "Epoch 51 train loss: 0.2193, eval loss 0.6671529412269592\n",
      "optimal threshold: -0.6062\n",
      "Epoch 52 train loss: 0.2772, eval loss 0.6668075919151306\n",
      "optimal threshold: -0.6184\n",
      "Epoch 53 train loss: 0.2096, eval loss 0.6666034460067749\n",
      "optimal threshold: -0.6207\n",
      "Epoch 54 train loss: 0.3171, eval loss 0.6662607192993164\n",
      "optimal threshold: -0.6124\n",
      "Epoch 55 train loss: 0.2656, eval loss 0.665976345539093\n",
      "optimal threshold: -0.6151\n",
      "Epoch 56 train loss: 0.2883, eval loss 0.665762186050415\n",
      "optimal threshold: -0.8183\n",
      "Epoch 57 train loss: 0.2400, eval loss 0.6655324697494507\n",
      "optimal threshold: -0.8048\n",
      "Epoch 58 train loss: 0.1940, eval loss 0.6652025580406189\n",
      "optimal threshold: -0.8082\n",
      "Epoch 59 train loss: 0.3032, eval loss 0.6650095582008362\n",
      "optimal threshold: -0.8099\n",
      "Epoch 60 train loss: 0.2666, eval loss 0.6647996306419373\n",
      "optimal threshold: -0.8228\n",
      "Epoch 61 train loss: 0.1872, eval loss 0.6646278500556946\n",
      "optimal threshold: -0.8298\n",
      "Epoch 62 train loss: 0.4085, eval loss 0.6644173264503479\n",
      "optimal threshold: -0.5705\n",
      "Epoch 63 train loss: 0.1758, eval loss 0.6642016768455505\n",
      "optimal threshold: -0.5639\n",
      "Epoch 64 train loss: 0.2256, eval loss 0.6640063524246216\n",
      "optimal threshold: -0.5643\n",
      "Epoch 65 train loss: 0.2313, eval loss 0.6638283729553223\n",
      "optimal threshold: -0.8860\n",
      "Epoch 66 train loss: 0.2803, eval loss 0.6636647582054138\n",
      "optimal threshold: -0.8380\n",
      "Epoch 67 train loss: 0.2113, eval loss 0.6634520292282104\n",
      "optimal threshold: -0.8500\n",
      "Epoch 68 train loss: 0.2276, eval loss 0.6634047031402588\n",
      "optimal threshold: -0.8344\n",
      "Epoch 69 train loss: 0.2394, eval loss 0.6632289886474609\n",
      "optimal threshold: -0.7506\n",
      "Epoch 70 train loss: 0.3079, eval loss 0.6630257964134216\n",
      "optimal threshold: -0.4320\n",
      "Epoch 71 train loss: 0.2171, eval loss 0.6629637479782104\n",
      "optimal threshold: -0.4448\n",
      "Epoch 72 train loss: 0.2084, eval loss 0.6628640294075012\n",
      "optimal threshold: -0.4396\n",
      "Epoch 73 train loss: 0.2370, eval loss 0.662649929523468\n",
      "optimal threshold: -0.8089\n",
      "Epoch 74 train loss: 0.2968, eval loss 0.6625810265541077\n",
      "optimal threshold: -0.7992\n",
      "Epoch 75 train loss: 0.3270, eval loss 0.6623468399047852\n",
      "optimal threshold: -0.7932\n",
      "Epoch 76 train loss: 0.2539, eval loss 0.6622267365455627\n",
      "optimal threshold: -0.7981\n",
      "Epoch 77 train loss: 0.2457, eval loss 0.6621984839439392\n",
      "optimal threshold: -0.8022\n",
      "Epoch 78 train loss: 0.2094, eval loss 0.6619893312454224\n",
      "optimal threshold: -0.8077\n",
      "Epoch 79 train loss: 0.2069, eval loss 0.6618605256080627\n",
      "optimal threshold: -0.4152\n",
      "Epoch 80 train loss: 0.2414, eval loss 0.661687433719635\n",
      "optimal threshold: -0.4164\n",
      "Epoch 81 train loss: 0.2543, eval loss 0.6615841388702393\n",
      "optimal threshold: -0.4263\n",
      "Epoch 82 train loss: 0.1852, eval loss 0.6615419387817383\n",
      "optimal threshold: -0.8116\n",
      "Epoch 83 train loss: 0.2664, eval loss 0.6614344716072083\n",
      "optimal threshold: -0.7898\n",
      "Epoch 84 train loss: 0.1761, eval loss 0.6613430976867676\n",
      "optimal threshold: -0.7018\n",
      "Epoch 85 train loss: 0.2164, eval loss 0.6612843871116638\n",
      "optimal threshold: -0.7010\n",
      "Epoch 86 train loss: 0.3139, eval loss 0.6611449122428894\n",
      "optimal threshold: -0.4079\n",
      "Epoch 87 train loss: 0.2429, eval loss 0.6610580086708069\n",
      "optimal threshold: -0.7075\n",
      "Epoch 88 train loss: 0.3642, eval loss 0.6610028743743896\n",
      "optimal threshold: -0.4123\n",
      "Epoch 89 train loss: 0.2306, eval loss 0.6609628796577454\n",
      "optimal threshold: -0.7202\n",
      "Epoch 90 train loss: 0.2941, eval loss 0.6607760190963745\n",
      "optimal threshold: -0.7216\n",
      "Epoch 91 train loss: 0.1435, eval loss 0.660619854927063\n",
      "optimal threshold: -0.8148\n",
      "Epoch 92 train loss: 0.1943, eval loss 0.6605166792869568\n",
      "optimal threshold: -0.8179\n",
      "Epoch 93 train loss: 0.1338, eval loss 0.6605504155158997\n",
      "optimal threshold: -0.7912\n",
      "Epoch 94 train loss: 0.2467, eval loss 0.6603711247444153\n",
      "optimal threshold: -0.7881\n",
      "Epoch 95 train loss: 0.2060, eval loss 0.6603449583053589\n",
      "optimal threshold: -0.7916\n",
      "Epoch 96 train loss: 0.2135, eval loss 0.6603784561157227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7888\n",
      "Epoch 97 train loss: 0.2409, eval loss 0.6602209806442261\n",
      "optimal threshold: -0.7911\n",
      "Epoch 98 train loss: 0.1942, eval loss 0.6601320505142212\n",
      "optimal threshold: -0.7761\n",
      "Epoch 99 train loss: 0.2239, eval loss 0.6600792407989502\n",
      "optimal threshold: -0.7778\n",
      "Epoch 100 train loss: 0.1709, eval loss 0.6600625514984131\n",
      "optimal threshold: -0.7723\n",
      "Epoch 101 train loss: 0.1738, eval loss 0.659925103187561\n",
      "optimal threshold: -0.7777\n",
      "Epoch 102 train loss: 0.1378, eval loss 0.6598845720291138\n",
      "optimal threshold: -0.7816\n",
      "Epoch 103 train loss: 0.2012, eval loss 0.6598010659217834\n",
      "optimal threshold: -0.7757\n",
      "Epoch 104 train loss: 0.1950, eval loss 0.6596502661705017\n",
      "optimal threshold: -0.7798\n",
      "Epoch 105 train loss: 0.2914, eval loss 0.6595907807350159\n",
      "optimal threshold: -0.7825\n",
      "Epoch 106 train loss: 0.1623, eval loss 0.6595740914344788\n",
      "optimal threshold: -0.7804\n",
      "Epoch 107 train loss: 0.1866, eval loss 0.6595578193664551\n",
      "optimal threshold: -0.7822\n",
      "Epoch 108 train loss: 0.2114, eval loss 0.6594484448432922\n",
      "optimal threshold: -0.7619\n",
      "Epoch 109 train loss: 0.2997, eval loss 0.6594172716140747\n",
      "optimal threshold: -0.7608\n",
      "Epoch 110 train loss: 0.2176, eval loss 0.6593910455703735\n",
      "optimal threshold: -0.7554\n",
      "Epoch 111 train loss: 0.2290, eval loss 0.6592805981636047\n",
      "optimal threshold: -0.7549\n",
      "Epoch 112 train loss: 0.1461, eval loss 0.6592477560043335\n",
      "optimal threshold: -0.7355\n",
      "Epoch 113 train loss: 0.2894, eval loss 0.6591172218322754\n",
      "optimal threshold: -0.7434\n",
      "Epoch 114 train loss: 0.2087, eval loss 0.6591614484786987\n",
      "optimal threshold: -0.7412\n",
      "Epoch 115 train loss: 0.2155, eval loss 0.6590020060539246\n",
      "optimal threshold: -0.7464\n",
      "Epoch 116 train loss: 0.1626, eval loss 0.658935546875\n",
      "optimal threshold: -0.5679\n",
      "Epoch 117 train loss: 0.1531, eval loss 0.6589796543121338\n",
      "optimal threshold: -0.5634\n",
      "Epoch 118 train loss: 0.1412, eval loss 0.6589179039001465\n",
      "optimal threshold: -0.3920\n",
      "Epoch 119 train loss: 0.1636, eval loss 0.6588810682296753\n",
      "optimal threshold: -0.3860\n",
      "Epoch 120 train loss: 0.1292, eval loss 0.6587263345718384\n",
      "optimal threshold: -0.5559\n",
      "Epoch 121 train loss: 0.1636, eval loss 0.658710777759552\n",
      "optimal threshold: -0.5588\n",
      "Epoch 122 train loss: 0.1855, eval loss 0.6587110161781311\n",
      "optimal threshold: -0.5566\n",
      "Epoch 123 train loss: 0.1828, eval loss 0.6587064266204834\n",
      "optimal threshold: -0.5458\n",
      "Epoch 124 train loss: 0.2835, eval loss 0.6585923433303833\n",
      "optimal threshold: -0.5476\n",
      "Epoch 125 train loss: 0.2371, eval loss 0.6585325002670288\n",
      "optimal threshold: -0.5518\n",
      "Epoch 126 train loss: 0.1640, eval loss 0.6585637331008911\n",
      "optimal threshold: -0.5490\n",
      "Epoch 127 train loss: 0.2263, eval loss 0.658477783203125\n",
      "optimal threshold: -0.5508\n",
      "Epoch 128 train loss: 0.2349, eval loss 0.6584789156913757\n",
      "optimal threshold: -0.5401\n",
      "Epoch 129 train loss: 0.1804, eval loss 0.6583836674690247\n",
      "optimal threshold: -0.5438\n",
      "Epoch 130 train loss: 0.3055, eval loss 0.6583859920501709\n",
      "optimal threshold: -0.5390\n",
      "Epoch 131 train loss: 0.2169, eval loss 0.658301591873169\n",
      "optimal threshold: -0.5420\n",
      "Epoch 132 train loss: 0.1398, eval loss 0.6583465337753296\n",
      "optimal threshold: -0.5441\n",
      "Epoch 133 train loss: 0.1663, eval loss 0.6583319306373596\n",
      "optimal threshold: -0.5474\n",
      "Epoch 134 train loss: 0.2295, eval loss 0.6582499742507935\n",
      "optimal threshold: -0.5439\n",
      "Epoch 135 train loss: 0.3040, eval loss 0.6581831574440002\n",
      "optimal threshold: -0.5440\n",
      "Epoch 136 train loss: 0.1925, eval loss 0.6581959128379822\n",
      "optimal threshold: -0.5460\n",
      "Epoch 137 train loss: 0.1425, eval loss 0.6582137942314148\n",
      "optimal threshold: -0.5490\n",
      "Epoch 138 train loss: 0.2540, eval loss 0.6581177711486816\n",
      "optimal threshold: -0.5457\n",
      "Epoch 139 train loss: 0.2275, eval loss 0.6582098007202148\n",
      "optimal threshold: -0.5302\n",
      "Epoch 140 train loss: 0.2165, eval loss 0.6581521034240723\n",
      "optimal threshold: -0.5619\n",
      "Epoch 141 train loss: 0.1611, eval loss 0.6581366658210754\n",
      "optimal threshold: -0.5556\n",
      "Epoch 142 train loss: 0.2694, eval loss 0.6580654978752136\n",
      "optimal threshold: -0.5586\n",
      "Epoch 143 train loss: 0.2101, eval loss 0.6581381559371948\n",
      "optimal threshold: -0.5418\n",
      "Epoch 144 train loss: 0.1733, eval loss 0.6581616997718811\n",
      "optimal threshold: -0.5388\n",
      "Epoch 145 train loss: 0.2421, eval loss 0.6581565141677856\n",
      "optimal threshold: -0.5358\n",
      "Epoch 146 train loss: 0.1414, eval loss 0.6581186652183533\n",
      "optimal threshold: -0.5403\n",
      "Epoch 147 train loss: 0.1997, eval loss 0.6580527424812317\n",
      "optimal threshold: -0.5308\n",
      "Epoch 148 train loss: 0.2169, eval loss 0.6579657196998596\n",
      "optimal threshold: -0.5295\n",
      "Epoch 149 train loss: 0.1744, eval loss 0.6578980684280396\n",
      "optimal threshold: -0.5279\n",
      "Epoch 150 train loss: 0.1445, eval loss 0.6577736735343933\n",
      "optimal threshold: -0.5254\n",
      "Epoch 151 train loss: 0.2509, eval loss 0.6578183770179749\n",
      "optimal threshold: -0.5288\n",
      "Epoch 152 train loss: 0.2380, eval loss 0.6579095721244812\n",
      "optimal threshold: -0.5296\n",
      "Epoch 153 train loss: 0.2184, eval loss 0.6579045057296753\n",
      "optimal threshold: -0.5247\n",
      "Epoch 154 train loss: 0.1955, eval loss 0.6578719615936279\n",
      "optimal threshold: -0.5203\n",
      "Epoch 155 train loss: 0.1999, eval loss 0.6577968597412109\n",
      "optimal threshold: -0.5190\n",
      "Epoch 156 train loss: 0.1757, eval loss 0.6577216386795044\n",
      "optimal threshold: -0.6577\n",
      "Epoch 157 train loss: 0.2271, eval loss 0.657798707485199\n",
      "optimal threshold: -0.6753\n",
      "Epoch 158 train loss: 0.1652, eval loss 0.6577255129814148\n",
      "optimal threshold: -0.6048\n",
      "Epoch 159 train loss: 0.1745, eval loss 0.6577627062797546\n",
      "optimal threshold: -0.6761\n",
      "Epoch 160 train loss: 0.2494, eval loss 0.657673716545105\n",
      "optimal threshold: -0.5233\n",
      "Epoch 161 train loss: 0.2735, eval loss 0.6577618718147278\n",
      "optimal threshold: -0.6757\n",
      "Epoch 162 train loss: 0.2216, eval loss 0.6577603220939636\n",
      "optimal threshold: -0.6764\n",
      "Epoch 163 train loss: 0.2088, eval loss 0.657747209072113\n",
      "optimal threshold: -0.6793\n",
      "Epoch 164 train loss: 0.1973, eval loss 0.6577553153038025\n",
      "optimal threshold: -0.6953\n",
      "Epoch 165 train loss: 0.1649, eval loss 0.6576903462409973\n",
      "optimal threshold: -0.7005\n",
      "Epoch 166 train loss: 0.2470, eval loss 0.6577286720275879\n",
      "optimal threshold: -0.6989\n",
      "Epoch 167 train loss: 0.1587, eval loss 0.6576018929481506\n",
      "optimal threshold: -0.6724\n",
      "Epoch 168 train loss: 0.1618, eval loss 0.6576467752456665\n",
      "optimal threshold: -0.6661\n",
      "Epoch 169 train loss: 0.2733, eval loss 0.6575822830200195\n",
      "optimal threshold: -0.7101\n",
      "Epoch 170 train loss: 0.1723, eval loss 0.6576270461082458\n",
      "optimal threshold: -0.7006\n",
      "Epoch 171 train loss: 0.2090, eval loss 0.6575841307640076\n",
      "optimal threshold: -0.7023\n",
      "Epoch 172 train loss: 0.1898, eval loss 0.6575109958648682\n",
      "optimal threshold: -0.6664\n",
      "Epoch 173 train loss: 0.1924, eval loss 0.6574919819831848\n",
      "optimal threshold: -0.6993\n",
      "Epoch 174 train loss: 0.1853, eval loss 0.657442033290863\n",
      "optimal threshold: -0.6866\n",
      "Epoch 175 train loss: 0.1988, eval loss 0.6575297713279724\n",
      "optimal threshold: -0.6815\n",
      "Epoch 176 train loss: 0.1686, eval loss 0.6574856042861938\n",
      "optimal threshold: -0.6588\n",
      "Epoch 177 train loss: 0.1682, eval loss 0.657455325126648\n",
      "optimal threshold: -0.6810\n",
      "Epoch 178 train loss: 0.1726, eval loss 0.657436728477478\n",
      "optimal threshold: -0.6764\n",
      "Epoch 179 train loss: 0.1928, eval loss 0.6574134230613708\n",
      "optimal threshold: -0.6493\n",
      "Epoch 180 train loss: 0.1637, eval loss 0.6573904752731323\n",
      "optimal threshold: -0.6529\n",
      "Epoch 181 train loss: 0.1780, eval loss 0.6573682427406311\n",
      "optimal threshold: -0.6591\n",
      "Epoch 182 train loss: 0.1652, eval loss 0.6574472784996033\n",
      "optimal threshold: -0.6705\n",
      "Epoch 183 train loss: 0.1713, eval loss 0.657486617565155\n",
      "optimal threshold: -0.7145\n",
      "Epoch 184 train loss: 0.1724, eval loss 0.6574071645736694\n",
      "optimal threshold: -0.6545\n",
      "Epoch 185 train loss: 0.1722, eval loss 0.6573368906974792\n",
      "optimal threshold: -0.7104\n",
      "Epoch 186 train loss: 0.1848, eval loss 0.6574187278747559\n",
      "optimal threshold: -0.6393\n",
      "Epoch 187 train loss: 0.1725, eval loss 0.6574690937995911\n",
      "optimal threshold: -0.6538\n",
      "Epoch 188 train loss: 0.1559, eval loss 0.6574290990829468\n",
      "optimal threshold: -0.6735\n",
      "Epoch 189 train loss: 0.1808, eval loss 0.6575606465339661\n",
      "optimal threshold: -0.6599\n",
      "Epoch 190 train loss: 0.2026, eval loss 0.657447338104248\n",
      "optimal threshold: -0.6247\n",
      "Epoch 191 train loss: 0.2083, eval loss 0.6573150157928467\n",
      "optimal threshold: -0.6598\n",
      "Epoch 192 train loss: 0.1565, eval loss 0.6573578715324402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5080\n",
      "Epoch 193 train loss: 0.2090, eval loss 0.6574252247810364\n",
      "optimal threshold: -0.6383\n",
      "Epoch 194 train loss: 0.2005, eval loss 0.6574456095695496\n",
      "optimal threshold: -0.6475\n",
      "Epoch 195 train loss: 0.1785, eval loss 0.6574311256408691\n",
      "optimal threshold: -0.6640\n",
      "Epoch 196 train loss: 0.1734, eval loss 0.6574828624725342\n",
      "optimal threshold: -0.6481\n",
      "Epoch 197 train loss: 0.1474, eval loss 0.6574345827102661\n",
      "optimal threshold: -0.6583\n",
      "Epoch 198 train loss: 0.1943, eval loss 0.6574317216873169\n",
      "optimal threshold: -0.6374\n",
      "Epoch 199 train loss: 0.1092, eval loss 0.6574069857597351\n",
      "optimal threshold: -0.6403\n",
      "Epoch 200 train loss: 0.1827, eval loss 0.657402753829956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 13:25:53,700] Trial 39 finished with value: 0.18607832491397858 and parameters: {'learning_rate_exp': -5.119670604508084, 'dropout_p': 0.36281278111797377, 'l2_reg_exp': -3.360051317465629, 'batch_size': 93, 'N': 502}. Best is trial 14 with value: 0.10548289865255356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6407\n"
     ]
    }
   ],
   "source": [
    "#direction= 'minimize' or 'maximize' \n",
    "study = optuna.create_study(direction='minimize')\n",
    "#study = optuna.create_study(sampler=TPESampler(), #direction=\"maximize\") by default the sampler = TPESampler()\n",
    "study.optimize(objective, n_trials=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.10548289865255356\n",
      "Best hyperparameters: {'learning_rate_exp': -3.417577960778775, 'dropout_p': 0.3591737999088154, 'l2_reg_exp': -5.100636178875623, 'batch_size': 9, 'N': 336}\n"
     ]
    }
   ],
   "source": [
    "trial = study.best_trial\n",
    "print('Accuracy: {}'.format(trial.value)) #0.9733333333333333\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))\n",
    "#{'n_estimators': 11, 'max_depth': 27.827767703750034}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7492\n",
      "Epoch 0 train loss: 0.4975, eval loss 0.6605998277664185\n",
      "optimal threshold: -0.6368\n",
      "Epoch 1 train loss: 0.1797, eval loss 0.6588138937950134\n",
      "optimal threshold: -0.5117\n",
      "Epoch 2 train loss: 0.5560, eval loss 0.6581278443336487\n",
      "optimal threshold: -0.6422\n",
      "Epoch 3 train loss: 0.2625, eval loss 0.6592444181442261\n",
      "optimal threshold: -0.4382\n",
      "Epoch 4 train loss: 0.2043, eval loss 0.6578590869903564\n",
      "optimal threshold: -0.5597\n",
      "Epoch 5 train loss: 0.1943, eval loss 0.6601585745811462\n",
      "optimal threshold: -0.5946\n",
      "Epoch 6 train loss: 0.3689, eval loss 0.6580578684806824\n",
      "optimal threshold: -0.5629\n",
      "Epoch 7 train loss: 0.1232, eval loss 0.6621609330177307\n",
      "optimal threshold: -0.4729\n",
      "Epoch 8 train loss: 0.1733, eval loss 0.6645522117614746\n",
      "optimal threshold: -0.5294\n",
      "Epoch 9 train loss: 0.0559, eval loss 0.6708927750587463\n",
      "optimal threshold: -0.5429\n",
      "Epoch 10 train loss: 0.1244, eval loss 0.6696640849113464\n",
      "optimal threshold: -0.5705\n",
      "Epoch 11 train loss: 0.0926, eval loss 0.6763636469841003\n",
      "optimal threshold: -0.4690\n",
      "Epoch 12 train loss: 0.1212, eval loss 0.6770602464675903\n",
      "optimal threshold: -0.5950\n",
      "Epoch 13 train loss: 0.1409, eval loss 0.6836735010147095\n",
      "optimal threshold: -0.6217\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 300\n",
    "early_stopping_patience = 10\n",
    "model = MLP(\n",
    "    input_size=X_train.shape[1], \n",
    "    dropout_p=trial.params['dropout_p'],\n",
    "    N=trial.params['N']\n",
    ")\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=10**trial.params['learning_rate_exp'], \n",
    "    weight_decay=10**trial.params['learning_rate_exp']\n",
    ")\n",
    "positive_weight = torch.tensor(compute_class_weight(class_weight='balanced',classes=np.unique(y_train),y=np.ravel(y_train))[1:])\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(weight = positive_weight)\n",
    "\n",
    "train_dataset = MyDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=trial.params['batch_size'])\n",
    "\n",
    "steps_without_improvement = 0\n",
    "\n",
    "best_val_loss = np.inf\n",
    "best_model = None\n",
    "best_threshold = None\n",
    "\n",
    "for epoch_num in range(max_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # model training\n",
    "\n",
    "    # model evaluation, early stopping\n",
    "    valid_metrics=evaluate_model(\n",
    "    model, \n",
    "    X_valid, \n",
    "    y_valid, \n",
    "    loss_fn)\n",
    "    val_loss = valid_metrics['loss']\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        steps_without_improvement = 0\n",
    "        best_model = deepcopy(model)\n",
    "        best_threshold = valid_metrics['threshold']\n",
    "    else:\n",
    "        steps_without_improvement += 1\n",
    "    if early_stopping_patience == steps_without_improvement:\n",
    "        break\n",
    "    print(f\"Epoch {epoch_num} train loss: {loss.item():.4f}, eval loss {valid_metrics['loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPE Sampler:\n",
      "AUROC: 90.83%\n",
      "F1: 69.71%\n",
      "Precision: 66.74%\n",
      "Recall: 72.96%\n"
     ]
    }
   ],
   "source": [
    "test_metrics = evaluate_model(best_model, X_test, y_test, loss_fn, best_threshold)\n",
    "print(\"TPE Sampler:\")\n",
    "print(f\"AUROC: {100 * test_metrics['AUROC']:.2f}%\")\n",
    "print(f\"F1: {100 * test_metrics['F1-score']:.2f}%\")\n",
    "print(f\"Precision: {100 * test_metrics['precision']:.2f}%\")\n",
    "print(f\"Recall: {100 * test_metrics['recall']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:01:05,123] A new study created in memory with name: no-name-1fa684cb-6ad5-4b51-a18e-ac6da751a55e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4950\n",
      "Epoch 0 train loss: 0.7186, eval loss 0.7323850989341736\n",
      "optimal threshold: -0.6644\n",
      "Epoch 1 train loss: 0.6377, eval loss 0.6859854459762573\n",
      "optimal threshold: -0.6273\n",
      "Epoch 2 train loss: 0.6363, eval loss 0.6705894470214844\n",
      "optimal threshold: -0.4192\n",
      "Epoch 3 train loss: 0.6290, eval loss 0.6636227369308472\n",
      "optimal threshold: -0.3351\n",
      "Epoch 4 train loss: 0.6079, eval loss 0.6609492897987366\n",
      "optimal threshold: -0.3656\n",
      "Epoch 5 train loss: 0.6101, eval loss 0.6599127054214478\n",
      "optimal threshold: -0.6547\n",
      "Epoch 6 train loss: 0.5833, eval loss 0.6596865057945251\n",
      "optimal threshold: -0.6523\n",
      "Epoch 7 train loss: 0.5572, eval loss 0.65892094373703\n",
      "optimal threshold: -0.4711\n",
      "Epoch 8 train loss: 0.5707, eval loss 0.6589905619621277\n",
      "optimal threshold: -0.7219\n",
      "Epoch 9 train loss: 0.5982, eval loss 0.6587505340576172\n",
      "optimal threshold: -0.5397\n",
      "Epoch 10 train loss: 0.5932, eval loss 0.6590614318847656\n",
      "optimal threshold: -0.5772\n",
      "Epoch 11 train loss: 0.5713, eval loss 0.6604552865028381\n",
      "optimal threshold: -0.5841\n",
      "Epoch 12 train loss: 0.6130, eval loss 0.6594253182411194\n",
      "optimal threshold: -0.5644\n",
      "Epoch 13 train loss: 0.5647, eval loss 0.6584329009056091\n",
      "optimal threshold: -0.6598\n",
      "Epoch 14 train loss: 0.5619, eval loss 0.6593696475028992\n",
      "optimal threshold: -0.5945\n",
      "Epoch 15 train loss: 0.5660, eval loss 0.6589223146438599\n",
      "optimal threshold: -0.6882\n",
      "Epoch 16 train loss: 0.5535, eval loss 0.6602887511253357\n",
      "optimal threshold: -0.6230\n",
      "Epoch 17 train loss: 0.5726, eval loss 0.6601632833480835\n",
      "optimal threshold: -0.5157\n",
      "Epoch 18 train loss: 0.5580, eval loss 0.6609352231025696\n",
      "optimal threshold: -0.5737\n",
      "Epoch 19 train loss: 0.5518, eval loss 0.6608303785324097\n",
      "optimal threshold: -0.5947\n",
      "Epoch 20 train loss: 0.5412, eval loss 0.6610496044158936\n",
      "optimal threshold: -0.5693\n",
      "Epoch 21 train loss: 0.5810, eval loss 0.6606143712997437\n",
      "optimal threshold: -0.6682\n",
      "Epoch 22 train loss: 0.5330, eval loss 0.6607266068458557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:01:14,065] Trial 0 finished with value: 0.5673678517341614 and parameters: {'learning_rate_exp': -3.03631502989729, 'dropout_p': 0.4488729029792222, 'l2_reg_exp': -4.262990614707818, 'batch_size': 429, 'N': 140}. Best is trial 0 with value: 0.5673678517341614.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6478\n",
      "optimal threshold: -0.0955\n",
      "Epoch 0 train loss: 1.3954, eval loss 1.3942855596542358\n",
      "optimal threshold: -0.1146\n",
      "Epoch 1 train loss: 1.3817, eval loss 1.3827717304229736\n",
      "optimal threshold: -0.1232\n",
      "Epoch 2 train loss: 1.3712, eval loss 1.3713794946670532\n",
      "optimal threshold: -0.1415\n",
      "Epoch 3 train loss: 1.3588, eval loss 1.3600431680679321\n",
      "optimal threshold: -0.1614\n",
      "Epoch 4 train loss: 1.3481, eval loss 1.3486852645874023\n",
      "optimal threshold: -0.1781\n",
      "Epoch 5 train loss: 1.3355, eval loss 1.3372588157653809\n",
      "optimal threshold: -0.2021\n",
      "Epoch 6 train loss: 1.3245, eval loss 1.325730323791504\n",
      "optimal threshold: -0.2173\n",
      "Epoch 7 train loss: 1.3117, eval loss 1.3140829801559448\n",
      "optimal threshold: -0.2419\n",
      "Epoch 8 train loss: 1.2993, eval loss 1.3022840023040771\n",
      "optimal threshold: -0.2616\n",
      "Epoch 9 train loss: 1.2891, eval loss 1.2903614044189453\n",
      "optimal threshold: -0.2823\n",
      "Epoch 10 train loss: 1.2765, eval loss 1.2783411741256714\n",
      "optimal threshold: -0.3049\n",
      "Epoch 11 train loss: 1.2654, eval loss 1.266261339187622\n",
      "optimal threshold: -0.3299\n",
      "Epoch 12 train loss: 1.2515, eval loss 1.2541427612304688\n",
      "optimal threshold: -0.3525\n",
      "Epoch 13 train loss: 1.2392, eval loss 1.2420369386672974\n",
      "optimal threshold: -0.3717\n",
      "Epoch 14 train loss: 1.2264, eval loss 1.229984164237976\n",
      "optimal threshold: -0.3936\n",
      "Epoch 15 train loss: 1.2141, eval loss 1.2180088758468628\n",
      "optimal threshold: -0.4188\n",
      "Epoch 16 train loss: 1.2027, eval loss 1.2061526775360107\n",
      "optimal threshold: -0.4399\n",
      "Epoch 17 train loss: 1.1894, eval loss 1.1944093704223633\n",
      "optimal threshold: -0.4609\n",
      "Epoch 18 train loss: 1.1788, eval loss 1.1828094720840454\n",
      "optimal threshold: -0.4798\n",
      "Epoch 19 train loss: 1.1669, eval loss 1.171368956565857\n",
      "optimal threshold: -0.5153\n",
      "Epoch 20 train loss: 1.1548, eval loss 1.160081386566162\n",
      "optimal threshold: -0.5165\n",
      "Epoch 21 train loss: 1.1425, eval loss 1.1489523649215698\n",
      "optimal threshold: -0.5523\n",
      "Epoch 22 train loss: 1.1313, eval loss 1.1379796266555786\n",
      "optimal threshold: -0.5720\n",
      "Epoch 23 train loss: 1.1210, eval loss 1.1271733045578003\n",
      "optimal threshold: -0.5793\n",
      "Epoch 24 train loss: 1.1070, eval loss 1.1165393590927124\n",
      "optimal threshold: -0.6161\n",
      "Epoch 25 train loss: 1.0986, eval loss 1.1060584783554077\n",
      "optimal threshold: -0.6304\n",
      "Epoch 26 train loss: 1.0888, eval loss 1.095740556716919\n",
      "optimal threshold: -0.6553\n",
      "Epoch 27 train loss: 1.0769, eval loss 1.0855951309204102\n",
      "optimal threshold: -0.6478\n",
      "Epoch 28 train loss: 1.0688, eval loss 1.0756107568740845\n",
      "optimal threshold: -0.6597\n",
      "Epoch 29 train loss: 1.0548, eval loss 1.0657734870910645\n",
      "optimal threshold: -0.6722\n",
      "Epoch 30 train loss: 1.0461, eval loss 1.0560871362686157\n",
      "optimal threshold: -0.6895\n",
      "Epoch 31 train loss: 1.0340, eval loss 1.046547770500183\n",
      "optimal threshold: -0.7057\n",
      "Epoch 32 train loss: 1.0270, eval loss 1.037142038345337\n",
      "optimal threshold: -0.7058\n",
      "Epoch 33 train loss: 1.0148, eval loss 1.0278576612472534\n",
      "optimal threshold: -0.7181\n",
      "Epoch 34 train loss: 1.0048, eval loss 1.0187046527862549\n",
      "optimal threshold: -0.7325\n",
      "Epoch 35 train loss: 0.9968, eval loss 1.009672999382019\n",
      "optimal threshold: -0.7409\n",
      "Epoch 36 train loss: 0.9856, eval loss 1.000747561454773\n",
      "optimal threshold: -0.7514\n",
      "Epoch 37 train loss: 0.9777, eval loss 0.9919372797012329\n",
      "optimal threshold: -0.7549\n",
      "Epoch 38 train loss: 0.9663, eval loss 0.9832655191421509\n",
      "optimal threshold: -0.7597\n",
      "Epoch 39 train loss: 0.9610, eval loss 0.9747251868247986\n",
      "optimal threshold: -0.7409\n",
      "Epoch 40 train loss: 0.9521, eval loss 0.9663296937942505\n",
      "optimal threshold: -0.7501\n",
      "Epoch 41 train loss: 0.9439, eval loss 0.9580893516540527\n",
      "optimal threshold: -0.7567\n",
      "Epoch 42 train loss: 0.9350, eval loss 0.9499966502189636\n",
      "optimal threshold: -0.7609\n",
      "Epoch 43 train loss: 0.9240, eval loss 0.9420556426048279\n",
      "optimal threshold: -0.7659\n",
      "Epoch 44 train loss: 0.9166, eval loss 0.9342723488807678\n",
      "optimal threshold: -0.7835\n",
      "Epoch 45 train loss: 0.9107, eval loss 0.926641047000885\n",
      "optimal threshold: -0.7808\n",
      "Epoch 46 train loss: 0.9046, eval loss 0.9191704988479614\n",
      "optimal threshold: -0.7959\n",
      "Epoch 47 train loss: 0.8947, eval loss 0.9118548035621643\n",
      "optimal threshold: -0.8013\n",
      "Epoch 48 train loss: 0.8897, eval loss 0.9046940803527832\n",
      "optimal threshold: -0.7992\n",
      "Epoch 49 train loss: 0.8825, eval loss 0.8976906538009644\n",
      "optimal threshold: -0.8041\n",
      "Epoch 50 train loss: 0.8728, eval loss 0.8908510804176331\n",
      "optimal threshold: -0.8001\n",
      "Epoch 51 train loss: 0.8663, eval loss 0.8841764330863953\n",
      "optimal threshold: -0.7969\n",
      "Epoch 52 train loss: 0.8611, eval loss 0.8776720762252808\n",
      "optimal threshold: -0.8076\n",
      "Epoch 53 train loss: 0.8496, eval loss 0.8713262677192688\n",
      "optimal threshold: -0.7501\n",
      "Epoch 54 train loss: 0.8485, eval loss 0.8651513457298279\n",
      "optimal threshold: -0.9443\n",
      "Epoch 55 train loss: 0.8412, eval loss 0.8591340184211731\n",
      "optimal threshold: -0.7541\n",
      "Epoch 56 train loss: 0.8358, eval loss 0.8532854318618774\n",
      "optimal threshold: -0.7545\n",
      "Epoch 57 train loss: 0.8293, eval loss 0.8475940227508545\n",
      "optimal threshold: -0.7554\n",
      "Epoch 58 train loss: 0.8254, eval loss 0.842062771320343\n",
      "optimal threshold: -0.7685\n",
      "Epoch 59 train loss: 0.8203, eval loss 0.8366851806640625\n",
      "optimal threshold: -0.7656\n",
      "Epoch 60 train loss: 0.8162, eval loss 0.8314753770828247\n",
      "optimal threshold: -0.7686\n",
      "Epoch 61 train loss: 0.8103, eval loss 0.8264374732971191\n",
      "optimal threshold: -0.7649\n",
      "Epoch 62 train loss: 0.8108, eval loss 0.8215569853782654\n",
      "optimal threshold: -0.9990\n",
      "Epoch 63 train loss: 0.8005, eval loss 0.8168437480926514\n",
      "optimal threshold: -1.0019\n",
      "Epoch 64 train loss: 0.7935, eval loss 0.8122851252555847\n",
      "optimal threshold: -1.0050\n",
      "Epoch 65 train loss: 0.7904, eval loss 0.8078933954238892\n",
      "optimal threshold: -1.0076\n",
      "Epoch 66 train loss: 0.7877, eval loss 0.8036598563194275\n",
      "optimal threshold: -1.0116\n",
      "Epoch 67 train loss: 0.7853, eval loss 0.7995702028274536\n",
      "optimal threshold: -1.0140\n",
      "Epoch 68 train loss: 0.7805, eval loss 0.7956399321556091\n",
      "optimal threshold: -1.0140\n",
      "Epoch 69 train loss: 0.7775, eval loss 0.7918474674224854\n",
      "optimal threshold: -1.0175\n",
      "Epoch 70 train loss: 0.7711, eval loss 0.788195013999939\n",
      "optimal threshold: -1.0231\n",
      "Epoch 71 train loss: 0.7711, eval loss 0.7846787571907043\n",
      "optimal threshold: -0.8191\n",
      "Epoch 72 train loss: 0.7702, eval loss 0.7812888026237488\n",
      "optimal threshold: -0.8300\n",
      "Epoch 73 train loss: 0.7691, eval loss 0.7780318260192871\n",
      "optimal threshold: -0.8332\n",
      "Epoch 74 train loss: 0.7622, eval loss 0.7748994827270508\n",
      "optimal threshold: -0.8311\n",
      "Epoch 75 train loss: 0.7622, eval loss 0.7718802690505981\n",
      "optimal threshold: -0.8293\n",
      "Epoch 76 train loss: 0.7546, eval loss 0.7689732909202576\n",
      "optimal threshold: -0.9649\n",
      "Epoch 77 train loss: 0.7532, eval loss 0.7661824226379395\n",
      "optimal threshold: -0.9734\n",
      "Epoch 78 train loss: 0.7526, eval loss 0.7634897232055664\n",
      "optimal threshold: -1.0135\n",
      "Epoch 79 train loss: 0.7511, eval loss 0.7609035968780518\n",
      "optimal threshold: -0.7399\n",
      "Epoch 80 train loss: 0.7473, eval loss 0.7584185600280762\n",
      "optimal threshold: -0.7408\n",
      "Epoch 81 train loss: 0.7450, eval loss 0.7560178637504578\n",
      "optimal threshold: -1.0150\n",
      "Epoch 82 train loss: 0.7387, eval loss 0.7537137269973755\n",
      "optimal threshold: -1.0160\n",
      "Epoch 83 train loss: 0.7383, eval loss 0.7514864802360535\n",
      "optimal threshold: -1.0159\n",
      "Epoch 84 train loss: 0.7396, eval loss 0.749354898929596\n",
      "optimal threshold: -0.7205\n",
      "Epoch 85 train loss: 0.7423, eval loss 0.7472987174987793\n",
      "optimal threshold: -0.7208\n",
      "Epoch 86 train loss: 0.7364, eval loss 0.7453227639198303\n",
      "optimal threshold: -0.7154\n",
      "Epoch 87 train loss: 0.7367, eval loss 0.7434189915657043\n",
      "optimal threshold: -0.7187\n",
      "Epoch 88 train loss: 0.7306, eval loss 0.7416024208068848\n",
      "optimal threshold: -0.7148\n",
      "Epoch 89 train loss: 0.7271, eval loss 0.7398613095283508\n",
      "optimal threshold: -0.6797\n",
      "Epoch 90 train loss: 0.7369, eval loss 0.7381798624992371\n",
      "optimal threshold: -0.6871\n",
      "Epoch 91 train loss: 0.7276, eval loss 0.7365594506263733\n",
      "optimal threshold: -0.6907\n",
      "Epoch 92 train loss: 0.7297, eval loss 0.7350062727928162\n",
      "optimal threshold: -0.6933\n",
      "Epoch 93 train loss: 0.7221, eval loss 0.7335070371627808\n",
      "optimal threshold: -0.6787\n",
      "Epoch 94 train loss: 0.7225, eval loss 0.7320560216903687\n",
      "optimal threshold: -0.6787\n",
      "Epoch 95 train loss: 0.7225, eval loss 0.730663001537323\n",
      "optimal threshold: -0.6967\n",
      "Epoch 96 train loss: 0.7181, eval loss 0.729323148727417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6929\n",
      "Epoch 97 train loss: 0.7203, eval loss 0.7280243635177612\n",
      "optimal threshold: -0.6669\n",
      "Epoch 98 train loss: 0.7152, eval loss 0.7267761826515198\n",
      "optimal threshold: -0.6645\n",
      "Epoch 99 train loss: 0.7151, eval loss 0.725569486618042\n",
      "optimal threshold: -0.6576\n",
      "Epoch 100 train loss: 0.7194, eval loss 0.7244046330451965\n",
      "optimal threshold: -0.6567\n",
      "Epoch 101 train loss: 0.7172, eval loss 0.7232728600502014\n",
      "optimal threshold: -0.6752\n",
      "Epoch 102 train loss: 0.7141, eval loss 0.7221821546554565\n",
      "optimal threshold: -0.6789\n",
      "Epoch 103 train loss: 0.7116, eval loss 0.7211252450942993\n",
      "optimal threshold: -0.6815\n",
      "Epoch 104 train loss: 0.7084, eval loss 0.7201058864593506\n",
      "optimal threshold: -0.6445\n",
      "Epoch 105 train loss: 0.7082, eval loss 0.7191115021705627\n",
      "optimal threshold: -0.6669\n",
      "Epoch 106 train loss: 0.7079, eval loss 0.7181493639945984\n",
      "optimal threshold: -0.6743\n",
      "Epoch 107 train loss: 0.7069, eval loss 0.7172152400016785\n",
      "optimal threshold: -0.6738\n",
      "Epoch 108 train loss: 0.7083, eval loss 0.7163193225860596\n",
      "optimal threshold: -0.6697\n",
      "Epoch 109 train loss: 0.7156, eval loss 0.7154483199119568\n",
      "optimal threshold: -0.6664\n",
      "Epoch 110 train loss: 0.7130, eval loss 0.7145983576774597\n",
      "optimal threshold: -0.6471\n",
      "Epoch 111 train loss: 0.7122, eval loss 0.7137703895568848\n",
      "optimal threshold: -0.6628\n",
      "Epoch 112 train loss: 0.7099, eval loss 0.7129558324813843\n",
      "optimal threshold: -0.6564\n",
      "Epoch 113 train loss: 0.7047, eval loss 0.7121652960777283\n",
      "optimal threshold: -0.6546\n",
      "Epoch 114 train loss: 0.7044, eval loss 0.7114062905311584\n",
      "optimal threshold: -0.6625\n",
      "Epoch 115 train loss: 0.6991, eval loss 0.7106674909591675\n",
      "optimal threshold: -0.6579\n",
      "Epoch 116 train loss: 0.7004, eval loss 0.709945023059845\n",
      "optimal threshold: -0.6628\n",
      "Epoch 117 train loss: 0.7002, eval loss 0.7092368006706238\n",
      "optimal threshold: -0.6653\n",
      "Epoch 118 train loss: 0.7012, eval loss 0.7085519433021545\n",
      "optimal threshold: -0.6635\n",
      "Epoch 119 train loss: 0.7010, eval loss 0.7078729271888733\n",
      "optimal threshold: -0.6689\n",
      "Epoch 120 train loss: 0.7030, eval loss 0.7072038054466248\n",
      "optimal threshold: -0.6622\n",
      "Epoch 121 train loss: 0.6922, eval loss 0.7065512537956238\n",
      "optimal threshold: -0.6620\n",
      "Epoch 122 train loss: 0.7012, eval loss 0.705910861492157\n",
      "optimal threshold: -0.6632\n",
      "Epoch 123 train loss: 0.6993, eval loss 0.7052827477455139\n",
      "optimal threshold: -0.6660\n",
      "Epoch 124 train loss: 0.6978, eval loss 0.7046641707420349\n",
      "optimal threshold: -0.6717\n",
      "Epoch 125 train loss: 0.6990, eval loss 0.704067051410675\n",
      "optimal threshold: -0.6780\n",
      "Epoch 126 train loss: 0.6905, eval loss 0.7034744024276733\n",
      "optimal threshold: -0.6746\n",
      "Epoch 127 train loss: 0.7028, eval loss 0.7028974294662476\n",
      "optimal threshold: -0.6746\n",
      "Epoch 128 train loss: 0.6938, eval loss 0.7023385167121887\n",
      "optimal threshold: -0.6859\n",
      "Epoch 129 train loss: 0.6936, eval loss 0.7017768621444702\n",
      "optimal threshold: -0.6862\n",
      "Epoch 130 train loss: 0.6893, eval loss 0.7012346386909485\n",
      "optimal threshold: -0.6869\n",
      "Epoch 131 train loss: 0.6874, eval loss 0.7006987929344177\n",
      "optimal threshold: -0.6826\n",
      "Epoch 132 train loss: 0.6896, eval loss 0.700170636177063\n",
      "optimal threshold: -0.6871\n",
      "Epoch 133 train loss: 0.6908, eval loss 0.6996504068374634\n",
      "optimal threshold: -0.6875\n",
      "Epoch 134 train loss: 0.6899, eval loss 0.6991448998451233\n",
      "optimal threshold: -0.7057\n",
      "Epoch 135 train loss: 0.6907, eval loss 0.6986411213874817\n",
      "optimal threshold: -0.7051\n",
      "Epoch 136 train loss: 0.6857, eval loss 0.6981562972068787\n",
      "optimal threshold: -0.7062\n",
      "Epoch 137 train loss: 0.6940, eval loss 0.6976681351661682\n",
      "optimal threshold: -0.8723\n",
      "Epoch 138 train loss: 0.6894, eval loss 0.6971901059150696\n",
      "optimal threshold: -0.8740\n",
      "Epoch 139 train loss: 0.6824, eval loss 0.6967210173606873\n",
      "optimal threshold: -0.7215\n",
      "Epoch 140 train loss: 0.6846, eval loss 0.6962643265724182\n",
      "optimal threshold: -0.8261\n",
      "Epoch 141 train loss: 0.6816, eval loss 0.6958017945289612\n",
      "optimal threshold: -0.8310\n",
      "Epoch 142 train loss: 0.6787, eval loss 0.6953526139259338\n",
      "optimal threshold: -0.8432\n",
      "Epoch 143 train loss: 0.6807, eval loss 0.6949170231819153\n",
      "optimal threshold: -0.8255\n",
      "Epoch 144 train loss: 0.6794, eval loss 0.6944774389266968\n",
      "optimal threshold: -0.8270\n",
      "Epoch 145 train loss: 0.6806, eval loss 0.6940415501594543\n",
      "optimal threshold: -0.8254\n",
      "Epoch 146 train loss: 0.6817, eval loss 0.6936010718345642\n",
      "optimal threshold: -0.8267\n",
      "Epoch 147 train loss: 0.6809, eval loss 0.6931935548782349\n",
      "optimal threshold: -0.8286\n",
      "Epoch 148 train loss: 0.6848, eval loss 0.6927747130393982\n",
      "optimal threshold: -0.8269\n",
      "Epoch 149 train loss: 0.6860, eval loss 0.6923725008964539\n",
      "optimal threshold: -0.8316\n",
      "Epoch 150 train loss: 0.6807, eval loss 0.6919801235198975\n",
      "optimal threshold: -0.8285\n",
      "Epoch 151 train loss: 0.6771, eval loss 0.6915856003761292\n",
      "optimal threshold: -0.8297\n",
      "Epoch 152 train loss: 0.6855, eval loss 0.6911998987197876\n",
      "optimal threshold: -0.8303\n",
      "Epoch 153 train loss: 0.6747, eval loss 0.6908135414123535\n",
      "optimal threshold: -0.8455\n",
      "Epoch 154 train loss: 0.6794, eval loss 0.6904240250587463\n",
      "optimal threshold: -0.8460\n",
      "Epoch 155 train loss: 0.6762, eval loss 0.6900516748428345\n",
      "optimal threshold: -0.8465\n",
      "Epoch 156 train loss: 0.6729, eval loss 0.6896805763244629\n",
      "optimal threshold: -0.8774\n",
      "Epoch 157 train loss: 0.6746, eval loss 0.6893107295036316\n",
      "optimal threshold: -0.8732\n",
      "Epoch 158 train loss: 0.6751, eval loss 0.688933789730072\n",
      "optimal threshold: -0.8714\n",
      "Epoch 159 train loss: 0.6771, eval loss 0.688580334186554\n",
      "optimal threshold: -0.8725\n",
      "Epoch 160 train loss: 0.6759, eval loss 0.688230574131012\n",
      "optimal threshold: -0.8779\n",
      "Epoch 161 train loss: 0.6735, eval loss 0.6878783702850342\n",
      "optimal threshold: -0.8821\n",
      "Epoch 162 train loss: 0.6714, eval loss 0.6875436305999756\n",
      "optimal threshold: -0.8876\n",
      "Epoch 163 train loss: 0.6730, eval loss 0.6872045993804932\n",
      "optimal threshold: -0.8120\n",
      "Epoch 164 train loss: 0.6739, eval loss 0.6868749260902405\n",
      "optimal threshold: -0.8719\n",
      "Epoch 165 train loss: 0.6744, eval loss 0.6865416169166565\n",
      "optimal threshold: -0.8709\n",
      "Epoch 166 train loss: 0.6720, eval loss 0.686218798160553\n",
      "optimal threshold: -0.8708\n",
      "Epoch 167 train loss: 0.6679, eval loss 0.6859028935432434\n",
      "optimal threshold: -0.8757\n",
      "Epoch 168 train loss: 0.6672, eval loss 0.6855771541595459\n",
      "optimal threshold: -0.8690\n",
      "Epoch 169 train loss: 0.6669, eval loss 0.6852533221244812\n",
      "optimal threshold: -0.8696\n",
      "Epoch 170 train loss: 0.6717, eval loss 0.6849458813667297\n",
      "optimal threshold: -0.8728\n",
      "Epoch 171 train loss: 0.6708, eval loss 0.6846460103988647\n",
      "optimal threshold: -0.8721\n",
      "Epoch 172 train loss: 0.6648, eval loss 0.6843490600585938\n",
      "optimal threshold: -0.8411\n",
      "Epoch 173 train loss: 0.6697, eval loss 0.684053897857666\n",
      "optimal threshold: -0.8417\n",
      "Epoch 174 train loss: 0.6739, eval loss 0.6837589144706726\n",
      "optimal threshold: -0.8424\n",
      "Epoch 175 train loss: 0.6630, eval loss 0.6834691166877747\n",
      "optimal threshold: -0.8717\n",
      "Epoch 176 train loss: 0.6633, eval loss 0.683184027671814\n",
      "optimal threshold: -0.8774\n",
      "Epoch 177 train loss: 0.6638, eval loss 0.6829097270965576\n",
      "optimal threshold: -0.8791\n",
      "Epoch 178 train loss: 0.6608, eval loss 0.6826186776161194\n",
      "optimal threshold: -0.8770\n",
      "Epoch 179 train loss: 0.6653, eval loss 0.6823421716690063\n",
      "optimal threshold: -0.8751\n",
      "Epoch 180 train loss: 0.6665, eval loss 0.6820639371871948\n",
      "optimal threshold: -0.8704\n",
      "Epoch 181 train loss: 0.6533, eval loss 0.6817873120307922\n",
      "optimal threshold: -0.8970\n",
      "Epoch 182 train loss: 0.6668, eval loss 0.6815313696861267\n",
      "optimal threshold: -0.9017\n",
      "Epoch 183 train loss: 0.6607, eval loss 0.6812803745269775\n",
      "optimal threshold: -0.8787\n",
      "Epoch 184 train loss: 0.6610, eval loss 0.6810067296028137\n",
      "optimal threshold: -0.8777\n",
      "Epoch 185 train loss: 0.6645, eval loss 0.6807489991188049\n",
      "optimal threshold: -0.8724\n",
      "Epoch 186 train loss: 0.6639, eval loss 0.6804959774017334\n",
      "optimal threshold: -0.8727\n",
      "Epoch 187 train loss: 0.6640, eval loss 0.6802438497543335\n",
      "optimal threshold: -0.8739\n",
      "Epoch 188 train loss: 0.6603, eval loss 0.6800041794776917\n",
      "optimal threshold: -0.8740\n",
      "Epoch 189 train loss: 0.6560, eval loss 0.6797568798065186\n",
      "optimal threshold: -0.8759\n",
      "Epoch 190 train loss: 0.6603, eval loss 0.6795176863670349\n",
      "optimal threshold: -0.8775\n",
      "Epoch 191 train loss: 0.6671, eval loss 0.6792823672294617\n",
      "optimal threshold: -0.6013\n",
      "Epoch 192 train loss: 0.6634, eval loss 0.6790520548820496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5920\n",
      "Epoch 193 train loss: 0.6600, eval loss 0.6788268685340881\n",
      "optimal threshold: -0.8774\n",
      "Epoch 194 train loss: 0.6529, eval loss 0.6785951256752014\n",
      "optimal threshold: -0.6392\n",
      "Epoch 195 train loss: 0.6482, eval loss 0.6783715486526489\n",
      "optimal threshold: -0.6397\n",
      "Epoch 196 train loss: 0.6590, eval loss 0.6781538128852844\n",
      "optimal threshold: -0.6415\n",
      "Epoch 197 train loss: 0.6559, eval loss 0.6779372096061707\n",
      "optimal threshold: -0.6460\n",
      "Epoch 198 train loss: 0.6561, eval loss 0.6777235269546509\n",
      "optimal threshold: -0.6425\n",
      "Epoch 199 train loss: 0.6556, eval loss 0.6775049567222595\n",
      "optimal threshold: -0.6470\n",
      "Epoch 200 train loss: 0.6509, eval loss 0.6773014068603516\n",
      "optimal threshold: -0.6505\n",
      "Epoch 201 train loss: 0.6549, eval loss 0.6770994067192078\n",
      "optimal threshold: -0.6486\n",
      "Epoch 202 train loss: 0.6543, eval loss 0.6768832802772522\n",
      "optimal threshold: -0.6490\n",
      "Epoch 203 train loss: 0.6602, eval loss 0.6766735315322876\n",
      "optimal threshold: -0.6295\n",
      "Epoch 204 train loss: 0.6578, eval loss 0.6764715313911438\n",
      "optimal threshold: -0.6304\n",
      "Epoch 205 train loss: 0.6533, eval loss 0.6762721538543701\n",
      "optimal threshold: -0.6279\n",
      "Epoch 206 train loss: 0.6534, eval loss 0.676067054271698\n",
      "optimal threshold: -0.6297\n",
      "Epoch 207 train loss: 0.6516, eval loss 0.6758825778961182\n",
      "optimal threshold: -0.6271\n",
      "Epoch 208 train loss: 0.6459, eval loss 0.6756921410560608\n",
      "optimal threshold: -0.6323\n",
      "Epoch 209 train loss: 0.6578, eval loss 0.6755021810531616\n",
      "optimal threshold: -0.6260\n",
      "Epoch 210 train loss: 0.6512, eval loss 0.6753114461898804\n",
      "optimal threshold: -0.6629\n",
      "Epoch 211 train loss: 0.6506, eval loss 0.6751306653022766\n",
      "optimal threshold: -0.6578\n",
      "Epoch 212 train loss: 0.6504, eval loss 0.6749516129493713\n",
      "optimal threshold: -0.6558\n",
      "Epoch 213 train loss: 0.6464, eval loss 0.6747572422027588\n",
      "optimal threshold: -0.6403\n",
      "Epoch 214 train loss: 0.6513, eval loss 0.6745833158493042\n",
      "optimal threshold: -0.6393\n",
      "Epoch 215 train loss: 0.6531, eval loss 0.6744037866592407\n",
      "optimal threshold: -0.6401\n",
      "Epoch 216 train loss: 0.6420, eval loss 0.674235999584198\n",
      "optimal threshold: -0.6183\n",
      "Epoch 217 train loss: 0.6499, eval loss 0.6740664839744568\n",
      "optimal threshold: -0.6205\n",
      "Epoch 218 train loss: 0.6456, eval loss 0.673901379108429\n",
      "optimal threshold: -0.6488\n",
      "Epoch 219 train loss: 0.6471, eval loss 0.6737348437309265\n",
      "optimal threshold: -0.6493\n",
      "Epoch 220 train loss: 0.6474, eval loss 0.6735787391662598\n",
      "optimal threshold: -0.6143\n",
      "Epoch 221 train loss: 0.6534, eval loss 0.6734234690666199\n",
      "optimal threshold: -0.6451\n",
      "Epoch 222 train loss: 0.6482, eval loss 0.6732521653175354\n",
      "optimal threshold: -0.6265\n",
      "Epoch 223 train loss: 0.6497, eval loss 0.6730973720550537\n",
      "optimal threshold: -0.6210\n",
      "Epoch 224 train loss: 0.6472, eval loss 0.6729499101638794\n",
      "optimal threshold: -0.6238\n",
      "Epoch 225 train loss: 0.6427, eval loss 0.6727868318557739\n",
      "optimal threshold: -0.6113\n",
      "Epoch 226 train loss: 0.6469, eval loss 0.6726239323616028\n",
      "optimal threshold: -0.6150\n",
      "Epoch 227 train loss: 0.6534, eval loss 0.6724795699119568\n",
      "optimal threshold: -0.6141\n",
      "Epoch 228 train loss: 0.6446, eval loss 0.6723265051841736\n",
      "optimal threshold: -0.6145\n",
      "Epoch 229 train loss: 0.6454, eval loss 0.672183096408844\n",
      "optimal threshold: -0.6222\n",
      "Epoch 230 train loss: 0.6539, eval loss 0.6720333695411682\n",
      "optimal threshold: -0.6176\n",
      "Epoch 231 train loss: 0.6445, eval loss 0.6718904972076416\n",
      "optimal threshold: -0.6165\n",
      "Epoch 232 train loss: 0.6501, eval loss 0.671743631362915\n",
      "optimal threshold: -0.6169\n",
      "Epoch 233 train loss: 0.6469, eval loss 0.6716099381446838\n",
      "optimal threshold: -0.6174\n",
      "Epoch 234 train loss: 0.6453, eval loss 0.6714720726013184\n",
      "optimal threshold: -0.6144\n",
      "Epoch 235 train loss: 0.6372, eval loss 0.6713455319404602\n",
      "optimal threshold: -0.6155\n",
      "Epoch 236 train loss: 0.6408, eval loss 0.6712074875831604\n",
      "optimal threshold: -0.6272\n",
      "Epoch 237 train loss: 0.6471, eval loss 0.6710604429244995\n",
      "optimal threshold: -0.6264\n",
      "Epoch 238 train loss: 0.6434, eval loss 0.6709322929382324\n",
      "optimal threshold: -0.6249\n",
      "Epoch 239 train loss: 0.6440, eval loss 0.670802116394043\n",
      "optimal threshold: -0.6086\n",
      "Epoch 240 train loss: 0.6418, eval loss 0.6706711053848267\n",
      "optimal threshold: -0.6084\n",
      "Epoch 241 train loss: 0.6450, eval loss 0.670545756816864\n",
      "optimal threshold: -0.6080\n",
      "Epoch 242 train loss: 0.6370, eval loss 0.6704215407371521\n",
      "optimal threshold: -0.6077\n",
      "Epoch 243 train loss: 0.6517, eval loss 0.6702924370765686\n",
      "optimal threshold: -0.6073\n",
      "Epoch 244 train loss: 0.6465, eval loss 0.6701635122299194\n",
      "optimal threshold: -0.7440\n",
      "Epoch 245 train loss: 0.6446, eval loss 0.6700447201728821\n",
      "optimal threshold: -0.6072\n",
      "Epoch 246 train loss: 0.6438, eval loss 0.6699192523956299\n",
      "optimal threshold: -0.6065\n",
      "Epoch 247 train loss: 0.6448, eval loss 0.6697983741760254\n",
      "optimal threshold: -0.7398\n",
      "Epoch 248 train loss: 0.6375, eval loss 0.6696925759315491\n",
      "optimal threshold: -0.7385\n",
      "Epoch 249 train loss: 0.6393, eval loss 0.6695774793624878\n",
      "optimal threshold: -0.7318\n",
      "Epoch 250 train loss: 0.6449, eval loss 0.6694610118865967\n",
      "optimal threshold: -0.7304\n",
      "Epoch 251 train loss: 0.6489, eval loss 0.6693405508995056\n",
      "optimal threshold: -0.7303\n",
      "Epoch 252 train loss: 0.6427, eval loss 0.6692338585853577\n",
      "optimal threshold: -0.7323\n",
      "Epoch 253 train loss: 0.6518, eval loss 0.6691179275512695\n",
      "optimal threshold: -0.7322\n",
      "Epoch 254 train loss: 0.6391, eval loss 0.6690221428871155\n",
      "optimal threshold: -0.7330\n",
      "Epoch 255 train loss: 0.6376, eval loss 0.6689110398292542\n",
      "optimal threshold: -0.7290\n",
      "Epoch 256 train loss: 0.6414, eval loss 0.668813943862915\n",
      "optimal threshold: -0.7282\n",
      "Epoch 257 train loss: 0.6370, eval loss 0.6686885952949524\n",
      "optimal threshold: -0.7277\n",
      "Epoch 258 train loss: 0.6417, eval loss 0.6685758829116821\n",
      "optimal threshold: -0.7261\n",
      "Epoch 259 train loss: 0.6373, eval loss 0.6684783697128296\n",
      "optimal threshold: -0.7251\n",
      "Epoch 260 train loss: 0.6453, eval loss 0.6683676242828369\n",
      "optimal threshold: -0.7301\n",
      "Epoch 261 train loss: 0.6380, eval loss 0.6682770252227783\n",
      "optimal threshold: -0.7266\n",
      "Epoch 262 train loss: 0.6344, eval loss 0.6681773066520691\n",
      "optimal threshold: -0.7231\n",
      "Epoch 263 train loss: 0.6443, eval loss 0.6680821776390076\n",
      "optimal threshold: -0.7254\n",
      "Epoch 264 train loss: 0.6395, eval loss 0.6679868698120117\n",
      "optimal threshold: -0.7257\n",
      "Epoch 265 train loss: 0.6345, eval loss 0.6678908467292786\n",
      "optimal threshold: -0.7246\n",
      "Epoch 266 train loss: 0.6376, eval loss 0.6677853465080261\n",
      "optimal threshold: -0.7244\n",
      "Epoch 267 train loss: 0.6383, eval loss 0.6676899194717407\n",
      "optimal threshold: -0.7790\n",
      "Epoch 268 train loss: 0.6441, eval loss 0.6676023006439209\n",
      "optimal threshold: -0.7788\n",
      "Epoch 269 train loss: 0.6460, eval loss 0.6675130128860474\n",
      "optimal threshold: -0.7801\n",
      "Epoch 270 train loss: 0.6433, eval loss 0.6674253344535828\n",
      "optimal threshold: -0.7805\n",
      "Epoch 271 train loss: 0.6307, eval loss 0.6673424243927002\n",
      "optimal threshold: -0.7773\n",
      "Epoch 272 train loss: 0.6323, eval loss 0.667243242263794\n",
      "optimal threshold: -0.7765\n",
      "Epoch 273 train loss: 0.6414, eval loss 0.667148768901825\n",
      "optimal threshold: -0.7733\n",
      "Epoch 274 train loss: 0.6295, eval loss 0.6670677065849304\n",
      "optimal threshold: -0.7767\n",
      "Epoch 275 train loss: 0.6301, eval loss 0.6669851541519165\n",
      "optimal threshold: -0.7760\n",
      "Epoch 276 train loss: 0.6360, eval loss 0.6668980121612549\n",
      "optimal threshold: -0.7764\n",
      "Epoch 277 train loss: 0.6336, eval loss 0.666824460029602\n",
      "optimal threshold: -0.7740\n",
      "Epoch 278 train loss: 0.6365, eval loss 0.6667212247848511\n",
      "optimal threshold: -0.7689\n",
      "Epoch 279 train loss: 0.6339, eval loss 0.666648805141449\n",
      "optimal threshold: -0.7771\n",
      "Epoch 280 train loss: 0.6318, eval loss 0.6665614247322083\n",
      "optimal threshold: -0.7683\n",
      "Epoch 281 train loss: 0.6446, eval loss 0.6664860248565674\n",
      "optimal threshold: -0.7388\n",
      "Epoch 282 train loss: 0.6377, eval loss 0.6664003133773804\n",
      "optimal threshold: -0.7415\n",
      "Epoch 283 train loss: 0.6374, eval loss 0.6663358807563782\n",
      "optimal threshold: -0.7427\n",
      "Epoch 284 train loss: 0.6378, eval loss 0.6662538647651672\n",
      "optimal threshold: -0.7429\n",
      "Epoch 285 train loss: 0.6267, eval loss 0.6661695837974548\n",
      "optimal threshold: -0.7460\n",
      "Epoch 286 train loss: 0.6473, eval loss 0.6661182641983032\n",
      "optimal threshold: -0.7462\n",
      "Epoch 287 train loss: 0.6415, eval loss 0.6660328507423401\n",
      "optimal threshold: -0.7621\n",
      "Epoch 288 train loss: 0.6276, eval loss 0.6659458875656128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7625\n",
      "Epoch 289 train loss: 0.6387, eval loss 0.6658827066421509\n",
      "optimal threshold: -0.7622\n",
      "Epoch 290 train loss: 0.6321, eval loss 0.6658141016960144\n",
      "optimal threshold: -0.7613\n",
      "Epoch 291 train loss: 0.6352, eval loss 0.6657313704490662\n",
      "optimal threshold: -0.7616\n",
      "Epoch 292 train loss: 0.6341, eval loss 0.6656613945960999\n",
      "optimal threshold: -0.7690\n",
      "Epoch 293 train loss: 0.6323, eval loss 0.6656015515327454\n",
      "optimal threshold: -0.7684\n",
      "Epoch 294 train loss: 0.6277, eval loss 0.6655323505401611\n",
      "optimal threshold: -0.7668\n",
      "Epoch 295 train loss: 0.6384, eval loss 0.665451169013977\n",
      "optimal threshold: -0.5968\n",
      "Epoch 296 train loss: 0.6277, eval loss 0.6653792262077332\n",
      "optimal threshold: -0.5956\n",
      "Epoch 297 train loss: 0.6288, eval loss 0.6653094291687012\n",
      "optimal threshold: -0.5967\n",
      "Epoch 298 train loss: 0.6371, eval loss 0.665249764919281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:04:24,604] Trial 1 finished with value: 0.6375452280044556 and parameters: {'learning_rate_exp': -5.47138689884566, 'dropout_p': 0.05102510670507641, 'l2_reg_exp': -5.777596491777686, 'batch_size': 487, 'N': 460}. Best is trial 0 with value: 0.5673678517341614.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5976\n",
      "Epoch 299 train loss: 0.6375, eval loss 0.6651955246925354\n",
      "optimal threshold: -0.2043\n",
      "Epoch 0 train loss: 1.4185, eval loss 1.4143792390823364\n",
      "optimal threshold: -0.2382\n",
      "Epoch 1 train loss: 1.3942, eval loss 1.4058321714401245\n",
      "optimal threshold: -0.2726\n",
      "Epoch 2 train loss: 1.3973, eval loss 1.3975192308425903\n",
      "optimal threshold: -0.3056\n",
      "Epoch 3 train loss: 1.3853, eval loss 1.3894044160842896\n",
      "optimal threshold: -0.3362\n",
      "Epoch 4 train loss: 1.3926, eval loss 1.3814582824707031\n",
      "optimal threshold: -0.3708\n",
      "Epoch 5 train loss: 1.3851, eval loss 1.373618245124817\n",
      "optimal threshold: -0.4097\n",
      "Epoch 6 train loss: 1.3535, eval loss 1.3658826351165771\n",
      "optimal threshold: -0.4493\n",
      "Epoch 7 train loss: 1.3555, eval loss 1.3582239151000977\n",
      "optimal threshold: -0.4876\n",
      "Epoch 8 train loss: 1.3452, eval loss 1.35063636302948\n",
      "optimal threshold: -0.5253\n",
      "Epoch 9 train loss: 1.3403, eval loss 1.3431212902069092\n",
      "optimal threshold: -0.2160\n",
      "Epoch 10 train loss: 1.3372, eval loss 1.3356703519821167\n",
      "optimal threshold: -0.2260\n",
      "Epoch 11 train loss: 1.3324, eval loss 1.328270435333252\n",
      "optimal threshold: -0.2385\n",
      "Epoch 12 train loss: 1.3305, eval loss 1.3209123611450195\n",
      "optimal threshold: -0.2514\n",
      "Epoch 13 train loss: 1.3086, eval loss 1.3136091232299805\n",
      "optimal threshold: -0.2608\n",
      "Epoch 14 train loss: 1.3004, eval loss 1.3062968254089355\n",
      "optimal threshold: -0.2747\n",
      "Epoch 15 train loss: 1.3052, eval loss 1.2990458011627197\n",
      "optimal threshold: -0.2853\n",
      "Epoch 16 train loss: 1.2976, eval loss 1.2917851209640503\n",
      "optimal threshold: -0.2975\n",
      "Epoch 17 train loss: 1.2874, eval loss 1.2845889329910278\n",
      "optimal threshold: -0.3116\n",
      "Epoch 18 train loss: 1.2751, eval loss 1.2773768901824951\n",
      "optimal threshold: -0.3279\n",
      "Epoch 19 train loss: 1.2921, eval loss 1.2701951265335083\n",
      "optimal threshold: -0.3425\n",
      "Epoch 20 train loss: 1.2653, eval loss 1.2630438804626465\n",
      "optimal threshold: -0.3557\n",
      "Epoch 21 train loss: 1.2422, eval loss 1.2558653354644775\n",
      "optimal threshold: -0.3684\n",
      "Epoch 22 train loss: 1.2550, eval loss 1.2487220764160156\n",
      "optimal threshold: -0.3844\n",
      "Epoch 23 train loss: 1.2360, eval loss 1.2416645288467407\n",
      "optimal threshold: -0.3979\n",
      "Epoch 24 train loss: 1.2212, eval loss 1.2345956563949585\n",
      "optimal threshold: -0.4125\n",
      "Epoch 25 train loss: 1.2255, eval loss 1.2275911569595337\n",
      "optimal threshold: -0.4282\n",
      "Epoch 26 train loss: 1.2204, eval loss 1.2206367254257202\n",
      "optimal threshold: -0.4543\n",
      "Epoch 27 train loss: 1.2100, eval loss 1.2137075662612915\n",
      "optimal threshold: -0.4589\n",
      "Epoch 28 train loss: 1.2159, eval loss 1.2068407535552979\n",
      "optimal threshold: -0.4726\n",
      "Epoch 29 train loss: 1.2027, eval loss 1.2000232934951782\n",
      "optimal threshold: -0.4984\n",
      "Epoch 30 train loss: 1.1904, eval loss 1.1932379007339478\n",
      "optimal threshold: -0.5010\n",
      "Epoch 31 train loss: 1.1903, eval loss 1.1865297555923462\n",
      "optimal threshold: -0.5173\n",
      "Epoch 32 train loss: 1.1797, eval loss 1.1798923015594482\n",
      "optimal threshold: -0.5364\n",
      "Epoch 33 train loss: 1.1778, eval loss 1.173303484916687\n",
      "optimal threshold: -0.5513\n",
      "Epoch 34 train loss: 1.1666, eval loss 1.1667951345443726\n",
      "optimal threshold: -0.5711\n",
      "Epoch 35 train loss: 1.1561, eval loss 1.160340428352356\n",
      "optimal threshold: -0.5782\n",
      "Epoch 36 train loss: 1.1666, eval loss 1.1539723873138428\n",
      "optimal threshold: -0.5916\n",
      "Epoch 37 train loss: 1.1374, eval loss 1.1476842164993286\n",
      "optimal threshold: -0.6112\n",
      "Epoch 38 train loss: 1.1428, eval loss 1.1414817571640015\n",
      "optimal threshold: -0.6261\n",
      "Epoch 39 train loss: 1.1431, eval loss 1.1353400945663452\n",
      "optimal threshold: -0.6413\n",
      "Epoch 40 train loss: 1.1168, eval loss 1.1292823553085327\n",
      "optimal threshold: -0.6613\n",
      "Epoch 41 train loss: 1.1332, eval loss 1.1232966184616089\n",
      "optimal threshold: -0.6756\n",
      "Epoch 42 train loss: 1.1190, eval loss 1.1173864603042603\n",
      "optimal threshold: -0.6870\n",
      "Epoch 43 train loss: 1.1149, eval loss 1.1115283966064453\n",
      "optimal threshold: -0.6872\n",
      "Epoch 44 train loss: 1.1177, eval loss 1.1057435274124146\n",
      "optimal threshold: -0.7019\n",
      "Epoch 45 train loss: 1.1055, eval loss 1.100083827972412\n",
      "optimal threshold: -0.6961\n",
      "Epoch 46 train loss: 1.1033, eval loss 1.0944591760635376\n",
      "optimal threshold: -0.7431\n",
      "Epoch 47 train loss: 1.0821, eval loss 1.0889019966125488\n",
      "optimal threshold: -0.7607\n",
      "Epoch 48 train loss: 1.0908, eval loss 1.0834007263183594\n",
      "optimal threshold: -0.7734\n",
      "Epoch 49 train loss: 1.0756, eval loss 1.0780103206634521\n",
      "optimal threshold: -0.7813\n",
      "Epoch 50 train loss: 1.0756, eval loss 1.0726580619812012\n",
      "optimal threshold: -0.8044\n",
      "Epoch 51 train loss: 1.0779, eval loss 1.0673717260360718\n",
      "optimal threshold: -0.8161\n",
      "Epoch 52 train loss: 1.0733, eval loss 1.0621755123138428\n",
      "optimal threshold: -0.8255\n",
      "Epoch 53 train loss: 1.0573, eval loss 1.0569924116134644\n",
      "optimal threshold: -0.8325\n",
      "Epoch 54 train loss: 1.0655, eval loss 1.0519081354141235\n",
      "optimal threshold: -0.8391\n",
      "Epoch 55 train loss: 1.0701, eval loss 1.0468746423721313\n",
      "optimal threshold: -0.8509\n",
      "Epoch 56 train loss: 1.0421, eval loss 1.0419055223464966\n",
      "optimal threshold: -0.8600\n",
      "Epoch 57 train loss: 1.0365, eval loss 1.0369718074798584\n",
      "optimal threshold: -0.8428\n",
      "Epoch 58 train loss: 1.0393, eval loss 1.0320613384246826\n",
      "optimal threshold: -0.8376\n",
      "Epoch 59 train loss: 1.0259, eval loss 1.0272313356399536\n",
      "optimal threshold: -0.8663\n",
      "Epoch 60 train loss: 1.0193, eval loss 1.0224345922470093\n",
      "optimal threshold: -0.8557\n",
      "Epoch 61 train loss: 1.0224, eval loss 1.0176628828048706\n",
      "optimal threshold: -0.8615\n",
      "Epoch 62 train loss: 1.0266, eval loss 1.0129624605178833\n",
      "optimal threshold: -0.9090\n",
      "Epoch 63 train loss: 1.0056, eval loss 1.0082930326461792\n",
      "optimal threshold: -0.8893\n",
      "Epoch 64 train loss: 1.0061, eval loss 1.0036842823028564\n",
      "optimal threshold: -0.8738\n",
      "Epoch 65 train loss: 0.9989, eval loss 0.9990840554237366\n",
      "optimal threshold: -0.8822\n",
      "Epoch 66 train loss: 1.0065, eval loss 0.9945257902145386\n",
      "optimal threshold: -0.8908\n",
      "Epoch 67 train loss: 1.0053, eval loss 0.9899900555610657\n",
      "optimal threshold: -0.8938\n",
      "Epoch 68 train loss: 0.9860, eval loss 0.9855045080184937\n",
      "optimal threshold: -0.9091\n",
      "Epoch 69 train loss: 0.9806, eval loss 0.9810380339622498\n",
      "optimal threshold: -0.9137\n",
      "Epoch 70 train loss: 0.9992, eval loss 0.9766010046005249\n",
      "optimal threshold: -0.9195\n",
      "Epoch 71 train loss: 0.9657, eval loss 0.9722340703010559\n",
      "optimal threshold: -0.9257\n",
      "Epoch 72 train loss: 0.9733, eval loss 0.9678907990455627\n",
      "optimal threshold: -0.9298\n",
      "Epoch 73 train loss: 0.9721, eval loss 0.9635482430458069\n",
      "optimal threshold: -0.9108\n",
      "Epoch 74 train loss: 0.9588, eval loss 0.9592281579971313\n",
      "optimal threshold: -0.9223\n",
      "Epoch 75 train loss: 0.9626, eval loss 0.954933226108551\n",
      "optimal threshold: -0.9235\n",
      "Epoch 76 train loss: 0.9678, eval loss 0.9506709575653076\n",
      "optimal threshold: -0.9226\n",
      "Epoch 77 train loss: 0.9570, eval loss 0.9464486837387085\n",
      "optimal threshold: -0.9239\n",
      "Epoch 78 train loss: 0.9545, eval loss 0.9422444105148315\n",
      "optimal threshold: -0.9869\n",
      "Epoch 79 train loss: 0.9450, eval loss 0.9380854964256287\n",
      "optimal threshold: -0.9925\n",
      "Epoch 80 train loss: 0.9704, eval loss 0.9339682459831238\n",
      "optimal threshold: -0.9866\n",
      "Epoch 81 train loss: 0.9312, eval loss 0.9298843145370483\n",
      "optimal threshold: -0.9847\n",
      "Epoch 82 train loss: 0.9487, eval loss 0.9258133769035339\n",
      "optimal threshold: -0.9997\n",
      "Epoch 83 train loss: 0.9468, eval loss 0.9217482805252075\n",
      "optimal threshold: -1.0024\n",
      "Epoch 84 train loss: 0.9097, eval loss 0.9177223443984985\n",
      "optimal threshold: -0.9983\n",
      "Epoch 85 train loss: 0.8977, eval loss 0.9137484431266785\n",
      "optimal threshold: -0.9474\n",
      "Epoch 86 train loss: 0.9019, eval loss 0.9097888469696045\n",
      "optimal threshold: -0.9476\n",
      "Epoch 87 train loss: 0.9023, eval loss 0.9058768153190613\n",
      "optimal threshold: -0.9749\n",
      "Epoch 88 train loss: 0.9450, eval loss 0.902000367641449\n",
      "optimal threshold: -0.9670\n",
      "Epoch 89 train loss: 0.9335, eval loss 0.8981505036354065\n",
      "optimal threshold: -1.0200\n",
      "Epoch 90 train loss: 0.9009, eval loss 0.8943502902984619\n",
      "optimal threshold: -1.0227\n",
      "Epoch 91 train loss: 0.8968, eval loss 0.8905876278877258\n",
      "optimal threshold: -1.0220\n",
      "Epoch 92 train loss: 0.9148, eval loss 0.8868631720542908\n",
      "optimal threshold: -1.0238\n",
      "Epoch 93 train loss: 0.8952, eval loss 0.8831738829612732\n",
      "optimal threshold: -1.0253\n",
      "Epoch 94 train loss: 0.9049, eval loss 0.8795152306556702\n",
      "optimal threshold: -1.0270\n",
      "Epoch 95 train loss: 0.8766, eval loss 0.8759161829948425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -1.0287\n",
      "Epoch 96 train loss: 0.8913, eval loss 0.8723573088645935\n",
      "optimal threshold: -1.0293\n",
      "Epoch 97 train loss: 0.8975, eval loss 0.8688141703605652\n",
      "optimal threshold: -1.0305\n",
      "Epoch 98 train loss: 0.8766, eval loss 0.8653342127799988\n",
      "optimal threshold: -1.0322\n",
      "Epoch 99 train loss: 0.8846, eval loss 0.8618984818458557\n",
      "optimal threshold: -1.0225\n",
      "Epoch 100 train loss: 0.8632, eval loss 0.8585178256034851\n",
      "optimal threshold: -1.0222\n",
      "Epoch 101 train loss: 0.9024, eval loss 0.8551469445228577\n",
      "optimal threshold: -1.0231\n",
      "Epoch 102 train loss: 0.8288, eval loss 0.8518672585487366\n",
      "optimal threshold: -1.0240\n",
      "Epoch 103 train loss: 0.8603, eval loss 0.8486383557319641\n",
      "optimal threshold: -0.9644\n",
      "Epoch 104 train loss: 0.8693, eval loss 0.8454498648643494\n",
      "optimal threshold: -0.9636\n",
      "Epoch 105 train loss: 0.8644, eval loss 0.8423076272010803\n",
      "optimal threshold: -0.9624\n",
      "Epoch 106 train loss: 0.8479, eval loss 0.8392259478569031\n",
      "optimal threshold: -1.0241\n",
      "Epoch 107 train loss: 0.8833, eval loss 0.8361743092536926\n",
      "optimal threshold: -1.0238\n",
      "Epoch 108 train loss: 0.8230, eval loss 0.8331860899925232\n",
      "optimal threshold: -1.0233\n",
      "Epoch 109 train loss: 0.8608, eval loss 0.8302333354949951\n",
      "optimal threshold: -0.9623\n",
      "Epoch 110 train loss: 0.8329, eval loss 0.8273155689239502\n",
      "optimal threshold: -0.9456\n",
      "Epoch 111 train loss: 0.8316, eval loss 0.8244830369949341\n",
      "optimal threshold: -0.9371\n",
      "Epoch 112 train loss: 0.8423, eval loss 0.8216782808303833\n",
      "optimal threshold: -0.9361\n",
      "Epoch 113 train loss: 0.8327, eval loss 0.8189196586608887\n",
      "optimal threshold: -0.9346\n",
      "Epoch 114 train loss: 0.8290, eval loss 0.8162123560905457\n",
      "optimal threshold: -0.9333\n",
      "Epoch 115 train loss: 0.8241, eval loss 0.81354159116745\n",
      "optimal threshold: -0.9318\n",
      "Epoch 116 train loss: 0.8255, eval loss 0.8109018802642822\n",
      "optimal threshold: -0.9340\n",
      "Epoch 117 train loss: 0.8305, eval loss 0.8083556294441223\n",
      "optimal threshold: -0.9364\n",
      "Epoch 118 train loss: 0.8217, eval loss 0.8058869242668152\n",
      "optimal threshold: -0.9312\n",
      "Epoch 119 train loss: 0.8127, eval loss 0.803438663482666\n",
      "optimal threshold: -0.9493\n",
      "Epoch 120 train loss: 0.8772, eval loss 0.8010174036026001\n",
      "optimal threshold: -0.9370\n",
      "Epoch 121 train loss: 0.7991, eval loss 0.7986548542976379\n",
      "optimal threshold: -0.9394\n",
      "Epoch 122 train loss: 0.7936, eval loss 0.796381950378418\n",
      "optimal threshold: -0.9371\n",
      "Epoch 123 train loss: 0.8198, eval loss 0.7941291928291321\n",
      "optimal threshold: -0.9333\n",
      "Epoch 124 train loss: 0.7951, eval loss 0.7919155359268188\n",
      "optimal threshold: -0.9320\n",
      "Epoch 125 train loss: 0.7986, eval loss 0.7897571325302124\n",
      "optimal threshold: -0.9271\n",
      "Epoch 126 train loss: 0.8239, eval loss 0.7876755595207214\n",
      "optimal threshold: -0.9054\n",
      "Epoch 127 train loss: 0.8015, eval loss 0.7856007814407349\n",
      "optimal threshold: -0.9036\n",
      "Epoch 128 train loss: 0.8125, eval loss 0.7836040258407593\n",
      "optimal threshold: -0.9210\n",
      "Epoch 129 train loss: 0.8109, eval loss 0.7816459536552429\n",
      "optimal threshold: -0.9036\n",
      "Epoch 130 train loss: 0.7867, eval loss 0.7797233462333679\n",
      "optimal threshold: -0.8848\n",
      "Epoch 131 train loss: 0.7835, eval loss 0.7778726816177368\n",
      "optimal threshold: -0.8839\n",
      "Epoch 132 train loss: 0.7929, eval loss 0.7760637402534485\n",
      "optimal threshold: -0.9073\n",
      "Epoch 133 train loss: 0.7935, eval loss 0.7742797136306763\n",
      "optimal threshold: -0.9040\n",
      "Epoch 134 train loss: 0.7855, eval loss 0.772540807723999\n",
      "optimal threshold: -0.9017\n",
      "Epoch 135 train loss: 0.8081, eval loss 0.7708758115768433\n",
      "optimal threshold: -0.8700\n",
      "Epoch 136 train loss: 0.8143, eval loss 0.7692326307296753\n",
      "optimal threshold: -0.8973\n",
      "Epoch 137 train loss: 0.8130, eval loss 0.7676382660865784\n",
      "optimal threshold: -0.8948\n",
      "Epoch 138 train loss: 0.8067, eval loss 0.7660738229751587\n",
      "optimal threshold: -0.8654\n",
      "Epoch 139 train loss: 0.8002, eval loss 0.7645714282989502\n",
      "optimal threshold: -0.8904\n",
      "Epoch 140 train loss: 0.8054, eval loss 0.7630950808525085\n",
      "optimal threshold: -0.8636\n",
      "Epoch 141 train loss: 0.7850, eval loss 0.7616204619407654\n",
      "optimal threshold: -0.8605\n",
      "Epoch 142 train loss: 0.7626, eval loss 0.7601982951164246\n",
      "optimal threshold: -0.8612\n",
      "Epoch 143 train loss: 0.7886, eval loss 0.7588031888008118\n",
      "optimal threshold: -0.8585\n",
      "Epoch 144 train loss: 0.7570, eval loss 0.7574725151062012\n",
      "optimal threshold: -0.8590\n",
      "Epoch 145 train loss: 0.7692, eval loss 0.7561882138252258\n",
      "optimal threshold: -0.8066\n",
      "Epoch 146 train loss: 0.7767, eval loss 0.7549283504486084\n",
      "optimal threshold: -0.8087\n",
      "Epoch 147 train loss: 0.7877, eval loss 0.7537238001823425\n",
      "optimal threshold: -1.0455\n",
      "Epoch 148 train loss: 0.7851, eval loss 0.7525591850280762\n",
      "optimal threshold: -0.7967\n",
      "Epoch 149 train loss: 0.7877, eval loss 0.7514053583145142\n",
      "optimal threshold: -0.7948\n",
      "Epoch 150 train loss: 0.7375, eval loss 0.7503136396408081\n",
      "optimal threshold: -0.7988\n",
      "Epoch 151 train loss: 0.7606, eval loss 0.7491839528083801\n",
      "optimal threshold: -0.7962\n",
      "Epoch 152 train loss: 0.7359, eval loss 0.7481024861335754\n",
      "optimal threshold: -0.8013\n",
      "Epoch 153 train loss: 0.7428, eval loss 0.7470641732215881\n",
      "optimal threshold: -0.7994\n",
      "Epoch 154 train loss: 0.7322, eval loss 0.7460498213768005\n",
      "optimal threshold: -0.8047\n",
      "Epoch 155 train loss: 0.7728, eval loss 0.7450705766677856\n",
      "optimal threshold: -1.0422\n",
      "Epoch 156 train loss: 0.7570, eval loss 0.7441170811653137\n",
      "optimal threshold: -1.0413\n",
      "Epoch 157 train loss: 0.7729, eval loss 0.7431968450546265\n",
      "optimal threshold: -1.0375\n",
      "Epoch 158 train loss: 0.7962, eval loss 0.7423118948936462\n",
      "optimal threshold: -1.0341\n",
      "Epoch 159 train loss: 0.7727, eval loss 0.7414252161979675\n",
      "optimal threshold: -0.7943\n",
      "Epoch 160 train loss: 0.7832, eval loss 0.7405762672424316\n",
      "optimal threshold: -0.7930\n",
      "Epoch 161 train loss: 0.7289, eval loss 0.7397369742393494\n",
      "optimal threshold: -1.0316\n",
      "Epoch 162 train loss: 0.7836, eval loss 0.7389401197433472\n",
      "optimal threshold: -1.0660\n",
      "Epoch 163 train loss: 0.7653, eval loss 0.7381469011306763\n",
      "optimal threshold: -1.0149\n",
      "Epoch 164 train loss: 0.7455, eval loss 0.7373949885368347\n",
      "optimal threshold: -1.0644\n",
      "Epoch 165 train loss: 0.7515, eval loss 0.736646294593811\n",
      "optimal threshold: -1.0638\n",
      "Epoch 166 train loss: 0.7883, eval loss 0.7358977198600769\n",
      "optimal threshold: -1.0075\n",
      "Epoch 167 train loss: 0.7427, eval loss 0.7351807355880737\n",
      "optimal threshold: -1.0057\n",
      "Epoch 168 train loss: 0.7615, eval loss 0.7344666719436646\n",
      "optimal threshold: -1.0040\n",
      "Epoch 169 train loss: 0.7412, eval loss 0.7337597608566284\n",
      "optimal threshold: -1.0352\n",
      "Epoch 170 train loss: 0.7588, eval loss 0.7330995202064514\n",
      "optimal threshold: -1.0253\n",
      "Epoch 171 train loss: 0.7266, eval loss 0.7324509620666504\n",
      "optimal threshold: -1.0211\n",
      "Epoch 172 train loss: 0.7442, eval loss 0.7318158149719238\n",
      "optimal threshold: -1.0229\n",
      "Epoch 173 train loss: 0.7342, eval loss 0.731201708316803\n",
      "optimal threshold: -1.0165\n",
      "Epoch 174 train loss: 0.7860, eval loss 0.7306256294250488\n",
      "optimal threshold: -1.0151\n",
      "Epoch 175 train loss: 0.7424, eval loss 0.7300248742103577\n",
      "optimal threshold: -1.0168\n",
      "Epoch 176 train loss: 0.7446, eval loss 0.729461133480072\n",
      "optimal threshold: -1.0159\n",
      "Epoch 177 train loss: 0.7389, eval loss 0.7289074063301086\n",
      "optimal threshold: -1.0179\n",
      "Epoch 178 train loss: 0.7651, eval loss 0.7283419966697693\n",
      "optimal threshold: -1.0160\n",
      "Epoch 179 train loss: 0.7778, eval loss 0.7277987003326416\n",
      "optimal threshold: -1.0149\n",
      "Epoch 180 train loss: 0.7284, eval loss 0.7272726893424988\n",
      "optimal threshold: -1.0152\n",
      "Epoch 181 train loss: 0.7474, eval loss 0.726786196231842\n",
      "optimal threshold: -1.0154\n",
      "Epoch 182 train loss: 0.7373, eval loss 0.7262629270553589\n",
      "optimal threshold: -1.0309\n",
      "Epoch 183 train loss: 0.7670, eval loss 0.7257776260375977\n",
      "optimal threshold: -1.0318\n",
      "Epoch 184 train loss: 0.7380, eval loss 0.7252916097640991\n",
      "optimal threshold: -1.0309\n",
      "Epoch 185 train loss: 0.7333, eval loss 0.7248393893241882\n",
      "optimal threshold: -1.0304\n",
      "Epoch 186 train loss: 0.6899, eval loss 0.7243921756744385\n",
      "optimal threshold: -1.0293\n",
      "Epoch 187 train loss: 0.7195, eval loss 0.7239649295806885\n",
      "optimal threshold: -1.0282\n",
      "Epoch 188 train loss: 0.7389, eval loss 0.7235287427902222\n",
      "optimal threshold: -1.0339\n",
      "Epoch 189 train loss: 0.7051, eval loss 0.7230823636054993\n",
      "optimal threshold: -1.0315\n",
      "Epoch 190 train loss: 0.7697, eval loss 0.7226498126983643\n",
      "optimal threshold: -1.0361\n",
      "Epoch 191 train loss: 0.7242, eval loss 0.7222485542297363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -1.0533\n",
      "Epoch 192 train loss: 0.7148, eval loss 0.721847653388977\n",
      "optimal threshold: -1.0467\n",
      "Epoch 193 train loss: 0.7211, eval loss 0.7214527130126953\n",
      "optimal threshold: -1.0470\n",
      "Epoch 194 train loss: 0.7184, eval loss 0.7210504412651062\n",
      "optimal threshold: -1.0447\n",
      "Epoch 195 train loss: 0.7390, eval loss 0.7206658124923706\n",
      "optimal threshold: -1.0484\n",
      "Epoch 196 train loss: 0.7339, eval loss 0.7202762365341187\n",
      "optimal threshold: -1.0499\n",
      "Epoch 197 train loss: 0.7146, eval loss 0.719913899898529\n",
      "optimal threshold: -1.0470\n",
      "Epoch 198 train loss: 0.7232, eval loss 0.7195466160774231\n",
      "optimal threshold: -1.0464\n",
      "Epoch 199 train loss: 0.7631, eval loss 0.7191733121871948\n",
      "optimal threshold: -0.9922\n",
      "Epoch 200 train loss: 0.7225, eval loss 0.7188243865966797\n",
      "optimal threshold: -1.0458\n",
      "Epoch 201 train loss: 0.7292, eval loss 0.7184797525405884\n",
      "optimal threshold: -0.9922\n",
      "Epoch 202 train loss: 0.7269, eval loss 0.7181332111358643\n",
      "optimal threshold: -1.0068\n",
      "Epoch 203 train loss: 0.7478, eval loss 0.7177888751029968\n",
      "optimal threshold: -1.0081\n",
      "Epoch 204 train loss: 0.7314, eval loss 0.7174620628356934\n",
      "optimal threshold: -1.0077\n",
      "Epoch 205 train loss: 0.7035, eval loss 0.7171162962913513\n",
      "optimal threshold: -1.0080\n",
      "Epoch 206 train loss: 0.7225, eval loss 0.7168006896972656\n",
      "optimal threshold: -1.0091\n",
      "Epoch 207 train loss: 0.6888, eval loss 0.7164819240570068\n",
      "optimal threshold: -1.0099\n",
      "Epoch 208 train loss: 0.7407, eval loss 0.7161734700202942\n",
      "optimal threshold: -1.0099\n",
      "Epoch 209 train loss: 0.7434, eval loss 0.7158486843109131\n",
      "optimal threshold: -0.9962\n",
      "Epoch 210 train loss: 0.7451, eval loss 0.7155729532241821\n",
      "optimal threshold: -1.0119\n",
      "Epoch 211 train loss: 0.7222, eval loss 0.7152633666992188\n",
      "optimal threshold: -1.0177\n",
      "Epoch 212 train loss: 0.7305, eval loss 0.7149482369422913\n",
      "optimal threshold: -0.9925\n",
      "Epoch 213 train loss: 0.6971, eval loss 0.7146888375282288\n",
      "optimal threshold: -1.0307\n",
      "Epoch 214 train loss: 0.7150, eval loss 0.7144078016281128\n",
      "optimal threshold: -1.0282\n",
      "Epoch 215 train loss: 0.7355, eval loss 0.7141138315200806\n",
      "optimal threshold: -1.0270\n",
      "Epoch 216 train loss: 0.7221, eval loss 0.713835597038269\n",
      "optimal threshold: -1.0266\n",
      "Epoch 217 train loss: 0.7490, eval loss 0.7135502696037292\n",
      "optimal threshold: -1.0227\n",
      "Epoch 218 train loss: 0.7047, eval loss 0.7132546305656433\n",
      "optimal threshold: -1.0193\n",
      "Epoch 219 train loss: 0.7337, eval loss 0.7130001187324524\n",
      "optimal threshold: -1.0236\n",
      "Epoch 220 train loss: 0.7059, eval loss 0.7127599716186523\n",
      "optimal threshold: -1.0273\n",
      "Epoch 221 train loss: 0.7036, eval loss 0.7125083804130554\n",
      "optimal threshold: -1.0287\n",
      "Epoch 222 train loss: 0.7255, eval loss 0.7122136950492859\n",
      "optimal threshold: -1.0221\n",
      "Epoch 223 train loss: 0.7135, eval loss 0.7119641900062561\n",
      "optimal threshold: -1.0228\n",
      "Epoch 224 train loss: 0.7251, eval loss 0.7116904258728027\n",
      "optimal threshold: -1.0230\n",
      "Epoch 225 train loss: 0.7379, eval loss 0.7114466428756714\n",
      "optimal threshold: -1.0233\n",
      "Epoch 226 train loss: 0.7005, eval loss 0.711185872554779\n",
      "optimal threshold: -1.0222\n",
      "Epoch 227 train loss: 0.7256, eval loss 0.710900068283081\n",
      "optimal threshold: -1.0516\n",
      "Epoch 228 train loss: 0.7304, eval loss 0.7106329798698425\n",
      "optimal threshold: -1.0492\n",
      "Epoch 229 train loss: 0.7112, eval loss 0.7103835940361023\n",
      "optimal threshold: -1.0491\n",
      "Epoch 230 train loss: 0.6945, eval loss 0.7101446986198425\n",
      "optimal threshold: -1.0623\n",
      "Epoch 231 train loss: 0.7086, eval loss 0.7099026441574097\n",
      "optimal threshold: -1.0295\n",
      "Epoch 232 train loss: 0.7477, eval loss 0.7096555233001709\n",
      "optimal threshold: -1.0266\n",
      "Epoch 233 train loss: 0.7451, eval loss 0.709430456161499\n",
      "optimal threshold: -1.0268\n",
      "Epoch 234 train loss: 0.7225, eval loss 0.7091912031173706\n",
      "optimal threshold: -1.0307\n",
      "Epoch 235 train loss: 0.6801, eval loss 0.708965003490448\n",
      "optimal threshold: -1.0362\n",
      "Epoch 236 train loss: 0.7341, eval loss 0.7087239027023315\n",
      "optimal threshold: -1.0347\n",
      "Epoch 237 train loss: 0.7242, eval loss 0.7084736824035645\n",
      "optimal threshold: -1.0408\n",
      "Epoch 238 train loss: 0.7039, eval loss 0.7082635164260864\n",
      "optimal threshold: -1.0322\n",
      "Epoch 239 train loss: 0.7661, eval loss 0.7080320715904236\n",
      "optimal threshold: -1.0327\n",
      "Epoch 240 train loss: 0.7241, eval loss 0.7078262567520142\n",
      "optimal threshold: -0.9948\n",
      "Epoch 241 train loss: 0.6941, eval loss 0.7075678706169128\n",
      "optimal threshold: -0.9971\n",
      "Epoch 242 train loss: 0.7166, eval loss 0.7073361873626709\n",
      "optimal threshold: -0.9968\n",
      "Epoch 243 train loss: 0.7112, eval loss 0.7071327567100525\n",
      "optimal threshold: -0.9965\n",
      "Epoch 244 train loss: 0.6758, eval loss 0.7069042921066284\n",
      "optimal threshold: -0.9973\n",
      "Epoch 245 train loss: 0.6914, eval loss 0.7066744565963745\n",
      "optimal threshold: -0.9995\n",
      "Epoch 246 train loss: 0.7035, eval loss 0.7064643502235413\n",
      "optimal threshold: -1.0124\n",
      "Epoch 247 train loss: 0.6928, eval loss 0.706251323223114\n",
      "optimal threshold: -1.0141\n",
      "Epoch 248 train loss: 0.6996, eval loss 0.7060341835021973\n",
      "optimal threshold: -1.0081\n",
      "Epoch 249 train loss: 0.7108, eval loss 0.7058308124542236\n",
      "optimal threshold: -1.0066\n",
      "Epoch 250 train loss: 0.7010, eval loss 0.7056193351745605\n",
      "optimal threshold: -1.0154\n",
      "Epoch 251 train loss: 0.7298, eval loss 0.705396294593811\n",
      "optimal threshold: -1.0153\n",
      "Epoch 252 train loss: 0.7534, eval loss 0.7051927447319031\n",
      "optimal threshold: -1.0413\n",
      "Epoch 253 train loss: 0.6941, eval loss 0.704990565776825\n",
      "optimal threshold: -1.0419\n",
      "Epoch 254 train loss: 0.6906, eval loss 0.70479816198349\n",
      "optimal threshold: -1.0430\n",
      "Epoch 255 train loss: 0.7385, eval loss 0.7046013474464417\n",
      "optimal threshold: -1.0486\n",
      "Epoch 256 train loss: 0.7267, eval loss 0.7044259309768677\n",
      "optimal threshold: -1.0480\n",
      "Epoch 257 train loss: 0.7727, eval loss 0.7042379975318909\n",
      "optimal threshold: -1.0459\n",
      "Epoch 258 train loss: 0.7074, eval loss 0.7040031552314758\n",
      "optimal threshold: -1.0471\n",
      "Epoch 259 train loss: 0.6900, eval loss 0.7038045525550842\n",
      "optimal threshold: -1.0469\n",
      "Epoch 260 train loss: 0.6909, eval loss 0.7035916447639465\n",
      "optimal threshold: -1.0020\n",
      "Epoch 261 train loss: 0.6772, eval loss 0.7034096717834473\n",
      "optimal threshold: -1.0013\n",
      "Epoch 262 train loss: 0.6982, eval loss 0.7032055258750916\n",
      "optimal threshold: -1.0024\n",
      "Epoch 263 train loss: 0.7084, eval loss 0.7030285596847534\n",
      "optimal threshold: -0.9996\n",
      "Epoch 264 train loss: 0.6788, eval loss 0.702826201915741\n",
      "optimal threshold: -0.9998\n",
      "Epoch 265 train loss: 0.6519, eval loss 0.7026540637016296\n",
      "optimal threshold: -0.9980\n",
      "Epoch 266 train loss: 0.7101, eval loss 0.7024415731430054\n",
      "optimal threshold: -0.9975\n",
      "Epoch 267 train loss: 0.7175, eval loss 0.7022525072097778\n",
      "optimal threshold: -0.9948\n",
      "Epoch 268 train loss: 0.7160, eval loss 0.7020273804664612\n",
      "optimal threshold: -0.9942\n",
      "Epoch 269 train loss: 0.7349, eval loss 0.7018424272537231\n",
      "optimal threshold: -0.9929\n",
      "Epoch 270 train loss: 0.6988, eval loss 0.7016509175300598\n",
      "optimal threshold: -0.9939\n",
      "Epoch 271 train loss: 0.7063, eval loss 0.7014886736869812\n",
      "optimal threshold: -0.9843\n",
      "Epoch 272 train loss: 0.6736, eval loss 0.7013185620307922\n",
      "optimal threshold: -0.9927\n",
      "Epoch 273 train loss: 0.7132, eval loss 0.7011294960975647\n",
      "optimal threshold: -0.9918\n",
      "Epoch 274 train loss: 0.7346, eval loss 0.7009454369544983\n",
      "optimal threshold: -0.9911\n",
      "Epoch 275 train loss: 0.7019, eval loss 0.7007555961608887\n",
      "optimal threshold: -0.9794\n",
      "Epoch 276 train loss: 0.7261, eval loss 0.7005665302276611\n",
      "optimal threshold: -0.7915\n",
      "Epoch 277 train loss: 0.6725, eval loss 0.7004076838493347\n",
      "optimal threshold: -0.7916\n",
      "Epoch 278 train loss: 0.7113, eval loss 0.7002306580543518\n",
      "optimal threshold: -0.7918\n",
      "Epoch 279 train loss: 0.6542, eval loss 0.700065016746521\n",
      "optimal threshold: -0.7930\n",
      "Epoch 280 train loss: 0.6745, eval loss 0.6999120712280273\n",
      "optimal threshold: -0.7930\n",
      "Epoch 281 train loss: 0.7230, eval loss 0.699740469455719\n",
      "optimal threshold: -0.7878\n",
      "Epoch 282 train loss: 0.7140, eval loss 0.6995574831962585\n",
      "optimal threshold: -0.9922\n",
      "Epoch 283 train loss: 0.6599, eval loss 0.6993924379348755\n",
      "optimal threshold: -0.7894\n",
      "Epoch 284 train loss: 0.7274, eval loss 0.6992350220680237\n",
      "optimal threshold: -0.7906\n",
      "Epoch 285 train loss: 0.7032, eval loss 0.6990693807601929\n",
      "optimal threshold: -0.7947\n",
      "Epoch 286 train loss: 0.7259, eval loss 0.6988984942436218\n",
      "optimal threshold: -0.9897\n",
      "Epoch 287 train loss: 0.6948, eval loss 0.6987292766571045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7382\n",
      "Epoch 288 train loss: 0.7004, eval loss 0.6985482573509216\n",
      "optimal threshold: -0.7390\n",
      "Epoch 289 train loss: 0.6906, eval loss 0.6983641982078552\n",
      "optimal threshold: -0.9958\n",
      "Epoch 290 train loss: 0.7055, eval loss 0.698182225227356\n",
      "optimal threshold: -0.7413\n",
      "Epoch 291 train loss: 0.7201, eval loss 0.6980187296867371\n",
      "optimal threshold: -0.7418\n",
      "Epoch 292 train loss: 0.7178, eval loss 0.6978437304496765\n",
      "optimal threshold: -0.7309\n",
      "Epoch 293 train loss: 0.6946, eval loss 0.6976747512817383\n",
      "optimal threshold: -0.7309\n",
      "Epoch 294 train loss: 0.7208, eval loss 0.6975192427635193\n",
      "optimal threshold: -0.7304\n",
      "Epoch 295 train loss: 0.6911, eval loss 0.6973548531532288\n",
      "optimal threshold: -0.7341\n",
      "Epoch 296 train loss: 0.6945, eval loss 0.6972036361694336\n",
      "optimal threshold: -0.7979\n",
      "Epoch 297 train loss: 0.7217, eval loss 0.6970441937446594\n",
      "optimal threshold: -0.7974\n",
      "Epoch 298 train loss: 0.6790, eval loss 0.6968857049942017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:07:59,620] Trial 2 finished with value: 0.6857167482376099 and parameters: {'learning_rate_exp': -5.742628498423528, 'dropout_p': 0.5981192314563737, 'l2_reg_exp': -3.421720638221457, 'batch_size': 295, 'N': 447}. Best is trial 0 with value: 0.5673678517341614.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7987\n",
      "Epoch 299 train loss: 0.6857, eval loss 0.6967364549636841\n",
      "optimal threshold: -0.3401\n",
      "Epoch 0 train loss: 0.7917, eval loss 0.6650853753089905\n",
      "optimal threshold: -0.4386\n",
      "Epoch 1 train loss: 0.7003, eval loss 0.661765992641449\n",
      "optimal threshold: -0.3705\n",
      "Epoch 2 train loss: 0.7045, eval loss 0.6600625514984131\n",
      "optimal threshold: -0.4043\n",
      "Epoch 3 train loss: 0.6564, eval loss 0.6605260968208313\n",
      "optimal threshold: -0.4865\n",
      "Epoch 4 train loss: 0.6938, eval loss 0.6617300510406494\n",
      "optimal threshold: -0.5799\n",
      "Epoch 5 train loss: 0.6863, eval loss 0.6682774424552917\n",
      "optimal threshold: -0.5928\n",
      "Epoch 6 train loss: 0.6403, eval loss 0.6779700517654419\n",
      "optimal threshold: -0.8068\n",
      "Epoch 7 train loss: 0.6450, eval loss 0.6897907853126526\n",
      "optimal threshold: -0.5380\n",
      "Epoch 8 train loss: 0.6425, eval loss 0.7058224678039551\n",
      "optimal threshold: -0.4014\n",
      "Epoch 9 train loss: 0.5987, eval loss 0.7256419658660889\n",
      "optimal threshold: -0.4220\n",
      "Epoch 10 train loss: 0.5894, eval loss 0.7396281957626343\n",
      "optimal threshold: -0.5636\n",
      "Epoch 11 train loss: 0.5695, eval loss 0.758571982383728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:08:19,011] Trial 3 finished with value: 0.5553447008132935 and parameters: {'learning_rate_exp': -2.714887190148618, 'dropout_p': 0.05377366661422562, 'l2_reg_exp': -6.828686017723423, 'batch_size': 61, 'N': 247}. Best is trial 3 with value: 0.5553447008132935.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.3266\n",
      "optimal threshold: -0.1106\n",
      "Epoch 0 train loss: 1.3766, eval loss 1.3835225105285645\n",
      "optimal threshold: -0.1523\n",
      "Epoch 1 train loss: 1.3539, eval loss 1.3577845096588135\n",
      "optimal threshold: -0.1976\n",
      "Epoch 2 train loss: 1.3443, eval loss 1.3291640281677246\n",
      "optimal threshold: -0.2530\n",
      "Epoch 3 train loss: 1.3013, eval loss 1.296095371246338\n",
      "optimal threshold: -0.3290\n",
      "Epoch 4 train loss: 1.2602, eval loss 1.2588050365447998\n",
      "optimal threshold: -0.4075\n",
      "Epoch 5 train loss: 1.2263, eval loss 1.2173118591308594\n",
      "optimal threshold: -0.5040\n",
      "Epoch 6 train loss: 1.1884, eval loss 1.1738195419311523\n",
      "optimal threshold: -0.5820\n",
      "Epoch 7 train loss: 1.1462, eval loss 1.130179524421692\n",
      "optimal threshold: -0.6749\n",
      "Epoch 8 train loss: 1.0926, eval loss 1.0875061750411987\n",
      "optimal threshold: -0.7897\n",
      "Epoch 9 train loss: 1.0758, eval loss 1.0473381280899048\n",
      "optimal threshold: -0.7877\n",
      "Epoch 10 train loss: 1.0230, eval loss 1.0105642080307007\n",
      "optimal threshold: -0.8338\n",
      "Epoch 11 train loss: 1.0095, eval loss 0.9768365621566772\n",
      "optimal threshold: -0.9350\n",
      "Epoch 12 train loss: 0.9662, eval loss 0.9458803534507751\n",
      "optimal threshold: -0.9718\n",
      "Epoch 13 train loss: 0.9286, eval loss 0.917474091053009\n",
      "optimal threshold: -1.0040\n",
      "Epoch 14 train loss: 0.9325, eval loss 0.890941858291626\n",
      "optimal threshold: -1.0545\n",
      "Epoch 15 train loss: 0.9046, eval loss 0.8666386008262634\n",
      "optimal threshold: -1.0546\n",
      "Epoch 16 train loss: 0.8493, eval loss 0.8440951704978943\n",
      "optimal threshold: -1.0680\n",
      "Epoch 17 train loss: 0.8626, eval loss 0.8241553902626038\n",
      "optimal threshold: -1.0752\n",
      "Epoch 18 train loss: 0.8251, eval loss 0.8061703443527222\n",
      "optimal threshold: -1.0788\n",
      "Epoch 19 train loss: 0.8195, eval loss 0.7907460927963257\n",
      "optimal threshold: -1.0917\n",
      "Epoch 20 train loss: 0.7748, eval loss 0.7774157524108887\n",
      "optimal threshold: -0.9151\n",
      "Epoch 21 train loss: 0.8049, eval loss 0.7660736441612244\n",
      "optimal threshold: -0.9408\n",
      "Epoch 22 train loss: 0.7815, eval loss 0.7564969062805176\n",
      "optimal threshold: -0.9560\n",
      "Epoch 23 train loss: 0.7853, eval loss 0.7484922409057617\n",
      "optimal threshold: -0.9232\n",
      "Epoch 24 train loss: 0.7786, eval loss 0.741659939289093\n",
      "optimal threshold: -0.9336\n",
      "Epoch 25 train loss: 0.7598, eval loss 0.7360538244247437\n",
      "optimal threshold: -0.9145\n",
      "Epoch 26 train loss: 0.7497, eval loss 0.731200098991394\n",
      "optimal threshold: -0.7591\n",
      "Epoch 27 train loss: 0.7634, eval loss 0.7270312905311584\n",
      "optimal threshold: -0.7611\n",
      "Epoch 28 train loss: 0.7426, eval loss 0.7236742973327637\n",
      "optimal threshold: -0.7734\n",
      "Epoch 29 train loss: 0.7431, eval loss 0.720682680606842\n",
      "optimal threshold: -0.7470\n",
      "Epoch 30 train loss: 0.7392, eval loss 0.7181577086448669\n",
      "optimal threshold: -0.7331\n",
      "Epoch 31 train loss: 0.7314, eval loss 0.7158910632133484\n",
      "optimal threshold: -0.7327\n",
      "Epoch 32 train loss: 0.7120, eval loss 0.7137923240661621\n",
      "optimal threshold: -0.7015\n",
      "Epoch 33 train loss: 0.6925, eval loss 0.7117779850959778\n",
      "optimal threshold: -0.7050\n",
      "Epoch 34 train loss: 0.7028, eval loss 0.7100509405136108\n",
      "optimal threshold: -0.6991\n",
      "Epoch 35 train loss: 0.7555, eval loss 0.7084912657737732\n",
      "optimal threshold: -0.7181\n",
      "Epoch 36 train loss: 0.7241, eval loss 0.7069038152694702\n",
      "optimal threshold: -0.6667\n",
      "Epoch 37 train loss: 0.6910, eval loss 0.705375075340271\n",
      "optimal threshold: -0.6705\n",
      "Epoch 38 train loss: 0.7277, eval loss 0.7040238976478577\n",
      "optimal threshold: -0.7030\n",
      "Epoch 39 train loss: 0.7126, eval loss 0.7028071284294128\n",
      "optimal threshold: -0.7099\n",
      "Epoch 40 train loss: 0.7103, eval loss 0.7016344666481018\n",
      "optimal threshold: -0.6442\n",
      "Epoch 41 train loss: 0.7217, eval loss 0.7002666592597961\n",
      "optimal threshold: -0.6803\n",
      "Epoch 42 train loss: 0.7144, eval loss 0.6992150545120239\n",
      "optimal threshold: -0.6489\n",
      "Epoch 43 train loss: 0.6986, eval loss 0.6981008052825928\n",
      "optimal threshold: -0.6552\n",
      "Epoch 44 train loss: 0.7078, eval loss 0.697083592414856\n",
      "optimal threshold: -0.7494\n",
      "Epoch 45 train loss: 0.7074, eval loss 0.6960893869400024\n",
      "optimal threshold: -0.6685\n",
      "Epoch 46 train loss: 0.7190, eval loss 0.6951000094413757\n",
      "optimal threshold: -0.6652\n",
      "Epoch 47 train loss: 0.7252, eval loss 0.694115161895752\n",
      "optimal threshold: -0.6737\n",
      "Epoch 48 train loss: 0.6964, eval loss 0.6933193206787109\n",
      "optimal threshold: -0.6754\n",
      "Epoch 49 train loss: 0.7069, eval loss 0.6924523711204529\n",
      "optimal threshold: -0.9597\n",
      "Epoch 50 train loss: 0.7305, eval loss 0.6916547417640686\n",
      "optimal threshold: -0.7611\n",
      "Epoch 51 train loss: 0.6748, eval loss 0.6907892227172852\n",
      "optimal threshold: -0.8754\n",
      "Epoch 52 train loss: 0.6848, eval loss 0.6899882555007935\n",
      "optimal threshold: -0.9046\n",
      "Epoch 53 train loss: 0.6960, eval loss 0.6893205642700195\n",
      "optimal threshold: -0.8941\n",
      "Epoch 54 train loss: 0.7120, eval loss 0.6885794401168823\n",
      "optimal threshold: -0.8953\n",
      "Epoch 55 train loss: 0.6762, eval loss 0.6878738403320312\n",
      "optimal threshold: -0.8906\n",
      "Epoch 56 train loss: 0.6912, eval loss 0.6871048808097839\n",
      "optimal threshold: -0.8765\n",
      "Epoch 57 train loss: 0.6775, eval loss 0.6863370537757874\n",
      "optimal threshold: -0.8457\n",
      "Epoch 58 train loss: 0.6763, eval loss 0.6858810186386108\n",
      "optimal threshold: -0.9063\n",
      "Epoch 59 train loss: 0.7084, eval loss 0.6851053833961487\n",
      "optimal threshold: -0.8927\n",
      "Epoch 60 train loss: 0.7168, eval loss 0.6845383644104004\n",
      "optimal threshold: -0.9023\n",
      "Epoch 61 train loss: 0.7159, eval loss 0.6840869784355164\n",
      "optimal threshold: -0.6055\n",
      "Epoch 62 train loss: 0.6724, eval loss 0.683497965335846\n",
      "optimal threshold: -0.6092\n",
      "Epoch 63 train loss: 0.6789, eval loss 0.6829381585121155\n",
      "optimal threshold: -0.6030\n",
      "Epoch 64 train loss: 0.7271, eval loss 0.6822828054428101\n",
      "optimal threshold: -0.8057\n",
      "Epoch 65 train loss: 0.6910, eval loss 0.6819368004798889\n",
      "optimal threshold: -0.7885\n",
      "Epoch 66 train loss: 0.6505, eval loss 0.6813403367996216\n",
      "optimal threshold: -0.6715\n",
      "Epoch 67 train loss: 0.6443, eval loss 0.6808663010597229\n",
      "optimal threshold: -0.5724\n",
      "Epoch 68 train loss: 0.6881, eval loss 0.6803193092346191\n",
      "optimal threshold: -0.5409\n",
      "Epoch 69 train loss: 0.6801, eval loss 0.6798692941665649\n",
      "optimal threshold: -0.5478\n",
      "Epoch 70 train loss: 0.7043, eval loss 0.6794970035552979\n",
      "optimal threshold: -0.5558\n",
      "Epoch 71 train loss: 0.7016, eval loss 0.6790696978569031\n",
      "optimal threshold: -0.5617\n",
      "Epoch 72 train loss: 0.6638, eval loss 0.6786279678344727\n",
      "optimal threshold: -0.4426\n",
      "Epoch 73 train loss: 0.6529, eval loss 0.678027331829071\n",
      "optimal threshold: -0.4926\n",
      "Epoch 74 train loss: 0.7085, eval loss 0.6777816414833069\n",
      "optimal threshold: -0.5162\n",
      "Epoch 75 train loss: 0.6808, eval loss 0.6772071123123169\n",
      "optimal threshold: -0.5165\n",
      "Epoch 76 train loss: 0.6729, eval loss 0.676819920539856\n",
      "optimal threshold: -0.4962\n",
      "Epoch 77 train loss: 0.6567, eval loss 0.676467776298523\n",
      "optimal threshold: -0.4940\n",
      "Epoch 78 train loss: 0.6580, eval loss 0.6761296391487122\n",
      "optimal threshold: -0.4945\n",
      "Epoch 79 train loss: 0.6888, eval loss 0.6756579279899597\n",
      "optimal threshold: -0.5066\n",
      "Epoch 80 train loss: 0.6486, eval loss 0.6754236817359924\n",
      "optimal threshold: -0.5825\n",
      "Epoch 81 train loss: 0.6909, eval loss 0.6750136613845825\n",
      "optimal threshold: -0.5532\n",
      "Epoch 82 train loss: 0.6402, eval loss 0.6748381853103638\n",
      "optimal threshold: -0.5453\n",
      "Epoch 83 train loss: 0.6755, eval loss 0.6744226813316345\n",
      "optimal threshold: -0.5466\n",
      "Epoch 84 train loss: 0.6693, eval loss 0.6740537285804749\n",
      "optimal threshold: -0.5513\n",
      "Epoch 85 train loss: 0.6881, eval loss 0.673852264881134\n",
      "optimal threshold: -0.5436\n",
      "Epoch 86 train loss: 0.6400, eval loss 0.6736436486244202\n",
      "optimal threshold: -0.5416\n",
      "Epoch 87 train loss: 0.6610, eval loss 0.6733875274658203\n",
      "optimal threshold: -0.5373\n",
      "Epoch 88 train loss: 0.6419, eval loss 0.6728758811950684\n",
      "optimal threshold: -0.5557\n",
      "Epoch 89 train loss: 0.6519, eval loss 0.6726712584495544\n",
      "optimal threshold: -0.5492\n",
      "Epoch 90 train loss: 0.6600, eval loss 0.6723300218582153\n",
      "optimal threshold: -0.5550\n",
      "Epoch 91 train loss: 0.6290, eval loss 0.6721433401107788\n",
      "optimal threshold: -0.5512\n",
      "Epoch 92 train loss: 0.6751, eval loss 0.6719480752944946\n",
      "optimal threshold: -0.5444\n",
      "Epoch 93 train loss: 0.6875, eval loss 0.6716261506080627\n",
      "optimal threshold: -0.5510\n",
      "Epoch 94 train loss: 0.6561, eval loss 0.6714207530021667\n",
      "optimal threshold: -0.5547\n",
      "Epoch 95 train loss: 0.6451, eval loss 0.6711429953575134\n",
      "optimal threshold: -0.5463\n",
      "Epoch 96 train loss: 0.6365, eval loss 0.6708670854568481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5460\n",
      "Epoch 97 train loss: 0.7168, eval loss 0.6706991195678711\n",
      "optimal threshold: -0.5512\n",
      "Epoch 98 train loss: 0.6903, eval loss 0.670686662197113\n",
      "optimal threshold: -0.5472\n",
      "Epoch 99 train loss: 0.6794, eval loss 0.67034912109375\n",
      "optimal threshold: -0.5386\n",
      "Epoch 100 train loss: 0.6794, eval loss 0.6700243353843689\n",
      "optimal threshold: -0.5344\n",
      "Epoch 101 train loss: 0.6749, eval loss 0.6696656942367554\n",
      "optimal threshold: -0.5222\n",
      "Epoch 102 train loss: 0.7041, eval loss 0.6694240570068359\n",
      "optimal threshold: -0.5247\n",
      "Epoch 103 train loss: 0.6608, eval loss 0.6694625616073608\n",
      "optimal threshold: -0.5337\n",
      "Epoch 104 train loss: 0.6258, eval loss 0.6694630980491638\n",
      "optimal threshold: -0.5264\n",
      "Epoch 105 train loss: 0.7085, eval loss 0.6691509485244751\n",
      "optimal threshold: -0.5104\n",
      "Epoch 106 train loss: 0.6576, eval loss 0.6687952280044556\n",
      "optimal threshold: -0.4271\n",
      "Epoch 107 train loss: 0.6360, eval loss 0.6686359643936157\n",
      "optimal threshold: -0.4277\n",
      "Epoch 108 train loss: 0.6655, eval loss 0.6683993935585022\n",
      "optimal threshold: -0.4245\n",
      "Epoch 109 train loss: 0.6624, eval loss 0.6681891083717346\n",
      "optimal threshold: -0.4215\n",
      "Epoch 110 train loss: 0.6494, eval loss 0.6681200861930847\n",
      "optimal threshold: -0.4189\n",
      "Epoch 111 train loss: 0.6784, eval loss 0.6679812073707581\n",
      "optimal threshold: -0.4154\n",
      "Epoch 112 train loss: 0.6754, eval loss 0.6677150726318359\n",
      "optimal threshold: -0.4186\n",
      "Epoch 113 train loss: 0.6512, eval loss 0.6675942540168762\n",
      "optimal threshold: -0.4193\n",
      "Epoch 114 train loss: 0.6481, eval loss 0.6674214005470276\n",
      "optimal threshold: -0.4193\n",
      "Epoch 115 train loss: 0.6218, eval loss 0.667298436164856\n",
      "optimal threshold: -0.4149\n",
      "Epoch 116 train loss: 0.6582, eval loss 0.6670887470245361\n",
      "optimal threshold: -0.4155\n",
      "Epoch 117 train loss: 0.6448, eval loss 0.6669802665710449\n",
      "optimal threshold: -0.4113\n",
      "Epoch 118 train loss: 0.6470, eval loss 0.6668105721473694\n",
      "optimal threshold: -0.4153\n",
      "Epoch 119 train loss: 0.6750, eval loss 0.6667054891586304\n",
      "optimal threshold: -0.4298\n",
      "Epoch 120 train loss: 0.6571, eval loss 0.6666625142097473\n",
      "optimal threshold: -0.4243\n",
      "Epoch 121 train loss: 0.6616, eval loss 0.6664614677429199\n",
      "optimal threshold: -0.4336\n",
      "Epoch 122 train loss: 0.6163, eval loss 0.6662920713424683\n",
      "optimal threshold: -0.4288\n",
      "Epoch 123 train loss: 0.6516, eval loss 0.6661608219146729\n",
      "optimal threshold: -0.4213\n",
      "Epoch 124 train loss: 0.6357, eval loss 0.6659591197967529\n",
      "optimal threshold: -0.4174\n",
      "Epoch 125 train loss: 0.6277, eval loss 0.6657810211181641\n",
      "optimal threshold: -0.4155\n",
      "Epoch 126 train loss: 0.6772, eval loss 0.6656056642532349\n",
      "optimal threshold: -0.4191\n",
      "Epoch 127 train loss: 0.6543, eval loss 0.6655656695365906\n",
      "optimal threshold: -0.4230\n",
      "Epoch 128 train loss: 0.6439, eval loss 0.6654126048088074\n",
      "optimal threshold: -0.4295\n",
      "Epoch 129 train loss: 0.6500, eval loss 0.6652688384056091\n",
      "optimal threshold: -0.4350\n",
      "Epoch 130 train loss: 0.6415, eval loss 0.6651156544685364\n",
      "optimal threshold: -0.4368\n",
      "Epoch 131 train loss: 0.6501, eval loss 0.6650282740592957\n",
      "optimal threshold: -0.4282\n",
      "Epoch 132 train loss: 0.6382, eval loss 0.6648578643798828\n",
      "optimal threshold: -0.4528\n",
      "Epoch 133 train loss: 0.6350, eval loss 0.6647747755050659\n",
      "optimal threshold: -0.4497\n",
      "Epoch 134 train loss: 0.6548, eval loss 0.664654016494751\n",
      "optimal threshold: -0.4505\n",
      "Epoch 135 train loss: 0.6238, eval loss 0.6646326780319214\n",
      "optimal threshold: -0.4562\n",
      "Epoch 136 train loss: 0.6571, eval loss 0.6645936965942383\n",
      "optimal threshold: -0.4466\n",
      "Epoch 137 train loss: 0.6062, eval loss 0.6643692851066589\n",
      "optimal threshold: -0.4382\n",
      "Epoch 138 train loss: 0.6251, eval loss 0.6641892790794373\n",
      "optimal threshold: -0.4495\n",
      "Epoch 139 train loss: 0.6124, eval loss 0.6642848253250122\n",
      "optimal threshold: -0.4525\n",
      "Epoch 140 train loss: 0.6353, eval loss 0.6641207337379456\n",
      "optimal threshold: -0.4269\n",
      "Epoch 141 train loss: 0.6242, eval loss 0.6640212535858154\n",
      "optimal threshold: -0.4203\n",
      "Epoch 142 train loss: 0.6219, eval loss 0.6639186143875122\n",
      "optimal threshold: -0.8522\n",
      "Epoch 143 train loss: 0.6542, eval loss 0.6638364195823669\n",
      "optimal threshold: -0.8453\n",
      "Epoch 144 train loss: 0.6365, eval loss 0.6636242270469666\n",
      "optimal threshold: -0.4382\n",
      "Epoch 145 train loss: 0.6389, eval loss 0.6635373830795288\n",
      "optimal threshold: -0.4366\n",
      "Epoch 146 train loss: 0.6743, eval loss 0.6635103225708008\n",
      "optimal threshold: -0.8374\n",
      "Epoch 147 train loss: 0.6604, eval loss 0.6634345650672913\n",
      "optimal threshold: -0.4325\n",
      "Epoch 148 train loss: 0.6199, eval loss 0.6633775234222412\n",
      "optimal threshold: -0.8346\n",
      "Epoch 149 train loss: 0.6438, eval loss 0.663308322429657\n",
      "optimal threshold: -0.8365\n",
      "Epoch 150 train loss: 0.6319, eval loss 0.6632722020149231\n",
      "optimal threshold: -0.4340\n",
      "Epoch 151 train loss: 0.6196, eval loss 0.6633003950119019\n",
      "optimal threshold: -0.4342\n",
      "Epoch 152 train loss: 0.6588, eval loss 0.6632499694824219\n",
      "optimal threshold: -0.4957\n",
      "Epoch 153 train loss: 0.6448, eval loss 0.6631011962890625\n",
      "optimal threshold: -0.4564\n",
      "Epoch 154 train loss: 0.6447, eval loss 0.6630281805992126\n",
      "optimal threshold: -0.4451\n",
      "Epoch 155 train loss: 0.6225, eval loss 0.6627894639968872\n",
      "optimal threshold: -0.4402\n",
      "Epoch 156 train loss: 0.6627, eval loss 0.6628005504608154\n",
      "optimal threshold: -0.4430\n",
      "Epoch 157 train loss: 0.6469, eval loss 0.6626135110855103\n",
      "optimal threshold: -0.4440\n",
      "Epoch 158 train loss: 0.6433, eval loss 0.6626007556915283\n",
      "optimal threshold: -0.4431\n",
      "Epoch 159 train loss: 0.6298, eval loss 0.6625248193740845\n",
      "optimal threshold: -0.5085\n",
      "Epoch 160 train loss: 0.6167, eval loss 0.6624677777290344\n",
      "optimal threshold: -0.5055\n",
      "Epoch 161 train loss: 0.6580, eval loss 0.6624080538749695\n",
      "optimal threshold: -0.4776\n",
      "Epoch 162 train loss: 0.6471, eval loss 0.6623483896255493\n",
      "optimal threshold: -0.4759\n",
      "Epoch 163 train loss: 0.6146, eval loss 0.66231769323349\n",
      "optimal threshold: -0.4916\n",
      "Epoch 164 train loss: 0.6294, eval loss 0.6622651219367981\n",
      "optimal threshold: -0.4903\n",
      "Epoch 165 train loss: 0.6258, eval loss 0.6621935963630676\n",
      "optimal threshold: -0.4950\n",
      "Epoch 166 train loss: 0.6356, eval loss 0.6622069478034973\n",
      "optimal threshold: -0.4941\n",
      "Epoch 167 train loss: 0.6117, eval loss 0.6621378064155579\n",
      "optimal threshold: -0.4982\n",
      "Epoch 168 train loss: 0.6421, eval loss 0.6621635556221008\n",
      "optimal threshold: -0.4988\n",
      "Epoch 169 train loss: 0.7108, eval loss 0.6620222926139832\n",
      "optimal threshold: -0.4941\n",
      "Epoch 170 train loss: 0.6376, eval loss 0.6619551777839661\n",
      "optimal threshold: -0.4922\n",
      "Epoch 171 train loss: 0.6195, eval loss 0.6618675589561462\n",
      "optimal threshold: -0.4949\n",
      "Epoch 172 train loss: 0.6421, eval loss 0.6618969440460205\n",
      "optimal threshold: -0.4958\n",
      "Epoch 173 train loss: 0.5993, eval loss 0.6619316935539246\n",
      "optimal threshold: -0.4929\n",
      "Epoch 174 train loss: 0.6203, eval loss 0.6616936922073364\n",
      "optimal threshold: -0.4933\n",
      "Epoch 175 train loss: 0.6788, eval loss 0.6615417003631592\n",
      "optimal threshold: -0.4901\n",
      "Epoch 176 train loss: 0.6440, eval loss 0.6615872383117676\n",
      "optimal threshold: -0.4947\n",
      "Epoch 177 train loss: 0.6576, eval loss 0.6615328192710876\n",
      "optimal threshold: -0.4841\n",
      "Epoch 178 train loss: 0.6638, eval loss 0.6614682078361511\n",
      "optimal threshold: -0.4879\n",
      "Epoch 179 train loss: 0.6527, eval loss 0.6614612340927124\n",
      "optimal threshold: -0.4866\n",
      "Epoch 180 train loss: 0.6436, eval loss 0.6612847447395325\n",
      "optimal threshold: -0.4944\n",
      "Epoch 181 train loss: 0.6653, eval loss 0.6613306999206543\n",
      "optimal threshold: -0.5039\n",
      "Epoch 182 train loss: 0.6071, eval loss 0.6613326072692871\n",
      "optimal threshold: -0.4875\n",
      "Epoch 183 train loss: 0.6517, eval loss 0.661195695400238\n",
      "optimal threshold: -0.4934\n",
      "Epoch 184 train loss: 0.6302, eval loss 0.661322832107544\n",
      "optimal threshold: -0.5045\n",
      "Epoch 185 train loss: 0.6280, eval loss 0.6613010168075562\n",
      "optimal threshold: -0.5013\n",
      "Epoch 186 train loss: 0.6546, eval loss 0.6612803936004639\n",
      "optimal threshold: -0.5015\n",
      "Epoch 187 train loss: 0.6420, eval loss 0.661199152469635\n",
      "optimal threshold: -0.5274\n",
      "Epoch 188 train loss: 0.6281, eval loss 0.6610966324806213\n",
      "optimal threshold: -0.4910\n",
      "Epoch 189 train loss: 0.6703, eval loss 0.6609619855880737\n",
      "optimal threshold: -0.4997\n",
      "Epoch 190 train loss: 0.6361, eval loss 0.6609352231025696\n",
      "optimal threshold: -0.4922\n",
      "Epoch 191 train loss: 0.6410, eval loss 0.6608671545982361\n",
      "optimal threshold: -0.5211\n",
      "Epoch 192 train loss: 0.6318, eval loss 0.6609395742416382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4906\n",
      "Epoch 193 train loss: 0.6446, eval loss 0.6608377695083618\n",
      "optimal threshold: -0.4808\n",
      "Epoch 194 train loss: 0.6101, eval loss 0.6606999039649963\n",
      "optimal threshold: -0.4964\n",
      "Epoch 195 train loss: 0.6203, eval loss 0.6607270240783691\n",
      "optimal threshold: -0.4952\n",
      "Epoch 196 train loss: 0.6532, eval loss 0.6607489585876465\n",
      "optimal threshold: -0.4948\n",
      "Epoch 197 train loss: 0.6315, eval loss 0.6607855558395386\n",
      "optimal threshold: -0.4930\n",
      "Epoch 198 train loss: 0.6308, eval loss 0.6606943607330322\n",
      "optimal threshold: -0.4915\n",
      "Epoch 199 train loss: 0.6421, eval loss 0.6605556011199951\n",
      "optimal threshold: -0.4942\n",
      "Epoch 200 train loss: 0.6094, eval loss 0.6605994701385498\n",
      "optimal threshold: -0.4964\n",
      "Epoch 201 train loss: 0.6667, eval loss 0.6606450080871582\n",
      "optimal threshold: -0.4948\n",
      "Epoch 202 train loss: 0.6181, eval loss 0.6605737805366516\n",
      "optimal threshold: -0.5099\n",
      "Epoch 203 train loss: 0.6527, eval loss 0.6605942249298096\n",
      "optimal threshold: -0.4901\n",
      "Epoch 204 train loss: 0.6221, eval loss 0.6604472398757935\n",
      "optimal threshold: -0.4883\n",
      "Epoch 205 train loss: 0.6268, eval loss 0.6604057550430298\n",
      "optimal threshold: -0.5224\n",
      "Epoch 206 train loss: 0.6379, eval loss 0.6603772640228271\n",
      "optimal threshold: -0.5386\n",
      "Epoch 207 train loss: 0.6201, eval loss 0.6603050231933594\n",
      "optimal threshold: -0.5366\n",
      "Epoch 208 train loss: 0.6095, eval loss 0.6602501273155212\n",
      "optimal threshold: -0.5381\n",
      "Epoch 209 train loss: 0.6456, eval loss 0.6602434515953064\n",
      "optimal threshold: -0.5390\n",
      "Epoch 210 train loss: 0.6545, eval loss 0.6602005958557129\n",
      "optimal threshold: -0.5331\n",
      "Epoch 211 train loss: 0.6092, eval loss 0.6601440906524658\n",
      "optimal threshold: -0.5389\n",
      "Epoch 212 train loss: 0.6333, eval loss 0.6602304577827454\n",
      "optimal threshold: -0.5477\n",
      "Epoch 213 train loss: 0.6178, eval loss 0.6601155996322632\n",
      "optimal threshold: -0.5331\n",
      "Epoch 214 train loss: 0.6224, eval loss 0.6601385474205017\n",
      "optimal threshold: -0.5391\n",
      "Epoch 215 train loss: 0.6488, eval loss 0.6601226329803467\n",
      "optimal threshold: -0.5333\n",
      "Epoch 216 train loss: 0.6518, eval loss 0.6601467132568359\n",
      "optimal threshold: -0.5256\n",
      "Epoch 217 train loss: 0.6195, eval loss 0.6600453853607178\n",
      "optimal threshold: -0.5306\n",
      "Epoch 218 train loss: 0.5931, eval loss 0.6600853800773621\n",
      "optimal threshold: -0.5244\n",
      "Epoch 219 train loss: 0.6413, eval loss 0.6600186824798584\n",
      "optimal threshold: -0.5207\n",
      "Epoch 220 train loss: 0.6310, eval loss 0.6598662734031677\n",
      "optimal threshold: -0.5247\n",
      "Epoch 221 train loss: 0.6342, eval loss 0.6598716974258423\n",
      "optimal threshold: -0.5326\n",
      "Epoch 222 train loss: 0.6338, eval loss 0.6598337292671204\n",
      "optimal threshold: -0.5176\n",
      "Epoch 223 train loss: 0.6037, eval loss 0.659716010093689\n",
      "optimal threshold: -0.5317\n",
      "Epoch 224 train loss: 0.6060, eval loss 0.6596848368644714\n",
      "optimal threshold: -0.5279\n",
      "Epoch 225 train loss: 0.6604, eval loss 0.6595675945281982\n",
      "optimal threshold: -0.5297\n",
      "Epoch 226 train loss: 0.6155, eval loss 0.6596278548240662\n",
      "optimal threshold: -0.5335\n",
      "Epoch 227 train loss: 0.6030, eval loss 0.6596567630767822\n",
      "optimal threshold: -0.5325\n",
      "Epoch 228 train loss: 0.6131, eval loss 0.6596121191978455\n",
      "optimal threshold: -0.5295\n",
      "Epoch 229 train loss: 0.5796, eval loss 0.6596078872680664\n",
      "optimal threshold: -0.5289\n",
      "Epoch 230 train loss: 0.6263, eval loss 0.6595864295959473\n",
      "optimal threshold: -0.5255\n",
      "Epoch 231 train loss: 0.6332, eval loss 0.6595321893692017\n",
      "optimal threshold: -0.5291\n",
      "Epoch 232 train loss: 0.6192, eval loss 0.659593403339386\n",
      "optimal threshold: -0.5284\n",
      "Epoch 233 train loss: 0.5976, eval loss 0.6595867872238159\n",
      "optimal threshold: -0.7055\n",
      "Epoch 234 train loss: 0.6568, eval loss 0.6595628261566162\n",
      "optimal threshold: -0.5257\n",
      "Epoch 235 train loss: 0.6443, eval loss 0.6595224142074585\n",
      "optimal threshold: -0.5227\n",
      "Epoch 236 train loss: 0.6321, eval loss 0.6594020128250122\n",
      "optimal threshold: -0.5229\n",
      "Epoch 237 train loss: 0.6137, eval loss 0.6593377590179443\n",
      "optimal threshold: -0.5260\n",
      "Epoch 238 train loss: 0.6232, eval loss 0.6595184206962585\n",
      "optimal threshold: -0.5265\n",
      "Epoch 239 train loss: 0.5971, eval loss 0.6595324277877808\n",
      "optimal threshold: -0.7267\n",
      "Epoch 240 train loss: 0.6642, eval loss 0.6597008109092712\n",
      "optimal threshold: -0.7109\n",
      "Epoch 241 train loss: 0.6275, eval loss 0.6596527695655823\n",
      "optimal threshold: -0.7184\n",
      "Epoch 242 train loss: 0.6181, eval loss 0.6596075892448425\n",
      "optimal threshold: -0.7134\n",
      "Epoch 243 train loss: 0.6239, eval loss 0.6595146656036377\n",
      "optimal threshold: -0.7125\n",
      "Epoch 244 train loss: 0.6097, eval loss 0.6594552993774414\n",
      "optimal threshold: -0.7181\n",
      "Epoch 245 train loss: 0.6423, eval loss 0.6594289541244507\n",
      "optimal threshold: -0.7108\n",
      "Epoch 246 train loss: 0.6509, eval loss 0.6594107151031494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:10:07,928] Trial 4 finished with value: 0.6423218250274658 and parameters: {'learning_rate_exp': -4.543674803621247, 'dropout_p': 0.5831414702579255, 'l2_reg_exp': -2.39068651515102, 'batch_size': 458, 'N': 176}. Best is trial 3 with value: 0.5553447008132935.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7195\n",
      "optimal threshold: -0.4058\n",
      "Epoch 0 train loss: 1.3946, eval loss 1.3931068181991577\n",
      "optimal threshold: -0.1619\n",
      "Epoch 1 train loss: 1.3573, eval loss 1.352640151977539\n",
      "optimal threshold: -0.2369\n",
      "Epoch 2 train loss: 1.3160, eval loss 1.3089572191238403\n",
      "optimal threshold: -0.3209\n",
      "Epoch 3 train loss: 1.2721, eval loss 1.26156485080719\n",
      "optimal threshold: -0.4234\n",
      "Epoch 4 train loss: 1.2247, eval loss 1.2116029262542725\n",
      "optimal threshold: -0.5072\n",
      "Epoch 5 train loss: 1.1739, eval loss 1.1613233089447021\n",
      "optimal threshold: -0.5976\n",
      "Epoch 6 train loss: 1.1349, eval loss 1.1125288009643555\n",
      "optimal threshold: -0.6747\n",
      "Epoch 7 train loss: 1.0902, eval loss 1.067681074142456\n",
      "optimal threshold: -0.7437\n",
      "Epoch 8 train loss: 1.0480, eval loss 1.0270638465881348\n",
      "optimal threshold: -0.7956\n",
      "Epoch 9 train loss: 1.0170, eval loss 0.9902977347373962\n",
      "optimal threshold: -0.8394\n",
      "Epoch 10 train loss: 0.9741, eval loss 0.9567954540252686\n",
      "optimal threshold: -0.8637\n",
      "Epoch 11 train loss: 0.9494, eval loss 0.9260748624801636\n",
      "optimal threshold: -0.9005\n",
      "Epoch 12 train loss: 0.9142, eval loss 0.8978052735328674\n",
      "optimal threshold: -0.9159\n",
      "Epoch 13 train loss: 0.8900, eval loss 0.8716377019882202\n",
      "optimal threshold: -0.9246\n",
      "Epoch 14 train loss: 0.8740, eval loss 0.8475660085678101\n",
      "optimal threshold: -0.9256\n",
      "Epoch 15 train loss: 0.8456, eval loss 0.8258807063102722\n",
      "optimal threshold: -0.9381\n",
      "Epoch 16 train loss: 0.8320, eval loss 0.8067914247512817\n",
      "optimal threshold: -0.9249\n",
      "Epoch 17 train loss: 0.8072, eval loss 0.7901395559310913\n",
      "optimal threshold: -0.7427\n",
      "Epoch 18 train loss: 0.8007, eval loss 0.7758866548538208\n",
      "optimal threshold: -0.7817\n",
      "Epoch 19 train loss: 0.7781, eval loss 0.7637563943862915\n",
      "optimal threshold: -0.7451\n",
      "Epoch 20 train loss: 0.7743, eval loss 0.7534422278404236\n",
      "optimal threshold: -0.7251\n",
      "Epoch 21 train loss: 0.7526, eval loss 0.7446545958518982\n",
      "optimal threshold: -0.8882\n",
      "Epoch 22 train loss: 0.7534, eval loss 0.7372604012489319\n",
      "optimal threshold: -0.6918\n",
      "Epoch 23 train loss: 0.7513, eval loss 0.7310230731964111\n",
      "optimal threshold: -0.9198\n",
      "Epoch 24 train loss: 0.7427, eval loss 0.7255969643592834\n",
      "optimal threshold: -0.9114\n",
      "Epoch 25 train loss: 0.7424, eval loss 0.7209538817405701\n",
      "optimal threshold: -0.8907\n",
      "Epoch 26 train loss: 0.7467, eval loss 0.7169526815414429\n",
      "optimal threshold: -0.9109\n",
      "Epoch 27 train loss: 0.7482, eval loss 0.7133966088294983\n",
      "optimal threshold: -0.9797\n",
      "Epoch 28 train loss: 0.7367, eval loss 0.7102972269058228\n",
      "optimal threshold: -0.9843\n",
      "Epoch 29 train loss: 0.7366, eval loss 0.7075027823448181\n",
      "optimal threshold: -0.9858\n",
      "Epoch 30 train loss: 0.7315, eval loss 0.704975962638855\n",
      "optimal threshold: -0.9784\n",
      "Epoch 31 train loss: 0.7208, eval loss 0.7027145624160767\n",
      "optimal threshold: -0.9152\n",
      "Epoch 32 train loss: 0.7212, eval loss 0.7005413174629211\n",
      "optimal threshold: -0.8332\n",
      "Epoch 33 train loss: 0.7013, eval loss 0.6985607743263245\n",
      "optimal threshold: -0.8224\n",
      "Epoch 34 train loss: 0.7179, eval loss 0.6967424154281616\n",
      "optimal threshold: -0.8219\n",
      "Epoch 35 train loss: 0.7195, eval loss 0.6950292587280273\n",
      "optimal threshold: -0.7677\n",
      "Epoch 36 train loss: 0.7183, eval loss 0.6934852004051208\n",
      "optimal threshold: -0.8249\n",
      "Epoch 37 train loss: 0.7233, eval loss 0.6919069886207581\n",
      "optimal threshold: -0.8112\n",
      "Epoch 38 train loss: 0.7138, eval loss 0.6904484033584595\n",
      "optimal threshold: -0.8347\n",
      "Epoch 39 train loss: 0.7042, eval loss 0.6890273690223694\n",
      "optimal threshold: -0.7990\n",
      "Epoch 40 train loss: 0.7037, eval loss 0.6878039836883545\n",
      "optimal threshold: -0.8090\n",
      "Epoch 41 train loss: 0.7119, eval loss 0.6865341067314148\n",
      "optimal threshold: -0.8072\n",
      "Epoch 42 train loss: 0.7169, eval loss 0.6853421926498413\n",
      "optimal threshold: -0.8196\n",
      "Epoch 43 train loss: 0.6887, eval loss 0.6841995120048523\n",
      "optimal threshold: -0.9296\n",
      "Epoch 44 train loss: 0.7044, eval loss 0.6831024885177612\n",
      "optimal threshold: -0.4936\n",
      "Epoch 45 train loss: 0.6967, eval loss 0.6820910573005676\n",
      "optimal threshold: -0.9367\n",
      "Epoch 46 train loss: 0.6982, eval loss 0.6811038255691528\n",
      "optimal threshold: -0.6234\n",
      "Epoch 47 train loss: 0.6985, eval loss 0.6802376508712769\n",
      "optimal threshold: -0.6438\n",
      "Epoch 48 train loss: 0.7048, eval loss 0.6793582439422607\n",
      "optimal threshold: -0.6750\n",
      "Epoch 49 train loss: 0.7092, eval loss 0.6784843802452087\n",
      "optimal threshold: -0.6665\n",
      "Epoch 50 train loss: 0.6829, eval loss 0.677628755569458\n",
      "optimal threshold: -0.6369\n",
      "Epoch 51 train loss: 0.6892, eval loss 0.6768817901611328\n",
      "optimal threshold: -0.6149\n",
      "Epoch 52 train loss: 0.7013, eval loss 0.6761552691459656\n",
      "optimal threshold: -0.6118\n",
      "Epoch 53 train loss: 0.6810, eval loss 0.6754587888717651\n",
      "optimal threshold: -0.6125\n",
      "Epoch 54 train loss: 0.6986, eval loss 0.6747992634773254\n",
      "optimal threshold: -0.6254\n",
      "Epoch 55 train loss: 0.6966, eval loss 0.6741587519645691\n",
      "optimal threshold: -0.6326\n",
      "Epoch 56 train loss: 0.6855, eval loss 0.6734883189201355\n",
      "optimal threshold: -0.6255\n",
      "Epoch 57 train loss: 0.6817, eval loss 0.672922670841217\n",
      "optimal threshold: -0.6826\n",
      "Epoch 58 train loss: 0.6947, eval loss 0.6723352074623108\n",
      "optimal threshold: -0.6917\n",
      "Epoch 59 train loss: 0.6993, eval loss 0.6718173623085022\n",
      "optimal threshold: -0.6996\n",
      "Epoch 60 train loss: 0.6734, eval loss 0.6712750196456909\n",
      "optimal threshold: -0.6806\n",
      "Epoch 61 train loss: 0.6845, eval loss 0.6706936359405518\n",
      "optimal threshold: -0.6767\n",
      "Epoch 62 train loss: 0.6782, eval loss 0.6702996492385864\n",
      "optimal threshold: -0.6842\n",
      "Epoch 63 train loss: 0.6825, eval loss 0.6698030233383179\n",
      "optimal threshold: -0.6989\n",
      "Epoch 64 train loss: 0.6626, eval loss 0.6694200038909912\n",
      "optimal threshold: -0.6788\n",
      "Epoch 65 train loss: 0.6599, eval loss 0.6690080165863037\n",
      "optimal threshold: -0.6791\n",
      "Epoch 66 train loss: 0.6634, eval loss 0.6686887741088867\n",
      "optimal threshold: -0.6792\n",
      "Epoch 67 train loss: 0.6659, eval loss 0.6682723760604858\n",
      "optimal threshold: -0.6954\n",
      "Epoch 68 train loss: 0.6919, eval loss 0.6678678393363953\n",
      "optimal threshold: -0.6477\n",
      "Epoch 69 train loss: 0.6666, eval loss 0.667457103729248\n",
      "optimal threshold: -0.6516\n",
      "Epoch 70 train loss: 0.6700, eval loss 0.6671695113182068\n",
      "optimal threshold: -0.6426\n",
      "Epoch 71 train loss: 0.6698, eval loss 0.6667768359184265\n",
      "optimal threshold: -0.6429\n",
      "Epoch 72 train loss: 0.6810, eval loss 0.6665022969245911\n",
      "optimal threshold: -0.6377\n",
      "Epoch 73 train loss: 0.6770, eval loss 0.6661524772644043\n",
      "optimal threshold: -0.7357\n",
      "Epoch 74 train loss: 0.6851, eval loss 0.6658936738967896\n",
      "optimal threshold: -0.7371\n",
      "Epoch 75 train loss: 0.6848, eval loss 0.6655833125114441\n",
      "optimal threshold: -0.7424\n",
      "Epoch 76 train loss: 0.6887, eval loss 0.6653218865394592\n",
      "optimal threshold: -0.7439\n",
      "Epoch 77 train loss: 0.6498, eval loss 0.6650547981262207\n",
      "optimal threshold: -0.7423\n",
      "Epoch 78 train loss: 0.6956, eval loss 0.6648015975952148\n",
      "optimal threshold: -0.6566\n",
      "Epoch 79 train loss: 0.6681, eval loss 0.6645817160606384\n",
      "optimal threshold: -0.6581\n",
      "Epoch 80 train loss: 0.6660, eval loss 0.664429247379303\n",
      "optimal threshold: -0.6495\n",
      "Epoch 81 train loss: 0.6602, eval loss 0.6641460657119751\n",
      "optimal threshold: -0.4601\n",
      "Epoch 82 train loss: 0.6801, eval loss 0.6639288067817688\n",
      "optimal threshold: -0.4565\n",
      "Epoch 83 train loss: 0.6738, eval loss 0.6637081503868103\n",
      "optimal threshold: -0.4585\n",
      "Epoch 84 train loss: 0.6576, eval loss 0.6634986996650696\n",
      "optimal threshold: -0.4616\n",
      "Epoch 85 train loss: 0.6559, eval loss 0.6632959246635437\n",
      "optimal threshold: -0.7000\n",
      "Epoch 86 train loss: 0.6697, eval loss 0.6631103157997131\n",
      "optimal threshold: -0.4664\n",
      "Epoch 87 train loss: 0.6761, eval loss 0.6628963351249695\n",
      "optimal threshold: -0.6889\n",
      "Epoch 88 train loss: 0.6618, eval loss 0.6626230478286743\n",
      "optimal threshold: -0.7059\n",
      "Epoch 89 train loss: 0.6760, eval loss 0.6625136137008667\n",
      "optimal threshold: -0.7098\n",
      "Epoch 90 train loss: 0.6676, eval loss 0.6623961925506592\n",
      "optimal threshold: -0.7075\n",
      "Epoch 91 train loss: 0.6951, eval loss 0.6622275710105896\n",
      "optimal threshold: -0.7198\n",
      "Epoch 92 train loss: 0.6620, eval loss 0.6622519493103027\n",
      "optimal threshold: -0.7161\n",
      "Epoch 93 train loss: 0.6749, eval loss 0.6620499491691589\n",
      "optimal threshold: -0.7274\n",
      "Epoch 94 train loss: 0.6822, eval loss 0.6619560718536377\n",
      "optimal threshold: -0.7246\n",
      "Epoch 95 train loss: 0.6566, eval loss 0.6617542505264282\n",
      "optimal threshold: -0.7058\n",
      "Epoch 96 train loss: 0.6918, eval loss 0.6616281270980835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7082\n",
      "Epoch 97 train loss: 0.6579, eval loss 0.6614498496055603\n",
      "optimal threshold: -0.7080\n",
      "Epoch 98 train loss: 0.6765, eval loss 0.6613199710845947\n",
      "optimal threshold: -0.7476\n",
      "Epoch 99 train loss: 0.6577, eval loss 0.66126948595047\n",
      "optimal threshold: -0.7328\n",
      "Epoch 100 train loss: 0.6631, eval loss 0.6610296368598938\n",
      "optimal threshold: -0.7433\n",
      "Epoch 101 train loss: 0.6589, eval loss 0.6608836650848389\n",
      "optimal threshold: -0.7475\n",
      "Epoch 102 train loss: 0.6640, eval loss 0.6608502268791199\n",
      "optimal threshold: -0.7496\n",
      "Epoch 103 train loss: 0.6563, eval loss 0.6606922149658203\n",
      "optimal threshold: -0.7303\n",
      "Epoch 104 train loss: 0.6580, eval loss 0.6605311632156372\n",
      "optimal threshold: -0.7471\n",
      "Epoch 105 train loss: 0.6622, eval loss 0.6604667901992798\n",
      "optimal threshold: -0.7393\n",
      "Epoch 106 train loss: 0.6640, eval loss 0.6603031754493713\n",
      "optimal threshold: -0.7531\n",
      "Epoch 107 train loss: 0.6813, eval loss 0.6602001786231995\n",
      "optimal threshold: -0.7401\n",
      "Epoch 108 train loss: 0.6609, eval loss 0.6601390838623047\n",
      "optimal threshold: -0.7472\n",
      "Epoch 109 train loss: 0.6755, eval loss 0.6600443124771118\n",
      "optimal threshold: -0.7408\n",
      "Epoch 110 train loss: 0.6750, eval loss 0.6599112749099731\n",
      "optimal threshold: -0.7652\n",
      "Epoch 111 train loss: 0.6567, eval loss 0.6598894596099854\n",
      "optimal threshold: -0.7586\n",
      "Epoch 112 train loss: 0.6630, eval loss 0.6597551107406616\n",
      "optimal threshold: -0.6776\n",
      "Epoch 113 train loss: 0.6731, eval loss 0.6595923900604248\n",
      "optimal threshold: -0.6681\n",
      "Epoch 114 train loss: 0.6694, eval loss 0.659477174282074\n",
      "optimal threshold: -0.7461\n",
      "Epoch 115 train loss: 0.6630, eval loss 0.6594415307044983\n",
      "optimal threshold: -0.6801\n",
      "Epoch 116 train loss: 0.6966, eval loss 0.6594686508178711\n",
      "optimal threshold: -0.6786\n",
      "Epoch 117 train loss: 0.6904, eval loss 0.6593917608261108\n",
      "optimal threshold: -0.7018\n",
      "Epoch 118 train loss: 0.6675, eval loss 0.6592869758605957\n",
      "optimal threshold: -0.6980\n",
      "Epoch 119 train loss: 0.6374, eval loss 0.6591610908508301\n",
      "optimal threshold: -0.6942\n",
      "Epoch 120 train loss: 0.6676, eval loss 0.6591492295265198\n",
      "optimal threshold: -0.6898\n",
      "Epoch 121 train loss: 0.6512, eval loss 0.6590265035629272\n",
      "optimal threshold: -0.6989\n",
      "Epoch 122 train loss: 0.6571, eval loss 0.6589291095733643\n",
      "optimal threshold: -0.6954\n",
      "Epoch 123 train loss: 0.6668, eval loss 0.6587598919868469\n",
      "optimal threshold: -0.6994\n",
      "Epoch 124 train loss: 0.6760, eval loss 0.6587395668029785\n",
      "optimal threshold: -0.6974\n",
      "Epoch 125 train loss: 0.6551, eval loss 0.6586443781852722\n",
      "optimal threshold: -0.6968\n",
      "Epoch 126 train loss: 0.6539, eval loss 0.6585569977760315\n",
      "optimal threshold: -0.6968\n",
      "Epoch 127 train loss: 0.6451, eval loss 0.658453106880188\n",
      "optimal threshold: -0.6993\n",
      "Epoch 128 train loss: 0.6619, eval loss 0.6584372520446777\n",
      "optimal threshold: -0.6898\n",
      "Epoch 129 train loss: 0.6742, eval loss 0.6583420038223267\n",
      "optimal threshold: -0.6924\n",
      "Epoch 130 train loss: 0.6661, eval loss 0.6582391858100891\n",
      "optimal threshold: -0.7033\n",
      "Epoch 131 train loss: 0.6806, eval loss 0.6582911014556885\n",
      "optimal threshold: -0.6999\n",
      "Epoch 132 train loss: 0.6629, eval loss 0.6582140922546387\n",
      "optimal threshold: -0.7014\n",
      "Epoch 133 train loss: 0.6593, eval loss 0.6581063270568848\n",
      "optimal threshold: -0.6941\n",
      "Epoch 134 train loss: 0.6609, eval loss 0.6580665707588196\n",
      "optimal threshold: -0.7043\n",
      "Epoch 135 train loss: 0.6625, eval loss 0.6579324007034302\n",
      "optimal threshold: -0.7092\n",
      "Epoch 136 train loss: 0.6536, eval loss 0.6579951047897339\n",
      "optimal threshold: -0.6897\n",
      "Epoch 137 train loss: 0.6333, eval loss 0.6579151749610901\n",
      "optimal threshold: -0.6901\n",
      "Epoch 138 train loss: 0.6563, eval loss 0.6579352617263794\n",
      "optimal threshold: -0.6838\n",
      "Epoch 139 train loss: 0.6665, eval loss 0.6577670574188232\n",
      "optimal threshold: -0.7041\n",
      "Epoch 140 train loss: 0.6561, eval loss 0.657817006111145\n",
      "optimal threshold: -0.7052\n",
      "Epoch 141 train loss: 0.6611, eval loss 0.6577768325805664\n",
      "optimal threshold: -0.7397\n",
      "Epoch 142 train loss: 0.6620, eval loss 0.6577447652816772\n",
      "optimal threshold: -0.7374\n",
      "Epoch 143 train loss: 0.6825, eval loss 0.6577162742614746\n",
      "optimal threshold: -0.7298\n",
      "Epoch 144 train loss: 0.6566, eval loss 0.657564640045166\n",
      "optimal threshold: -0.7351\n",
      "Epoch 145 train loss: 0.6578, eval loss 0.6575040817260742\n",
      "optimal threshold: -0.7503\n",
      "Epoch 146 train loss: 0.6754, eval loss 0.6574335098266602\n",
      "optimal threshold: -0.7001\n",
      "Epoch 147 train loss: 0.6452, eval loss 0.6573665142059326\n",
      "optimal threshold: -0.7444\n",
      "Epoch 148 train loss: 0.6407, eval loss 0.6572826504707336\n",
      "optimal threshold: -0.7562\n",
      "Epoch 149 train loss: 0.6676, eval loss 0.6573067903518677\n",
      "optimal threshold: -0.7487\n",
      "Epoch 150 train loss: 0.6619, eval loss 0.6571612358093262\n",
      "optimal threshold: -0.6652\n",
      "Epoch 151 train loss: 0.6626, eval loss 0.657111644744873\n",
      "optimal threshold: -0.6599\n",
      "Epoch 152 train loss: 0.6762, eval loss 0.6570543050765991\n",
      "optimal threshold: -0.6589\n",
      "Epoch 153 train loss: 0.6705, eval loss 0.657015860080719\n",
      "optimal threshold: -0.6689\n",
      "Epoch 154 train loss: 0.6613, eval loss 0.6569458246231079\n",
      "optimal threshold: -0.6684\n",
      "Epoch 155 train loss: 0.6519, eval loss 0.6570221781730652\n",
      "optimal threshold: -0.6630\n",
      "Epoch 156 train loss: 0.6533, eval loss 0.6569393873214722\n",
      "optimal threshold: -0.6581\n",
      "Epoch 157 train loss: 0.6576, eval loss 0.6568399667739868\n",
      "optimal threshold: -0.6508\n",
      "Epoch 158 train loss: 0.6586, eval loss 0.6566748023033142\n",
      "optimal threshold: -0.6966\n",
      "Epoch 159 train loss: 0.6791, eval loss 0.6566377282142639\n",
      "optimal threshold: -0.6694\n",
      "Epoch 160 train loss: 0.6457, eval loss 0.6567158102989197\n",
      "optimal threshold: -0.6596\n",
      "Epoch 161 train loss: 0.6536, eval loss 0.6565473079681396\n",
      "optimal threshold: -0.6561\n",
      "Epoch 162 train loss: 0.6468, eval loss 0.6565587520599365\n",
      "optimal threshold: -0.6629\n",
      "Epoch 163 train loss: 0.6625, eval loss 0.6565664410591125\n",
      "optimal threshold: -0.6600\n",
      "Epoch 164 train loss: 0.6493, eval loss 0.6565065383911133\n",
      "optimal threshold: -0.6582\n",
      "Epoch 165 train loss: 0.6491, eval loss 0.6564568877220154\n",
      "optimal threshold: -0.6579\n",
      "Epoch 166 train loss: 0.6922, eval loss 0.656429648399353\n",
      "optimal threshold: -0.6588\n",
      "Epoch 167 train loss: 0.6678, eval loss 0.6563825607299805\n",
      "optimal threshold: -0.6688\n",
      "Epoch 168 train loss: 0.6721, eval loss 0.6565048098564148\n",
      "optimal threshold: -0.6095\n",
      "Epoch 169 train loss: 0.6958, eval loss 0.6564452648162842\n",
      "optimal threshold: -0.6105\n",
      "Epoch 170 train loss: 0.6736, eval loss 0.6563872694969177\n",
      "optimal threshold: -0.6107\n",
      "Epoch 171 train loss: 0.6625, eval loss 0.6562825441360474\n",
      "optimal threshold: -0.6131\n",
      "Epoch 172 train loss: 0.6441, eval loss 0.6563296318054199\n",
      "optimal threshold: -0.6153\n",
      "Epoch 173 train loss: 0.6758, eval loss 0.6563129425048828\n",
      "optimal threshold: -0.6108\n",
      "Epoch 174 train loss: 0.6422, eval loss 0.6562890410423279\n",
      "optimal threshold: -0.6072\n",
      "Epoch 175 train loss: 0.6519, eval loss 0.656334638595581\n",
      "optimal threshold: -0.5983\n",
      "Epoch 176 train loss: 0.6673, eval loss 0.6562343239784241\n",
      "optimal threshold: -0.6028\n",
      "Epoch 177 train loss: 0.6428, eval loss 0.6561640501022339\n",
      "optimal threshold: -0.5980\n",
      "Epoch 178 train loss: 0.6649, eval loss 0.6562389135360718\n",
      "optimal threshold: -0.5877\n",
      "Epoch 179 train loss: 0.6621, eval loss 0.6561511158943176\n",
      "optimal threshold: -0.6058\n",
      "Epoch 180 train loss: 0.6602, eval loss 0.6562556624412537\n",
      "optimal threshold: -0.6039\n",
      "Epoch 181 train loss: 0.6546, eval loss 0.6561878323554993\n",
      "optimal threshold: -0.5990\n",
      "Epoch 182 train loss: 0.6695, eval loss 0.6561113595962524\n",
      "optimal threshold: -0.5976\n",
      "Epoch 183 train loss: 0.6393, eval loss 0.6560642123222351\n",
      "optimal threshold: -0.5985\n",
      "Epoch 184 train loss: 0.6397, eval loss 0.6560555100440979\n",
      "optimal threshold: -0.5975\n",
      "Epoch 185 train loss: 0.6429, eval loss 0.6559237837791443\n",
      "optimal threshold: -0.5932\n",
      "Epoch 186 train loss: 0.6516, eval loss 0.6559246778488159\n",
      "optimal threshold: -0.5921\n",
      "Epoch 187 train loss: 0.6610, eval loss 0.6558725833892822\n",
      "optimal threshold: -0.5921\n",
      "Epoch 188 train loss: 0.6482, eval loss 0.655925989151001\n",
      "optimal threshold: -0.5892\n",
      "Epoch 189 train loss: 0.6712, eval loss 0.6558718085289001\n",
      "optimal threshold: -0.5811\n",
      "Epoch 190 train loss: 0.6580, eval loss 0.655768632888794\n",
      "optimal threshold: -0.5873\n",
      "Epoch 191 train loss: 0.6474, eval loss 0.6558337211608887\n",
      "optimal threshold: -0.5952\n",
      "Epoch 192 train loss: 0.6593, eval loss 0.6558426022529602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5807\n",
      "Epoch 193 train loss: 0.6463, eval loss 0.6557849645614624\n",
      "optimal threshold: -0.5765\n",
      "Epoch 194 train loss: 0.6276, eval loss 0.6557137966156006\n",
      "optimal threshold: -0.5758\n",
      "Epoch 195 train loss: 0.6506, eval loss 0.6557366251945496\n",
      "optimal threshold: -0.5708\n",
      "Epoch 196 train loss: 0.6402, eval loss 0.6555896401405334\n",
      "optimal threshold: -0.5726\n",
      "Epoch 197 train loss: 0.6519, eval loss 0.6556053757667542\n",
      "optimal threshold: -0.5733\n",
      "Epoch 198 train loss: 0.6655, eval loss 0.6555894017219543\n",
      "optimal threshold: -0.5904\n",
      "Epoch 199 train loss: 0.6462, eval loss 0.6555666327476501\n",
      "optimal threshold: -0.5701\n",
      "Epoch 200 train loss: 0.6563, eval loss 0.6555011868476868\n",
      "optimal threshold: -0.5820\n",
      "Epoch 201 train loss: 0.6609, eval loss 0.6555324196815491\n",
      "optimal threshold: -0.5777\n",
      "Epoch 202 train loss: 0.6487, eval loss 0.6554920077323914\n",
      "optimal threshold: -0.5810\n",
      "Epoch 203 train loss: 0.6540, eval loss 0.65561443567276\n",
      "optimal threshold: -0.5800\n",
      "Epoch 204 train loss: 0.6607, eval loss 0.6555411219596863\n",
      "optimal threshold: -0.5796\n",
      "Epoch 205 train loss: 0.6451, eval loss 0.6555631160736084\n",
      "optimal threshold: -0.5741\n",
      "Epoch 206 train loss: 0.6587, eval loss 0.6554606556892395\n",
      "optimal threshold: -0.5743\n",
      "Epoch 207 train loss: 0.6423, eval loss 0.6554689407348633\n",
      "optimal threshold: -0.5761\n",
      "Epoch 208 train loss: 0.6557, eval loss 0.6554281711578369\n",
      "optimal threshold: -0.5752\n",
      "Epoch 209 train loss: 0.6707, eval loss 0.6554096937179565\n",
      "optimal threshold: -0.5759\n",
      "Epoch 210 train loss: 0.6445, eval loss 0.6553537845611572\n",
      "optimal threshold: -0.5987\n",
      "Epoch 211 train loss: 0.6604, eval loss 0.6552519798278809\n",
      "optimal threshold: -0.6013\n",
      "Epoch 212 train loss: 0.6354, eval loss 0.6552668809890747\n",
      "optimal threshold: -0.6001\n",
      "Epoch 213 train loss: 0.6641, eval loss 0.6553047895431519\n",
      "optimal threshold: -0.5972\n",
      "Epoch 214 train loss: 0.6370, eval loss 0.6552257537841797\n",
      "optimal threshold: -0.5968\n",
      "Epoch 215 train loss: 0.6383, eval loss 0.6552426815032959\n",
      "optimal threshold: -0.6031\n",
      "Epoch 216 train loss: 0.6648, eval loss 0.655309796333313\n",
      "optimal threshold: -0.6017\n",
      "Epoch 217 train loss: 0.6504, eval loss 0.6552749872207642\n",
      "optimal threshold: -0.5888\n",
      "Epoch 218 train loss: 0.6727, eval loss 0.6551933288574219\n",
      "optimal threshold: -0.5791\n",
      "Epoch 219 train loss: 0.6592, eval loss 0.6551336646080017\n",
      "optimal threshold: -0.5875\n",
      "Epoch 220 train loss: 0.6326, eval loss 0.6552205681800842\n",
      "optimal threshold: -0.6023\n",
      "Epoch 221 train loss: 0.6473, eval loss 0.6552994847297668\n",
      "optimal threshold: -0.5994\n",
      "Epoch 222 train loss: 0.6517, eval loss 0.6553204655647278\n",
      "optimal threshold: -0.6171\n",
      "Epoch 223 train loss: 0.6537, eval loss 0.6552138328552246\n",
      "optimal threshold: -0.6162\n",
      "Epoch 224 train loss: 0.6488, eval loss 0.6551790237426758\n",
      "optimal threshold: -0.6176\n",
      "Epoch 225 train loss: 0.6511, eval loss 0.6552449464797974\n",
      "optimal threshold: -0.6033\n",
      "Epoch 226 train loss: 0.6455, eval loss 0.6552824378013611\n",
      "optimal threshold: -0.5701\n",
      "Epoch 227 train loss: 0.6394, eval loss 0.6551449298858643\n",
      "optimal threshold: -0.5740\n",
      "Epoch 228 train loss: 0.6585, eval loss 0.6551684737205505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:12:17,705] Trial 5 finished with value: 0.6623961925506592 and parameters: {'learning_rate_exp': -4.759701586578014, 'dropout_p': 0.19826275177198038, 'l2_reg_exp': -3.878788436425824, 'batch_size': 321, 'N': 202}. Best is trial 3 with value: 0.5553447008132935.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5702\n",
      "optimal threshold: -0.5249\n",
      "Epoch 0 train loss: 0.6668, eval loss 0.6637534499168396\n",
      "optimal threshold: -0.5211\n",
      "Epoch 1 train loss: 0.6390, eval loss 0.6610378623008728\n",
      "optimal threshold: -0.4811\n",
      "Epoch 2 train loss: 0.6131, eval loss 0.6609277725219727\n",
      "optimal threshold: -0.4666\n",
      "Epoch 3 train loss: 0.6395, eval loss 0.6605851054191589\n",
      "optimal threshold: -0.5731\n",
      "Epoch 4 train loss: 0.6332, eval loss 0.6655388474464417\n",
      "optimal threshold: -0.4579\n",
      "Epoch 5 train loss: 0.6008, eval loss 0.6670329570770264\n",
      "optimal threshold: -0.3736\n",
      "Epoch 6 train loss: 0.6007, eval loss 0.6698744297027588\n",
      "optimal threshold: -0.3246\n",
      "Epoch 7 train loss: 0.5846, eval loss 0.680188775062561\n",
      "optimal threshold: -0.2732\n",
      "Epoch 8 train loss: 0.5470, eval loss 0.6866059303283691\n",
      "optimal threshold: -0.5506\n",
      "Epoch 9 train loss: 0.5027, eval loss 0.6916054487228394\n",
      "optimal threshold: -0.3878\n",
      "Epoch 10 train loss: 0.5524, eval loss 0.6928461194038391\n",
      "optimal threshold: -0.3424\n",
      "Epoch 11 train loss: 0.4826, eval loss 0.7009607553482056\n",
      "optimal threshold: -0.7310\n",
      "Epoch 12 train loss: 0.4720, eval loss 0.7135190367698669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:12:26,909] Trial 6 finished with value: 0.45701825618743896 and parameters: {'learning_rate_exp': -2.3571744697916563, 'dropout_p': 0.2194373767267711, 'l2_reg_exp': -5.721051262044693, 'batch_size': 357, 'N': 266}. Best is trial 6 with value: 0.45701825618743896.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5894\n",
      "optimal threshold: -0.0079\n",
      "Epoch 0 train loss: 1.5602, eval loss 1.4943450689315796\n",
      "optimal threshold: -0.0141\n",
      "Epoch 1 train loss: 1.5182, eval loss 1.491307258605957\n",
      "optimal threshold: -0.0203\n",
      "Epoch 2 train loss: 1.5589, eval loss 1.488304615020752\n",
      "optimal threshold: -0.0269\n",
      "Epoch 3 train loss: 1.5426, eval loss 1.4852749109268188\n",
      "optimal threshold: 0.0645\n",
      "Epoch 4 train loss: 1.5112, eval loss 1.4822973012924194\n",
      "optimal threshold: 0.0586\n",
      "Epoch 5 train loss: 1.5999, eval loss 1.4792636632919312\n",
      "optimal threshold: 0.0528\n",
      "Epoch 6 train loss: 1.5307, eval loss 1.4762215614318848\n",
      "optimal threshold: 0.0495\n",
      "Epoch 7 train loss: 1.5509, eval loss 1.4731507301330566\n",
      "optimal threshold: 0.0438\n",
      "Epoch 8 train loss: 1.5239, eval loss 1.470078706741333\n",
      "optimal threshold: 0.0387\n",
      "Epoch 9 train loss: 1.5605, eval loss 1.4669889211654663\n",
      "optimal threshold: 0.0337\n",
      "Epoch 10 train loss: 1.4206, eval loss 1.4638773202896118\n",
      "optimal threshold: 0.0290\n",
      "Epoch 11 train loss: 1.4424, eval loss 1.460733413696289\n",
      "optimal threshold: 0.0254\n",
      "Epoch 12 train loss: 1.5607, eval loss 1.457572102546692\n",
      "optimal threshold: 0.0207\n",
      "Epoch 13 train loss: 1.4819, eval loss 1.454391598701477\n",
      "optimal threshold: 0.0157\n",
      "Epoch 14 train loss: 1.5120, eval loss 1.4511756896972656\n",
      "optimal threshold: 0.0098\n",
      "Epoch 15 train loss: 1.4582, eval loss 1.4479385614395142\n",
      "optimal threshold: 0.0094\n",
      "Epoch 16 train loss: 1.4055, eval loss 1.444651484489441\n",
      "optimal threshold: 0.0031\n",
      "Epoch 17 train loss: 1.4887, eval loss 1.4413152933120728\n",
      "optimal threshold: -0.0020\n",
      "Epoch 18 train loss: 1.4908, eval loss 1.4379600286483765\n",
      "optimal threshold: -0.0079\n",
      "Epoch 19 train loss: 1.4021, eval loss 1.4345735311508179\n",
      "optimal threshold: -0.0121\n",
      "Epoch 20 train loss: 1.4717, eval loss 1.4311295747756958\n",
      "optimal threshold: -0.0175\n",
      "Epoch 21 train loss: 1.2926, eval loss 1.427628517150879\n",
      "optimal threshold: -0.0228\n",
      "Epoch 22 train loss: 1.2400, eval loss 1.4241355657577515\n",
      "optimal threshold: -0.0292\n",
      "Epoch 23 train loss: 1.4230, eval loss 1.4206067323684692\n",
      "optimal threshold: -0.0272\n",
      "Epoch 24 train loss: 1.3218, eval loss 1.417040467262268\n",
      "optimal threshold: -0.0336\n",
      "Epoch 25 train loss: 1.4665, eval loss 1.4134025573730469\n",
      "optimal threshold: -0.0392\n",
      "Epoch 26 train loss: 1.4981, eval loss 1.4097049236297607\n",
      "optimal threshold: -0.0451\n",
      "Epoch 27 train loss: 1.2777, eval loss 1.4059289693832397\n",
      "optimal threshold: -0.0510\n",
      "Epoch 28 train loss: 1.3858, eval loss 1.402093529701233\n",
      "optimal threshold: -0.0579\n",
      "Epoch 29 train loss: 1.3926, eval loss 1.3982142210006714\n",
      "optimal threshold: -0.0640\n",
      "Epoch 30 train loss: 1.4028, eval loss 1.394273042678833\n",
      "optimal threshold: -0.0698\n",
      "Epoch 31 train loss: 1.3327, eval loss 1.390289068222046\n",
      "optimal threshold: -0.0772\n",
      "Epoch 32 train loss: 1.1794, eval loss 1.386204719543457\n",
      "optimal threshold: -0.0793\n",
      "Epoch 33 train loss: 1.3439, eval loss 1.3820691108703613\n",
      "optimal threshold: -0.0867\n",
      "Epoch 34 train loss: 1.4139, eval loss 1.3778464794158936\n",
      "optimal threshold: -0.0964\n",
      "Epoch 35 train loss: 1.2243, eval loss 1.3735404014587402\n",
      "optimal threshold: -0.1018\n",
      "Epoch 36 train loss: 1.3553, eval loss 1.3691706657409668\n",
      "optimal threshold: -0.1076\n",
      "Epoch 37 train loss: 1.3022, eval loss 1.364701271057129\n",
      "optimal threshold: -0.1148\n",
      "Epoch 38 train loss: 1.2129, eval loss 1.360169529914856\n",
      "optimal threshold: -0.1199\n",
      "Epoch 39 train loss: 1.2853, eval loss 1.3555375337600708\n",
      "optimal threshold: -0.1274\n",
      "Epoch 40 train loss: 1.3568, eval loss 1.350842833518982\n",
      "optimal threshold: -0.1367\n",
      "Epoch 41 train loss: 1.1670, eval loss 1.3460215330123901\n",
      "optimal threshold: -0.1505\n",
      "Epoch 42 train loss: 1.4290, eval loss 1.3410896062850952\n",
      "optimal threshold: -0.1593\n",
      "Epoch 43 train loss: 1.2156, eval loss 1.3360651731491089\n",
      "optimal threshold: -0.1687\n",
      "Epoch 44 train loss: 1.2812, eval loss 1.3310014009475708\n",
      "optimal threshold: -0.1775\n",
      "Epoch 45 train loss: 1.3720, eval loss 1.325831651687622\n",
      "optimal threshold: -0.1846\n",
      "Epoch 46 train loss: 1.4864, eval loss 1.3205907344818115\n",
      "optimal threshold: -0.1941\n",
      "Epoch 47 train loss: 1.3237, eval loss 1.315251111984253\n",
      "optimal threshold: -0.2019\n",
      "Epoch 48 train loss: 1.2186, eval loss 1.3098154067993164\n",
      "optimal threshold: -0.2066\n",
      "Epoch 49 train loss: 1.2866, eval loss 1.304308533668518\n",
      "optimal threshold: -0.2145\n",
      "Epoch 50 train loss: 1.0592, eval loss 1.2986925840377808\n",
      "optimal threshold: -0.2177\n",
      "Epoch 51 train loss: 0.8047, eval loss 1.2930381298065186\n",
      "optimal threshold: -0.2289\n",
      "Epoch 52 train loss: 1.2148, eval loss 1.2872503995895386\n",
      "optimal threshold: -0.2392\n",
      "Epoch 53 train loss: 1.2946, eval loss 1.2814351320266724\n",
      "optimal threshold: -0.2469\n",
      "Epoch 54 train loss: 1.1453, eval loss 1.2755850553512573\n",
      "optimal threshold: -0.2553\n",
      "Epoch 55 train loss: 1.0839, eval loss 1.2696713209152222\n",
      "optimal threshold: -0.2684\n",
      "Epoch 56 train loss: 0.9925, eval loss 1.2636511325836182\n",
      "optimal threshold: -0.2810\n",
      "Epoch 57 train loss: 1.1009, eval loss 1.2576544284820557\n",
      "optimal threshold: -0.2903\n",
      "Epoch 58 train loss: 1.1468, eval loss 1.2515366077423096\n",
      "optimal threshold: -0.3016\n",
      "Epoch 59 train loss: 0.9279, eval loss 1.245347261428833\n",
      "optimal threshold: -0.3114\n",
      "Epoch 60 train loss: 1.3432, eval loss 1.2391401529312134\n",
      "optimal threshold: -0.3217\n",
      "Epoch 61 train loss: 1.0989, eval loss 1.2329133749008179\n",
      "optimal threshold: -0.3336\n",
      "Epoch 62 train loss: 0.9231, eval loss 1.2265994548797607\n",
      "optimal threshold: -0.3473\n",
      "Epoch 63 train loss: 0.6693, eval loss 1.2202935218811035\n",
      "optimal threshold: -0.3598\n",
      "Epoch 64 train loss: 1.0787, eval loss 1.213985800743103\n",
      "optimal threshold: -0.3701\n",
      "Epoch 65 train loss: 1.0667, eval loss 1.2076884508132935\n",
      "optimal threshold: -0.3703\n",
      "Epoch 66 train loss: 0.7660, eval loss 1.2013299465179443\n",
      "optimal threshold: -0.3937\n",
      "Epoch 67 train loss: 1.1521, eval loss 1.1950414180755615\n",
      "optimal threshold: -0.4028\n",
      "Epoch 68 train loss: 1.2086, eval loss 1.1887296438217163\n",
      "optimal threshold: -0.4114\n",
      "Epoch 69 train loss: 1.1472, eval loss 1.1824544668197632\n",
      "optimal threshold: -0.4227\n",
      "Epoch 70 train loss: 1.0555, eval loss 1.1761276721954346\n",
      "optimal threshold: -0.4593\n",
      "Epoch 71 train loss: 0.8825, eval loss 1.1698484420776367\n",
      "optimal threshold: -0.4737\n",
      "Epoch 72 train loss: 1.2903, eval loss 1.1636096239089966\n",
      "optimal threshold: -0.4652\n",
      "Epoch 73 train loss: 0.8808, eval loss 1.1573699712753296\n",
      "optimal threshold: -0.4986\n",
      "Epoch 74 train loss: 1.2005, eval loss 1.1511479616165161\n",
      "optimal threshold: -0.5093\n",
      "Epoch 75 train loss: 0.8944, eval loss 1.144990086555481\n",
      "optimal threshold: -0.5214\n",
      "Epoch 76 train loss: 0.9390, eval loss 1.138864517211914\n",
      "optimal threshold: -0.5421\n",
      "Epoch 77 train loss: 0.8120, eval loss 1.132773756980896\n",
      "optimal threshold: -0.5529\n",
      "Epoch 78 train loss: 1.1949, eval loss 1.1267123222351074\n",
      "optimal threshold: -0.5638\n",
      "Epoch 79 train loss: 0.5997, eval loss 1.1207547187805176\n",
      "optimal threshold: -0.5760\n",
      "Epoch 80 train loss: 0.7452, eval loss 1.114801049232483\n",
      "optimal threshold: -0.5846\n",
      "Epoch 81 train loss: 0.9412, eval loss 1.1089822053909302\n",
      "optimal threshold: -0.5992\n",
      "Epoch 82 train loss: 1.0347, eval loss 1.103232741355896\n",
      "optimal threshold: -0.6161\n",
      "Epoch 83 train loss: 0.9333, eval loss 1.097544550895691\n",
      "optimal threshold: -0.6331\n",
      "Epoch 84 train loss: 0.6171, eval loss 1.0918773412704468\n",
      "optimal threshold: -0.6442\n",
      "Epoch 85 train loss: 0.6565, eval loss 1.0863134860992432\n",
      "optimal threshold: -0.6528\n",
      "Epoch 86 train loss: 0.6259, eval loss 1.0808250904083252\n",
      "optimal threshold: -0.6693\n",
      "Epoch 87 train loss: 0.8031, eval loss 1.0754237174987793\n",
      "optimal threshold: -0.6707\n",
      "Epoch 88 train loss: 0.7767, eval loss 1.070119857788086\n",
      "optimal threshold: -0.6876\n",
      "Epoch 89 train loss: 1.1030, eval loss 1.0648657083511353\n",
      "optimal threshold: -0.7002\n",
      "Epoch 90 train loss: 0.4721, eval loss 1.0597361326217651\n",
      "optimal threshold: -0.7150\n",
      "Epoch 91 train loss: 0.5723, eval loss 1.054636836051941\n",
      "optimal threshold: -0.7216\n",
      "Epoch 92 train loss: 1.1286, eval loss 1.0496089458465576\n",
      "optimal threshold: -0.7334\n",
      "Epoch 93 train loss: 0.3422, eval loss 1.044683814048767\n",
      "optimal threshold: -0.7377\n",
      "Epoch 94 train loss: 0.9332, eval loss 1.039832592010498\n",
      "optimal threshold: -0.7535\n",
      "Epoch 95 train loss: 1.0750, eval loss 1.0350866317749023\n",
      "optimal threshold: -0.7613\n",
      "Epoch 96 train loss: 0.4473, eval loss 1.0304173231124878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7721\n",
      "Epoch 97 train loss: 0.4972, eval loss 1.0258228778839111\n",
      "optimal threshold: -0.7834\n",
      "Epoch 98 train loss: 0.7708, eval loss 1.0212972164154053\n",
      "optimal threshold: -0.7930\n",
      "Epoch 99 train loss: 0.8764, eval loss 1.0169042348861694\n",
      "optimal threshold: -0.8005\n",
      "Epoch 100 train loss: 0.3946, eval loss 1.0126265287399292\n",
      "optimal threshold: -0.8095\n",
      "Epoch 101 train loss: 0.4140, eval loss 1.0083577632904053\n",
      "optimal threshold: -0.8184\n",
      "Epoch 102 train loss: 0.7624, eval loss 1.0041733980178833\n",
      "optimal threshold: -0.8273\n",
      "Epoch 103 train loss: 0.7216, eval loss 1.0000834465026855\n",
      "optimal threshold: -0.8775\n",
      "Epoch 104 train loss: 0.5067, eval loss 0.9960640668869019\n",
      "optimal threshold: -0.8921\n",
      "Epoch 105 train loss: 0.5238, eval loss 0.9921545386314392\n",
      "optimal threshold: -0.8938\n",
      "Epoch 106 train loss: 0.5940, eval loss 0.9882997274398804\n",
      "optimal threshold: -0.9095\n",
      "Epoch 107 train loss: 0.6753, eval loss 0.9844989776611328\n",
      "optimal threshold: -0.9067\n",
      "Epoch 108 train loss: 0.5577, eval loss 0.9808337092399597\n",
      "optimal threshold: -0.9145\n",
      "Epoch 109 train loss: 1.0010, eval loss 0.9771987795829773\n",
      "optimal threshold: -0.9223\n",
      "Epoch 110 train loss: 0.9598, eval loss 0.9736136198043823\n",
      "optimal threshold: -0.9271\n",
      "Epoch 111 train loss: 0.8808, eval loss 0.9701334238052368\n",
      "optimal threshold: -0.9369\n",
      "Epoch 112 train loss: 0.7058, eval loss 0.9666386842727661\n",
      "optimal threshold: -0.9406\n",
      "Epoch 113 train loss: 0.8870, eval loss 0.963204562664032\n",
      "optimal threshold: -0.9474\n",
      "Epoch 114 train loss: 0.6701, eval loss 0.9599127769470215\n",
      "optimal threshold: -0.9522\n",
      "Epoch 115 train loss: 0.7982, eval loss 0.9566394090652466\n",
      "optimal threshold: -0.9582\n",
      "Epoch 116 train loss: 0.3821, eval loss 0.9534262418746948\n",
      "optimal threshold: -0.9898\n",
      "Epoch 117 train loss: 0.6104, eval loss 0.9502447843551636\n",
      "optimal threshold: -0.9816\n",
      "Epoch 118 train loss: 1.0593, eval loss 0.947134256362915\n",
      "optimal threshold: -0.9891\n",
      "Epoch 119 train loss: 0.4808, eval loss 0.9440745711326599\n",
      "optimal threshold: -0.9930\n",
      "Epoch 120 train loss: 0.9711, eval loss 0.9410208463668823\n",
      "optimal threshold: -0.9995\n",
      "Epoch 121 train loss: 0.4241, eval loss 0.9380439519882202\n",
      "optimal threshold: -1.0059\n",
      "Epoch 122 train loss: 0.4415, eval loss 0.9350736737251282\n",
      "optimal threshold: -1.0114\n",
      "Epoch 123 train loss: 0.5716, eval loss 0.9321801066398621\n",
      "optimal threshold: -1.0189\n",
      "Epoch 124 train loss: 0.6058, eval loss 0.9293109178543091\n",
      "optimal threshold: -1.0234\n",
      "Epoch 125 train loss: 0.6919, eval loss 0.9264688491821289\n",
      "optimal threshold: -0.9542\n",
      "Epoch 126 train loss: 0.6155, eval loss 0.9236515760421753\n",
      "optimal threshold: -0.9567\n",
      "Epoch 127 train loss: 0.3764, eval loss 0.9208967685699463\n",
      "optimal threshold: -0.9851\n",
      "Epoch 128 train loss: 0.4754, eval loss 0.918171763420105\n",
      "optimal threshold: -0.9852\n",
      "Epoch 129 train loss: 0.6649, eval loss 0.9155074954032898\n",
      "optimal threshold: -0.9866\n",
      "Epoch 130 train loss: 0.7864, eval loss 0.912874698638916\n",
      "optimal threshold: -0.9891\n",
      "Epoch 131 train loss: 0.8720, eval loss 0.9102872014045715\n",
      "optimal threshold: -0.9892\n",
      "Epoch 132 train loss: 0.8790, eval loss 0.9077333211898804\n",
      "optimal threshold: -0.9888\n",
      "Epoch 133 train loss: 0.6465, eval loss 0.9052100777626038\n",
      "optimal threshold: -0.9905\n",
      "Epoch 134 train loss: 0.4590, eval loss 0.9027039408683777\n",
      "optimal threshold: -0.9921\n",
      "Epoch 135 train loss: 0.6940, eval loss 0.900216281414032\n",
      "optimal threshold: -0.9939\n",
      "Epoch 136 train loss: 0.3537, eval loss 0.8977553844451904\n",
      "optimal threshold: -1.0008\n",
      "Epoch 137 train loss: 0.3855, eval loss 0.8953443765640259\n",
      "optimal threshold: -1.0253\n",
      "Epoch 138 train loss: 0.5521, eval loss 0.8929516077041626\n",
      "optimal threshold: -0.9665\n",
      "Epoch 139 train loss: 0.3058, eval loss 0.8905978202819824\n",
      "optimal threshold: -0.9689\n",
      "Epoch 140 train loss: 0.4744, eval loss 0.8882692456245422\n",
      "optimal threshold: -0.9954\n",
      "Epoch 141 train loss: 0.2677, eval loss 0.8859540820121765\n",
      "optimal threshold: -0.9956\n",
      "Epoch 142 train loss: 0.7357, eval loss 0.8836634755134583\n",
      "optimal threshold: -0.9963\n",
      "Epoch 143 train loss: 0.6885, eval loss 0.8814010620117188\n",
      "optimal threshold: -0.9972\n",
      "Epoch 144 train loss: 0.7344, eval loss 0.879177451133728\n",
      "optimal threshold: -0.9974\n",
      "Epoch 145 train loss: 0.4286, eval loss 0.8769928812980652\n",
      "optimal threshold: -1.0125\n",
      "Epoch 146 train loss: 0.5067, eval loss 0.8748131990432739\n",
      "optimal threshold: -1.0118\n",
      "Epoch 147 train loss: 0.2500, eval loss 0.8726869821548462\n",
      "optimal threshold: -1.0101\n",
      "Epoch 148 train loss: 0.3783, eval loss 0.8705781102180481\n",
      "optimal threshold: -1.0087\n",
      "Epoch 149 train loss: 0.5114, eval loss 0.8684892058372498\n",
      "optimal threshold: -1.0077\n",
      "Epoch 150 train loss: 0.7032, eval loss 0.8664574027061462\n",
      "optimal threshold: -1.0082\n",
      "Epoch 151 train loss: 0.7140, eval loss 0.864427924156189\n",
      "optimal threshold: -1.0007\n",
      "Epoch 152 train loss: 0.4691, eval loss 0.8624261617660522\n",
      "optimal threshold: -1.0027\n",
      "Epoch 153 train loss: 0.5790, eval loss 0.8604398369789124\n",
      "optimal threshold: -0.9946\n",
      "Epoch 154 train loss: 0.5865, eval loss 0.8584801554679871\n",
      "optimal threshold: -0.9926\n",
      "Epoch 155 train loss: 0.5062, eval loss 0.8565430641174316\n",
      "optimal threshold: -0.9917\n",
      "Epoch 156 train loss: 0.2802, eval loss 0.8546335101127625\n",
      "optimal threshold: -0.9988\n",
      "Epoch 157 train loss: 0.3660, eval loss 0.852742075920105\n",
      "optimal threshold: -0.9950\n",
      "Epoch 158 train loss: 0.4105, eval loss 0.8508707284927368\n",
      "optimal threshold: -0.9827\n",
      "Epoch 159 train loss: 0.4593, eval loss 0.8490309119224548\n",
      "optimal threshold: -0.9914\n",
      "Epoch 160 train loss: 0.6735, eval loss 0.8472186923027039\n",
      "optimal threshold: -0.9895\n",
      "Epoch 161 train loss: 0.4538, eval loss 0.845414400100708\n",
      "optimal threshold: -0.9740\n",
      "Epoch 162 train loss: 0.2651, eval loss 0.8436282873153687\n",
      "optimal threshold: -0.9714\n",
      "Epoch 163 train loss: 0.5762, eval loss 0.8418633341789246\n",
      "optimal threshold: -0.9795\n",
      "Epoch 164 train loss: 0.2780, eval loss 0.8401308059692383\n",
      "optimal threshold: -0.9821\n",
      "Epoch 165 train loss: 0.5278, eval loss 0.838447630405426\n",
      "optimal threshold: -0.9825\n",
      "Epoch 166 train loss: 0.3728, eval loss 0.8367717266082764\n",
      "optimal threshold: -0.9835\n",
      "Epoch 167 train loss: 0.5289, eval loss 0.8351236581802368\n",
      "optimal threshold: -0.9655\n",
      "Epoch 168 train loss: 0.2830, eval loss 0.833513081073761\n",
      "optimal threshold: -0.9610\n",
      "Epoch 169 train loss: 0.3933, eval loss 0.8319128155708313\n",
      "optimal threshold: -0.9580\n",
      "Epoch 170 train loss: 0.6492, eval loss 0.8303127884864807\n",
      "optimal threshold: -0.9694\n",
      "Epoch 171 train loss: 0.3211, eval loss 0.8287562727928162\n",
      "optimal threshold: -0.9537\n",
      "Epoch 172 train loss: 0.3745, eval loss 0.8272203207015991\n",
      "optimal threshold: -0.9510\n",
      "Epoch 173 train loss: 0.9627, eval loss 0.8257124423980713\n",
      "optimal threshold: -0.9450\n",
      "Epoch 174 train loss: 0.7807, eval loss 0.824226975440979\n",
      "optimal threshold: -0.9450\n",
      "Epoch 175 train loss: 0.4519, eval loss 0.822746217250824\n",
      "optimal threshold: -0.9440\n",
      "Epoch 176 train loss: 0.5262, eval loss 0.8212847709655762\n",
      "optimal threshold: -0.9488\n",
      "Epoch 177 train loss: 0.5164, eval loss 0.8198582530021667\n",
      "optimal threshold: -0.9458\n",
      "Epoch 178 train loss: 0.9446, eval loss 0.818449079990387\n",
      "optimal threshold: -0.9439\n",
      "Epoch 179 train loss: 0.7130, eval loss 0.8170686364173889\n",
      "optimal threshold: -0.9408\n",
      "Epoch 180 train loss: 0.7171, eval loss 0.8157011270523071\n",
      "optimal threshold: -0.9381\n",
      "Epoch 181 train loss: 0.8260, eval loss 0.8143408298492432\n",
      "optimal threshold: -0.9354\n",
      "Epoch 182 train loss: 0.4137, eval loss 0.813018262386322\n",
      "optimal threshold: -0.9426\n",
      "Epoch 183 train loss: 0.7133, eval loss 0.8117161393165588\n",
      "optimal threshold: -0.9324\n",
      "Epoch 184 train loss: 0.4014, eval loss 0.8104320764541626\n",
      "optimal threshold: -0.9471\n",
      "Epoch 185 train loss: 0.9405, eval loss 0.809146523475647\n",
      "optimal threshold: -0.9446\n",
      "Epoch 186 train loss: 0.9827, eval loss 0.8079158067703247\n",
      "optimal threshold: -0.9422\n",
      "Epoch 187 train loss: 0.4646, eval loss 0.8067049980163574\n",
      "optimal threshold: -0.9925\n",
      "Epoch 188 train loss: 0.1724, eval loss 0.8054817914962769\n",
      "optimal threshold: -0.9908\n",
      "Epoch 189 train loss: 0.2817, eval loss 0.8042815923690796\n",
      "optimal threshold: -0.9893\n",
      "Epoch 190 train loss: 0.4031, eval loss 0.8030955791473389\n",
      "optimal threshold: -0.9877\n",
      "Epoch 191 train loss: 0.2893, eval loss 0.8019141554832458\n",
      "optimal threshold: -0.9844\n",
      "Epoch 192 train loss: 0.1794, eval loss 0.8007408380508423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.9700\n",
      "Epoch 193 train loss: 0.2420, eval loss 0.7995981574058533\n",
      "optimal threshold: -0.9788\n",
      "Epoch 194 train loss: 0.2272, eval loss 0.7984744310379028\n",
      "optimal threshold: -0.9829\n",
      "Epoch 195 train loss: 0.3484, eval loss 0.797370970249176\n",
      "optimal threshold: -0.9795\n",
      "Epoch 196 train loss: 0.3441, eval loss 0.7962948083877563\n",
      "optimal threshold: -0.9779\n",
      "Epoch 197 train loss: 0.3286, eval loss 0.7952220439910889\n",
      "optimal threshold: -0.9792\n",
      "Epoch 198 train loss: 0.4555, eval loss 0.7941599488258362\n",
      "optimal threshold: -0.9788\n",
      "Epoch 199 train loss: 0.3874, eval loss 0.7931190729141235\n",
      "optimal threshold: -0.9772\n",
      "Epoch 200 train loss: 0.4707, eval loss 0.7920799851417542\n",
      "optimal threshold: -0.9761\n",
      "Epoch 201 train loss: 0.1577, eval loss 0.7910633087158203\n",
      "optimal threshold: -0.9688\n",
      "Epoch 202 train loss: 0.5161, eval loss 0.7900469303131104\n",
      "optimal threshold: -0.9681\n",
      "Epoch 203 train loss: 0.5432, eval loss 0.78907710313797\n",
      "optimal threshold: -0.9517\n",
      "Epoch 204 train loss: 0.3518, eval loss 0.7880972027778625\n",
      "optimal threshold: -1.0046\n",
      "Epoch 205 train loss: 0.3330, eval loss 0.7871389985084534\n",
      "optimal threshold: -1.0328\n",
      "Epoch 206 train loss: 0.5645, eval loss 0.7861931920051575\n",
      "optimal threshold: -1.0309\n",
      "Epoch 207 train loss: 0.1922, eval loss 0.7852596044540405\n",
      "optimal threshold: -1.0301\n",
      "Epoch 208 train loss: 0.5577, eval loss 0.7843483090400696\n",
      "optimal threshold: -1.0217\n",
      "Epoch 209 train loss: 0.8277, eval loss 0.7834315299987793\n",
      "optimal threshold: -1.0190\n",
      "Epoch 210 train loss: 0.6657, eval loss 0.7825211882591248\n",
      "optimal threshold: -1.0174\n",
      "Epoch 211 train loss: 0.4896, eval loss 0.7816416621208191\n",
      "optimal threshold: -1.0201\n",
      "Epoch 212 train loss: 0.3560, eval loss 0.7807798385620117\n",
      "optimal threshold: -1.0157\n",
      "Epoch 213 train loss: 0.3391, eval loss 0.7799267768859863\n",
      "optimal threshold: -1.0145\n",
      "Epoch 214 train loss: 0.4186, eval loss 0.7790913581848145\n",
      "optimal threshold: -1.0091\n",
      "Epoch 215 train loss: 0.2290, eval loss 0.7782518267631531\n",
      "optimal threshold: -1.0072\n",
      "Epoch 216 train loss: 0.2046, eval loss 0.7774524688720703\n",
      "optimal threshold: -1.0261\n",
      "Epoch 217 train loss: 0.3635, eval loss 0.776629626750946\n",
      "optimal threshold: -1.0242\n",
      "Epoch 218 train loss: 0.1339, eval loss 0.775824248790741\n",
      "optimal threshold: -1.0219\n",
      "Epoch 219 train loss: 0.5028, eval loss 0.7750349044799805\n",
      "optimal threshold: -1.0190\n",
      "Epoch 220 train loss: 0.1858, eval loss 0.7742414474487305\n",
      "optimal threshold: -1.0165\n",
      "Epoch 221 train loss: 0.1670, eval loss 0.7734445333480835\n",
      "optimal threshold: -1.0138\n",
      "Epoch 222 train loss: 0.4681, eval loss 0.7726695537567139\n",
      "optimal threshold: -1.0321\n",
      "Epoch 223 train loss: 0.2359, eval loss 0.7719223499298096\n",
      "optimal threshold: -1.0392\n",
      "Epoch 224 train loss: 0.4481, eval loss 0.7711651921272278\n",
      "optimal threshold: -1.0427\n",
      "Epoch 225 train loss: 0.2795, eval loss 0.7704224586486816\n",
      "optimal threshold: -1.0350\n",
      "Epoch 226 train loss: 0.2712, eval loss 0.7697086334228516\n",
      "optimal threshold: -1.0333\n",
      "Epoch 227 train loss: 0.6581, eval loss 0.7689841985702515\n",
      "optimal threshold: -1.0460\n",
      "Epoch 228 train loss: 0.3932, eval loss 0.7682710886001587\n",
      "optimal threshold: -1.0350\n",
      "Epoch 229 train loss: 0.3632, eval loss 0.7675663828849792\n",
      "optimal threshold: -1.0267\n",
      "Epoch 230 train loss: 0.2288, eval loss 0.766886293888092\n",
      "optimal threshold: -1.0241\n",
      "Epoch 231 train loss: 0.4344, eval loss 0.7661938071250916\n",
      "optimal threshold: -1.0222\n",
      "Epoch 232 train loss: 0.3080, eval loss 0.7655338644981384\n",
      "optimal threshold: -1.0215\n",
      "Epoch 233 train loss: 0.3428, eval loss 0.764859139919281\n",
      "optimal threshold: -0.9945\n",
      "Epoch 234 train loss: 0.2000, eval loss 0.764193058013916\n",
      "optimal threshold: -1.0263\n",
      "Epoch 235 train loss: 0.2039, eval loss 0.7635289430618286\n",
      "optimal threshold: -1.0194\n",
      "Epoch 236 train loss: 0.5389, eval loss 0.7628920078277588\n",
      "optimal threshold: -1.0215\n",
      "Epoch 237 train loss: 0.6699, eval loss 0.7622475028038025\n",
      "optimal threshold: -1.0091\n",
      "Epoch 238 train loss: 0.5454, eval loss 0.7616147994995117\n",
      "optimal threshold: -1.0003\n",
      "Epoch 239 train loss: 0.2666, eval loss 0.7610076069831848\n",
      "optimal threshold: -0.9986\n",
      "Epoch 240 train loss: 0.4679, eval loss 0.7604199051856995\n",
      "optimal threshold: -0.9745\n",
      "Epoch 241 train loss: 0.6475, eval loss 0.7598130106925964\n",
      "optimal threshold: -0.9744\n",
      "Epoch 242 train loss: 0.5783, eval loss 0.7592224478721619\n",
      "optimal threshold: -0.9736\n",
      "Epoch 243 train loss: 0.3442, eval loss 0.7586333155632019\n",
      "optimal threshold: -1.0085\n",
      "Epoch 244 train loss: 0.4839, eval loss 0.7580574750900269\n",
      "optimal threshold: -1.0028\n",
      "Epoch 245 train loss: 0.4350, eval loss 0.7574746608734131\n",
      "optimal threshold: -1.0056\n",
      "Epoch 246 train loss: 0.6971, eval loss 0.7569040060043335\n",
      "optimal threshold: -1.0044\n",
      "Epoch 247 train loss: 0.4115, eval loss 0.7563593983650208\n",
      "optimal threshold: -0.9946\n",
      "Epoch 248 train loss: 0.3682, eval loss 0.7557932734489441\n",
      "optimal threshold: -0.9926\n",
      "Epoch 249 train loss: 0.2261, eval loss 0.7552434206008911\n",
      "optimal threshold: -0.9900\n",
      "Epoch 250 train loss: 0.1682, eval loss 0.754676878452301\n",
      "optimal threshold: -0.9961\n",
      "Epoch 251 train loss: 0.5174, eval loss 0.7541382312774658\n",
      "optimal threshold: -0.9935\n",
      "Epoch 252 train loss: 0.6231, eval loss 0.7536134123802185\n",
      "optimal threshold: -0.9801\n",
      "Epoch 253 train loss: 0.6148, eval loss 0.7530738115310669\n",
      "optimal threshold: -0.9773\n",
      "Epoch 254 train loss: 0.3996, eval loss 0.7525527477264404\n",
      "optimal threshold: -0.9750\n",
      "Epoch 255 train loss: 0.4234, eval loss 0.752045214176178\n",
      "optimal threshold: -0.9819\n",
      "Epoch 256 train loss: 0.3022, eval loss 0.751562237739563\n",
      "optimal threshold: -0.9826\n",
      "Epoch 257 train loss: 0.2494, eval loss 0.7510569095611572\n",
      "optimal threshold: -0.9755\n",
      "Epoch 258 train loss: 0.6152, eval loss 0.7505596876144409\n",
      "optimal threshold: -0.9739\n",
      "Epoch 259 train loss: 0.1714, eval loss 0.7500717639923096\n",
      "optimal threshold: -0.9772\n",
      "Epoch 260 train loss: 0.3991, eval loss 0.7495855689048767\n",
      "optimal threshold: -0.9754\n",
      "Epoch 261 train loss: 0.3056, eval loss 0.7490919232368469\n",
      "optimal threshold: -0.9739\n",
      "Epoch 262 train loss: 0.1705, eval loss 0.7486236095428467\n",
      "optimal threshold: -0.9734\n",
      "Epoch 263 train loss: 0.3439, eval loss 0.7481601238250732\n",
      "optimal threshold: -0.9710\n",
      "Epoch 264 train loss: 0.1678, eval loss 0.7476925849914551\n",
      "optimal threshold: -0.9699\n",
      "Epoch 265 train loss: 1.1227, eval loss 0.7472335696220398\n",
      "optimal threshold: -0.9692\n",
      "Epoch 266 train loss: 0.1465, eval loss 0.7467815279960632\n",
      "optimal threshold: -0.9678\n",
      "Epoch 267 train loss: 0.6031, eval loss 0.7463334202766418\n",
      "optimal threshold: -0.9650\n",
      "Epoch 268 train loss: 0.9929, eval loss 0.7458982467651367\n",
      "optimal threshold: -0.9642\n",
      "Epoch 269 train loss: 0.2480, eval loss 0.7454591989517212\n",
      "optimal threshold: -0.9638\n",
      "Epoch 270 train loss: 0.0995, eval loss 0.7450191378593445\n",
      "optimal threshold: -0.9617\n",
      "Epoch 271 train loss: 0.2630, eval loss 0.744598925113678\n",
      "optimal threshold: -0.9613\n",
      "Epoch 272 train loss: 0.1039, eval loss 0.7441920042037964\n",
      "optimal threshold: -0.9624\n",
      "Epoch 273 train loss: 0.2260, eval loss 0.7437635660171509\n",
      "optimal threshold: -0.9600\n",
      "Epoch 274 train loss: 0.1167, eval loss 0.7433413863182068\n",
      "optimal threshold: -0.9573\n",
      "Epoch 275 train loss: 0.5930, eval loss 0.7429138422012329\n",
      "optimal threshold: -0.9564\n",
      "Epoch 276 train loss: 0.3398, eval loss 0.7425113916397095\n",
      "optimal threshold: -0.9569\n",
      "Epoch 277 train loss: 0.5589, eval loss 0.7421272993087769\n",
      "optimal threshold: -0.9563\n",
      "Epoch 278 train loss: 0.5136, eval loss 0.7417383193969727\n",
      "optimal threshold: -0.9553\n",
      "Epoch 279 train loss: 0.0894, eval loss 0.7413539290428162\n",
      "optimal threshold: -0.9163\n",
      "Epoch 280 train loss: 0.0556, eval loss 0.7409470677375793\n",
      "optimal threshold: -0.9158\n",
      "Epoch 281 train loss: 0.7088, eval loss 0.740572452545166\n",
      "optimal threshold: -0.9146\n",
      "Epoch 282 train loss: 0.2973, eval loss 0.7401794791221619\n",
      "optimal threshold: -0.9139\n",
      "Epoch 283 train loss: 0.7558, eval loss 0.7397909164428711\n",
      "optimal threshold: -0.9142\n",
      "Epoch 284 train loss: 0.1833, eval loss 0.7394030690193176\n",
      "optimal threshold: -0.9107\n",
      "Epoch 285 train loss: 0.2566, eval loss 0.7390132546424866\n",
      "optimal threshold: -0.7186\n",
      "Epoch 286 train loss: 0.3310, eval loss 0.7386485934257507\n",
      "optimal threshold: -0.7743\n",
      "Epoch 287 train loss: 0.0793, eval loss 0.7383155822753906\n",
      "optimal threshold: -0.7109\n",
      "Epoch 288 train loss: 0.4096, eval loss 0.7379398941993713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.9179\n",
      "Epoch 289 train loss: 0.1369, eval loss 0.7375904321670532\n",
      "optimal threshold: -0.7119\n",
      "Epoch 290 train loss: 0.4066, eval loss 0.7372288703918457\n",
      "optimal threshold: -0.7094\n",
      "Epoch 291 train loss: 0.3872, eval loss 0.7368818521499634\n",
      "optimal threshold: -0.7085\n",
      "Epoch 292 train loss: 0.5244, eval loss 0.7365455627441406\n",
      "optimal threshold: -0.7072\n",
      "Epoch 293 train loss: 0.4055, eval loss 0.7362181544303894\n",
      "optimal threshold: -0.7131\n",
      "Epoch 294 train loss: 0.9024, eval loss 0.7358646988868713\n",
      "optimal threshold: -0.7121\n",
      "Epoch 295 train loss: 0.6463, eval loss 0.7355167269706726\n",
      "optimal threshold: -0.7125\n",
      "Epoch 296 train loss: 0.1724, eval loss 0.7351986765861511\n",
      "optimal threshold: -0.7115\n",
      "Epoch 297 train loss: 0.0779, eval loss 0.7348805069923401\n",
      "optimal threshold: -0.7097\n",
      "Epoch 298 train loss: 0.1485, eval loss 0.7345434427261353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:17:16,932] Trial 7 finished with value: 0.603831946849823 and parameters: {'learning_rate_exp': -5.7784195094097495, 'dropout_p': 0.5105169807673813, 'l2_reg_exp': -6.654315324386599, 'batch_size': 67, 'N': 63}. Best is trial 6 with value: 0.45701825618743896.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7062\n",
      "Epoch 299 train loss: 0.6038, eval loss 0.7342162132263184\n",
      "optimal threshold: -0.3708\n",
      "Epoch 0 train loss: 0.8031, eval loss 0.6827287673950195\n",
      "optimal threshold: -0.4106\n",
      "Epoch 1 train loss: 0.7701, eval loss 0.6791986227035522\n",
      "optimal threshold: -0.5781\n",
      "Epoch 2 train loss: 0.7351, eval loss 0.673801064491272\n",
      "optimal threshold: -0.4561\n",
      "Epoch 3 train loss: 0.7769, eval loss 0.6880913972854614\n",
      "optimal threshold: -0.6279\n",
      "Epoch 4 train loss: 0.7180, eval loss 0.6783486008644104\n",
      "optimal threshold: -0.6305\n",
      "Epoch 5 train loss: 0.6792, eval loss 0.6959999203681946\n",
      "optimal threshold: -0.6261\n",
      "Epoch 6 train loss: 0.6687, eval loss 0.7062594890594482\n",
      "optimal threshold: -0.5934\n",
      "Epoch 7 train loss: 0.6074, eval loss 0.7420374155044556\n",
      "optimal threshold: -0.7542\n",
      "Epoch 8 train loss: 0.7034, eval loss 0.7426369786262512\n",
      "optimal threshold: -0.4914\n",
      "Epoch 9 train loss: 0.6899, eval loss 0.7620471715927124\n",
      "optimal threshold: -0.5496\n",
      "Epoch 10 train loss: 0.5658, eval loss 0.7654798030853271\n",
      "optimal threshold: -0.3446\n",
      "Epoch 11 train loss: 0.6291, eval loss 0.7800484895706177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:17:36,722] Trial 8 finished with value: 0.5914099812507629 and parameters: {'learning_rate_exp': -2.0296168429697206, 'dropout_p': 0.14369921774269784, 'l2_reg_exp': -6.383984431653242, 'batch_size': 88, 'N': 396}. Best is trial 6 with value: 0.45701825618743896.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5808\n",
      "optimal threshold: -0.5413\n",
      "Epoch 0 train loss: 0.6594, eval loss 0.6685562133789062\n",
      "optimal threshold: -0.5381\n",
      "Epoch 1 train loss: 0.6380, eval loss 0.6644775867462158\n",
      "optimal threshold: -0.4013\n",
      "Epoch 2 train loss: 0.6530, eval loss 0.6615296006202698\n",
      "optimal threshold: -0.2952\n",
      "Epoch 3 train loss: 0.6197, eval loss 0.6651342511177063\n",
      "optimal threshold: -0.3686\n",
      "Epoch 4 train loss: 0.6026, eval loss 0.6619963049888611\n",
      "optimal threshold: -0.3619\n",
      "Epoch 5 train loss: 0.6175, eval loss 0.661993145942688\n",
      "optimal threshold: -0.3817\n",
      "Epoch 6 train loss: 0.6138, eval loss 0.6629666090011597\n",
      "optimal threshold: -0.5533\n",
      "Epoch 7 train loss: 0.5893, eval loss 0.6666611433029175\n",
      "optimal threshold: -0.4646\n",
      "Epoch 8 train loss: 0.6436, eval loss 0.6643144488334656\n",
      "optimal threshold: -0.6342\n",
      "Epoch 9 train loss: 0.5913, eval loss 0.6715266108512878\n",
      "optimal threshold: -0.4826\n",
      "Epoch 10 train loss: 0.5642, eval loss 0.6738345623016357\n",
      "optimal threshold: -0.5267\n",
      "Epoch 11 train loss: 0.5524, eval loss 0.6743237376213074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:17:43,877] Trial 9 finished with value: 0.6383844614028931 and parameters: {'learning_rate_exp': -2.253169294109366, 'dropout_p': 0.4790413280035907, 'l2_reg_exp': -4.404257729245214, 'batch_size': 389, 'N': 324}. Best is trial 6 with value: 0.45701825618743896.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5472\n",
      "optimal threshold: -0.1038\n",
      "Epoch 0 train loss: 1.4359, eval loss 1.4371211528778076\n",
      "optimal threshold: -0.1384\n",
      "Epoch 1 train loss: 1.4272, eval loss 1.425221562385559\n",
      "optimal threshold: -0.1723\n",
      "Epoch 2 train loss: 1.4143, eval loss 1.4135215282440186\n",
      "optimal threshold: -0.2077\n",
      "Epoch 3 train loss: 1.3954, eval loss 1.4019478559494019\n",
      "optimal threshold: -0.2465\n",
      "Epoch 4 train loss: 1.3819, eval loss 1.3904528617858887\n",
      "optimal threshold: -0.2845\n",
      "Epoch 5 train loss: 1.3690, eval loss 1.378971815109253\n",
      "optimal threshold: -0.3228\n",
      "Epoch 6 train loss: 1.3650, eval loss 1.3675003051757812\n",
      "optimal threshold: -0.1698\n",
      "Epoch 7 train loss: 1.3530, eval loss 1.3560150861740112\n",
      "optimal threshold: -0.1932\n",
      "Epoch 8 train loss: 1.3414, eval loss 1.3444936275482178\n",
      "optimal threshold: -0.2088\n",
      "Epoch 9 train loss: 1.3270, eval loss 1.3329544067382812\n",
      "optimal threshold: -0.2184\n",
      "Epoch 10 train loss: 1.3125, eval loss 1.3213781118392944\n",
      "optimal threshold: -0.2444\n",
      "Epoch 11 train loss: 1.2986, eval loss 1.3097745180130005\n",
      "optimal threshold: -0.2662\n",
      "Epoch 12 train loss: 1.2844, eval loss 1.2981644868850708\n",
      "optimal threshold: -0.2753\n",
      "Epoch 13 train loss: 1.2776, eval loss 1.2865746021270752\n",
      "optimal threshold: -0.3021\n",
      "Epoch 14 train loss: 1.2601, eval loss 1.275048851966858\n",
      "optimal threshold: -0.3217\n",
      "Epoch 15 train loss: 1.2530, eval loss 1.2635362148284912\n",
      "optimal threshold: -0.3356\n",
      "Epoch 16 train loss: 1.2401, eval loss 1.2520999908447266\n",
      "optimal threshold: -0.3553\n",
      "Epoch 17 train loss: 1.2369, eval loss 1.240725040435791\n",
      "optimal threshold: -0.3793\n",
      "Epoch 18 train loss: 1.2173, eval loss 1.2294328212738037\n",
      "optimal threshold: -0.4067\n",
      "Epoch 19 train loss: 1.2073, eval loss 1.2182403802871704\n",
      "optimal threshold: -0.4331\n",
      "Epoch 20 train loss: 1.2017, eval loss 1.2071441411972046\n",
      "optimal threshold: -0.4501\n",
      "Epoch 21 train loss: 1.1811, eval loss 1.1961681842803955\n",
      "optimal threshold: -0.4692\n",
      "Epoch 22 train loss: 1.1721, eval loss 1.1853113174438477\n",
      "optimal threshold: -0.5118\n",
      "Epoch 23 train loss: 1.1634, eval loss 1.1745774745941162\n",
      "optimal threshold: -0.5351\n",
      "Epoch 24 train loss: 1.1442, eval loss 1.1639899015426636\n",
      "optimal threshold: -0.5430\n",
      "Epoch 25 train loss: 1.1368, eval loss 1.1535427570343018\n",
      "optimal threshold: -0.5658\n",
      "Epoch 26 train loss: 1.1275, eval loss 1.143242597579956\n",
      "optimal threshold: -0.5761\n",
      "Epoch 27 train loss: 1.1117, eval loss 1.1330757141113281\n",
      "optimal threshold: -0.6041\n",
      "Epoch 28 train loss: 1.1131, eval loss 1.1230542659759521\n",
      "optimal threshold: -0.6276\n",
      "Epoch 29 train loss: 1.0988, eval loss 1.1131845712661743\n",
      "optimal threshold: -0.6250\n",
      "Epoch 30 train loss: 1.0821, eval loss 1.1034648418426514\n",
      "optimal threshold: -0.6439\n",
      "Epoch 31 train loss: 1.0695, eval loss 1.093847393989563\n",
      "optimal threshold: -0.6631\n",
      "Epoch 32 train loss: 1.0765, eval loss 1.084359884262085\n",
      "optimal threshold: -0.6811\n",
      "Epoch 33 train loss: 1.0666, eval loss 1.074995756149292\n",
      "optimal threshold: -0.6909\n",
      "Epoch 34 train loss: 1.0325, eval loss 1.0657356977462769\n",
      "optimal threshold: -0.6992\n",
      "Epoch 35 train loss: 1.0486, eval loss 1.0565931797027588\n",
      "optimal threshold: -0.7073\n",
      "Epoch 36 train loss: 1.0275, eval loss 1.0475497245788574\n",
      "optimal threshold: -0.7446\n",
      "Epoch 37 train loss: 1.0217, eval loss 1.0385900735855103\n",
      "optimal threshold: -0.7590\n",
      "Epoch 38 train loss: 1.0157, eval loss 1.0297493934631348\n",
      "optimal threshold: -0.7633\n",
      "Epoch 39 train loss: 1.0132, eval loss 1.020999789237976\n",
      "optimal threshold: -0.7747\n",
      "Epoch 40 train loss: 1.0018, eval loss 1.0123560428619385\n",
      "optimal threshold: -0.7830\n",
      "Epoch 41 train loss: 0.9941, eval loss 1.0038278102874756\n",
      "optimal threshold: -0.7848\n",
      "Epoch 42 train loss: 0.9810, eval loss 0.9953972101211548\n",
      "optimal threshold: -0.7825\n",
      "Epoch 43 train loss: 0.9651, eval loss 0.9870669841766357\n",
      "optimal threshold: -0.8114\n",
      "Epoch 44 train loss: 0.9704, eval loss 0.9788543581962585\n",
      "optimal threshold: -0.8166\n",
      "Epoch 45 train loss: 0.9539, eval loss 0.970754861831665\n",
      "optimal threshold: -0.8246\n",
      "Epoch 46 train loss: 0.9394, eval loss 0.9627725481987\n",
      "optimal threshold: -0.8127\n",
      "Epoch 47 train loss: 0.9414, eval loss 0.954910159111023\n",
      "optimal threshold: -0.8208\n",
      "Epoch 48 train loss: 0.9206, eval loss 0.947181761264801\n",
      "optimal threshold: -0.8207\n",
      "Epoch 49 train loss: 0.9172, eval loss 0.9395837783813477\n",
      "optimal threshold: -0.8274\n",
      "Epoch 50 train loss: 0.9155, eval loss 0.9321069717407227\n",
      "optimal threshold: -0.8322\n",
      "Epoch 51 train loss: 0.9054, eval loss 0.924764096736908\n",
      "optimal threshold: -0.8376\n",
      "Epoch 52 train loss: 0.8985, eval loss 0.9175602197647095\n",
      "optimal threshold: -0.8424\n",
      "Epoch 53 train loss: 0.8983, eval loss 0.9104989171028137\n",
      "optimal threshold: -0.8499\n",
      "Epoch 54 train loss: 0.8936, eval loss 0.903586745262146\n",
      "optimal threshold: -0.8560\n",
      "Epoch 55 train loss: 0.8837, eval loss 0.8968143463134766\n",
      "optimal threshold: -0.8576\n",
      "Epoch 56 train loss: 0.8940, eval loss 0.8901766538619995\n",
      "optimal threshold: -0.8890\n",
      "Epoch 57 train loss: 0.8757, eval loss 0.883701741695404\n",
      "optimal threshold: -0.8805\n",
      "Epoch 58 train loss: 0.8688, eval loss 0.8773685693740845\n",
      "optimal threshold: -0.8616\n",
      "Epoch 59 train loss: 0.8606, eval loss 0.8711780905723572\n",
      "optimal threshold: -0.8500\n",
      "Epoch 60 train loss: 0.8381, eval loss 0.8651513457298279\n",
      "optimal threshold: -0.8563\n",
      "Epoch 61 train loss: 0.8504, eval loss 0.8592813611030579\n",
      "optimal threshold: -0.8585\n",
      "Epoch 62 train loss: 0.8511, eval loss 0.8535593748092651\n",
      "optimal threshold: -0.8585\n",
      "Epoch 63 train loss: 0.8437, eval loss 0.8479750752449036\n",
      "optimal threshold: -0.8600\n",
      "Epoch 64 train loss: 0.8237, eval loss 0.8425484895706177\n",
      "optimal threshold: -0.8579\n",
      "Epoch 65 train loss: 0.8331, eval loss 0.8372610211372375\n",
      "optimal threshold: -0.8612\n",
      "Epoch 66 train loss: 0.8331, eval loss 0.83213210105896\n",
      "optimal threshold: -0.8611\n",
      "Epoch 67 train loss: 0.8118, eval loss 0.8271695375442505\n",
      "optimal threshold: -0.8642\n",
      "Epoch 68 train loss: 0.8128, eval loss 0.8223723769187927\n",
      "optimal threshold: -0.8645\n",
      "Epoch 69 train loss: 0.8125, eval loss 0.8176814913749695\n",
      "optimal threshold: -0.8655\n",
      "Epoch 70 train loss: 0.8172, eval loss 0.8131580352783203\n",
      "optimal threshold: -0.8631\n",
      "Epoch 71 train loss: 0.7819, eval loss 0.8087900876998901\n",
      "optimal threshold: -0.8640\n",
      "Epoch 72 train loss: 0.7952, eval loss 0.8045644164085388\n",
      "optimal threshold: -0.8710\n",
      "Epoch 73 train loss: 0.7827, eval loss 0.8004754185676575\n",
      "optimal threshold: -0.8645\n",
      "Epoch 74 train loss: 0.7958, eval loss 0.7965277433395386\n",
      "optimal threshold: -0.8658\n",
      "Epoch 75 train loss: 0.7745, eval loss 0.792730987071991\n",
      "optimal threshold: -0.8638\n",
      "Epoch 76 train loss: 0.7723, eval loss 0.7890834212303162\n",
      "optimal threshold: -0.8596\n",
      "Epoch 77 train loss: 0.7685, eval loss 0.7855498790740967\n",
      "optimal threshold: -0.8586\n",
      "Epoch 78 train loss: 0.7534, eval loss 0.7821596264839172\n",
      "optimal threshold: -0.8582\n",
      "Epoch 79 train loss: 0.7550, eval loss 0.7788903713226318\n",
      "optimal threshold: -0.8615\n",
      "Epoch 80 train loss: 0.7741, eval loss 0.7757219076156616\n",
      "optimal threshold: -0.8586\n",
      "Epoch 81 train loss: 0.7631, eval loss 0.7726802229881287\n",
      "optimal threshold: -0.8578\n",
      "Epoch 82 train loss: 0.7483, eval loss 0.7697626352310181\n",
      "optimal threshold: -0.8670\n",
      "Epoch 83 train loss: 0.7531, eval loss 0.7669471502304077\n",
      "optimal threshold: -0.8651\n",
      "Epoch 84 train loss: 0.7423, eval loss 0.7642416954040527\n",
      "optimal threshold: -0.8649\n",
      "Epoch 85 train loss: 0.7559, eval loss 0.7616513967514038\n",
      "optimal threshold: -0.8589\n",
      "Epoch 86 train loss: 0.7466, eval loss 0.7591807246208191\n",
      "optimal threshold: -0.8508\n",
      "Epoch 87 train loss: 0.7835, eval loss 0.7568020820617676\n",
      "optimal threshold: -0.8530\n",
      "Epoch 88 train loss: 0.7694, eval loss 0.754514753818512\n",
      "optimal threshold: -0.8583\n",
      "Epoch 89 train loss: 0.7217, eval loss 0.7523302435874939\n",
      "optimal threshold: -0.8535\n",
      "Epoch 90 train loss: 0.7623, eval loss 0.7502400875091553\n",
      "optimal threshold: -0.8646\n",
      "Epoch 91 train loss: 0.7511, eval loss 0.7482196688652039\n",
      "optimal threshold: -0.8627\n",
      "Epoch 92 train loss: 0.7556, eval loss 0.7462854981422424\n",
      "optimal threshold: -0.8551\n",
      "Epoch 93 train loss: 0.7476, eval loss 0.744440495967865\n",
      "optimal threshold: -0.8529\n",
      "Epoch 94 train loss: 0.7918, eval loss 0.7426777482032776\n",
      "optimal threshold: -0.8500\n",
      "Epoch 95 train loss: 0.7160, eval loss 0.7409871220588684\n",
      "optimal threshold: -0.8519\n",
      "Epoch 96 train loss: 0.7335, eval loss 0.7393521666526794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.9895\n",
      "Epoch 97 train loss: 0.7229, eval loss 0.7377871870994568\n",
      "optimal threshold: -0.9880\n",
      "Epoch 98 train loss: 0.7529, eval loss 0.7362746000289917\n",
      "optimal threshold: -0.9872\n",
      "Epoch 99 train loss: 0.7422, eval loss 0.7348470091819763\n",
      "optimal threshold: -0.9934\n",
      "Epoch 100 train loss: 0.7264, eval loss 0.7334577441215515\n",
      "optimal threshold: -0.9977\n",
      "Epoch 101 train loss: 0.7571, eval loss 0.7321114540100098\n",
      "optimal threshold: -0.9996\n",
      "Epoch 102 train loss: 0.7240, eval loss 0.7308101058006287\n",
      "optimal threshold: -1.0003\n",
      "Epoch 103 train loss: 0.7537, eval loss 0.7295624613761902\n",
      "optimal threshold: -0.9799\n",
      "Epoch 104 train loss: 0.7254, eval loss 0.7283722758293152\n",
      "optimal threshold: -1.0303\n",
      "Epoch 105 train loss: 0.7327, eval loss 0.7272209525108337\n",
      "optimal threshold: -1.0313\n",
      "Epoch 106 train loss: 0.7162, eval loss 0.7261098027229309\n",
      "optimal threshold: -1.0128\n",
      "Epoch 107 train loss: 0.7362, eval loss 0.7250645160675049\n",
      "optimal threshold: -0.9924\n",
      "Epoch 108 train loss: 0.7331, eval loss 0.724044919013977\n",
      "optimal threshold: -0.9925\n",
      "Epoch 109 train loss: 0.7373, eval loss 0.7230510711669922\n",
      "optimal threshold: -1.0003\n",
      "Epoch 110 train loss: 0.7165, eval loss 0.7220932841300964\n",
      "optimal threshold: -1.0067\n",
      "Epoch 111 train loss: 0.7487, eval loss 0.7211713790893555\n",
      "optimal threshold: -1.0028\n",
      "Epoch 112 train loss: 0.7571, eval loss 0.7202645540237427\n",
      "optimal threshold: -1.0128\n",
      "Epoch 113 train loss: 0.7491, eval loss 0.7193876504898071\n",
      "optimal threshold: -0.9761\n",
      "Epoch 114 train loss: 0.7143, eval loss 0.718560516834259\n",
      "optimal threshold: -0.9817\n",
      "Epoch 115 train loss: 0.7359, eval loss 0.717754602432251\n",
      "optimal threshold: -1.0113\n",
      "Epoch 116 train loss: 0.7160, eval loss 0.7169650793075562\n",
      "optimal threshold: -0.9678\n",
      "Epoch 117 train loss: 0.7146, eval loss 0.7162107229232788\n",
      "optimal threshold: -0.9660\n",
      "Epoch 118 train loss: 0.7136, eval loss 0.7154537439346313\n",
      "optimal threshold: -0.9637\n",
      "Epoch 119 train loss: 0.7479, eval loss 0.7147088050842285\n",
      "optimal threshold: -0.9634\n",
      "Epoch 120 train loss: 0.7214, eval loss 0.7139933109283447\n",
      "optimal threshold: -0.9393\n",
      "Epoch 121 train loss: 0.7006, eval loss 0.7132924199104309\n",
      "optimal threshold: -0.9355\n",
      "Epoch 122 train loss: 0.7333, eval loss 0.7126197218894958\n",
      "optimal threshold: -0.9352\n",
      "Epoch 123 train loss: 0.7196, eval loss 0.7119487524032593\n",
      "optimal threshold: -0.8775\n",
      "Epoch 124 train loss: 0.7147, eval loss 0.7113139033317566\n",
      "optimal threshold: -0.8755\n",
      "Epoch 125 train loss: 0.7138, eval loss 0.7106786370277405\n",
      "optimal threshold: -0.7224\n",
      "Epoch 126 train loss: 0.7202, eval loss 0.7100604176521301\n",
      "optimal threshold: -0.7242\n",
      "Epoch 127 train loss: 0.7050, eval loss 0.7094443440437317\n",
      "optimal threshold: -0.8901\n",
      "Epoch 128 train loss: 0.7192, eval loss 0.7088392972946167\n",
      "optimal threshold: -0.7856\n",
      "Epoch 129 train loss: 0.6797, eval loss 0.7082666754722595\n",
      "optimal threshold: -0.6715\n",
      "Epoch 130 train loss: 0.7228, eval loss 0.7076993584632874\n",
      "optimal threshold: -0.6745\n",
      "Epoch 131 train loss: 0.7171, eval loss 0.7071294784545898\n",
      "optimal threshold: -0.6670\n",
      "Epoch 132 train loss: 0.6879, eval loss 0.7065582275390625\n",
      "optimal threshold: -0.6669\n",
      "Epoch 133 train loss: 0.7276, eval loss 0.7060221433639526\n",
      "optimal threshold: -0.8933\n",
      "Epoch 134 train loss: 0.7049, eval loss 0.7054886221885681\n",
      "optimal threshold: -0.8898\n",
      "Epoch 135 train loss: 0.6833, eval loss 0.7049704790115356\n",
      "optimal threshold: -0.6916\n",
      "Epoch 136 train loss: 0.7244, eval loss 0.7044402956962585\n",
      "optimal threshold: -0.6869\n",
      "Epoch 137 train loss: 0.7055, eval loss 0.7039113640785217\n",
      "optimal threshold: -0.6907\n",
      "Epoch 138 train loss: 0.7377, eval loss 0.7034052014350891\n",
      "optimal threshold: -0.6793\n",
      "Epoch 139 train loss: 0.6951, eval loss 0.702898383140564\n",
      "optimal threshold: -0.7341\n",
      "Epoch 140 train loss: 0.7231, eval loss 0.7024189829826355\n",
      "optimal threshold: -0.7312\n",
      "Epoch 141 train loss: 0.7089, eval loss 0.7019359469413757\n",
      "optimal threshold: -0.7376\n",
      "Epoch 142 train loss: 0.7191, eval loss 0.7014660239219666\n",
      "optimal threshold: -0.7344\n",
      "Epoch 143 train loss: 0.6942, eval loss 0.7009938359260559\n",
      "optimal threshold: -0.7395\n",
      "Epoch 144 train loss: 0.7003, eval loss 0.7005487084388733\n",
      "optimal threshold: -0.7365\n",
      "Epoch 145 train loss: 0.7191, eval loss 0.7001073956489563\n",
      "optimal threshold: -0.7319\n",
      "Epoch 146 train loss: 0.6995, eval loss 0.6996577978134155\n",
      "optimal threshold: -0.7408\n",
      "Epoch 147 train loss: 0.7116, eval loss 0.6992061734199524\n",
      "optimal threshold: -0.7396\n",
      "Epoch 148 train loss: 0.7339, eval loss 0.6987676620483398\n",
      "optimal threshold: -0.7304\n",
      "Epoch 149 train loss: 0.6565, eval loss 0.6983323693275452\n",
      "optimal threshold: -0.7233\n",
      "Epoch 150 train loss: 0.7241, eval loss 0.697920560836792\n",
      "optimal threshold: -0.7240\n",
      "Epoch 151 train loss: 0.7548, eval loss 0.6974976658821106\n",
      "optimal threshold: -0.7205\n",
      "Epoch 152 train loss: 0.7024, eval loss 0.6970906257629395\n",
      "optimal threshold: -0.7260\n",
      "Epoch 153 train loss: 0.6830, eval loss 0.6966843008995056\n",
      "optimal threshold: -0.7417\n",
      "Epoch 154 train loss: 0.6865, eval loss 0.6962770223617554\n",
      "optimal threshold: -0.6374\n",
      "Epoch 155 train loss: 0.7057, eval loss 0.6958926320075989\n",
      "optimal threshold: -0.6352\n",
      "Epoch 156 train loss: 0.7228, eval loss 0.6955105662345886\n",
      "optimal threshold: -0.7035\n",
      "Epoch 157 train loss: 0.6863, eval loss 0.6951148509979248\n",
      "optimal threshold: -0.6331\n",
      "Epoch 158 train loss: 0.6653, eval loss 0.694713830947876\n",
      "optimal threshold: -0.7011\n",
      "Epoch 159 train loss: 0.6754, eval loss 0.6943416595458984\n",
      "optimal threshold: -0.7038\n",
      "Epoch 160 train loss: 0.7014, eval loss 0.6939669847488403\n",
      "optimal threshold: -0.7044\n",
      "Epoch 161 train loss: 0.6857, eval loss 0.6935943365097046\n",
      "optimal threshold: -0.7030\n",
      "Epoch 162 train loss: 0.7139, eval loss 0.6932201385498047\n",
      "optimal threshold: -0.7029\n",
      "Epoch 163 train loss: 0.6952, eval loss 0.6928701400756836\n",
      "optimal threshold: -0.9617\n",
      "Epoch 164 train loss: 0.7053, eval loss 0.6925162672996521\n",
      "optimal threshold: -0.9609\n",
      "Epoch 165 train loss: 0.6963, eval loss 0.6921553015708923\n",
      "optimal threshold: -0.9614\n",
      "Epoch 166 train loss: 0.7013, eval loss 0.6918131113052368\n",
      "optimal threshold: -0.6422\n",
      "Epoch 167 train loss: 0.7058, eval loss 0.691474437713623\n",
      "optimal threshold: -0.6361\n",
      "Epoch 168 train loss: 0.7014, eval loss 0.6911301612854004\n",
      "optimal threshold: -0.6390\n",
      "Epoch 169 train loss: 0.7003, eval loss 0.6907830238342285\n",
      "optimal threshold: -0.6389\n",
      "Epoch 170 train loss: 0.6897, eval loss 0.6904486417770386\n",
      "optimal threshold: -0.9629\n",
      "Epoch 171 train loss: 0.6765, eval loss 0.6901166439056396\n",
      "optimal threshold: -0.9634\n",
      "Epoch 172 train loss: 0.6953, eval loss 0.689786434173584\n",
      "optimal threshold: -0.6243\n",
      "Epoch 173 train loss: 0.7215, eval loss 0.6894652843475342\n",
      "optimal threshold: -0.6217\n",
      "Epoch 174 train loss: 0.7300, eval loss 0.6891602277755737\n",
      "optimal threshold: -0.9874\n",
      "Epoch 175 train loss: 0.7206, eval loss 0.6888474822044373\n",
      "optimal threshold: -0.9901\n",
      "Epoch 176 train loss: 0.6744, eval loss 0.6885214447975159\n",
      "optimal threshold: -0.9837\n",
      "Epoch 177 train loss: 0.6587, eval loss 0.6882044672966003\n",
      "optimal threshold: -0.9802\n",
      "Epoch 178 train loss: 0.6877, eval loss 0.6878743171691895\n",
      "optimal threshold: -0.9829\n",
      "Epoch 179 train loss: 0.6604, eval loss 0.6875882148742676\n",
      "optimal threshold: -0.9878\n",
      "Epoch 180 train loss: 0.7509, eval loss 0.6872851252555847\n",
      "optimal threshold: -0.5782\n",
      "Epoch 181 train loss: 0.6775, eval loss 0.6869891285896301\n",
      "optimal threshold: -0.7593\n",
      "Epoch 182 train loss: 0.7144, eval loss 0.686697781085968\n",
      "optimal threshold: -0.7661\n",
      "Epoch 183 train loss: 0.7446, eval loss 0.6864002346992493\n",
      "optimal threshold: -0.7701\n",
      "Epoch 184 train loss: 0.7179, eval loss 0.6861156821250916\n",
      "optimal threshold: -0.7701\n",
      "Epoch 185 train loss: 0.6857, eval loss 0.6858369708061218\n",
      "optimal threshold: -0.7742\n",
      "Epoch 186 train loss: 0.6701, eval loss 0.6855619549751282\n",
      "optimal threshold: -0.7754\n",
      "Epoch 187 train loss: 0.6751, eval loss 0.685303807258606\n",
      "optimal threshold: -0.5893\n",
      "Epoch 188 train loss: 0.7071, eval loss 0.685005784034729\n",
      "optimal threshold: -0.5669\n",
      "Epoch 189 train loss: 0.6923, eval loss 0.6847358345985413\n",
      "optimal threshold: -0.5702\n",
      "Epoch 190 train loss: 0.6587, eval loss 0.6844615340232849\n",
      "optimal threshold: -0.5934\n",
      "Epoch 191 train loss: 0.6866, eval loss 0.6842085719108582\n",
      "optimal threshold: -0.5659\n",
      "Epoch 192 train loss: 0.6808, eval loss 0.6839412450790405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5624\n",
      "Epoch 193 train loss: 0.6731, eval loss 0.68369060754776\n",
      "optimal threshold: -0.5721\n",
      "Epoch 194 train loss: 0.6795, eval loss 0.6834485530853271\n",
      "optimal threshold: -0.5632\n",
      "Epoch 195 train loss: 0.7187, eval loss 0.6832024455070496\n",
      "optimal threshold: -0.5609\n",
      "Epoch 196 train loss: 0.6850, eval loss 0.6829476356506348\n",
      "optimal threshold: -0.5601\n",
      "Epoch 197 train loss: 0.6672, eval loss 0.6826834082603455\n",
      "optimal threshold: -0.5635\n",
      "Epoch 198 train loss: 0.6911, eval loss 0.6824588179588318\n",
      "optimal threshold: -0.5613\n",
      "Epoch 199 train loss: 0.6791, eval loss 0.6822018623352051\n",
      "optimal threshold: -0.5633\n",
      "Epoch 200 train loss: 0.6843, eval loss 0.6819465756416321\n",
      "optimal threshold: -0.5634\n",
      "Epoch 201 train loss: 0.6844, eval loss 0.681723952293396\n",
      "optimal threshold: -0.5690\n",
      "Epoch 202 train loss: 0.6964, eval loss 0.6814932823181152\n",
      "optimal threshold: -0.5529\n",
      "Epoch 203 train loss: 0.7260, eval loss 0.6812534928321838\n",
      "optimal threshold: -0.5548\n",
      "Epoch 204 train loss: 0.7288, eval loss 0.6810328960418701\n",
      "optimal threshold: -0.5528\n",
      "Epoch 205 train loss: 0.6652, eval loss 0.6808000802993774\n",
      "optimal threshold: -0.5450\n",
      "Epoch 206 train loss: 0.6647, eval loss 0.6805669665336609\n",
      "optimal threshold: -0.5457\n",
      "Epoch 207 train loss: 0.6841, eval loss 0.6803429126739502\n",
      "optimal threshold: -0.4786\n",
      "Epoch 208 train loss: 0.6798, eval loss 0.6801370978355408\n",
      "optimal threshold: -0.5046\n",
      "Epoch 209 train loss: 0.6645, eval loss 0.6799190640449524\n",
      "optimal threshold: -0.4757\n",
      "Epoch 210 train loss: 0.6902, eval loss 0.6796852946281433\n",
      "optimal threshold: -0.6507\n",
      "Epoch 211 train loss: 0.6720, eval loss 0.6794604659080505\n",
      "optimal threshold: -0.6507\n",
      "Epoch 212 train loss: 0.6824, eval loss 0.6792563199996948\n",
      "optimal threshold: -0.6498\n",
      "Epoch 213 train loss: 0.6731, eval loss 0.6790378093719482\n",
      "optimal threshold: -0.6489\n",
      "Epoch 214 train loss: 0.6518, eval loss 0.6788244843482971\n",
      "optimal threshold: -0.6417\n",
      "Epoch 215 train loss: 0.6604, eval loss 0.6786283254623413\n",
      "optimal threshold: -0.5273\n",
      "Epoch 216 train loss: 0.7027, eval loss 0.6784416437149048\n",
      "optimal threshold: -0.6503\n",
      "Epoch 217 train loss: 0.6900, eval loss 0.6782400012016296\n",
      "optimal threshold: -0.5223\n",
      "Epoch 218 train loss: 0.6962, eval loss 0.6780509948730469\n",
      "optimal threshold: -0.5219\n",
      "Epoch 219 train loss: 0.7072, eval loss 0.6778577566146851\n",
      "optimal threshold: -0.5221\n",
      "Epoch 220 train loss: 0.6852, eval loss 0.6776798963546753\n",
      "optimal threshold: -0.6542\n",
      "Epoch 221 train loss: 0.6108, eval loss 0.6774815917015076\n",
      "optimal threshold: -0.6530\n",
      "Epoch 222 train loss: 0.7335, eval loss 0.677303671836853\n",
      "optimal threshold: -0.6529\n",
      "Epoch 223 train loss: 0.6925, eval loss 0.6771245002746582\n",
      "optimal threshold: -0.6525\n",
      "Epoch 224 train loss: 0.6839, eval loss 0.6769231557846069\n",
      "optimal threshold: -0.6526\n",
      "Epoch 225 train loss: 0.6732, eval loss 0.676741361618042\n",
      "optimal threshold: -0.6523\n",
      "Epoch 226 train loss: 0.6761, eval loss 0.6765556931495667\n",
      "optimal threshold: -0.6523\n",
      "Epoch 227 train loss: 0.7015, eval loss 0.6763789653778076\n",
      "optimal threshold: -0.6532\n",
      "Epoch 228 train loss: 0.7071, eval loss 0.6762146353721619\n",
      "optimal threshold: -0.6589\n",
      "Epoch 229 train loss: 0.6668, eval loss 0.6760745644569397\n",
      "optimal threshold: -0.6560\n",
      "Epoch 230 train loss: 0.6666, eval loss 0.6758880019187927\n",
      "optimal threshold: -0.6553\n",
      "Epoch 231 train loss: 0.6928, eval loss 0.6757208704948425\n",
      "optimal threshold: -0.6616\n",
      "Epoch 232 train loss: 0.7252, eval loss 0.6755399107933044\n",
      "optimal threshold: -0.6597\n",
      "Epoch 233 train loss: 0.6869, eval loss 0.6753807663917542\n",
      "optimal threshold: -0.6573\n",
      "Epoch 234 train loss: 0.6927, eval loss 0.675205647945404\n",
      "optimal threshold: -0.6561\n",
      "Epoch 235 train loss: 0.6467, eval loss 0.6750509738922119\n",
      "optimal threshold: -0.6595\n",
      "Epoch 236 train loss: 0.6575, eval loss 0.6748772263526917\n",
      "optimal threshold: -0.6568\n",
      "Epoch 237 train loss: 0.6500, eval loss 0.6747128367424011\n",
      "optimal threshold: -0.6620\n",
      "Epoch 238 train loss: 0.6754, eval loss 0.6745539307594299\n",
      "optimal threshold: -0.6581\n",
      "Epoch 239 train loss: 0.6956, eval loss 0.6744006276130676\n",
      "optimal threshold: -0.6577\n",
      "Epoch 240 train loss: 0.6626, eval loss 0.6742479801177979\n",
      "optimal threshold: -0.6592\n",
      "Epoch 241 train loss: 0.6750, eval loss 0.6740882992744446\n",
      "optimal threshold: -0.6595\n",
      "Epoch 242 train loss: 0.7046, eval loss 0.6739404201507568\n",
      "optimal threshold: -0.6616\n",
      "Epoch 243 train loss: 0.6875, eval loss 0.6738018989562988\n",
      "optimal threshold: -0.6626\n",
      "Epoch 244 train loss: 0.6474, eval loss 0.6736488342285156\n",
      "optimal threshold: -0.6616\n",
      "Epoch 245 train loss: 0.6982, eval loss 0.6735195517539978\n",
      "optimal threshold: -0.6620\n",
      "Epoch 246 train loss: 0.6823, eval loss 0.6733787655830383\n",
      "optimal threshold: -0.6327\n",
      "Epoch 247 train loss: 0.6943, eval loss 0.6732036471366882\n",
      "optimal threshold: -0.6311\n",
      "Epoch 248 train loss: 0.6441, eval loss 0.6730631589889526\n",
      "optimal threshold: -0.6309\n",
      "Epoch 249 train loss: 0.6652, eval loss 0.6729472875595093\n",
      "optimal threshold: -0.6308\n",
      "Epoch 250 train loss: 0.7039, eval loss 0.6728127002716064\n",
      "optimal threshold: -0.6313\n",
      "Epoch 251 train loss: 0.6822, eval loss 0.6726833581924438\n",
      "optimal threshold: -0.6512\n",
      "Epoch 252 train loss: 0.6697, eval loss 0.6725432276725769\n",
      "optimal threshold: -0.6482\n",
      "Epoch 253 train loss: 0.6492, eval loss 0.6723855137825012\n",
      "optimal threshold: -0.6442\n",
      "Epoch 254 train loss: 0.7039, eval loss 0.672257125377655\n",
      "optimal threshold: -0.6438\n",
      "Epoch 255 train loss: 0.6672, eval loss 0.6721395254135132\n",
      "optimal threshold: -0.6386\n",
      "Epoch 256 train loss: 0.6753, eval loss 0.6719875335693359\n",
      "optimal threshold: -0.6372\n",
      "Epoch 257 train loss: 0.6611, eval loss 0.671846866607666\n",
      "optimal threshold: -0.6531\n",
      "Epoch 258 train loss: 0.6286, eval loss 0.671739399433136\n",
      "optimal threshold: -0.6385\n",
      "Epoch 259 train loss: 0.6697, eval loss 0.6716256737709045\n",
      "optimal threshold: -0.6588\n",
      "Epoch 260 train loss: 0.6815, eval loss 0.6715067625045776\n",
      "optimal threshold: -0.6530\n",
      "Epoch 261 train loss: 0.6357, eval loss 0.6713817715644836\n",
      "optimal threshold: -0.6532\n",
      "Epoch 262 train loss: 0.6667, eval loss 0.6712551116943359\n",
      "optimal threshold: -0.6543\n",
      "Epoch 263 train loss: 0.6222, eval loss 0.6711255311965942\n",
      "optimal threshold: -0.6258\n",
      "Epoch 264 train loss: 0.6588, eval loss 0.6709902882575989\n",
      "optimal threshold: -0.6262\n",
      "Epoch 265 train loss: 0.6772, eval loss 0.6708764433860779\n",
      "optimal threshold: -0.6257\n",
      "Epoch 266 train loss: 0.6346, eval loss 0.670760452747345\n",
      "optimal threshold: -0.6353\n",
      "Epoch 267 train loss: 0.6892, eval loss 0.6706448197364807\n",
      "optimal threshold: -0.6274\n",
      "Epoch 268 train loss: 0.6632, eval loss 0.670514702796936\n",
      "optimal threshold: -0.6354\n",
      "Epoch 269 train loss: 0.6651, eval loss 0.6704186797142029\n",
      "optimal threshold: -0.6342\n",
      "Epoch 270 train loss: 0.6636, eval loss 0.6702868342399597\n",
      "optimal threshold: -0.6369\n",
      "Epoch 271 train loss: 0.6814, eval loss 0.6701735854148865\n",
      "optimal threshold: -0.6244\n",
      "Epoch 272 train loss: 0.6506, eval loss 0.6700596213340759\n",
      "optimal threshold: -0.6362\n",
      "Epoch 273 train loss: 0.6743, eval loss 0.6699543595314026\n",
      "optimal threshold: -0.6227\n",
      "Epoch 274 train loss: 0.6694, eval loss 0.669815719127655\n",
      "optimal threshold: -0.6250\n",
      "Epoch 275 train loss: 0.6211, eval loss 0.6697095036506653\n",
      "optimal threshold: -0.6172\n",
      "Epoch 276 train loss: 0.6810, eval loss 0.669604480266571\n",
      "optimal threshold: -0.6163\n",
      "Epoch 277 train loss: 0.6370, eval loss 0.6695114970207214\n",
      "optimal threshold: -0.6151\n",
      "Epoch 278 train loss: 0.6154, eval loss 0.6693951487541199\n",
      "optimal threshold: -0.6212\n",
      "Epoch 279 train loss: 0.6823, eval loss 0.6692978739738464\n",
      "optimal threshold: -0.6201\n",
      "Epoch 280 train loss: 0.6487, eval loss 0.6691904664039612\n",
      "optimal threshold: -0.6162\n",
      "Epoch 281 train loss: 0.6969, eval loss 0.669099748134613\n",
      "optimal threshold: -0.6176\n",
      "Epoch 282 train loss: 0.6589, eval loss 0.6690276861190796\n",
      "optimal threshold: -0.6193\n",
      "Epoch 283 train loss: 0.6350, eval loss 0.6689236760139465\n",
      "optimal threshold: -0.6162\n",
      "Epoch 284 train loss: 0.6929, eval loss 0.6688202619552612\n",
      "optimal threshold: -0.6162\n",
      "Epoch 285 train loss: 0.6407, eval loss 0.6687235832214355\n",
      "optimal threshold: -0.6209\n",
      "Epoch 286 train loss: 0.6687, eval loss 0.6686215996742249\n",
      "optimal threshold: -0.6194\n",
      "Epoch 287 train loss: 0.6944, eval loss 0.6685088872909546\n",
      "optimal threshold: -0.6280\n",
      "Epoch 288 train loss: 0.6249, eval loss 0.668412446975708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6292\n",
      "Epoch 289 train loss: 0.6430, eval loss 0.6683225631713867\n",
      "optimal threshold: -0.6175\n",
      "Epoch 290 train loss: 0.6116, eval loss 0.6682490706443787\n",
      "optimal threshold: -0.6195\n",
      "Epoch 291 train loss: 0.6105, eval loss 0.6681380271911621\n",
      "optimal threshold: -0.6232\n",
      "Epoch 292 train loss: 0.6826, eval loss 0.6680541038513184\n",
      "optimal threshold: -0.6203\n",
      "Epoch 293 train loss: 0.6627, eval loss 0.6679701805114746\n",
      "optimal threshold: -0.6181\n",
      "Epoch 294 train loss: 0.7068, eval loss 0.6678630113601685\n",
      "optimal threshold: -0.6176\n",
      "Epoch 295 train loss: 0.6343, eval loss 0.6677731275558472\n",
      "optimal threshold: -0.6191\n",
      "Epoch 296 train loss: 0.6711, eval loss 0.6677005887031555\n",
      "optimal threshold: -0.6186\n",
      "Epoch 297 train loss: 0.6363, eval loss 0.6676100492477417\n",
      "optimal threshold: -0.6182\n",
      "Epoch 298 train loss: 0.6642, eval loss 0.6675081849098206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:21:53,793] Trial 10 finished with value: 0.6412184238433838 and parameters: {'learning_rate_exp': -5.664163860192817, 'dropout_p': 0.15539736069054844, 'l2_reg_exp': -5.784715445636765, 'batch_size': 281, 'N': 458}. Best is trial 6 with value: 0.45701825618743896.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6181\n",
      "Epoch 299 train loss: 0.6412, eval loss 0.6674327254295349\n",
      "optimal threshold: -0.6609\n",
      "Epoch 0 train loss: 0.6837, eval loss 0.6676875352859497\n",
      "optimal threshold: -0.4458\n",
      "Epoch 1 train loss: 0.6632, eval loss 0.6610424518585205\n",
      "optimal threshold: -0.4812\n",
      "Epoch 2 train loss: 0.6485, eval loss 0.6595289707183838\n",
      "optimal threshold: -0.6967\n",
      "Epoch 3 train loss: 0.6467, eval loss 0.66047602891922\n",
      "optimal threshold: -0.6781\n",
      "Epoch 4 train loss: 0.6189, eval loss 0.6627734899520874\n",
      "optimal threshold: -0.6058\n",
      "Epoch 5 train loss: 0.6363, eval loss 0.663540780544281\n",
      "optimal threshold: -0.6219\n",
      "Epoch 6 train loss: 0.6172, eval loss 0.6684038043022156\n",
      "optimal threshold: -0.5181\n",
      "Epoch 7 train loss: 0.6174, eval loss 0.6711382269859314\n",
      "optimal threshold: -0.7150\n",
      "Epoch 8 train loss: 0.6082, eval loss 0.6773345470428467\n",
      "optimal threshold: -0.6341\n",
      "Epoch 9 train loss: 0.5908, eval loss 0.6853999495506287\n",
      "optimal threshold: -0.5808\n",
      "Epoch 10 train loss: 0.5896, eval loss 0.6914124488830566\n",
      "optimal threshold: -0.4636\n",
      "Epoch 11 train loss: 0.6065, eval loss 0.7021917104721069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:22:04,245] Trial 11 finished with value: 0.5703268647193909 and parameters: {'learning_rate_exp': -2.814673717837715, 'dropout_p': 0.17729808716029682, 'l2_reg_exp': -3.4650545335908194, 'batch_size': 326, 'N': 440}. Best is trial 6 with value: 0.45701825618743896.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6101\n",
      "optimal threshold: -0.5892\n",
      "Epoch 0 train loss: 1.1252, eval loss 1.106319785118103\n",
      "optimal threshold: -1.0326\n",
      "Epoch 1 train loss: 0.8322, eval loss 0.7941082715988159\n",
      "optimal threshold: -0.8675\n",
      "Epoch 2 train loss: 0.7397, eval loss 0.7145277261734009\n",
      "optimal threshold: -0.6039\n",
      "Epoch 3 train loss: 0.7230, eval loss 0.6941686868667603\n",
      "optimal threshold: -0.6136\n",
      "Epoch 4 train loss: 0.7194, eval loss 0.6818885207176208\n",
      "optimal threshold: -0.6221\n",
      "Epoch 5 train loss: 0.6970, eval loss 0.674092173576355\n",
      "optimal threshold: -0.5849\n",
      "Epoch 6 train loss: 0.6851, eval loss 0.6690420508384705\n",
      "optimal threshold: -0.6472\n",
      "Epoch 7 train loss: 0.6885, eval loss 0.6658172011375427\n",
      "optimal threshold: -0.5319\n",
      "Epoch 8 train loss: 0.6840, eval loss 0.6636596322059631\n",
      "optimal threshold: -0.4299\n",
      "Epoch 9 train loss: 0.6778, eval loss 0.661911129951477\n",
      "optimal threshold: -0.5451\n",
      "Epoch 10 train loss: 0.6779, eval loss 0.6603548526763916\n",
      "optimal threshold: -0.4921\n",
      "Epoch 11 train loss: 0.6564, eval loss 0.6593055725097656\n",
      "optimal threshold: -0.4224\n",
      "Epoch 12 train loss: 0.6688, eval loss 0.6586145162582397\n",
      "optimal threshold: -0.4475\n",
      "Epoch 13 train loss: 0.6453, eval loss 0.6580456495285034\n",
      "optimal threshold: -0.4500\n",
      "Epoch 14 train loss: 0.6439, eval loss 0.6574340462684631\n",
      "optimal threshold: -0.4345\n",
      "Epoch 15 train loss: 0.6581, eval loss 0.6573008298873901\n",
      "optimal threshold: -0.4465\n",
      "Epoch 16 train loss: 0.6442, eval loss 0.6570318341255188\n",
      "optimal threshold: -0.6454\n",
      "Epoch 17 train loss: 0.6417, eval loss 0.6565588116645813\n",
      "optimal threshold: -0.3908\n",
      "Epoch 18 train loss: 0.6430, eval loss 0.6564595699310303\n",
      "optimal threshold: -0.4234\n",
      "Epoch 19 train loss: 0.6488, eval loss 0.6561955213546753\n",
      "optimal threshold: -0.3918\n",
      "Epoch 20 train loss: 0.6360, eval loss 0.6561285853385925\n",
      "optimal threshold: -0.5197\n",
      "Epoch 21 train loss: 0.6478, eval loss 0.656519889831543\n",
      "optimal threshold: -0.5761\n",
      "Epoch 22 train loss: 0.6393, eval loss 0.6563721895217896\n",
      "optimal threshold: -0.5624\n",
      "Epoch 23 train loss: 0.6178, eval loss 0.656284749507904\n",
      "optimal threshold: -0.5580\n",
      "Epoch 24 train loss: 0.6344, eval loss 0.6562600135803223\n",
      "optimal threshold: -0.3812\n",
      "Epoch 25 train loss: 0.6377, eval loss 0.6568977236747742\n",
      "optimal threshold: -0.5491\n",
      "Epoch 26 train loss: 0.6323, eval loss 0.656582772731781\n",
      "optimal threshold: -0.4417\n",
      "Epoch 27 train loss: 0.6326, eval loss 0.6572163105010986\n",
      "optimal threshold: -0.5514\n",
      "Epoch 28 train loss: 0.6293, eval loss 0.6570349335670471\n",
      "optimal threshold: -0.5528\n",
      "Epoch 29 train loss: 0.6234, eval loss 0.6573653817176819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:22:19,538] Trial 12 finished with value: 0.6162843108177185 and parameters: {'learning_rate_exp': -3.6993339809273804, 'dropout_p': 0.1491007951170051, 'l2_reg_exp': -3.5386879835126184, 'batch_size': 457, 'N': 220}. Best is trial 6 with value: 0.45701825618743896.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5687\n",
      "optimal threshold: -0.2216\n",
      "Epoch 0 train loss: 1.3699, eval loss 1.3720568418502808\n",
      "optimal threshold: -0.1743\n",
      "Epoch 1 train loss: 1.3400, eval loss 1.3503895998001099\n",
      "optimal threshold: -0.2120\n",
      "Epoch 2 train loss: 1.3196, eval loss 1.3271623849868774\n",
      "optimal threshold: -0.2533\n",
      "Epoch 3 train loss: 1.2957, eval loss 1.3016053438186646\n",
      "optimal threshold: -0.3021\n",
      "Epoch 4 train loss: 1.2343, eval loss 1.2734237909317017\n",
      "optimal threshold: -0.3364\n",
      "Epoch 5 train loss: 1.2222, eval loss 1.2422124147415161\n",
      "optimal threshold: -0.4148\n",
      "Epoch 6 train loss: 1.1750, eval loss 1.20816969871521\n",
      "optimal threshold: -0.4800\n",
      "Epoch 7 train loss: 1.1396, eval loss 1.1717103719711304\n",
      "optimal threshold: -0.5488\n",
      "Epoch 8 train loss: 1.0843, eval loss 1.1341168880462646\n",
      "optimal threshold: -0.6132\n",
      "Epoch 9 train loss: 1.0670, eval loss 1.096760869026184\n",
      "optimal threshold: -0.6313\n",
      "Epoch 10 train loss: 1.0118, eval loss 1.06032395362854\n",
      "optimal threshold: -0.6889\n",
      "Epoch 11 train loss: 0.9853, eval loss 1.0251834392547607\n",
      "optimal threshold: -0.7560\n",
      "Epoch 12 train loss: 0.9528, eval loss 0.9918624758720398\n",
      "optimal threshold: -0.8022\n",
      "Epoch 13 train loss: 0.9148, eval loss 0.9605194926261902\n",
      "optimal threshold: -0.8928\n",
      "Epoch 14 train loss: 0.8877, eval loss 0.9313691854476929\n",
      "optimal threshold: -0.9245\n",
      "Epoch 15 train loss: 0.8327, eval loss 0.9044599533081055\n",
      "optimal threshold: -0.9229\n",
      "Epoch 16 train loss: 0.8396, eval loss 0.8797255754470825\n",
      "optimal threshold: -0.9292\n",
      "Epoch 17 train loss: 0.7706, eval loss 0.8571386337280273\n",
      "optimal threshold: -0.9310\n",
      "Epoch 18 train loss: 0.7754, eval loss 0.8366632461547852\n",
      "optimal threshold: -0.9119\n",
      "Epoch 19 train loss: 0.7906, eval loss 0.8182904720306396\n",
      "optimal threshold: -0.9243\n",
      "Epoch 20 train loss: 0.7614, eval loss 0.8021246194839478\n",
      "optimal threshold: -0.9156\n",
      "Epoch 21 train loss: 0.7391, eval loss 0.7881014943122864\n",
      "optimal threshold: -0.8899\n",
      "Epoch 22 train loss: 0.7177, eval loss 0.7758710384368896\n",
      "optimal threshold: -0.8795\n",
      "Epoch 23 train loss: 0.7098, eval loss 0.7653583288192749\n",
      "optimal threshold: -0.8644\n",
      "Epoch 24 train loss: 0.7050, eval loss 0.7563443183898926\n",
      "optimal threshold: -0.8600\n",
      "Epoch 25 train loss: 0.7482, eval loss 0.7486351728439331\n",
      "optimal threshold: -0.8581\n",
      "Epoch 26 train loss: 0.6921, eval loss 0.7420603036880493\n",
      "optimal threshold: -0.8823\n",
      "Epoch 27 train loss: 0.7006, eval loss 0.7364091277122498\n",
      "optimal threshold: -0.9241\n",
      "Epoch 28 train loss: 0.7047, eval loss 0.7315139770507812\n",
      "optimal threshold: -0.8261\n",
      "Epoch 29 train loss: 0.6912, eval loss 0.7272785902023315\n",
      "optimal threshold: -0.8454\n",
      "Epoch 30 train loss: 0.6997, eval loss 0.7236305475234985\n",
      "optimal threshold: -1.0456\n",
      "Epoch 31 train loss: 0.7027, eval loss 0.7203933596611023\n",
      "optimal threshold: -1.0261\n",
      "Epoch 32 train loss: 0.6763, eval loss 0.717598021030426\n",
      "optimal threshold: -1.0179\n",
      "Epoch 33 train loss: 0.6992, eval loss 0.7150483131408691\n",
      "optimal threshold: -0.7604\n",
      "Epoch 34 train loss: 0.7004, eval loss 0.7127113342285156\n",
      "optimal threshold: -0.7352\n",
      "Epoch 35 train loss: 0.7617, eval loss 0.7106804847717285\n",
      "optimal threshold: -0.7555\n",
      "Epoch 36 train loss: 0.7307, eval loss 0.7087306976318359\n",
      "optimal threshold: -0.7557\n",
      "Epoch 37 train loss: 0.6840, eval loss 0.7069827318191528\n",
      "optimal threshold: -0.7393\n",
      "Epoch 38 train loss: 0.7024, eval loss 0.705322802066803\n",
      "optimal threshold: -0.7327\n",
      "Epoch 39 train loss: 0.7641, eval loss 0.7037878632545471\n",
      "optimal threshold: -0.7263\n",
      "Epoch 40 train loss: 0.6959, eval loss 0.7023698091506958\n",
      "optimal threshold: -0.7294\n",
      "Epoch 41 train loss: 0.7049, eval loss 0.7010568976402283\n",
      "optimal threshold: -0.7282\n",
      "Epoch 42 train loss: 0.7419, eval loss 0.6997433304786682\n",
      "optimal threshold: -0.8142\n",
      "Epoch 43 train loss: 0.7303, eval loss 0.6984862089157104\n",
      "optimal threshold: -0.8111\n",
      "Epoch 44 train loss: 0.6856, eval loss 0.6972882151603699\n",
      "optimal threshold: -0.8259\n",
      "Epoch 45 train loss: 0.7166, eval loss 0.6962143778800964\n",
      "optimal threshold: -0.7310\n",
      "Epoch 46 train loss: 0.6865, eval loss 0.6951944231987\n",
      "optimal threshold: -0.7278\n",
      "Epoch 47 train loss: 0.7027, eval loss 0.6941326260566711\n",
      "optimal threshold: -0.7281\n",
      "Epoch 48 train loss: 0.7483, eval loss 0.6931110620498657\n",
      "optimal threshold: -0.7226\n",
      "Epoch 49 train loss: 0.7374, eval loss 0.6921895742416382\n",
      "optimal threshold: -0.7040\n",
      "Epoch 50 train loss: 0.7134, eval loss 0.6912823915481567\n",
      "optimal threshold: -0.7215\n",
      "Epoch 51 train loss: 0.7562, eval loss 0.690396249294281\n",
      "optimal threshold: -0.6875\n",
      "Epoch 52 train loss: 0.6708, eval loss 0.6895427107810974\n",
      "optimal threshold: -0.6930\n",
      "Epoch 53 train loss: 0.6540, eval loss 0.6887580156326294\n",
      "optimal threshold: -0.7329\n",
      "Epoch 54 train loss: 0.7067, eval loss 0.6880000829696655\n",
      "optimal threshold: -0.7549\n",
      "Epoch 55 train loss: 0.7787, eval loss 0.6872227787971497\n",
      "optimal threshold: -0.7626\n",
      "Epoch 56 train loss: 0.7367, eval loss 0.686513364315033\n",
      "optimal threshold: -0.7623\n",
      "Epoch 57 train loss: 0.7104, eval loss 0.6857770681381226\n",
      "optimal threshold: -0.8051\n",
      "Epoch 58 train loss: 0.7572, eval loss 0.6850959062576294\n",
      "optimal threshold: -0.7999\n",
      "Epoch 59 train loss: 0.7652, eval loss 0.6844752430915833\n",
      "optimal threshold: -0.7982\n",
      "Epoch 60 train loss: 0.6605, eval loss 0.6837791204452515\n",
      "optimal threshold: -0.8017\n",
      "Epoch 61 train loss: 0.6696, eval loss 0.6832258105278015\n",
      "optimal threshold: -0.8116\n",
      "Epoch 62 train loss: 0.7967, eval loss 0.6826413869857788\n",
      "optimal threshold: -0.8072\n",
      "Epoch 63 train loss: 0.7167, eval loss 0.6820120215415955\n",
      "optimal threshold: -0.7896\n",
      "Epoch 64 train loss: 0.7205, eval loss 0.6814150214195251\n",
      "optimal threshold: -0.8578\n",
      "Epoch 65 train loss: 0.7716, eval loss 0.6808799505233765\n",
      "optimal threshold: -0.8656\n",
      "Epoch 66 train loss: 0.7361, eval loss 0.6804245710372925\n",
      "optimal threshold: -0.8780\n",
      "Epoch 67 train loss: 0.7525, eval loss 0.6799181699752808\n",
      "optimal threshold: -0.6408\n",
      "Epoch 68 train loss: 0.7280, eval loss 0.6794469356536865\n",
      "optimal threshold: -0.6423\n",
      "Epoch 69 train loss: 0.7132, eval loss 0.678981363773346\n",
      "optimal threshold: -0.6128\n",
      "Epoch 70 train loss: 0.7441, eval loss 0.6785317659378052\n",
      "optimal threshold: -0.6129\n",
      "Epoch 71 train loss: 0.7599, eval loss 0.678059995174408\n",
      "optimal threshold: -0.6339\n",
      "Epoch 72 train loss: 0.7263, eval loss 0.6776731610298157\n",
      "optimal threshold: -0.6275\n",
      "Epoch 73 train loss: 0.7145, eval loss 0.6772037744522095\n",
      "optimal threshold: -0.6502\n",
      "Epoch 74 train loss: 0.7945, eval loss 0.676802933216095\n",
      "optimal threshold: -0.6415\n",
      "Epoch 75 train loss: 0.7466, eval loss 0.6763747334480286\n",
      "optimal threshold: -0.5974\n",
      "Epoch 76 train loss: 0.7246, eval loss 0.6759518384933472\n",
      "optimal threshold: -0.7075\n",
      "Epoch 77 train loss: 0.6819, eval loss 0.6756383180618286\n",
      "optimal threshold: -0.7078\n",
      "Epoch 78 train loss: 0.7518, eval loss 0.6753127574920654\n",
      "optimal threshold: -0.6932\n",
      "Epoch 79 train loss: 0.6393, eval loss 0.6749476194381714\n",
      "optimal threshold: -0.6964\n",
      "Epoch 80 train loss: 0.7302, eval loss 0.6746129393577576\n",
      "optimal threshold: -0.6960\n",
      "Epoch 81 train loss: 0.7213, eval loss 0.6743382811546326\n",
      "optimal threshold: -0.6947\n",
      "Epoch 82 train loss: 0.6920, eval loss 0.6739699244499207\n",
      "optimal threshold: -0.6989\n",
      "Epoch 83 train loss: 0.7282, eval loss 0.6736855506896973\n",
      "optimal threshold: -0.6907\n",
      "Epoch 84 train loss: 0.7129, eval loss 0.6733744740486145\n",
      "optimal threshold: -0.6814\n",
      "Epoch 85 train loss: 0.6504, eval loss 0.6730442047119141\n",
      "optimal threshold: -0.6873\n",
      "Epoch 86 train loss: 0.7729, eval loss 0.6727825999259949\n",
      "optimal threshold: -0.6938\n",
      "Epoch 87 train loss: 0.7256, eval loss 0.6725090146064758\n",
      "optimal threshold: -0.6893\n",
      "Epoch 88 train loss: 0.7507, eval loss 0.6722274422645569\n",
      "optimal threshold: -0.6898\n",
      "Epoch 89 train loss: 0.6884, eval loss 0.6719509363174438\n",
      "optimal threshold: -0.7045\n",
      "Epoch 90 train loss: 0.6551, eval loss 0.6716914772987366\n",
      "optimal threshold: -0.6964\n",
      "Epoch 91 train loss: 0.7658, eval loss 0.6714850068092346\n",
      "optimal threshold: -0.6879\n",
      "Epoch 92 train loss: 0.6926, eval loss 0.671294629573822\n",
      "optimal threshold: -0.6909\n",
      "Epoch 93 train loss: 0.7475, eval loss 0.6709496378898621\n",
      "optimal threshold: -0.6920\n",
      "Epoch 94 train loss: 0.6985, eval loss 0.6707085967063904\n",
      "optimal threshold: -0.6931\n",
      "Epoch 95 train loss: 0.7113, eval loss 0.6705042719841003\n",
      "optimal threshold: -0.7196\n",
      "Epoch 96 train loss: 0.6329, eval loss 0.6703446507453918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4603\n",
      "Epoch 97 train loss: 0.7380, eval loss 0.6701143383979797\n",
      "optimal threshold: -0.7135\n",
      "Epoch 98 train loss: 0.7278, eval loss 0.6699851155281067\n",
      "optimal threshold: -0.5954\n",
      "Epoch 99 train loss: 0.6784, eval loss 0.6697836518287659\n",
      "optimal threshold: -0.5957\n",
      "Epoch 100 train loss: 0.7179, eval loss 0.6695646047592163\n",
      "optimal threshold: -0.5951\n",
      "Epoch 101 train loss: 0.6387, eval loss 0.6694034337997437\n",
      "optimal threshold: -0.5926\n",
      "Epoch 102 train loss: 0.6502, eval loss 0.6692325472831726\n",
      "optimal threshold: -0.5987\n",
      "Epoch 103 train loss: 0.7322, eval loss 0.6690644025802612\n",
      "optimal threshold: -0.5946\n",
      "Epoch 104 train loss: 0.7249, eval loss 0.6688947081565857\n",
      "optimal threshold: -0.6029\n",
      "Epoch 105 train loss: 0.6981, eval loss 0.6687487959861755\n",
      "optimal threshold: -0.5747\n",
      "Epoch 106 train loss: 0.7473, eval loss 0.6686052083969116\n",
      "optimal threshold: -0.5855\n",
      "Epoch 107 train loss: 0.7303, eval loss 0.668398380279541\n",
      "optimal threshold: -0.5901\n",
      "Epoch 108 train loss: 0.7463, eval loss 0.6682853698730469\n",
      "optimal threshold: -0.6037\n",
      "Epoch 109 train loss: 0.7055, eval loss 0.6681183576583862\n",
      "optimal threshold: -0.6044\n",
      "Epoch 110 train loss: 0.7360, eval loss 0.6679713726043701\n",
      "optimal threshold: -0.5997\n",
      "Epoch 111 train loss: 0.6831, eval loss 0.6678006649017334\n",
      "optimal threshold: -0.6023\n",
      "Epoch 112 train loss: 0.7287, eval loss 0.6676530838012695\n",
      "optimal threshold: -0.6007\n",
      "Epoch 113 train loss: 0.7087, eval loss 0.6675013303756714\n",
      "optimal threshold: -0.6039\n",
      "Epoch 114 train loss: 0.6754, eval loss 0.6674036979675293\n",
      "optimal threshold: -0.6008\n",
      "Epoch 115 train loss: 0.7496, eval loss 0.6672751307487488\n",
      "optimal threshold: -0.5992\n",
      "Epoch 116 train loss: 0.7213, eval loss 0.6671085357666016\n",
      "optimal threshold: -0.5940\n",
      "Epoch 117 train loss: 0.7197, eval loss 0.6669434309005737\n",
      "optimal threshold: -0.5955\n",
      "Epoch 118 train loss: 0.6891, eval loss 0.6668787598609924\n",
      "optimal threshold: -0.6093\n",
      "Epoch 119 train loss: 0.6984, eval loss 0.6668174266815186\n",
      "optimal threshold: -0.5837\n",
      "Epoch 120 train loss: 0.7308, eval loss 0.6666982173919678\n",
      "optimal threshold: -0.6192\n",
      "Epoch 121 train loss: 0.7023, eval loss 0.6665211319923401\n",
      "optimal threshold: -0.6291\n",
      "Epoch 122 train loss: 0.6315, eval loss 0.6663386821746826\n",
      "optimal threshold: -0.6312\n",
      "Epoch 123 train loss: 0.7489, eval loss 0.6662223935127258\n",
      "optimal threshold: -0.6015\n",
      "Epoch 124 train loss: 0.6895, eval loss 0.6660727262496948\n",
      "optimal threshold: -0.6079\n",
      "Epoch 125 train loss: 0.6720, eval loss 0.666003406047821\n",
      "optimal threshold: -0.6258\n",
      "Epoch 126 train loss: 0.6113, eval loss 0.6659164428710938\n",
      "optimal threshold: -0.6174\n",
      "Epoch 127 train loss: 0.7699, eval loss 0.6657025814056396\n",
      "optimal threshold: -0.6187\n",
      "Epoch 128 train loss: 0.6710, eval loss 0.6656214594841003\n",
      "optimal threshold: -0.5948\n",
      "Epoch 129 train loss: 0.6892, eval loss 0.6655209064483643\n",
      "optimal threshold: -0.6282\n",
      "Epoch 130 train loss: 0.6644, eval loss 0.6654636263847351\n",
      "optimal threshold: -0.6304\n",
      "Epoch 131 train loss: 0.7066, eval loss 0.6653932929039001\n",
      "optimal threshold: -0.6259\n",
      "Epoch 132 train loss: 0.6267, eval loss 0.6652529239654541\n",
      "optimal threshold: -0.6332\n",
      "Epoch 133 train loss: 0.6777, eval loss 0.6651859879493713\n",
      "optimal threshold: -0.6326\n",
      "Epoch 134 train loss: 0.6844, eval loss 0.6650963425636292\n",
      "optimal threshold: -0.5831\n",
      "Epoch 135 train loss: 0.6300, eval loss 0.6649593710899353\n",
      "optimal threshold: -0.5909\n",
      "Epoch 136 train loss: 0.6840, eval loss 0.6649230122566223\n",
      "optimal threshold: -0.5898\n",
      "Epoch 137 train loss: 0.7083, eval loss 0.6648096442222595\n",
      "optimal threshold: -0.5882\n",
      "Epoch 138 train loss: 0.7554, eval loss 0.6647087931632996\n",
      "optimal threshold: -0.5925\n",
      "Epoch 139 train loss: 0.6603, eval loss 0.6646650433540344\n",
      "optimal threshold: -0.6973\n",
      "Epoch 140 train loss: 0.6615, eval loss 0.6646029949188232\n",
      "optimal threshold: -0.6226\n",
      "Epoch 141 train loss: 0.6540, eval loss 0.6645025014877319\n",
      "optimal threshold: -0.6228\n",
      "Epoch 142 train loss: 0.6979, eval loss 0.6644109487533569\n",
      "optimal threshold: -0.6219\n",
      "Epoch 143 train loss: 0.7051, eval loss 0.6643103957176208\n",
      "optimal threshold: -0.6222\n",
      "Epoch 144 train loss: 0.7197, eval loss 0.6642743945121765\n",
      "optimal threshold: -0.6209\n",
      "Epoch 145 train loss: 0.6156, eval loss 0.6642210483551025\n",
      "optimal threshold: -0.6249\n",
      "Epoch 146 train loss: 0.6477, eval loss 0.6642004251480103\n",
      "optimal threshold: -0.6157\n",
      "Epoch 147 train loss: 0.7463, eval loss 0.6641013026237488\n",
      "optimal threshold: -0.6884\n",
      "Epoch 148 train loss: 0.6878, eval loss 0.6639846563339233\n",
      "optimal threshold: -0.6861\n",
      "Epoch 149 train loss: 0.6611, eval loss 0.6638456583023071\n",
      "optimal threshold: -0.6820\n",
      "Epoch 150 train loss: 0.7004, eval loss 0.6637301445007324\n",
      "optimal threshold: -0.6035\n",
      "Epoch 151 train loss: 0.6326, eval loss 0.6637060642242432\n",
      "optimal threshold: -0.6039\n",
      "Epoch 152 train loss: 0.7432, eval loss 0.6636390089988708\n",
      "optimal threshold: -0.6091\n",
      "Epoch 153 train loss: 0.6660, eval loss 0.6635503172874451\n",
      "optimal threshold: -0.6059\n",
      "Epoch 154 train loss: 0.6350, eval loss 0.6635403633117676\n",
      "optimal threshold: -0.6073\n",
      "Epoch 155 train loss: 0.7116, eval loss 0.6634652018547058\n",
      "optimal threshold: -0.6025\n",
      "Epoch 156 train loss: 0.6473, eval loss 0.6633867621421814\n",
      "optimal threshold: -0.6058\n",
      "Epoch 157 train loss: 0.7042, eval loss 0.6633343696594238\n",
      "optimal threshold: -0.6084\n",
      "Epoch 158 train loss: 0.7078, eval loss 0.6632738709449768\n",
      "optimal threshold: -0.6087\n",
      "Epoch 159 train loss: 0.6819, eval loss 0.6632030010223389\n",
      "optimal threshold: -0.5964\n",
      "Epoch 160 train loss: 0.7357, eval loss 0.6631255745887756\n",
      "optimal threshold: -0.5946\n",
      "Epoch 161 train loss: 0.7143, eval loss 0.6630271077156067\n",
      "optimal threshold: -0.5951\n",
      "Epoch 162 train loss: 0.6921, eval loss 0.6629926562309265\n",
      "optimal threshold: -0.5988\n",
      "Epoch 163 train loss: 0.6846, eval loss 0.6629782319068909\n",
      "optimal threshold: -0.5977\n",
      "Epoch 164 train loss: 0.7349, eval loss 0.6629158854484558\n",
      "optimal threshold: -0.6016\n",
      "Epoch 165 train loss: 0.6737, eval loss 0.6629009246826172\n",
      "optimal threshold: -0.6969\n",
      "Epoch 166 train loss: 0.6655, eval loss 0.6628289818763733\n",
      "optimal threshold: -0.6979\n",
      "Epoch 167 train loss: 0.6615, eval loss 0.6627835035324097\n",
      "optimal threshold: -0.6991\n",
      "Epoch 168 train loss: 0.6339, eval loss 0.662763774394989\n",
      "optimal threshold: -0.5984\n",
      "Epoch 169 train loss: 0.6515, eval loss 0.6626629829406738\n",
      "optimal threshold: -0.8701\n",
      "Epoch 170 train loss: 0.7240, eval loss 0.6626152992248535\n",
      "optimal threshold: -0.8716\n",
      "Epoch 171 train loss: 0.5901, eval loss 0.6625499725341797\n",
      "optimal threshold: -0.6913\n",
      "Epoch 172 train loss: 0.6445, eval loss 0.662423849105835\n",
      "optimal threshold: -0.6901\n",
      "Epoch 173 train loss: 0.6558, eval loss 0.6623736619949341\n",
      "optimal threshold: -0.5626\n",
      "Epoch 174 train loss: 0.7279, eval loss 0.6623212099075317\n",
      "optimal threshold: -0.5522\n",
      "Epoch 175 train loss: 0.6773, eval loss 0.6623456478118896\n",
      "optimal threshold: -0.5539\n",
      "Epoch 176 train loss: 0.7062, eval loss 0.6623159050941467\n",
      "optimal threshold: -0.5568\n",
      "Epoch 177 train loss: 0.6679, eval loss 0.6622219681739807\n",
      "optimal threshold: -0.5563\n",
      "Epoch 178 train loss: 0.7070, eval loss 0.6621354222297668\n",
      "optimal threshold: -0.5521\n",
      "Epoch 179 train loss: 0.6712, eval loss 0.6620732545852661\n",
      "optimal threshold: -0.5542\n",
      "Epoch 180 train loss: 0.7037, eval loss 0.6620756387710571\n",
      "optimal threshold: -0.5557\n",
      "Epoch 181 train loss: 0.6275, eval loss 0.6620622873306274\n",
      "optimal threshold: -0.5561\n",
      "Epoch 182 train loss: 0.6731, eval loss 0.6620722413063049\n",
      "optimal threshold: -0.5537\n",
      "Epoch 183 train loss: 0.6568, eval loss 0.6620382070541382\n",
      "optimal threshold: -0.5475\n",
      "Epoch 184 train loss: 0.7589, eval loss 0.6619952917098999\n",
      "optimal threshold: -0.5497\n",
      "Epoch 185 train loss: 0.7045, eval loss 0.661963939666748\n",
      "optimal threshold: -0.5485\n",
      "Epoch 186 train loss: 0.5947, eval loss 0.66187983751297\n",
      "optimal threshold: -0.5501\n",
      "Epoch 187 train loss: 0.6952, eval loss 0.6618602871894836\n",
      "optimal threshold: -0.5468\n",
      "Epoch 188 train loss: 0.6844, eval loss 0.6617912650108337\n",
      "optimal threshold: -0.5450\n",
      "Epoch 189 train loss: 0.7126, eval loss 0.661723792552948\n",
      "optimal threshold: -0.5438\n",
      "Epoch 190 train loss: 0.6822, eval loss 0.6617111563682556\n",
      "optimal threshold: -0.5494\n",
      "Epoch 191 train loss: 0.6225, eval loss 0.6616871953010559\n",
      "optimal threshold: -0.5448\n",
      "Epoch 192 train loss: 0.6627, eval loss 0.6616527438163757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5439\n",
      "Epoch 193 train loss: 0.5980, eval loss 0.6616279482841492\n",
      "optimal threshold: -0.5420\n",
      "Epoch 194 train loss: 0.7414, eval loss 0.6615908145904541\n",
      "optimal threshold: -0.5420\n",
      "Epoch 195 train loss: 0.7239, eval loss 0.661566436290741\n",
      "optimal threshold: -0.5430\n",
      "Epoch 196 train loss: 0.6778, eval loss 0.661485493183136\n",
      "optimal threshold: -0.5402\n",
      "Epoch 197 train loss: 0.6786, eval loss 0.6614670753479004\n",
      "optimal threshold: -0.5334\n",
      "Epoch 198 train loss: 0.6153, eval loss 0.661407470703125\n",
      "optimal threshold: -0.5401\n",
      "Epoch 199 train loss: 0.7033, eval loss 0.6614378690719604\n",
      "optimal threshold: -0.5377\n",
      "Epoch 200 train loss: 0.6914, eval loss 0.6613746881484985\n",
      "optimal threshold: -0.5468\n",
      "Epoch 201 train loss: 0.6752, eval loss 0.6613622307777405\n",
      "optimal threshold: -0.5371\n",
      "Epoch 202 train loss: 0.6622, eval loss 0.6613496541976929\n",
      "optimal threshold: -0.5551\n",
      "Epoch 203 train loss: 0.6545, eval loss 0.6613686680793762\n",
      "optimal threshold: -0.5521\n",
      "Epoch 204 train loss: 0.6601, eval loss 0.6612873077392578\n",
      "optimal threshold: -0.5497\n",
      "Epoch 205 train loss: 0.5981, eval loss 0.6612210273742676\n",
      "optimal threshold: -0.5520\n",
      "Epoch 206 train loss: 0.6517, eval loss 0.6611815690994263\n",
      "optimal threshold: -0.5342\n",
      "Epoch 207 train loss: 0.6220, eval loss 0.6611737608909607\n",
      "optimal threshold: -0.5302\n",
      "Epoch 208 train loss: 0.6436, eval loss 0.6611230373382568\n",
      "optimal threshold: -0.5301\n",
      "Epoch 209 train loss: 0.6448, eval loss 0.6611092686653137\n",
      "optimal threshold: -0.5320\n",
      "Epoch 210 train loss: 0.6741, eval loss 0.6611473560333252\n",
      "optimal threshold: -0.5345\n",
      "Epoch 211 train loss: 0.6818, eval loss 0.6611286997795105\n",
      "optimal threshold: -0.5312\n",
      "Epoch 212 train loss: 0.6864, eval loss 0.6611388921737671\n",
      "optimal threshold: -0.5309\n",
      "Epoch 213 train loss: 0.6229, eval loss 0.6610966324806213\n",
      "optimal threshold: -0.5297\n",
      "Epoch 214 train loss: 0.6563, eval loss 0.6610528826713562\n",
      "optimal threshold: -0.5256\n",
      "Epoch 215 train loss: 0.6456, eval loss 0.6609849333763123\n",
      "optimal threshold: -0.5297\n",
      "Epoch 216 train loss: 0.6629, eval loss 0.6610080003738403\n",
      "optimal threshold: -0.5378\n",
      "Epoch 217 train loss: 0.7264, eval loss 0.6610345244407654\n",
      "optimal threshold: -0.5254\n",
      "Epoch 218 train loss: 0.6745, eval loss 0.6609469056129456\n",
      "optimal threshold: -0.5360\n",
      "Epoch 219 train loss: 0.6132, eval loss 0.6609025597572327\n",
      "optimal threshold: -0.5330\n",
      "Epoch 220 train loss: 0.6002, eval loss 0.6608243584632874\n",
      "optimal threshold: -0.5128\n",
      "Epoch 221 train loss: 0.6008, eval loss 0.6607975959777832\n",
      "optimal threshold: -0.5181\n",
      "Epoch 222 train loss: 0.5851, eval loss 0.6608436107635498\n",
      "optimal threshold: -0.5162\n",
      "Epoch 223 train loss: 0.7075, eval loss 0.6608179211616516\n",
      "optimal threshold: -0.5156\n",
      "Epoch 224 train loss: 0.6296, eval loss 0.6607843637466431\n",
      "optimal threshold: -0.5157\n",
      "Epoch 225 train loss: 0.6888, eval loss 0.6607682704925537\n",
      "optimal threshold: -0.5207\n",
      "Epoch 226 train loss: 0.6551, eval loss 0.6608049273490906\n",
      "optimal threshold: -0.5153\n",
      "Epoch 227 train loss: 0.6311, eval loss 0.6607237458229065\n",
      "optimal threshold: -0.5149\n",
      "Epoch 228 train loss: 0.6717, eval loss 0.6607195138931274\n",
      "optimal threshold: -0.5176\n",
      "Epoch 229 train loss: 0.6344, eval loss 0.6607128381729126\n",
      "optimal threshold: -0.5227\n",
      "Epoch 230 train loss: 0.6478, eval loss 0.6607075333595276\n",
      "optimal threshold: -0.5176\n",
      "Epoch 231 train loss: 0.6993, eval loss 0.6606774926185608\n",
      "optimal threshold: -0.5130\n",
      "Epoch 232 train loss: 0.6017, eval loss 0.6606652140617371\n",
      "optimal threshold: -0.5073\n",
      "Epoch 233 train loss: 0.6346, eval loss 0.6606439352035522\n",
      "optimal threshold: -0.5152\n",
      "Epoch 234 train loss: 0.6893, eval loss 0.6606720089912415\n",
      "optimal threshold: -0.5136\n",
      "Epoch 235 train loss: 0.6765, eval loss 0.6606290936470032\n",
      "optimal threshold: -0.5051\n",
      "Epoch 236 train loss: 0.6319, eval loss 0.6605762839317322\n",
      "optimal threshold: -0.5358\n",
      "Epoch 237 train loss: 0.6936, eval loss 0.6605780720710754\n",
      "optimal threshold: -0.5365\n",
      "Epoch 238 train loss: 0.6734, eval loss 0.66055828332901\n",
      "optimal threshold: -0.5325\n",
      "Epoch 239 train loss: 0.6260, eval loss 0.6604700088500977\n",
      "optimal threshold: -0.5121\n",
      "Epoch 240 train loss: 0.6679, eval loss 0.6604819893836975\n",
      "optimal threshold: -0.5426\n",
      "Epoch 241 train loss: 0.6432, eval loss 0.6604728102684021\n",
      "optimal threshold: -0.5181\n",
      "Epoch 242 train loss: 0.6205, eval loss 0.6604529023170471\n",
      "optimal threshold: -0.5390\n",
      "Epoch 243 train loss: 0.6618, eval loss 0.6603855490684509\n",
      "optimal threshold: -0.5399\n",
      "Epoch 244 train loss: 0.6263, eval loss 0.6603766679763794\n",
      "optimal threshold: -0.5268\n",
      "Epoch 245 train loss: 0.6038, eval loss 0.6603257060050964\n",
      "optimal threshold: -0.5251\n",
      "Epoch 246 train loss: 0.6361, eval loss 0.6602941155433655\n",
      "optimal threshold: -0.5324\n",
      "Epoch 247 train loss: 0.6849, eval loss 0.6603173613548279\n",
      "optimal threshold: -0.5324\n",
      "Epoch 248 train loss: 0.7101, eval loss 0.6602991223335266\n",
      "optimal threshold: -0.5303\n",
      "Epoch 249 train loss: 0.6441, eval loss 0.6603081226348877\n",
      "optimal threshold: -0.5269\n",
      "Epoch 250 train loss: 0.6527, eval loss 0.6602230668067932\n",
      "optimal threshold: -0.5247\n",
      "Epoch 251 train loss: 0.6525, eval loss 0.6602466702461243\n",
      "optimal threshold: -0.5232\n",
      "Epoch 252 train loss: 0.6756, eval loss 0.6602081656455994\n",
      "optimal threshold: -0.5219\n",
      "Epoch 253 train loss: 0.6022, eval loss 0.6601929664611816\n",
      "optimal threshold: -0.5234\n",
      "Epoch 254 train loss: 0.7186, eval loss 0.6601940393447876\n",
      "optimal threshold: -0.5261\n",
      "Epoch 255 train loss: 0.6503, eval loss 0.6601908206939697\n",
      "optimal threshold: -0.5295\n",
      "Epoch 256 train loss: 0.6103, eval loss 0.6602433323860168\n",
      "optimal threshold: -0.5077\n",
      "Epoch 257 train loss: 0.5806, eval loss 0.660256564617157\n",
      "optimal threshold: -0.5412\n",
      "Epoch 258 train loss: 0.6036, eval loss 0.6602664589881897\n",
      "optimal threshold: -0.5013\n",
      "Epoch 259 train loss: 0.6102, eval loss 0.6602107882499695\n",
      "optimal threshold: -0.5025\n",
      "Epoch 260 train loss: 0.6578, eval loss 0.6602135896682739\n",
      "optimal threshold: -0.5027\n",
      "Epoch 261 train loss: 0.6422, eval loss 0.6601781845092773\n",
      "optimal threshold: -0.4975\n",
      "Epoch 262 train loss: 0.6777, eval loss 0.6601061820983887\n",
      "optimal threshold: -0.5014\n",
      "Epoch 263 train loss: 0.6563, eval loss 0.6601230502128601\n",
      "optimal threshold: -0.5044\n",
      "Epoch 264 train loss: 0.6415, eval loss 0.6600930094718933\n",
      "optimal threshold: -0.5085\n",
      "Epoch 265 train loss: 0.6114, eval loss 0.6601181626319885\n",
      "optimal threshold: -0.6160\n",
      "Epoch 266 train loss: 0.6459, eval loss 0.6600320339202881\n",
      "optimal threshold: -0.5058\n",
      "Epoch 267 train loss: 0.6919, eval loss 0.6600142121315002\n",
      "optimal threshold: -0.5078\n",
      "Epoch 268 train loss: 0.7656, eval loss 0.6600397229194641\n",
      "optimal threshold: -0.5077\n",
      "Epoch 269 train loss: 0.6421, eval loss 0.6600239872932434\n",
      "optimal threshold: -0.5084\n",
      "Epoch 270 train loss: 0.6642, eval loss 0.6600111722946167\n",
      "optimal threshold: -0.5085\n",
      "Epoch 271 train loss: 0.5933, eval loss 0.6600215435028076\n",
      "optimal threshold: -0.5070\n",
      "Epoch 272 train loss: 0.6499, eval loss 0.6600093245506287\n",
      "optimal threshold: -0.4544\n",
      "Epoch 273 train loss: 0.6153, eval loss 0.6600030064582825\n",
      "optimal threshold: -0.4505\n",
      "Epoch 274 train loss: 0.5640, eval loss 0.6599501967430115\n",
      "optimal threshold: -0.4495\n",
      "Epoch 275 train loss: 0.6464, eval loss 0.6599859595298767\n",
      "optimal threshold: -0.4466\n",
      "Epoch 276 train loss: 0.6143, eval loss 0.6599141955375671\n",
      "optimal threshold: -0.5052\n",
      "Epoch 277 train loss: 0.5822, eval loss 0.6599400639533997\n",
      "optimal threshold: -0.5054\n",
      "Epoch 278 train loss: 0.6561, eval loss 0.6598958969116211\n",
      "optimal threshold: -0.6225\n",
      "Epoch 279 train loss: 0.6582, eval loss 0.6598889827728271\n",
      "optimal threshold: -0.5054\n",
      "Epoch 280 train loss: 0.6260, eval loss 0.6598103642463684\n",
      "optimal threshold: -0.6210\n",
      "Epoch 281 train loss: 0.6228, eval loss 0.659844696521759\n",
      "optimal threshold: -0.6172\n",
      "Epoch 282 train loss: 0.6812, eval loss 0.6597766280174255\n",
      "optimal threshold: -0.4458\n",
      "Epoch 283 train loss: 0.6032, eval loss 0.6598067879676819\n",
      "optimal threshold: -0.4471\n",
      "Epoch 284 train loss: 0.5823, eval loss 0.6597501039505005\n",
      "optimal threshold: -0.4485\n",
      "Epoch 285 train loss: 0.6767, eval loss 0.6597761511802673\n",
      "optimal threshold: -0.4505\n",
      "Epoch 286 train loss: 0.6431, eval loss 0.6597293615341187\n",
      "optimal threshold: -0.4470\n",
      "Epoch 287 train loss: 0.6482, eval loss 0.6597236394882202\n",
      "optimal threshold: -0.4450\n",
      "Epoch 288 train loss: 0.6426, eval loss 0.6597188711166382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6221\n",
      "Epoch 289 train loss: 0.6438, eval loss 0.6597045660018921\n",
      "optimal threshold: -0.6218\n",
      "Epoch 290 train loss: 0.6804, eval loss 0.6597113013267517\n",
      "optimal threshold: -0.6196\n",
      "Epoch 291 train loss: 0.6781, eval loss 0.6597067713737488\n",
      "optimal threshold: -0.4543\n",
      "Epoch 292 train loss: 0.6176, eval loss 0.6596722602844238\n",
      "optimal threshold: -0.6201\n",
      "Epoch 293 train loss: 0.6674, eval loss 0.6596939563751221\n",
      "optimal threshold: -0.6096\n",
      "Epoch 294 train loss: 0.6559, eval loss 0.6596641540527344\n",
      "optimal threshold: -0.6083\n",
      "Epoch 295 train loss: 0.5982, eval loss 0.65968257188797\n",
      "optimal threshold: -0.6130\n",
      "Epoch 296 train loss: 0.6056, eval loss 0.6596931219100952\n",
      "optimal threshold: -0.6195\n",
      "Epoch 297 train loss: 0.5866, eval loss 0.6597504615783691\n",
      "optimal threshold: -0.4446\n",
      "Epoch 298 train loss: 0.5724, eval loss 0.6597606539726257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:25:35,560] Trial 13 finished with value: 0.6191921234130859 and parameters: {'learning_rate_exp': -4.9941578766051755, 'dropout_p': 0.14690117812235498, 'l2_reg_exp': -2.5903269268234785, 'batch_size': 172, 'N': 147}. Best is trial 6 with value: 0.45701825618743896.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6181\n",
      "Epoch 299 train loss: 0.6192, eval loss 0.6597855091094971\n",
      "optimal threshold: -0.9738\n",
      "Epoch 0 train loss: 0.7255, eval loss 0.7498229742050171\n",
      "optimal threshold: -0.7502\n",
      "Epoch 1 train loss: 0.7595, eval loss 0.6978283524513245\n",
      "optimal threshold: -0.6583\n",
      "Epoch 2 train loss: 0.7727, eval loss 0.6813921928405762\n",
      "optimal threshold: -0.4696\n",
      "Epoch 3 train loss: 0.7551, eval loss 0.6723272204399109\n",
      "optimal threshold: -0.3950\n",
      "Epoch 4 train loss: 0.7674, eval loss 0.6672939658164978\n",
      "optimal threshold: -0.4048\n",
      "Epoch 5 train loss: 0.6637, eval loss 0.6643431186676025\n",
      "optimal threshold: -0.3972\n",
      "Epoch 6 train loss: 0.7349, eval loss 0.662641167640686\n",
      "optimal threshold: -0.4060\n",
      "Epoch 7 train loss: 0.6813, eval loss 0.6616445183753967\n",
      "optimal threshold: -0.4234\n",
      "Epoch 8 train loss: 0.6715, eval loss 0.6601691246032715\n",
      "optimal threshold: -0.3911\n",
      "Epoch 9 train loss: 0.6134, eval loss 0.6590287685394287\n",
      "optimal threshold: -0.3888\n",
      "Epoch 10 train loss: 0.6844, eval loss 0.6584999561309814\n",
      "optimal threshold: -0.4341\n",
      "Epoch 11 train loss: 0.6880, eval loss 0.6584539413452148\n",
      "optimal threshold: -0.3689\n",
      "Epoch 12 train loss: 0.8110, eval loss 0.6573781371116638\n",
      "optimal threshold: -0.4381\n",
      "Epoch 13 train loss: 0.7097, eval loss 0.6570097804069519\n",
      "optimal threshold: -0.4541\n",
      "Epoch 14 train loss: 0.6461, eval loss 0.6572017669677734\n",
      "optimal threshold: -0.4371\n",
      "Epoch 15 train loss: 0.6715, eval loss 0.6564648747444153\n",
      "optimal threshold: -0.4818\n",
      "Epoch 16 train loss: 0.7004, eval loss 0.6569294929504395\n",
      "optimal threshold: -0.3936\n",
      "Epoch 17 train loss: 0.6966, eval loss 0.6568109393119812\n",
      "optimal threshold: -0.4844\n",
      "Epoch 18 train loss: 0.6409, eval loss 0.6568856835365295\n",
      "optimal threshold: -0.5162\n",
      "Epoch 19 train loss: 0.6741, eval loss 0.6564533114433289\n",
      "optimal threshold: -0.5213\n",
      "Epoch 20 train loss: 0.7370, eval loss 0.6567389369010925\n",
      "optimal threshold: -0.5233\n",
      "Epoch 21 train loss: 0.6576, eval loss 0.6561633944511414\n",
      "optimal threshold: -0.5441\n",
      "Epoch 22 train loss: 0.6592, eval loss 0.6566556096076965\n",
      "optimal threshold: -0.6732\n",
      "Epoch 23 train loss: 0.7106, eval loss 0.6566959023475647\n",
      "optimal threshold: -0.6266\n",
      "Epoch 24 train loss: 0.6832, eval loss 0.6562501788139343\n",
      "optimal threshold: -0.6374\n",
      "Epoch 25 train loss: 0.7204, eval loss 0.6559369564056396\n",
      "optimal threshold: -0.6451\n",
      "Epoch 26 train loss: 0.6604, eval loss 0.6561242938041687\n",
      "optimal threshold: -0.6270\n",
      "Epoch 27 train loss: 0.6722, eval loss 0.6569750905036926\n",
      "optimal threshold: -0.5442\n",
      "Epoch 28 train loss: 0.7174, eval loss 0.6563348174095154\n",
      "optimal threshold: -0.6343\n",
      "Epoch 29 train loss: 0.6812, eval loss 0.6568607687950134\n",
      "optimal threshold: -0.4191\n",
      "Epoch 30 train loss: 0.6781, eval loss 0.6572887301445007\n",
      "optimal threshold: -0.5967\n",
      "Epoch 31 train loss: 0.6577, eval loss 0.6565479636192322\n",
      "optimal threshold: -0.5707\n",
      "Epoch 32 train loss: 0.7508, eval loss 0.6571095585823059\n",
      "optimal threshold: -0.6527\n",
      "Epoch 33 train loss: 0.6056, eval loss 0.6566266417503357\n",
      "optimal threshold: -0.5797\n",
      "Epoch 34 train loss: 0.6605, eval loss 0.6570751667022705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:26:45,181] Trial 14 finished with value: 0.7013633847236633 and parameters: {'learning_rate_exp': -4.163837662416018, 'dropout_p': 0.41341601926963234, 'l2_reg_exp': -2.4736392775107863, 'batch_size': 50, 'N': 332}. Best is trial 6 with value: 0.45701825618743896.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5716\n",
      "optimal threshold: -0.3021\n",
      "Epoch 0 train loss: 1.2896, eval loss 1.2673128843307495\n",
      "optimal threshold: -0.6105\n",
      "Epoch 1 train loss: 1.2120, eval loss 1.1243113279342651\n",
      "optimal threshold: -0.8684\n",
      "Epoch 2 train loss: 1.1238, eval loss 0.9964551329612732\n",
      "optimal threshold: -0.9253\n",
      "Epoch 3 train loss: 1.0215, eval loss 0.8997666239738464\n",
      "optimal threshold: -0.9472\n",
      "Epoch 4 train loss: 0.8227, eval loss 0.828924834728241\n",
      "optimal threshold: -1.0117\n",
      "Epoch 5 train loss: 0.7118, eval loss 0.7796105146408081\n",
      "optimal threshold: -0.9666\n",
      "Epoch 6 train loss: 0.6210, eval loss 0.7501726150512695\n",
      "optimal threshold: -0.9513\n",
      "Epoch 7 train loss: 0.5691, eval loss 0.733527660369873\n",
      "optimal threshold: -1.0236\n",
      "Epoch 8 train loss: 0.5528, eval loss 0.722826361656189\n",
      "optimal threshold: -0.8296\n",
      "Epoch 9 train loss: 0.4662, eval loss 0.7156820893287659\n",
      "optimal threshold: -0.8432\n",
      "Epoch 10 train loss: 0.4736, eval loss 0.7099340558052063\n",
      "optimal threshold: -0.8441\n",
      "Epoch 11 train loss: 0.3693, eval loss 0.7052332162857056\n",
      "optimal threshold: -0.8501\n",
      "Epoch 12 train loss: 0.5237, eval loss 0.7014681100845337\n",
      "optimal threshold: -0.8535\n",
      "Epoch 13 train loss: 0.4241, eval loss 0.6979234218597412\n",
      "optimal threshold: -0.8411\n",
      "Epoch 14 train loss: 0.4871, eval loss 0.694781482219696\n",
      "optimal threshold: -0.8405\n",
      "Epoch 15 train loss: 0.4025, eval loss 0.6917961239814758\n",
      "optimal threshold: -0.8188\n",
      "Epoch 16 train loss: 0.6625, eval loss 0.6891332864761353\n",
      "optimal threshold: -0.8492\n",
      "Epoch 17 train loss: 0.4361, eval loss 0.6869561672210693\n",
      "optimal threshold: -0.8924\n",
      "Epoch 18 train loss: 0.4243, eval loss 0.6849567294120789\n",
      "optimal threshold: -0.9563\n",
      "Epoch 19 train loss: 0.5038, eval loss 0.6829495429992676\n",
      "optimal threshold: -0.9384\n",
      "Epoch 20 train loss: 0.3669, eval loss 0.681101381778717\n",
      "optimal threshold: -0.9435\n",
      "Epoch 21 train loss: 0.3476, eval loss 0.679522693157196\n",
      "optimal threshold: -0.9215\n",
      "Epoch 22 train loss: 0.3670, eval loss 0.6780397891998291\n",
      "optimal threshold: -0.9323\n",
      "Epoch 23 train loss: 0.3847, eval loss 0.6766864657402039\n",
      "optimal threshold: -0.9311\n",
      "Epoch 24 train loss: 0.4390, eval loss 0.6753773093223572\n",
      "optimal threshold: -0.6128\n",
      "Epoch 25 train loss: 0.5534, eval loss 0.6741318702697754\n",
      "optimal threshold: -0.7415\n",
      "Epoch 26 train loss: 0.4160, eval loss 0.6731148958206177\n",
      "optimal threshold: -0.6093\n",
      "Epoch 27 train loss: 0.4932, eval loss 0.6721205115318298\n",
      "optimal threshold: -0.6247\n",
      "Epoch 28 train loss: 0.3528, eval loss 0.6714012622833252\n",
      "optimal threshold: -0.5986\n",
      "Epoch 29 train loss: 0.4038, eval loss 0.6705116629600525\n",
      "optimal threshold: -0.5980\n",
      "Epoch 30 train loss: 0.3868, eval loss 0.669837236404419\n",
      "optimal threshold: -0.5861\n",
      "Epoch 31 train loss: 0.3677, eval loss 0.6690605282783508\n",
      "optimal threshold: -0.7224\n",
      "Epoch 32 train loss: 0.3433, eval loss 0.6684165000915527\n",
      "optimal threshold: -0.6509\n",
      "Epoch 33 train loss: 0.4092, eval loss 0.6677723526954651\n",
      "optimal threshold: -0.6596\n",
      "Epoch 34 train loss: 0.4102, eval loss 0.6672002673149109\n",
      "optimal threshold: -0.6751\n",
      "Epoch 35 train loss: 0.3830, eval loss 0.6668240427970886\n",
      "optimal threshold: -0.6590\n",
      "Epoch 36 train loss: 0.3795, eval loss 0.6661635637283325\n",
      "optimal threshold: -0.6531\n",
      "Epoch 37 train loss: 0.3670, eval loss 0.665678083896637\n",
      "optimal threshold: -0.6631\n",
      "Epoch 38 train loss: 0.4188, eval loss 0.6654829382896423\n",
      "optimal threshold: -0.6822\n",
      "Epoch 39 train loss: 0.3961, eval loss 0.6648828983306885\n",
      "optimal threshold: -0.5742\n",
      "Epoch 40 train loss: 0.4502, eval loss 0.6645586490631104\n",
      "optimal threshold: -0.6003\n",
      "Epoch 41 train loss: 0.4218, eval loss 0.6642844080924988\n",
      "optimal threshold: -0.5777\n",
      "Epoch 42 train loss: 0.3846, eval loss 0.6638203263282776\n",
      "optimal threshold: -0.5759\n",
      "Epoch 43 train loss: 0.4035, eval loss 0.6634905934333801\n",
      "optimal threshold: -0.5903\n",
      "Epoch 44 train loss: 0.4667, eval loss 0.6632522344589233\n",
      "optimal threshold: -0.5360\n",
      "Epoch 45 train loss: 0.3276, eval loss 0.6628469228744507\n",
      "optimal threshold: -0.6138\n",
      "Epoch 46 train loss: 0.2409, eval loss 0.6626722812652588\n",
      "optimal threshold: -0.5427\n",
      "Epoch 47 train loss: 0.3560, eval loss 0.6624789237976074\n",
      "optimal threshold: -0.6230\n",
      "Epoch 48 train loss: 0.3322, eval loss 0.6622709035873413\n",
      "optimal threshold: -0.5293\n",
      "Epoch 49 train loss: 0.4732, eval loss 0.6619049906730652\n",
      "optimal threshold: -0.6426\n",
      "Epoch 50 train loss: 0.3276, eval loss 0.6617833971977234\n",
      "optimal threshold: -0.6380\n",
      "Epoch 51 train loss: 0.3603, eval loss 0.6615355014801025\n",
      "optimal threshold: -0.5602\n",
      "Epoch 52 train loss: 0.5326, eval loss 0.6614261865615845\n",
      "optimal threshold: -0.5676\n",
      "Epoch 53 train loss: 0.3038, eval loss 0.6611285209655762\n",
      "optimal threshold: -0.6027\n",
      "Epoch 54 train loss: 0.3347, eval loss 0.660781741142273\n",
      "optimal threshold: -0.6036\n",
      "Epoch 55 train loss: 0.3059, eval loss 0.6607309579849243\n",
      "optimal threshold: -0.6082\n",
      "Epoch 56 train loss: 0.3670, eval loss 0.6605523824691772\n",
      "optimal threshold: -0.6259\n",
      "Epoch 57 train loss: 0.3857, eval loss 0.6604897975921631\n",
      "optimal threshold: -0.6251\n",
      "Epoch 58 train loss: 0.3350, eval loss 0.6602281928062439\n",
      "optimal threshold: -0.5468\n",
      "Epoch 59 train loss: 0.3945, eval loss 0.6598701477050781\n",
      "optimal threshold: -0.6403\n",
      "Epoch 60 train loss: 0.3312, eval loss 0.6599311828613281\n",
      "optimal threshold: -0.4340\n",
      "Epoch 61 train loss: 0.3393, eval loss 0.6598440408706665\n",
      "optimal threshold: -0.5677\n",
      "Epoch 62 train loss: 0.4132, eval loss 0.6595726013183594\n",
      "optimal threshold: -0.6214\n",
      "Epoch 63 train loss: 0.3486, eval loss 0.6596211194992065\n",
      "optimal threshold: -0.6208\n",
      "Epoch 64 train loss: 0.4121, eval loss 0.6594735383987427\n",
      "optimal threshold: -0.6243\n",
      "Epoch 65 train loss: 0.3152, eval loss 0.6593548655509949\n",
      "optimal threshold: -0.6320\n",
      "Epoch 66 train loss: 0.2691, eval loss 0.659143328666687\n",
      "optimal threshold: -0.6092\n",
      "Epoch 67 train loss: 0.3767, eval loss 0.6590935587882996\n",
      "optimal threshold: -0.6259\n",
      "Epoch 68 train loss: 0.3532, eval loss 0.659028172492981\n",
      "optimal threshold: -0.6174\n",
      "Epoch 69 train loss: 0.4337, eval loss 0.6590421795845032\n",
      "optimal threshold: -0.6457\n",
      "Epoch 70 train loss: 0.3396, eval loss 0.6590986251831055\n",
      "optimal threshold: -0.6406\n",
      "Epoch 71 train loss: 0.3787, eval loss 0.6588677763938904\n",
      "optimal threshold: -0.6083\n",
      "Epoch 72 train loss: 0.2505, eval loss 0.6587060689926147\n",
      "optimal threshold: -0.5146\n",
      "Epoch 73 train loss: 0.3497, eval loss 0.6584579348564148\n",
      "optimal threshold: -0.5017\n",
      "Epoch 74 train loss: 0.3599, eval loss 0.6582764387130737\n",
      "optimal threshold: -0.6158\n",
      "Epoch 75 train loss: 0.4438, eval loss 0.6582521200180054\n",
      "optimal threshold: -0.6026\n",
      "Epoch 76 train loss: 0.3635, eval loss 0.6582494974136353\n",
      "optimal threshold: -0.5966\n",
      "Epoch 77 train loss: 0.3556, eval loss 0.6580809354782104\n",
      "optimal threshold: -0.6098\n",
      "Epoch 78 train loss: 0.2856, eval loss 0.6582753658294678\n",
      "optimal threshold: -0.5506\n",
      "Epoch 79 train loss: 0.3575, eval loss 0.6582551598548889\n",
      "optimal threshold: -0.6066\n",
      "Epoch 80 train loss: 0.3841, eval loss 0.6582302451133728\n",
      "optimal threshold: -0.5687\n",
      "Epoch 81 train loss: 0.4299, eval loss 0.6582823395729065\n",
      "optimal threshold: -0.6232\n",
      "Epoch 82 train loss: 0.3744, eval loss 0.6580408215522766\n",
      "optimal threshold: -0.6309\n",
      "Epoch 83 train loss: 0.3368, eval loss 0.6581339836120605\n",
      "optimal threshold: -0.6191\n",
      "Epoch 84 train loss: 0.4195, eval loss 0.6578563451766968\n",
      "optimal threshold: -0.6196\n",
      "Epoch 85 train loss: 0.4070, eval loss 0.657833993434906\n",
      "optimal threshold: -0.6167\n",
      "Epoch 86 train loss: 0.3507, eval loss 0.6574852466583252\n",
      "optimal threshold: -0.6196\n",
      "Epoch 87 train loss: 0.3806, eval loss 0.6574429869651794\n",
      "optimal threshold: -0.6246\n",
      "Epoch 88 train loss: 0.2502, eval loss 0.657531201839447\n",
      "optimal threshold: -0.6194\n",
      "Epoch 89 train loss: 0.2792, eval loss 0.657501220703125\n",
      "optimal threshold: -0.6357\n",
      "Epoch 90 train loss: 0.4548, eval loss 0.6574230790138245\n",
      "optimal threshold: -0.6117\n",
      "Epoch 91 train loss: 0.3206, eval loss 0.6574126482009888\n",
      "optimal threshold: -0.6224\n",
      "Epoch 92 train loss: 0.3382, eval loss 0.6574152112007141\n",
      "optimal threshold: -0.6176\n",
      "Epoch 93 train loss: 0.4000, eval loss 0.6572748422622681\n",
      "optimal threshold: -0.6377\n",
      "Epoch 94 train loss: 0.3275, eval loss 0.6573268175125122\n",
      "optimal threshold: -0.6079\n",
      "Epoch 95 train loss: 0.4001, eval loss 0.6572437882423401\n",
      "optimal threshold: -0.6068\n",
      "Epoch 96 train loss: 0.2715, eval loss 0.6571073532104492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6077\n",
      "Epoch 97 train loss: 0.3233, eval loss 0.6571996212005615\n",
      "optimal threshold: -0.6350\n",
      "Epoch 98 train loss: 0.3504, eval loss 0.6570984721183777\n",
      "optimal threshold: -0.6155\n",
      "Epoch 99 train loss: 0.3755, eval loss 0.6569058299064636\n",
      "optimal threshold: -0.6196\n",
      "Epoch 100 train loss: 0.3146, eval loss 0.6571347713470459\n",
      "optimal threshold: -0.6337\n",
      "Epoch 101 train loss: 0.2991, eval loss 0.6569617986679077\n",
      "optimal threshold: -0.6250\n",
      "Epoch 102 train loss: 0.4445, eval loss 0.6569969654083252\n",
      "optimal threshold: -0.6267\n",
      "Epoch 103 train loss: 0.4011, eval loss 0.6567726731300354\n",
      "optimal threshold: -0.6268\n",
      "Epoch 104 train loss: 0.3094, eval loss 0.6568347811698914\n",
      "optimal threshold: -0.6227\n",
      "Epoch 105 train loss: 0.3383, eval loss 0.6568310260772705\n",
      "optimal threshold: -0.6335\n",
      "Epoch 106 train loss: 0.3042, eval loss 0.6569876074790955\n",
      "optimal threshold: -0.6239\n",
      "Epoch 107 train loss: 0.3960, eval loss 0.6568359136581421\n",
      "optimal threshold: -0.6192\n",
      "Epoch 108 train loss: 0.3733, eval loss 0.6569364070892334\n",
      "optimal threshold: -0.6138\n",
      "Epoch 109 train loss: 0.3003, eval loss 0.6567302346229553\n",
      "optimal threshold: -0.6194\n",
      "Epoch 110 train loss: 0.3997, eval loss 0.6567395329475403\n",
      "optimal threshold: -0.6263\n",
      "Epoch 111 train loss: 0.3340, eval loss 0.6567620635032654\n",
      "optimal threshold: -0.6133\n",
      "Epoch 112 train loss: 0.3480, eval loss 0.6566818356513977\n",
      "optimal threshold: -0.6151\n",
      "Epoch 113 train loss: 0.3756, eval loss 0.6567374467849731\n",
      "optimal threshold: -0.6434\n",
      "Epoch 114 train loss: 0.3040, eval loss 0.6566280722618103\n",
      "optimal threshold: -0.6441\n",
      "Epoch 115 train loss: 0.3675, eval loss 0.6567306518554688\n",
      "optimal threshold: -0.6470\n",
      "Epoch 116 train loss: 0.3216, eval loss 0.6567294001579285\n",
      "optimal threshold: -0.6336\n",
      "Epoch 117 train loss: 0.3685, eval loss 0.656853199005127\n",
      "optimal threshold: -0.6361\n",
      "Epoch 118 train loss: 0.2800, eval loss 0.6567094326019287\n",
      "optimal threshold: -0.6143\n",
      "Epoch 119 train loss: 0.3357, eval loss 0.6563867926597595\n",
      "optimal threshold: -0.6215\n",
      "Epoch 120 train loss: 0.3420, eval loss 0.6565536260604858\n",
      "optimal threshold: -0.6181\n",
      "Epoch 121 train loss: 0.3132, eval loss 0.6564288139343262\n",
      "optimal threshold: -0.6233\n",
      "Epoch 122 train loss: 0.3549, eval loss 0.6567015051841736\n",
      "optimal threshold: -0.6229\n",
      "Epoch 123 train loss: 0.3034, eval loss 0.6566241979598999\n",
      "optimal threshold: -0.6175\n",
      "Epoch 124 train loss: 0.2750, eval loss 0.6565510034561157\n",
      "optimal threshold: -0.6431\n",
      "Epoch 125 train loss: 0.3245, eval loss 0.6567283868789673\n",
      "optimal threshold: -0.6282\n",
      "Epoch 126 train loss: 0.3324, eval loss 0.6566987633705139\n",
      "optimal threshold: -0.6297\n",
      "Epoch 127 train loss: 0.2854, eval loss 0.6567250490188599\n",
      "optimal threshold: -0.6373\n",
      "Epoch 128 train loss: 0.2996, eval loss 0.6565899848937988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:29:20,507] Trial 15 finished with value: 0.3374786078929901 and parameters: {'learning_rate_exp': -4.685694142124306, 'dropout_p': 0.46719090289822385, 'l2_reg_exp': -4.447377565499641, 'batch_size': 78, 'N': 197}. Best is trial 15 with value: 0.3374786078929901.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6169\n",
      "optimal threshold: -0.9737\n",
      "Epoch 0 train loss: 0.5964, eval loss 0.7401706576347351\n",
      "optimal threshold: -0.5423\n",
      "Epoch 1 train loss: 0.4146, eval loss 0.6996002197265625\n",
      "optimal threshold: -0.8205\n",
      "Epoch 2 train loss: 0.2780, eval loss 0.68442702293396\n",
      "optimal threshold: -0.6872\n",
      "Epoch 3 train loss: 0.5877, eval loss 0.6760835647583008\n",
      "optimal threshold: -0.6937\n",
      "Epoch 4 train loss: 0.5074, eval loss 0.6709772348403931\n",
      "optimal threshold: -0.5817\n",
      "Epoch 5 train loss: 0.4453, eval loss 0.6678807735443115\n",
      "optimal threshold: -0.4639\n",
      "Epoch 6 train loss: 0.4088, eval loss 0.6656294465065002\n",
      "optimal threshold: -0.3936\n",
      "Epoch 7 train loss: 0.4495, eval loss 0.6639251112937927\n",
      "optimal threshold: -0.5013\n",
      "Epoch 8 train loss: 0.4125, eval loss 0.663314163684845\n",
      "optimal threshold: -0.4131\n",
      "Epoch 9 train loss: 0.3864, eval loss 0.6623284220695496\n",
      "optimal threshold: -0.4167\n",
      "Epoch 10 train loss: 0.3477, eval loss 0.6617060899734497\n",
      "optimal threshold: -0.4254\n",
      "Epoch 11 train loss: 0.3788, eval loss 0.6607414484024048\n",
      "optimal threshold: -0.8256\n",
      "Epoch 12 train loss: 0.3797, eval loss 0.6604991555213928\n",
      "optimal threshold: -0.7660\n",
      "Epoch 13 train loss: 0.3991, eval loss 0.659779965877533\n",
      "optimal threshold: -0.7133\n",
      "Epoch 14 train loss: 0.3331, eval loss 0.6592465043067932\n",
      "optimal threshold: -0.7041\n",
      "Epoch 15 train loss: 0.3356, eval loss 0.659074068069458\n",
      "optimal threshold: -0.7350\n",
      "Epoch 16 train loss: 0.3898, eval loss 0.6587435007095337\n",
      "optimal threshold: -0.7611\n",
      "Epoch 17 train loss: 0.2808, eval loss 0.659184992313385\n",
      "optimal threshold: -0.5412\n",
      "Epoch 18 train loss: 0.5375, eval loss 0.6581035256385803\n",
      "optimal threshold: -0.5504\n",
      "Epoch 19 train loss: 0.3447, eval loss 0.6584104895591736\n",
      "optimal threshold: -0.5796\n",
      "Epoch 20 train loss: 0.3294, eval loss 0.6587873101234436\n",
      "optimal threshold: -0.5576\n",
      "Epoch 21 train loss: 0.3695, eval loss 0.6593031287193298\n",
      "optimal threshold: -0.5807\n",
      "Epoch 22 train loss: 0.3138, eval loss 0.6580060124397278\n",
      "optimal threshold: -0.5854\n",
      "Epoch 23 train loss: 0.3334, eval loss 0.6581502556800842\n",
      "optimal threshold: -0.5388\n",
      "Epoch 24 train loss: 0.3327, eval loss 0.6582511067390442\n",
      "optimal threshold: -0.5849\n",
      "Epoch 25 train loss: 0.3526, eval loss 0.65772944688797\n",
      "optimal threshold: -0.5307\n",
      "Epoch 26 train loss: 0.4420, eval loss 0.6585013270378113\n",
      "optimal threshold: -0.5019\n",
      "Epoch 27 train loss: 0.3346, eval loss 0.6585540771484375\n",
      "optimal threshold: -0.4996\n",
      "Epoch 28 train loss: 0.2952, eval loss 0.6583014130592346\n",
      "optimal threshold: -0.5147\n",
      "Epoch 29 train loss: 0.2789, eval loss 0.6580800414085388\n",
      "optimal threshold: -0.5175\n",
      "Epoch 30 train loss: 0.2826, eval loss 0.6589608788490295\n",
      "optimal threshold: -0.5419\n",
      "Epoch 31 train loss: 0.2969, eval loss 0.6587290167808533\n",
      "optimal threshold: -0.5392\n",
      "Epoch 32 train loss: 0.3617, eval loss 0.6588171124458313\n",
      "optimal threshold: -0.5067\n",
      "Epoch 33 train loss: 0.3560, eval loss 0.6586485505104065\n",
      "optimal threshold: -0.5114\n",
      "Epoch 34 train loss: 0.2913, eval loss 0.6591997146606445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:33:15,500] Trial 16 finished with value: 0.2552145719528198 and parameters: {'learning_rate_exp': -4.3338373173806755, 'dropout_p': 0.4924947839678394, 'l2_reg_exp': -3.700732476486805, 'batch_size': 10, 'N': 200}. Best is trial 16 with value: 0.2552145719528198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5013\n",
      "optimal threshold: -0.1789\n",
      "Epoch 0 train loss: 1.3486, eval loss 1.3391282558441162\n",
      "optimal threshold: -0.3176\n",
      "Epoch 1 train loss: 1.2845, eval loss 1.2656267881393433\n",
      "optimal threshold: -0.4782\n",
      "Epoch 2 train loss: 1.1886, eval loss 1.1736689805984497\n",
      "optimal threshold: -0.6692\n",
      "Epoch 3 train loss: 1.0746, eval loss 1.0677647590637207\n",
      "optimal threshold: -0.8184\n",
      "Epoch 4 train loss: 0.9909, eval loss 0.9700710773468018\n",
      "optimal threshold: -0.9130\n",
      "Epoch 5 train loss: 0.8861, eval loss 0.8928799033164978\n",
      "optimal threshold: -0.8866\n",
      "Epoch 6 train loss: 0.8356, eval loss 0.8346059322357178\n",
      "optimal threshold: -0.8427\n",
      "Epoch 7 train loss: 0.8204, eval loss 0.7916527986526489\n",
      "optimal threshold: -0.7797\n",
      "Epoch 8 train loss: 0.7504, eval loss 0.7617855072021484\n",
      "optimal threshold: -0.8895\n",
      "Epoch 9 train loss: 0.7646, eval loss 0.7414669990539551\n",
      "optimal threshold: -0.8788\n",
      "Epoch 10 train loss: 0.7178, eval loss 0.7287276387214661\n",
      "optimal threshold: -0.7809\n",
      "Epoch 11 train loss: 0.7382, eval loss 0.7201924324035645\n",
      "optimal threshold: -0.8175\n",
      "Epoch 12 train loss: 0.7349, eval loss 0.7144820094108582\n",
      "optimal threshold: -0.8609\n",
      "Epoch 13 train loss: 0.6767, eval loss 0.7099559307098389\n",
      "optimal threshold: -0.8627\n",
      "Epoch 14 train loss: 0.7029, eval loss 0.7061045169830322\n",
      "optimal threshold: -0.8527\n",
      "Epoch 15 train loss: 0.6742, eval loss 0.7026879787445068\n",
      "optimal threshold: -0.8221\n",
      "Epoch 16 train loss: 0.7113, eval loss 0.6996400952339172\n",
      "optimal threshold: -0.6826\n",
      "Epoch 17 train loss: 0.6745, eval loss 0.6970672607421875\n",
      "optimal threshold: -0.6943\n",
      "Epoch 18 train loss: 0.6606, eval loss 0.6947693228721619\n",
      "optimal threshold: -0.7147\n",
      "Epoch 19 train loss: 0.7044, eval loss 0.6925650835037231\n",
      "optimal threshold: -0.6944\n",
      "Epoch 20 train loss: 0.6414, eval loss 0.690502405166626\n",
      "optimal threshold: -0.7420\n",
      "Epoch 21 train loss: 0.6531, eval loss 0.6887714266777039\n",
      "optimal threshold: -0.7270\n",
      "Epoch 22 train loss: 0.6784, eval loss 0.6874266862869263\n",
      "optimal threshold: -0.7045\n",
      "Epoch 23 train loss: 0.6651, eval loss 0.6854939460754395\n",
      "optimal threshold: -0.7112\n",
      "Epoch 24 train loss: 0.6516, eval loss 0.6842523813247681\n",
      "optimal threshold: -0.7375\n",
      "Epoch 25 train loss: 0.6043, eval loss 0.6828323602676392\n",
      "optimal threshold: -0.7168\n",
      "Epoch 26 train loss: 0.6618, eval loss 0.6813616156578064\n",
      "optimal threshold: -0.7076\n",
      "Epoch 27 train loss: 0.6282, eval loss 0.6802737712860107\n",
      "optimal threshold: -0.7147\n",
      "Epoch 28 train loss: 0.6527, eval loss 0.6792134642601013\n",
      "optimal threshold: -0.7382\n",
      "Epoch 29 train loss: 0.6622, eval loss 0.6780823469161987\n",
      "optimal threshold: -0.7054\n",
      "Epoch 30 train loss: 0.6386, eval loss 0.6771135926246643\n",
      "optimal threshold: -0.7727\n",
      "Epoch 31 train loss: 0.6104, eval loss 0.6765655279159546\n",
      "optimal threshold: -0.7913\n",
      "Epoch 32 train loss: 0.6058, eval loss 0.6755419373512268\n",
      "optimal threshold: -0.7813\n",
      "Epoch 33 train loss: 0.6625, eval loss 0.674518883228302\n",
      "optimal threshold: -0.7990\n",
      "Epoch 34 train loss: 0.6428, eval loss 0.6739088892936707\n",
      "optimal threshold: -0.8347\n",
      "Epoch 35 train loss: 0.6538, eval loss 0.673335611820221\n",
      "optimal threshold: -0.8598\n",
      "Epoch 36 train loss: 0.6506, eval loss 0.6725865006446838\n",
      "optimal threshold: -0.8284\n",
      "Epoch 37 train loss: 0.6598, eval loss 0.6717786192893982\n",
      "optimal threshold: -0.8270\n",
      "Epoch 38 train loss: 0.6413, eval loss 0.6713495850563049\n",
      "optimal threshold: -0.8356\n",
      "Epoch 39 train loss: 0.6721, eval loss 0.6709225177764893\n",
      "optimal threshold: -0.8382\n",
      "Epoch 40 train loss: 0.6454, eval loss 0.670409619808197\n",
      "optimal threshold: -0.8456\n",
      "Epoch 41 train loss: 0.6599, eval loss 0.6699811220169067\n",
      "optimal threshold: -0.8649\n",
      "Epoch 42 train loss: 0.6683, eval loss 0.6694082617759705\n",
      "optimal threshold: -0.4247\n",
      "Epoch 43 train loss: 0.6237, eval loss 0.6685077548027039\n",
      "optimal threshold: -0.4528\n",
      "Epoch 44 train loss: 0.6217, eval loss 0.6683456897735596\n",
      "optimal threshold: -0.4344\n",
      "Epoch 45 train loss: 0.6426, eval loss 0.6678702235221863\n",
      "optimal threshold: -0.4472\n",
      "Epoch 46 train loss: 0.6096, eval loss 0.6676344871520996\n",
      "optimal threshold: -0.4515\n",
      "Epoch 47 train loss: 0.6662, eval loss 0.6671071648597717\n",
      "optimal threshold: -0.4628\n",
      "Epoch 48 train loss: 0.5993, eval loss 0.6670125722885132\n",
      "optimal threshold: -0.4469\n",
      "Epoch 49 train loss: 0.6071, eval loss 0.6668106317520142\n",
      "optimal threshold: -0.4347\n",
      "Epoch 50 train loss: 0.5981, eval loss 0.6664418578147888\n",
      "optimal threshold: -0.4282\n",
      "Epoch 51 train loss: 0.6069, eval loss 0.6660301685333252\n",
      "optimal threshold: -0.4377\n",
      "Epoch 52 train loss: 0.6404, eval loss 0.6656495332717896\n",
      "optimal threshold: -0.4385\n",
      "Epoch 53 train loss: 0.6257, eval loss 0.6653675436973572\n",
      "optimal threshold: -0.4645\n",
      "Epoch 54 train loss: 0.6417, eval loss 0.6654038429260254\n",
      "optimal threshold: -0.4439\n",
      "Epoch 55 train loss: 0.6240, eval loss 0.6650760769844055\n",
      "optimal threshold: -0.4507\n",
      "Epoch 56 train loss: 0.6395, eval loss 0.6646617650985718\n",
      "optimal threshold: -0.4550\n",
      "Epoch 57 train loss: 0.6159, eval loss 0.6645606160163879\n",
      "optimal threshold: -0.4648\n",
      "Epoch 58 train loss: 0.5860, eval loss 0.6644697189331055\n",
      "optimal threshold: -0.4547\n",
      "Epoch 59 train loss: 0.6380, eval loss 0.6642309427261353\n",
      "optimal threshold: -0.4796\n",
      "Epoch 60 train loss: 0.6336, eval loss 0.6641846299171448\n",
      "optimal threshold: -0.4435\n",
      "Epoch 61 train loss: 0.6339, eval loss 0.664047122001648\n",
      "optimal threshold: -0.4118\n",
      "Epoch 62 train loss: 0.6330, eval loss 0.663477897644043\n",
      "optimal threshold: -0.4794\n",
      "Epoch 63 train loss: 0.5983, eval loss 0.6634203791618347\n",
      "optimal threshold: -0.5286\n",
      "Epoch 64 train loss: 0.5949, eval loss 0.6633245348930359\n",
      "optimal threshold: -0.5226\n",
      "Epoch 65 train loss: 0.6094, eval loss 0.663119375705719\n",
      "optimal threshold: -0.4038\n",
      "Epoch 66 train loss: 0.6169, eval loss 0.662866473197937\n",
      "optimal threshold: -0.4988\n",
      "Epoch 67 train loss: 0.6217, eval loss 0.6627234220504761\n",
      "optimal threshold: -0.5223\n",
      "Epoch 68 train loss: 0.6277, eval loss 0.6627401113510132\n",
      "optimal threshold: -0.5227\n",
      "Epoch 69 train loss: 0.6131, eval loss 0.6626973152160645\n",
      "optimal threshold: -0.5107\n",
      "Epoch 70 train loss: 0.6197, eval loss 0.6624787449836731\n",
      "optimal threshold: -0.5183\n",
      "Epoch 71 train loss: 0.6519, eval loss 0.6625723838806152\n",
      "optimal threshold: -0.5201\n",
      "Epoch 72 train loss: 0.6300, eval loss 0.66246098279953\n",
      "optimal threshold: -0.8985\n",
      "Epoch 73 train loss: 0.6084, eval loss 0.6621612906455994\n",
      "optimal threshold: -0.5550\n",
      "Epoch 74 train loss: 0.5876, eval loss 0.6621831655502319\n",
      "optimal threshold: -0.5255\n",
      "Epoch 75 train loss: 0.6228, eval loss 0.6618220806121826\n",
      "optimal threshold: -0.4622\n",
      "Epoch 76 train loss: 0.6163, eval loss 0.6618626713752747\n",
      "optimal threshold: -0.4629\n",
      "Epoch 77 train loss: 0.6003, eval loss 0.6615244746208191\n",
      "optimal threshold: -0.4506\n",
      "Epoch 78 train loss: 0.6103, eval loss 0.6614895462989807\n",
      "optimal threshold: -0.4692\n",
      "Epoch 79 train loss: 0.6581, eval loss 0.6615108251571655\n",
      "optimal threshold: -0.4557\n",
      "Epoch 80 train loss: 0.6083, eval loss 0.6614513397216797\n",
      "optimal threshold: -0.4552\n",
      "Epoch 81 train loss: 0.6056, eval loss 0.6613726615905762\n",
      "optimal threshold: -0.4566\n",
      "Epoch 82 train loss: 0.6081, eval loss 0.6612235307693481\n",
      "optimal threshold: -0.4543\n",
      "Epoch 83 train loss: 0.6004, eval loss 0.6610928177833557\n",
      "optimal threshold: -0.5312\n",
      "Epoch 84 train loss: 0.6031, eval loss 0.6613824367523193\n",
      "optimal threshold: -0.5217\n",
      "Epoch 85 train loss: 0.6310, eval loss 0.6614352464675903\n",
      "optimal threshold: -0.5223\n",
      "Epoch 86 train loss: 0.5919, eval loss 0.6613349318504333\n",
      "optimal threshold: -0.5232\n",
      "Epoch 87 train loss: 0.5942, eval loss 0.6609671711921692\n",
      "optimal threshold: -0.4933\n",
      "Epoch 88 train loss: 0.5861, eval loss 0.6610141396522522\n",
      "optimal threshold: -0.4900\n",
      "Epoch 89 train loss: 0.6131, eval loss 0.6610879302024841\n",
      "optimal threshold: -0.4892\n",
      "Epoch 90 train loss: 0.6231, eval loss 0.6609453558921814\n",
      "optimal threshold: -0.5036\n",
      "Epoch 91 train loss: 0.6088, eval loss 0.6608531475067139\n",
      "optimal threshold: -0.4964\n",
      "Epoch 92 train loss: 0.6170, eval loss 0.660908043384552\n",
      "optimal threshold: -0.4982\n",
      "Epoch 93 train loss: 0.6292, eval loss 0.6605714559555054\n",
      "optimal threshold: -0.4687\n",
      "Epoch 94 train loss: 0.6312, eval loss 0.6607145071029663\n",
      "optimal threshold: -0.4755\n",
      "Epoch 95 train loss: 0.5995, eval loss 0.6605190634727478\n",
      "optimal threshold: -0.5157\n",
      "Epoch 96 train loss: 0.6097, eval loss 0.6605110764503479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5055\n",
      "Epoch 97 train loss: 0.6011, eval loss 0.6604666113853455\n",
      "optimal threshold: -0.5032\n",
      "Epoch 98 train loss: 0.5997, eval loss 0.6604499816894531\n",
      "optimal threshold: -0.5141\n",
      "Epoch 99 train loss: 0.6004, eval loss 0.6607897877693176\n",
      "optimal threshold: -0.4872\n",
      "Epoch 100 train loss: 0.6142, eval loss 0.6606324315071106\n",
      "optimal threshold: -0.4970\n",
      "Epoch 101 train loss: 0.5968, eval loss 0.6607574820518494\n",
      "optimal threshold: -0.4907\n",
      "Epoch 102 train loss: 0.6149, eval loss 0.6606631875038147\n",
      "optimal threshold: -0.4965\n",
      "Epoch 103 train loss: 0.6007, eval loss 0.6606253981590271\n",
      "optimal threshold: -0.4763\n",
      "Epoch 104 train loss: 0.5997, eval loss 0.6604040265083313\n",
      "optimal threshold: -0.4794\n",
      "Epoch 105 train loss: 0.6071, eval loss 0.6603395938873291\n",
      "optimal threshold: -0.4734\n",
      "Epoch 106 train loss: 0.5931, eval loss 0.6603108644485474\n",
      "optimal threshold: -0.4852\n",
      "Epoch 107 train loss: 0.6172, eval loss 0.66028892993927\n",
      "optimal threshold: -0.4851\n",
      "Epoch 108 train loss: 0.6163, eval loss 0.6602764129638672\n",
      "optimal threshold: -0.4911\n",
      "Epoch 109 train loss: 0.6034, eval loss 0.6604112982749939\n",
      "optimal threshold: -0.4809\n",
      "Epoch 110 train loss: 0.5990, eval loss 0.6602742075920105\n",
      "optimal threshold: -0.4954\n",
      "Epoch 111 train loss: 0.6225, eval loss 0.6603497266769409\n",
      "optimal threshold: -0.4921\n",
      "Epoch 112 train loss: 0.5967, eval loss 0.6601691246032715\n",
      "optimal threshold: -0.4748\n",
      "Epoch 113 train loss: 0.6126, eval loss 0.6601197719573975\n",
      "optimal threshold: -0.4776\n",
      "Epoch 114 train loss: 0.5856, eval loss 0.6600801944732666\n",
      "optimal threshold: -0.3984\n",
      "Epoch 115 train loss: 0.5852, eval loss 0.6598786115646362\n",
      "optimal threshold: -0.4129\n",
      "Epoch 116 train loss: 0.6040, eval loss 0.6602500677108765\n",
      "optimal threshold: -0.4141\n",
      "Epoch 117 train loss: 0.5996, eval loss 0.6602166891098022\n",
      "optimal threshold: -0.4165\n",
      "Epoch 118 train loss: 0.5791, eval loss 0.6603800058364868\n",
      "optimal threshold: -0.4350\n",
      "Epoch 119 train loss: 0.5881, eval loss 0.6603726744651794\n",
      "optimal threshold: -0.4248\n",
      "Epoch 120 train loss: 0.5904, eval loss 0.660197377204895\n",
      "optimal threshold: -0.4506\n",
      "Epoch 121 train loss: 0.6010, eval loss 0.6601613759994507\n",
      "optimal threshold: -0.4647\n",
      "Epoch 122 train loss: 0.6027, eval loss 0.6598861217498779\n",
      "optimal threshold: -0.4063\n",
      "Epoch 123 train loss: 0.6255, eval loss 0.659634530544281\n",
      "optimal threshold: -0.4238\n",
      "Epoch 124 train loss: 0.5982, eval loss 0.659957766532898\n",
      "optimal threshold: -0.4723\n",
      "Epoch 125 train loss: 0.6102, eval loss 0.6600914001464844\n",
      "optimal threshold: -0.4108\n",
      "Epoch 126 train loss: 0.5868, eval loss 0.6600627899169922\n",
      "optimal threshold: -0.4066\n",
      "Epoch 127 train loss: 0.6233, eval loss 0.6600741147994995\n",
      "optimal threshold: -0.4109\n",
      "Epoch 128 train loss: 0.6123, eval loss 0.660295844078064\n",
      "optimal threshold: -0.3906\n",
      "Epoch 129 train loss: 0.6111, eval loss 0.6601141691207886\n",
      "optimal threshold: -0.4015\n",
      "Epoch 130 train loss: 0.5925, eval loss 0.6600635647773743\n",
      "optimal threshold: -0.4123\n",
      "Epoch 131 train loss: 0.6137, eval loss 0.6597856283187866\n",
      "optimal threshold: -0.4039\n",
      "Epoch 132 train loss: 0.6348, eval loss 0.6599535942077637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:34:24,586] Trial 17 finished with value: 0.5642330050468445 and parameters: {'learning_rate_exp': -4.159850749750241, 'dropout_p': 0.4793880315013473, 'l2_reg_exp': -3.666074089629841, 'batch_size': 312, 'N': 102}. Best is trial 16 with value: 0.2552145719528198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4112\n",
      "optimal threshold: -0.9005\n",
      "Epoch 0 train loss: 0.7723, eval loss 0.7132331132888794\n",
      "optimal threshold: -0.6988\n",
      "Epoch 1 train loss: 0.7394, eval loss 0.674829363822937\n",
      "optimal threshold: -0.7801\n",
      "Epoch 2 train loss: 0.7412, eval loss 0.6694514751434326\n",
      "optimal threshold: -0.7511\n",
      "Epoch 3 train loss: 0.6874, eval loss 0.6641019582748413\n",
      "optimal threshold: -0.9569\n",
      "Epoch 4 train loss: 0.6497, eval loss 0.6657259464263916\n",
      "optimal threshold: -0.9163\n",
      "Epoch 5 train loss: 0.6287, eval loss 0.6632210612297058\n",
      "optimal threshold: -0.9301\n",
      "Epoch 6 train loss: 0.6451, eval loss 0.6641449332237244\n",
      "optimal threshold: -0.9518\n",
      "Epoch 7 train loss: 0.6179, eval loss 0.6635296940803528\n",
      "optimal threshold: -0.8912\n",
      "Epoch 8 train loss: 0.6535, eval loss 0.6631230711936951\n",
      "optimal threshold: -0.7678\n",
      "Epoch 9 train loss: 0.6374, eval loss 0.6649130582809448\n",
      "optimal threshold: -0.7554\n",
      "Epoch 10 train loss: 0.6030, eval loss 0.6671982407569885\n",
      "optimal threshold: -0.7912\n",
      "Epoch 11 train loss: 0.5873, eval loss 0.6640183329582214\n",
      "optimal threshold: -0.9004\n",
      "Epoch 12 train loss: 0.5686, eval loss 0.6679563522338867\n",
      "optimal threshold: -0.6412\n",
      "Epoch 13 train loss: 0.6073, eval loss 0.6682157516479492\n",
      "optimal threshold: -0.4930\n",
      "Epoch 14 train loss: 0.5511, eval loss 0.6701756119728088\n",
      "optimal threshold: -0.5791\n",
      "Epoch 15 train loss: 0.5433, eval loss 0.6719917058944702\n",
      "optimal threshold: -0.7252\n",
      "Epoch 16 train loss: 0.5416, eval loss 0.6724749207496643\n",
      "optimal threshold: -0.5891\n",
      "Epoch 17 train loss: 0.5274, eval loss 0.6769384145736694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:34:37,486] Trial 18 finished with value: 0.5033701062202454 and parameters: {'learning_rate_exp': -3.246913989235395, 'dropout_p': 0.27089703594007924, 'l2_reg_exp': -5.04050210734194, 'batch_size': 414, 'N': 443}. Best is trial 16 with value: 0.2552145719528198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5213\n",
      "optimal threshold: 0.0439\n",
      "Epoch 0 train loss: 1.4868, eval loss 1.4868359565734863\n",
      "optimal threshold: 0.0308\n",
      "Epoch 1 train loss: 1.4746, eval loss 1.4779034852981567\n",
      "optimal threshold: 0.0176\n",
      "Epoch 2 train loss: 1.4544, eval loss 1.4691401720046997\n",
      "optimal threshold: 0.0054\n",
      "Epoch 3 train loss: 1.4583, eval loss 1.4605053663253784\n",
      "optimal threshold: -0.0062\n",
      "Epoch 4 train loss: 1.4625, eval loss 1.4519731998443604\n",
      "optimal threshold: -0.0228\n",
      "Epoch 5 train loss: 1.4502, eval loss 1.4435614347457886\n",
      "optimal threshold: -0.0506\n",
      "Epoch 6 train loss: 1.4379, eval loss 1.4352368116378784\n",
      "optimal threshold: -0.0516\n",
      "Epoch 7 train loss: 1.4353, eval loss 1.426937222480774\n",
      "optimal threshold: -0.0650\n",
      "Epoch 8 train loss: 1.4130, eval loss 1.4186689853668213\n",
      "optimal threshold: -0.0730\n",
      "Epoch 9 train loss: 1.3973, eval loss 1.4104323387145996\n",
      "optimal threshold: -0.0882\n",
      "Epoch 10 train loss: 1.4063, eval loss 1.402244210243225\n",
      "optimal threshold: -0.1027\n",
      "Epoch 11 train loss: 1.3926, eval loss 1.3940788507461548\n",
      "optimal threshold: -0.0964\n",
      "Epoch 12 train loss: 1.3892, eval loss 1.3859167098999023\n",
      "optimal threshold: -0.1112\n",
      "Epoch 13 train loss: 1.3688, eval loss 1.3777357339859009\n",
      "optimal threshold: -0.1260\n",
      "Epoch 14 train loss: 1.3649, eval loss 1.369530200958252\n",
      "optimal threshold: -0.1384\n",
      "Epoch 15 train loss: 1.3496, eval loss 1.3613284826278687\n",
      "optimal threshold: -0.1512\n",
      "Epoch 16 train loss: 1.3417, eval loss 1.353118658065796\n",
      "optimal threshold: -0.1633\n",
      "Epoch 17 train loss: 1.3396, eval loss 1.3448933362960815\n",
      "optimal threshold: -0.1564\n",
      "Epoch 18 train loss: 1.3452, eval loss 1.3366533517837524\n",
      "optimal threshold: -0.1740\n",
      "Epoch 19 train loss: 1.3171, eval loss 1.3284093141555786\n",
      "optimal threshold: -0.1875\n",
      "Epoch 20 train loss: 1.3116, eval loss 1.3201557397842407\n",
      "optimal threshold: -0.2091\n",
      "Epoch 21 train loss: 1.3072, eval loss 1.3119045495986938\n",
      "optimal threshold: -0.2235\n",
      "Epoch 22 train loss: 1.2976, eval loss 1.3036549091339111\n",
      "optimal threshold: -0.2368\n",
      "Epoch 23 train loss: 1.2806, eval loss 1.2954034805297852\n",
      "optimal threshold: -0.2549\n",
      "Epoch 24 train loss: 1.2942, eval loss 1.2871745824813843\n",
      "optimal threshold: -0.2702\n",
      "Epoch 25 train loss: 1.2672, eval loss 1.278988003730774\n",
      "optimal threshold: -0.2841\n",
      "Epoch 26 train loss: 1.2829, eval loss 1.270812749862671\n",
      "optimal threshold: -0.2986\n",
      "Epoch 27 train loss: 1.2560, eval loss 1.2626433372497559\n",
      "optimal threshold: -0.3153\n",
      "Epoch 28 train loss: 1.2381, eval loss 1.254454255104065\n",
      "optimal threshold: -0.3317\n",
      "Epoch 29 train loss: 1.2401, eval loss 1.2463730573654175\n",
      "optimal threshold: -0.3475\n",
      "Epoch 30 train loss: 1.2241, eval loss 1.2383344173431396\n",
      "optimal threshold: -0.3616\n",
      "Epoch 31 train loss: 1.2158, eval loss 1.2303884029388428\n",
      "optimal threshold: -0.3792\n",
      "Epoch 32 train loss: 1.2237, eval loss 1.222487449645996\n",
      "optimal threshold: -0.3939\n",
      "Epoch 33 train loss: 1.2043, eval loss 1.2146741151809692\n",
      "optimal threshold: -0.4119\n",
      "Epoch 34 train loss: 1.1972, eval loss 1.2069008350372314\n",
      "optimal threshold: -0.4275\n",
      "Epoch 35 train loss: 1.1957, eval loss 1.1992406845092773\n",
      "optimal threshold: -0.4406\n",
      "Epoch 36 train loss: 1.1816, eval loss 1.1916812658309937\n",
      "optimal threshold: -0.4561\n",
      "Epoch 37 train loss: 1.1943, eval loss 1.1841892004013062\n",
      "optimal threshold: -0.4708\n",
      "Epoch 38 train loss: 1.1726, eval loss 1.1768015623092651\n",
      "optimal threshold: -0.4862\n",
      "Epoch 39 train loss: 1.1625, eval loss 1.1695111989974976\n",
      "optimal threshold: -0.4970\n",
      "Epoch 40 train loss: 1.1397, eval loss 1.1623022556304932\n",
      "optimal threshold: -0.5160\n",
      "Epoch 41 train loss: 1.1259, eval loss 1.1552085876464844\n",
      "optimal threshold: -0.5283\n",
      "Epoch 42 train loss: 1.1532, eval loss 1.1482250690460205\n",
      "optimal threshold: -0.5294\n",
      "Epoch 43 train loss: 1.1342, eval loss 1.1413427591323853\n",
      "optimal threshold: -0.5426\n",
      "Epoch 44 train loss: 1.1261, eval loss 1.1345537900924683\n",
      "optimal threshold: -0.5596\n",
      "Epoch 45 train loss: 1.1341, eval loss 1.1278481483459473\n",
      "optimal threshold: -0.5684\n",
      "Epoch 46 train loss: 1.1033, eval loss 1.121250033378601\n",
      "optimal threshold: -0.5833\n",
      "Epoch 47 train loss: 1.0974, eval loss 1.1147695779800415\n",
      "optimal threshold: -0.6000\n",
      "Epoch 48 train loss: 1.0902, eval loss 1.1084060668945312\n",
      "optimal threshold: -0.6152\n",
      "Epoch 49 train loss: 1.0992, eval loss 1.1021339893341064\n",
      "optimal threshold: -0.6275\n",
      "Epoch 50 train loss: 1.1004, eval loss 1.0959327220916748\n",
      "optimal threshold: -0.6387\n",
      "Epoch 51 train loss: 1.0748, eval loss 1.0898531675338745\n",
      "optimal threshold: -0.6514\n",
      "Epoch 52 train loss: 1.0527, eval loss 1.083832025527954\n",
      "optimal threshold: -0.6612\n",
      "Epoch 53 train loss: 1.1000, eval loss 1.0779110193252563\n",
      "optimal threshold: -0.7092\n",
      "Epoch 54 train loss: 1.0529, eval loss 1.0720878839492798\n",
      "optimal threshold: -0.6911\n",
      "Epoch 55 train loss: 1.0622, eval loss 1.0663223266601562\n",
      "optimal threshold: -0.6948\n",
      "Epoch 56 train loss: 1.0371, eval loss 1.0606448650360107\n",
      "optimal threshold: -0.7247\n",
      "Epoch 57 train loss: 1.0680, eval loss 1.0550605058670044\n",
      "optimal threshold: -0.7353\n",
      "Epoch 58 train loss: 1.0291, eval loss 1.049539566040039\n",
      "optimal threshold: -0.7447\n",
      "Epoch 59 train loss: 1.0573, eval loss 1.044091820716858\n",
      "optimal threshold: -0.7803\n",
      "Epoch 60 train loss: 1.0123, eval loss 1.0387251377105713\n",
      "optimal threshold: -0.7653\n",
      "Epoch 61 train loss: 1.0085, eval loss 1.0334346294403076\n",
      "optimal threshold: -0.7721\n",
      "Epoch 62 train loss: 1.0327, eval loss 1.0281885862350464\n",
      "optimal threshold: -0.7807\n",
      "Epoch 63 train loss: 1.0081, eval loss 1.0230251550674438\n",
      "optimal threshold: -0.7894\n",
      "Epoch 64 train loss: 1.0117, eval loss 1.0178899765014648\n",
      "optimal threshold: -0.8012\n",
      "Epoch 65 train loss: 0.9891, eval loss 1.0128320455551147\n",
      "optimal threshold: -0.7984\n",
      "Epoch 66 train loss: 1.0049, eval loss 1.0078237056732178\n",
      "optimal threshold: -0.8063\n",
      "Epoch 67 train loss: 0.9870, eval loss 1.0028650760650635\n",
      "optimal threshold: -0.8005\n",
      "Epoch 68 train loss: 0.9818, eval loss 0.9979538917541504\n",
      "optimal threshold: -0.8040\n",
      "Epoch 69 train loss: 1.0031, eval loss 0.9930828213691711\n",
      "optimal threshold: -0.8508\n",
      "Epoch 70 train loss: 0.9798, eval loss 0.9882475137710571\n",
      "optimal threshold: -0.8267\n",
      "Epoch 71 train loss: 0.9580, eval loss 0.9834690093994141\n",
      "optimal threshold: -0.8282\n",
      "Epoch 72 train loss: 0.9457, eval loss 0.9787341952323914\n",
      "optimal threshold: -0.8408\n",
      "Epoch 73 train loss: 0.9587, eval loss 0.9740344882011414\n",
      "optimal threshold: -0.8476\n",
      "Epoch 74 train loss: 0.9781, eval loss 0.9693633317947388\n",
      "optimal threshold: -0.8544\n",
      "Epoch 75 train loss: 0.9631, eval loss 0.9647430181503296\n",
      "optimal threshold: -0.8599\n",
      "Epoch 76 train loss: 0.9778, eval loss 0.9601741433143616\n",
      "optimal threshold: -0.8561\n",
      "Epoch 77 train loss: 0.9537, eval loss 0.9556583166122437\n",
      "optimal threshold: -0.8596\n",
      "Epoch 78 train loss: 0.9704, eval loss 0.95115727186203\n",
      "optimal threshold: -0.8690\n",
      "Epoch 79 train loss: 0.9441, eval loss 0.9467295408248901\n",
      "optimal threshold: -0.8693\n",
      "Epoch 80 train loss: 0.9543, eval loss 0.9423342943191528\n",
      "optimal threshold: -0.8632\n",
      "Epoch 81 train loss: 0.9383, eval loss 0.9379891157150269\n",
      "optimal threshold: -0.8682\n",
      "Epoch 82 train loss: 0.9585, eval loss 0.9336841106414795\n",
      "optimal threshold: -0.8708\n",
      "Epoch 83 train loss: 0.9297, eval loss 0.929422914981842\n",
      "optimal threshold: -0.8673\n",
      "Epoch 84 train loss: 0.9202, eval loss 0.925179660320282\n",
      "optimal threshold: -0.8697\n",
      "Epoch 85 train loss: 0.9121, eval loss 0.9209812879562378\n",
      "optimal threshold: -0.9350\n",
      "Epoch 86 train loss: 0.9240, eval loss 0.9168238639831543\n",
      "optimal threshold: -0.9386\n",
      "Epoch 87 train loss: 0.8997, eval loss 0.9126889109611511\n",
      "optimal threshold: -0.9768\n",
      "Epoch 88 train loss: 0.8953, eval loss 0.9086015224456787\n",
      "optimal threshold: -0.9832\n",
      "Epoch 89 train loss: 0.9029, eval loss 0.9045688509941101\n",
      "optimal threshold: -0.9858\n",
      "Epoch 90 train loss: 0.9015, eval loss 0.9005627036094666\n",
      "optimal threshold: -0.9919\n",
      "Epoch 91 train loss: 0.8770, eval loss 0.8966016173362732\n",
      "optimal threshold: -0.9728\n",
      "Epoch 92 train loss: 0.9036, eval loss 0.892701268196106\n",
      "optimal threshold: -0.9744\n",
      "Epoch 93 train loss: 0.9095, eval loss 0.8888444304466248\n",
      "optimal threshold: -0.9768\n",
      "Epoch 94 train loss: 0.8888, eval loss 0.8850110769271851\n",
      "optimal threshold: -0.9791\n",
      "Epoch 95 train loss: 0.8833, eval loss 0.8812093138694763\n",
      "optimal threshold: -0.9851\n",
      "Epoch 96 train loss: 0.8400, eval loss 0.87748122215271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.9876\n",
      "Epoch 97 train loss: 0.8714, eval loss 0.8737636208534241\n",
      "optimal threshold: -0.9913\n",
      "Epoch 98 train loss: 0.8758, eval loss 0.8701269626617432\n",
      "optimal threshold: -0.9912\n",
      "Epoch 99 train loss: 0.8877, eval loss 0.8665515184402466\n",
      "optimal threshold: -0.9980\n",
      "Epoch 100 train loss: 0.8566, eval loss 0.8629883527755737\n",
      "optimal threshold: -1.0028\n",
      "Epoch 101 train loss: 0.8390, eval loss 0.8594922423362732\n",
      "optimal threshold: -0.9955\n",
      "Epoch 102 train loss: 0.8785, eval loss 0.856026291847229\n",
      "optimal threshold: -1.0045\n",
      "Epoch 103 train loss: 0.8739, eval loss 0.8526241779327393\n",
      "optimal threshold: -1.0054\n",
      "Epoch 104 train loss: 0.8425, eval loss 0.8492627739906311\n",
      "optimal threshold: -1.0063\n",
      "Epoch 105 train loss: 0.8351, eval loss 0.8459525108337402\n",
      "optimal threshold: -1.0079\n",
      "Epoch 106 train loss: 0.8410, eval loss 0.8427128195762634\n",
      "optimal threshold: -1.0138\n",
      "Epoch 107 train loss: 0.8485, eval loss 0.8395105004310608\n",
      "optimal threshold: -1.0149\n",
      "Epoch 108 train loss: 0.8405, eval loss 0.8363695740699768\n",
      "optimal threshold: -1.0197\n",
      "Epoch 109 train loss: 0.8057, eval loss 0.8332681655883789\n",
      "optimal threshold: -1.0228\n",
      "Epoch 110 train loss: 0.8712, eval loss 0.8302128911018372\n",
      "optimal threshold: -1.0230\n",
      "Epoch 111 train loss: 0.8273, eval loss 0.8272281885147095\n",
      "optimal threshold: -1.0380\n",
      "Epoch 112 train loss: 0.8313, eval loss 0.8242954015731812\n",
      "optimal threshold: -1.0400\n",
      "Epoch 113 train loss: 0.8450, eval loss 0.8213971257209778\n",
      "optimal threshold: -1.0406\n",
      "Epoch 114 train loss: 0.8513, eval loss 0.8185741901397705\n",
      "optimal threshold: -1.0420\n",
      "Epoch 115 train loss: 0.8452, eval loss 0.8157833218574524\n",
      "optimal threshold: -1.0416\n",
      "Epoch 116 train loss: 0.8268, eval loss 0.8130394816398621\n",
      "optimal threshold: -0.8721\n",
      "Epoch 117 train loss: 0.8382, eval loss 0.8103872537612915\n",
      "optimal threshold: -0.8723\n",
      "Epoch 118 train loss: 0.7845, eval loss 0.807772696018219\n",
      "optimal threshold: -0.8689\n",
      "Epoch 119 train loss: 0.8009, eval loss 0.8051919937133789\n",
      "optimal threshold: -0.8686\n",
      "Epoch 120 train loss: 0.8118, eval loss 0.8026561141014099\n",
      "optimal threshold: -0.8653\n",
      "Epoch 121 train loss: 0.8249, eval loss 0.8001832365989685\n",
      "optimal threshold: -0.8722\n",
      "Epoch 122 train loss: 0.8126, eval loss 0.7977679371833801\n",
      "optimal threshold: -0.8298\n",
      "Epoch 123 train loss: 0.8156, eval loss 0.795394241809845\n",
      "optimal threshold: -0.8273\n",
      "Epoch 124 train loss: 0.8477, eval loss 0.7930771708488464\n",
      "optimal threshold: -0.8251\n",
      "Epoch 125 train loss: 0.8117, eval loss 0.7908114194869995\n",
      "optimal threshold: -0.8227\n",
      "Epoch 126 train loss: 0.8165, eval loss 0.7885839939117432\n",
      "optimal threshold: -1.1080\n",
      "Epoch 127 train loss: 0.7961, eval loss 0.7864382266998291\n",
      "optimal threshold: -1.1119\n",
      "Epoch 128 train loss: 0.8250, eval loss 0.7843198776245117\n",
      "optimal threshold: -1.1116\n",
      "Epoch 129 train loss: 0.7993, eval loss 0.7822209000587463\n",
      "optimal threshold: -1.1143\n",
      "Epoch 130 train loss: 0.8105, eval loss 0.7802110910415649\n",
      "optimal threshold: -0.8022\n",
      "Epoch 131 train loss: 0.8014, eval loss 0.7782445549964905\n",
      "optimal threshold: -0.8228\n",
      "Epoch 132 train loss: 0.8090, eval loss 0.7763029932975769\n",
      "optimal threshold: -0.8199\n",
      "Epoch 133 train loss: 0.7738, eval loss 0.7744119763374329\n",
      "optimal threshold: -0.8169\n",
      "Epoch 134 train loss: 0.7703, eval loss 0.7725692987442017\n",
      "optimal threshold: -0.8113\n",
      "Epoch 135 train loss: 0.7563, eval loss 0.7707772850990295\n",
      "optimal threshold: -0.8091\n",
      "Epoch 136 train loss: 0.7799, eval loss 0.7690547108650208\n",
      "optimal threshold: -0.8093\n",
      "Epoch 137 train loss: 0.7817, eval loss 0.7673227190971375\n",
      "optimal threshold: -0.8071\n",
      "Epoch 138 train loss: 0.7750, eval loss 0.7656720280647278\n",
      "optimal threshold: -0.8045\n",
      "Epoch 139 train loss: 0.7789, eval loss 0.7640654444694519\n",
      "optimal threshold: -0.8053\n",
      "Epoch 140 train loss: 0.7644, eval loss 0.7624825835227966\n",
      "optimal threshold: -0.8025\n",
      "Epoch 141 train loss: 0.7893, eval loss 0.7609625458717346\n",
      "optimal threshold: -0.7948\n",
      "Epoch 142 train loss: 0.8085, eval loss 0.7594553828239441\n",
      "optimal threshold: -0.7928\n",
      "Epoch 143 train loss: 0.8077, eval loss 0.7579889893531799\n",
      "optimal threshold: -0.7897\n",
      "Epoch 144 train loss: 0.7855, eval loss 0.7565532326698303\n",
      "optimal threshold: -0.7868\n",
      "Epoch 145 train loss: 0.7615, eval loss 0.7551679015159607\n",
      "optimal threshold: -0.7841\n",
      "Epoch 146 train loss: 0.7484, eval loss 0.7538133263587952\n",
      "optimal threshold: -0.7817\n",
      "Epoch 147 train loss: 0.7739, eval loss 0.752507746219635\n",
      "optimal threshold: -0.7797\n",
      "Epoch 148 train loss: 0.7582, eval loss 0.7512277960777283\n",
      "optimal threshold: -0.7775\n",
      "Epoch 149 train loss: 0.7689, eval loss 0.7499816417694092\n",
      "optimal threshold: -0.7728\n",
      "Epoch 150 train loss: 0.7530, eval loss 0.7487742304801941\n",
      "optimal threshold: -0.7712\n",
      "Epoch 151 train loss: 0.7609, eval loss 0.747600257396698\n",
      "optimal threshold: -0.7671\n",
      "Epoch 152 train loss: 0.7526, eval loss 0.7464497089385986\n",
      "optimal threshold: -0.7662\n",
      "Epoch 153 train loss: 0.7827, eval loss 0.7453215718269348\n",
      "optimal threshold: -0.7614\n",
      "Epoch 154 train loss: 0.7802, eval loss 0.7442494034767151\n",
      "optimal threshold: -0.7586\n",
      "Epoch 155 train loss: 0.7653, eval loss 0.7431773543357849\n",
      "optimal threshold: -0.7556\n",
      "Epoch 156 train loss: 0.7775, eval loss 0.7421377301216125\n",
      "optimal threshold: -0.7532\n",
      "Epoch 157 train loss: 0.7743, eval loss 0.7411434650421143\n",
      "optimal threshold: -0.7512\n",
      "Epoch 158 train loss: 0.7523, eval loss 0.7402040362358093\n",
      "optimal threshold: -0.7489\n",
      "Epoch 159 train loss: 0.7780, eval loss 0.7392457723617554\n",
      "optimal threshold: -0.7468\n",
      "Epoch 160 train loss: 0.7861, eval loss 0.7383351922035217\n",
      "optimal threshold: -0.7508\n",
      "Epoch 161 train loss: 0.7650, eval loss 0.7374824285507202\n",
      "optimal threshold: -1.0594\n",
      "Epoch 162 train loss: 0.7584, eval loss 0.7365948557853699\n",
      "optimal threshold: -1.0610\n",
      "Epoch 163 train loss: 0.7619, eval loss 0.735772430896759\n",
      "optimal threshold: -1.0594\n",
      "Epoch 164 train loss: 0.7590, eval loss 0.7349492907524109\n",
      "optimal threshold: -1.0642\n",
      "Epoch 165 train loss: 0.8026, eval loss 0.7341609001159668\n",
      "optimal threshold: -1.0652\n",
      "Epoch 166 train loss: 0.7394, eval loss 0.7333813905715942\n",
      "optimal threshold: -1.0735\n",
      "Epoch 167 train loss: 0.7707, eval loss 0.7326228022575378\n",
      "optimal threshold: -1.0727\n",
      "Epoch 168 train loss: 0.7474, eval loss 0.7318905591964722\n",
      "optimal threshold: -1.0562\n",
      "Epoch 169 train loss: 0.7674, eval loss 0.7311479449272156\n",
      "optimal threshold: -1.0631\n",
      "Epoch 170 train loss: 0.7440, eval loss 0.7304675579071045\n",
      "optimal threshold: -1.0815\n",
      "Epoch 171 train loss: 0.7420, eval loss 0.7297977209091187\n",
      "optimal threshold: -1.0681\n",
      "Epoch 172 train loss: 0.7204, eval loss 0.7291416525840759\n",
      "optimal threshold: -1.0369\n",
      "Epoch 173 train loss: 0.7633, eval loss 0.7284999489784241\n",
      "optimal threshold: -1.0386\n",
      "Epoch 174 train loss: 0.7386, eval loss 0.7278695106506348\n",
      "optimal threshold: -1.0357\n",
      "Epoch 175 train loss: 0.7363, eval loss 0.7272825241088867\n",
      "optimal threshold: -1.0313\n",
      "Epoch 176 train loss: 0.7831, eval loss 0.726655900478363\n",
      "optimal threshold: -1.0282\n",
      "Epoch 177 train loss: 0.7156, eval loss 0.7260711193084717\n",
      "optimal threshold: -1.0244\n",
      "Epoch 178 train loss: 0.7476, eval loss 0.7254970669746399\n",
      "optimal threshold: -1.0244\n",
      "Epoch 179 train loss: 0.7323, eval loss 0.7249306440353394\n",
      "optimal threshold: -1.0254\n",
      "Epoch 180 train loss: 0.7678, eval loss 0.7244006395339966\n",
      "optimal threshold: -1.0138\n",
      "Epoch 181 train loss: 0.7817, eval loss 0.723885178565979\n",
      "optimal threshold: -1.0123\n",
      "Epoch 182 train loss: 0.8029, eval loss 0.72336345911026\n",
      "optimal threshold: -1.0106\n",
      "Epoch 183 train loss: 0.7380, eval loss 0.7228779792785645\n",
      "optimal threshold: -0.6741\n",
      "Epoch 184 train loss: 0.7836, eval loss 0.7223889827728271\n",
      "optimal threshold: -0.9992\n",
      "Epoch 185 train loss: 0.7906, eval loss 0.7219259142875671\n",
      "optimal threshold: -0.9952\n",
      "Epoch 186 train loss: 0.7155, eval loss 0.721457302570343\n",
      "optimal threshold: -0.9949\n",
      "Epoch 187 train loss: 0.7275, eval loss 0.7209973931312561\n",
      "optimal threshold: -0.9886\n",
      "Epoch 188 train loss: 0.7045, eval loss 0.7205473780632019\n",
      "optimal threshold: -0.9877\n",
      "Epoch 189 train loss: 0.7734, eval loss 0.7200871706008911\n",
      "optimal threshold: -0.9826\n",
      "Epoch 190 train loss: 0.7470, eval loss 0.7196412682533264\n",
      "optimal threshold: -0.9815\n",
      "Epoch 191 train loss: 0.7693, eval loss 0.719221293926239\n",
      "optimal threshold: -0.6467\n",
      "Epoch 192 train loss: 0.7560, eval loss 0.7188377976417542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6457\n",
      "Epoch 193 train loss: 0.7252, eval loss 0.7184470891952515\n",
      "optimal threshold: -0.6872\n",
      "Epoch 194 train loss: 0.7410, eval loss 0.7180665135383606\n",
      "optimal threshold: -0.6811\n",
      "Epoch 195 train loss: 0.7329, eval loss 0.7176635265350342\n",
      "optimal threshold: -0.6844\n",
      "Epoch 196 train loss: 0.7504, eval loss 0.7172911763191223\n",
      "optimal threshold: -0.6836\n",
      "Epoch 197 train loss: 0.7421, eval loss 0.7169232964515686\n",
      "optimal threshold: -0.6861\n",
      "Epoch 198 train loss: 0.7795, eval loss 0.7165734171867371\n",
      "optimal threshold: -0.6876\n",
      "Epoch 199 train loss: 0.7450, eval loss 0.7162092924118042\n",
      "optimal threshold: -1.2015\n",
      "Epoch 200 train loss: 0.8037, eval loss 0.7158673405647278\n",
      "optimal threshold: -1.2055\n",
      "Epoch 201 train loss: 0.7459, eval loss 0.715537428855896\n",
      "optimal threshold: -0.9351\n",
      "Epoch 202 train loss: 0.7327, eval loss 0.7152107357978821\n",
      "optimal threshold: -0.9343\n",
      "Epoch 203 train loss: 0.7454, eval loss 0.7148584723472595\n",
      "optimal threshold: -0.9332\n",
      "Epoch 204 train loss: 0.7491, eval loss 0.7145134210586548\n",
      "optimal threshold: -0.9313\n",
      "Epoch 205 train loss: 0.6836, eval loss 0.7141625285148621\n",
      "optimal threshold: -0.6347\n",
      "Epoch 206 train loss: 0.7219, eval loss 0.7138447165489197\n",
      "optimal threshold: -0.6294\n",
      "Epoch 207 train loss: 0.7936, eval loss 0.7135281562805176\n",
      "optimal threshold: -0.6292\n",
      "Epoch 208 train loss: 0.7164, eval loss 0.7132314443588257\n",
      "optimal threshold: -0.6249\n",
      "Epoch 209 train loss: 0.6993, eval loss 0.7129481434822083\n",
      "optimal threshold: -0.6292\n",
      "Epoch 210 train loss: 0.7287, eval loss 0.71266108751297\n",
      "optimal threshold: -0.6312\n",
      "Epoch 211 train loss: 0.7642, eval loss 0.712348997592926\n",
      "optimal threshold: -0.6191\n",
      "Epoch 212 train loss: 0.7298, eval loss 0.7120592594146729\n",
      "optimal threshold: -0.6187\n",
      "Epoch 213 train loss: 0.7300, eval loss 0.7117881774902344\n",
      "optimal threshold: -0.6208\n",
      "Epoch 214 train loss: 0.7332, eval loss 0.7114819288253784\n",
      "optimal threshold: -0.6187\n",
      "Epoch 215 train loss: 0.7192, eval loss 0.7111921310424805\n",
      "optimal threshold: -0.6182\n",
      "Epoch 216 train loss: 0.7794, eval loss 0.7109049558639526\n",
      "optimal threshold: -0.6193\n",
      "Epoch 217 train loss: 0.7362, eval loss 0.7106406092643738\n",
      "optimal threshold: -0.6217\n",
      "Epoch 218 train loss: 0.7533, eval loss 0.710411012172699\n",
      "optimal threshold: -0.6324\n",
      "Epoch 219 train loss: 0.7283, eval loss 0.7101573944091797\n",
      "optimal threshold: -0.6305\n",
      "Epoch 220 train loss: 0.7019, eval loss 0.7098950743675232\n",
      "optimal threshold: -0.6294\n",
      "Epoch 221 train loss: 0.7169, eval loss 0.7096332907676697\n",
      "optimal threshold: -0.6099\n",
      "Epoch 222 train loss: 0.7238, eval loss 0.7093844413757324\n",
      "optimal threshold: -0.6084\n",
      "Epoch 223 train loss: 0.7245, eval loss 0.7091265916824341\n",
      "optimal threshold: -0.6058\n",
      "Epoch 224 train loss: 0.7413, eval loss 0.7089062333106995\n",
      "optimal threshold: -0.6079\n",
      "Epoch 225 train loss: 0.7457, eval loss 0.7086653709411621\n",
      "optimal threshold: -0.6076\n",
      "Epoch 226 train loss: 0.7401, eval loss 0.7084205746650696\n",
      "optimal threshold: -0.6056\n",
      "Epoch 227 train loss: 0.7637, eval loss 0.7081722021102905\n",
      "optimal threshold: -0.6039\n",
      "Epoch 228 train loss: 0.7473, eval loss 0.707918107509613\n",
      "optimal threshold: -0.6036\n",
      "Epoch 229 train loss: 0.7069, eval loss 0.7076925039291382\n",
      "optimal threshold: -0.6029\n",
      "Epoch 230 train loss: 0.6984, eval loss 0.7074514627456665\n",
      "optimal threshold: -0.6024\n",
      "Epoch 231 train loss: 0.7233, eval loss 0.7072218060493469\n",
      "optimal threshold: -0.6031\n",
      "Epoch 232 train loss: 0.7140, eval loss 0.707017183303833\n",
      "optimal threshold: -0.5991\n",
      "Epoch 233 train loss: 0.7202, eval loss 0.7068021297454834\n",
      "optimal threshold: -0.6246\n",
      "Epoch 234 train loss: 0.7336, eval loss 0.7065624594688416\n",
      "optimal threshold: -0.6261\n",
      "Epoch 235 train loss: 0.6697, eval loss 0.7063484191894531\n",
      "optimal threshold: -0.6239\n",
      "Epoch 236 train loss: 0.6988, eval loss 0.7061073780059814\n",
      "optimal threshold: -0.6074\n",
      "Epoch 237 train loss: 0.7810, eval loss 0.705898106098175\n",
      "optimal threshold: -0.6036\n",
      "Epoch 238 train loss: 0.7127, eval loss 0.7056729197502136\n",
      "optimal threshold: -0.6208\n",
      "Epoch 239 train loss: 0.7384, eval loss 0.7054712772369385\n",
      "optimal threshold: -0.6012\n",
      "Epoch 240 train loss: 0.7017, eval loss 0.7052700519561768\n",
      "optimal threshold: -0.5993\n",
      "Epoch 241 train loss: 0.7164, eval loss 0.7050492763519287\n",
      "optimal threshold: -0.5977\n",
      "Epoch 242 train loss: 0.7753, eval loss 0.7048367857933044\n",
      "optimal threshold: -0.6375\n",
      "Epoch 243 train loss: 0.7395, eval loss 0.7046306729316711\n",
      "optimal threshold: -0.6386\n",
      "Epoch 244 train loss: 0.7403, eval loss 0.7044341564178467\n",
      "optimal threshold: -0.6227\n",
      "Epoch 245 train loss: 0.7004, eval loss 0.7042288780212402\n",
      "optimal threshold: -0.6367\n",
      "Epoch 246 train loss: 0.7421, eval loss 0.7040117383003235\n",
      "optimal threshold: -0.6379\n",
      "Epoch 247 train loss: 0.7264, eval loss 0.7038122415542603\n",
      "optimal threshold: -0.6370\n",
      "Epoch 248 train loss: 0.7220, eval loss 0.7035832405090332\n",
      "optimal threshold: -0.5844\n",
      "Epoch 249 train loss: 0.7599, eval loss 0.7033693790435791\n",
      "optimal threshold: -0.5821\n",
      "Epoch 250 train loss: 0.7325, eval loss 0.7031527161598206\n",
      "optimal threshold: -0.5813\n",
      "Epoch 251 train loss: 0.7244, eval loss 0.7029703855514526\n",
      "optimal threshold: -0.5789\n",
      "Epoch 252 train loss: 0.7660, eval loss 0.7027665376663208\n",
      "optimal threshold: -0.5778\n",
      "Epoch 253 train loss: 0.7218, eval loss 0.7025717496871948\n",
      "optimal threshold: -0.5769\n",
      "Epoch 254 train loss: 0.7199, eval loss 0.7023909687995911\n",
      "optimal threshold: -0.5505\n",
      "Epoch 255 train loss: 0.7711, eval loss 0.7022057771682739\n",
      "optimal threshold: -0.5490\n",
      "Epoch 256 train loss: 0.6875, eval loss 0.7020056843757629\n",
      "optimal threshold: -0.6242\n",
      "Epoch 257 train loss: 0.7430, eval loss 0.7018211483955383\n",
      "optimal threshold: -0.5944\n",
      "Epoch 258 train loss: 0.7894, eval loss 0.7016087770462036\n",
      "optimal threshold: -0.6276\n",
      "Epoch 259 train loss: 0.7654, eval loss 0.7014321684837341\n",
      "optimal threshold: -0.5952\n",
      "Epoch 260 train loss: 0.7711, eval loss 0.7012597918510437\n",
      "optimal threshold: -0.5953\n",
      "Epoch 261 train loss: 0.7085, eval loss 0.7010729312896729\n",
      "optimal threshold: -0.5951\n",
      "Epoch 262 train loss: 0.7335, eval loss 0.7008888125419617\n",
      "optimal threshold: -0.6759\n",
      "Epoch 263 train loss: 0.7035, eval loss 0.7007058262825012\n",
      "optimal threshold: -0.6730\n",
      "Epoch 264 train loss: 0.7101, eval loss 0.7005242705345154\n",
      "optimal threshold: -0.6730\n",
      "Epoch 265 train loss: 0.7106, eval loss 0.7003593444824219\n",
      "optimal threshold: -0.6733\n",
      "Epoch 266 train loss: 0.7369, eval loss 0.7001929879188538\n",
      "optimal threshold: -0.6252\n",
      "Epoch 267 train loss: 0.7395, eval loss 0.700015664100647\n",
      "optimal threshold: -0.6207\n",
      "Epoch 268 train loss: 0.6742, eval loss 0.6998276710510254\n",
      "optimal threshold: -0.6173\n",
      "Epoch 269 train loss: 0.7304, eval loss 0.699658215045929\n",
      "optimal threshold: -0.6178\n",
      "Epoch 270 train loss: 0.7209, eval loss 0.699502170085907\n",
      "optimal threshold: -0.6178\n",
      "Epoch 271 train loss: 0.7906, eval loss 0.6993309259414673\n",
      "optimal threshold: -0.6210\n",
      "Epoch 272 train loss: 0.7407, eval loss 0.6991600394248962\n",
      "optimal threshold: -0.6173\n",
      "Epoch 273 train loss: 0.7429, eval loss 0.6989684104919434\n",
      "optimal threshold: -0.6115\n",
      "Epoch 274 train loss: 0.7294, eval loss 0.6987946629524231\n",
      "optimal threshold: -0.6174\n",
      "Epoch 275 train loss: 0.7343, eval loss 0.6986386179924011\n",
      "optimal threshold: -0.6128\n",
      "Epoch 276 train loss: 0.7448, eval loss 0.6984858512878418\n",
      "optimal threshold: -0.6098\n",
      "Epoch 277 train loss: 0.7446, eval loss 0.6983186602592468\n",
      "optimal threshold: -0.6190\n",
      "Epoch 278 train loss: 0.7140, eval loss 0.6981220841407776\n",
      "optimal threshold: -0.6162\n",
      "Epoch 279 train loss: 0.6976, eval loss 0.697964608669281\n",
      "optimal threshold: -0.6111\n",
      "Epoch 280 train loss: 0.7248, eval loss 0.697813093662262\n",
      "optimal threshold: -0.6079\n",
      "Epoch 281 train loss: 0.7323, eval loss 0.6976463198661804\n",
      "optimal threshold: -0.6064\n",
      "Epoch 282 train loss: 0.6492, eval loss 0.6974881887435913\n",
      "optimal threshold: -0.6059\n",
      "Epoch 283 train loss: 0.7424, eval loss 0.6973376274108887\n",
      "optimal threshold: -0.6198\n",
      "Epoch 284 train loss: 0.7599, eval loss 0.6971745491027832\n",
      "optimal threshold: -0.6162\n",
      "Epoch 285 train loss: 0.6957, eval loss 0.6969971060752869\n",
      "optimal threshold: -0.6067\n",
      "Epoch 286 train loss: 0.6920, eval loss 0.6968587636947632\n",
      "optimal threshold: -0.6195\n",
      "Epoch 287 train loss: 0.7653, eval loss 0.6966909170150757\n",
      "optimal threshold: -0.6211\n",
      "Epoch 288 train loss: 0.7612, eval loss 0.6965380311012268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6197\n",
      "Epoch 289 train loss: 0.7664, eval loss 0.6963851451873779\n",
      "optimal threshold: -0.6086\n",
      "Epoch 290 train loss: 0.7347, eval loss 0.6962273716926575\n",
      "optimal threshold: -0.6090\n",
      "Epoch 291 train loss: 0.7436, eval loss 0.6960821747779846\n",
      "optimal threshold: -0.6087\n",
      "Epoch 292 train loss: 0.7428, eval loss 0.6959323883056641\n",
      "optimal threshold: -0.6092\n",
      "Epoch 293 train loss: 0.7069, eval loss 0.6957719922065735\n",
      "optimal threshold: -0.6069\n",
      "Epoch 294 train loss: 0.6882, eval loss 0.6955741047859192\n",
      "optimal threshold: -0.6069\n",
      "Epoch 295 train loss: 0.7066, eval loss 0.6954227089881897\n",
      "optimal threshold: -0.6188\n",
      "Epoch 296 train loss: 0.6882, eval loss 0.6952701807022095\n",
      "optimal threshold: -0.6126\n",
      "Epoch 297 train loss: 0.7464, eval loss 0.6951236724853516\n",
      "optimal threshold: -0.6099\n",
      "Epoch 298 train loss: 0.7528, eval loss 0.6949695944786072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:38:17,497] Trial 19 finished with value: 0.7460097670555115 and parameters: {'learning_rate_exp': -5.656344787242499, 'dropout_p': 0.5701710032471186, 'l2_reg_exp': -3.9604872195477627, 'batch_size': 441, 'N': 472}. Best is trial 16 with value: 0.2552145719528198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6087\n",
      "Epoch 299 train loss: 0.7460, eval loss 0.6948133707046509\n",
      "optimal threshold: 0.0598\n",
      "Epoch 0 train loss: 1.4864, eval loss 1.4804425239562988\n",
      "optimal threshold: 0.0655\n",
      "Epoch 1 train loss: 1.4834, eval loss 1.4768486022949219\n",
      "optimal threshold: 0.0601\n",
      "Epoch 2 train loss: 1.4751, eval loss 1.4732578992843628\n",
      "optimal threshold: 0.0549\n",
      "Epoch 3 train loss: 1.4759, eval loss 1.4696706533432007\n",
      "optimal threshold: 0.0489\n",
      "Epoch 4 train loss: 1.4642, eval loss 1.4660893678665161\n",
      "optimal threshold: 0.0441\n",
      "Epoch 5 train loss: 1.4643, eval loss 1.4625194072723389\n",
      "optimal threshold: 0.0394\n",
      "Epoch 6 train loss: 1.4570, eval loss 1.4589375257492065\n",
      "optimal threshold: 0.0350\n",
      "Epoch 7 train loss: 1.4548, eval loss 1.4553474187850952\n",
      "optimal threshold: 0.0293\n",
      "Epoch 8 train loss: 1.4596, eval loss 1.4517449140548706\n",
      "optimal threshold: 0.0250\n",
      "Epoch 9 train loss: 1.4419, eval loss 1.4481288194656372\n",
      "optimal threshold: 0.0196\n",
      "Epoch 10 train loss: 1.4452, eval loss 1.4445081949234009\n",
      "optimal threshold: 0.0142\n",
      "Epoch 11 train loss: 1.4338, eval loss 1.4408701658248901\n",
      "optimal threshold: 0.0083\n",
      "Epoch 12 train loss: 1.4362, eval loss 1.4372104406356812\n",
      "optimal threshold: 0.0032\n",
      "Epoch 13 train loss: 1.4309, eval loss 1.4335124492645264\n",
      "optimal threshold: -0.0031\n",
      "Epoch 14 train loss: 1.4358, eval loss 1.4297962188720703\n",
      "optimal threshold: -0.0068\n",
      "Epoch 15 train loss: 1.4228, eval loss 1.4260473251342773\n",
      "optimal threshold: -0.0135\n",
      "Epoch 16 train loss: 1.4247, eval loss 1.422275185585022\n",
      "optimal threshold: -0.0177\n",
      "Epoch 17 train loss: 1.4172, eval loss 1.4184678792953491\n",
      "optimal threshold: -0.0208\n",
      "Epoch 18 train loss: 1.4160, eval loss 1.4146342277526855\n",
      "optimal threshold: -0.0263\n",
      "Epoch 19 train loss: 1.4148, eval loss 1.4107836484909058\n",
      "optimal threshold: -0.0343\n",
      "Epoch 20 train loss: 1.4057, eval loss 1.4068998098373413\n",
      "optimal threshold: -0.0359\n",
      "Epoch 21 train loss: 1.3989, eval loss 1.4029790163040161\n",
      "optimal threshold: -0.0431\n",
      "Epoch 22 train loss: 1.3956, eval loss 1.3990190029144287\n",
      "optimal threshold: -0.0489\n",
      "Epoch 23 train loss: 1.3943, eval loss 1.3950271606445312\n",
      "optimal threshold: -0.0557\n",
      "Epoch 24 train loss: 1.3848, eval loss 1.3909920454025269\n",
      "optimal threshold: -0.0608\n",
      "Epoch 25 train loss: 1.3818, eval loss 1.3869353532791138\n",
      "optimal threshold: -0.0672\n",
      "Epoch 26 train loss: 1.3750, eval loss 1.3828480243682861\n",
      "optimal threshold: -0.0750\n",
      "Epoch 27 train loss: 1.3772, eval loss 1.3787360191345215\n",
      "optimal threshold: -0.0819\n",
      "Epoch 28 train loss: 1.3716, eval loss 1.3745964765548706\n",
      "optimal threshold: -0.0882\n",
      "Epoch 29 train loss: 1.3616, eval loss 1.370431900024414\n",
      "optimal threshold: -0.0952\n",
      "Epoch 30 train loss: 1.3632, eval loss 1.3662179708480835\n",
      "optimal threshold: -0.1023\n",
      "Epoch 31 train loss: 1.3554, eval loss 1.3619908094406128\n",
      "optimal threshold: -0.1074\n",
      "Epoch 32 train loss: 1.3544, eval loss 1.357741355895996\n",
      "optimal threshold: -0.1142\n",
      "Epoch 33 train loss: 1.3469, eval loss 1.353471279144287\n",
      "optimal threshold: -0.1212\n",
      "Epoch 34 train loss: 1.3516, eval loss 1.3491870164871216\n",
      "optimal threshold: -0.1262\n",
      "Epoch 35 train loss: 1.3354, eval loss 1.3448691368103027\n",
      "optimal threshold: -0.1330\n",
      "Epoch 36 train loss: 1.3370, eval loss 1.3405413627624512\n",
      "optimal threshold: -0.1430\n",
      "Epoch 37 train loss: 1.3283, eval loss 1.336179256439209\n",
      "optimal threshold: -0.1497\n",
      "Epoch 38 train loss: 1.3257, eval loss 1.331825852394104\n",
      "optimal threshold: -0.1569\n",
      "Epoch 39 train loss: 1.3198, eval loss 1.3274530172348022\n",
      "optimal threshold: -0.1630\n",
      "Epoch 40 train loss: 1.3186, eval loss 1.323065161705017\n",
      "optimal threshold: -0.1702\n",
      "Epoch 41 train loss: 1.3134, eval loss 1.3186758756637573\n",
      "optimal threshold: -0.1766\n",
      "Epoch 42 train loss: 1.3066, eval loss 1.3142722845077515\n",
      "optimal threshold: -0.1856\n",
      "Epoch 43 train loss: 1.3023, eval loss 1.3098620176315308\n",
      "optimal threshold: -0.1916\n",
      "Epoch 44 train loss: 1.2889, eval loss 1.3054511547088623\n",
      "optimal threshold: -0.2001\n",
      "Epoch 45 train loss: 1.2994, eval loss 1.3010212182998657\n",
      "optimal threshold: -0.2108\n",
      "Epoch 46 train loss: 1.2935, eval loss 1.2966022491455078\n",
      "optimal threshold: -0.2183\n",
      "Epoch 47 train loss: 1.2891, eval loss 1.29218327999115\n",
      "optimal threshold: -0.2269\n",
      "Epoch 48 train loss: 1.2852, eval loss 1.2877728939056396\n",
      "optimal threshold: -0.2348\n",
      "Epoch 49 train loss: 1.2703, eval loss 1.2833517789840698\n",
      "optimal threshold: -0.2399\n",
      "Epoch 50 train loss: 1.2615, eval loss 1.278936743736267\n",
      "optimal threshold: -0.2504\n",
      "Epoch 51 train loss: 1.2709, eval loss 1.2745085954666138\n",
      "optimal threshold: -0.2591\n",
      "Epoch 52 train loss: 1.2589, eval loss 1.2701036930084229\n",
      "optimal threshold: -0.2671\n",
      "Epoch 53 train loss: 1.2505, eval loss 1.2656900882720947\n",
      "optimal threshold: -0.2768\n",
      "Epoch 54 train loss: 1.2484, eval loss 1.2612967491149902\n",
      "optimal threshold: -0.2761\n",
      "Epoch 55 train loss: 1.2460, eval loss 1.2568882703781128\n",
      "optimal threshold: -0.2871\n",
      "Epoch 56 train loss: 1.2432, eval loss 1.2525055408477783\n",
      "optimal threshold: -0.2953\n",
      "Epoch 57 train loss: 1.2389, eval loss 1.2481298446655273\n",
      "optimal threshold: -0.3021\n",
      "Epoch 58 train loss: 1.2367, eval loss 1.2437669038772583\n",
      "optimal threshold: -0.3101\n",
      "Epoch 59 train loss: 1.2368, eval loss 1.2394078969955444\n",
      "optimal threshold: -0.3190\n",
      "Epoch 60 train loss: 1.2211, eval loss 1.235068678855896\n",
      "optimal threshold: -0.3288\n",
      "Epoch 61 train loss: 1.2252, eval loss 1.230726957321167\n",
      "optimal threshold: -0.3360\n",
      "Epoch 62 train loss: 1.2105, eval loss 1.226402997970581\n",
      "optimal threshold: -0.3433\n",
      "Epoch 63 train loss: 1.2063, eval loss 1.2221041917800903\n",
      "optimal threshold: -0.3521\n",
      "Epoch 64 train loss: 1.2098, eval loss 1.2178107500076294\n",
      "optimal threshold: -0.3646\n",
      "Epoch 65 train loss: 1.2033, eval loss 1.2135344743728638\n",
      "optimal threshold: -0.3715\n",
      "Epoch 66 train loss: 1.1963, eval loss 1.2092808485031128\n",
      "optimal threshold: -0.3814\n",
      "Epoch 67 train loss: 1.1902, eval loss 1.2050423622131348\n",
      "optimal threshold: -0.3906\n",
      "Epoch 68 train loss: 1.1777, eval loss 1.2008082866668701\n",
      "optimal threshold: -0.3994\n",
      "Epoch 69 train loss: 1.1790, eval loss 1.196584939956665\n",
      "optimal threshold: -0.4084\n",
      "Epoch 70 train loss: 1.1811, eval loss 1.1923874616622925\n",
      "optimal threshold: -0.4167\n",
      "Epoch 71 train loss: 1.1796, eval loss 1.1882143020629883\n",
      "optimal threshold: -0.4275\n",
      "Epoch 72 train loss: 1.1685, eval loss 1.184051752090454\n",
      "optimal threshold: -0.4358\n",
      "Epoch 73 train loss: 1.1580, eval loss 1.1799081563949585\n",
      "optimal threshold: -0.4447\n",
      "Epoch 74 train loss: 1.1587, eval loss 1.175781488418579\n",
      "optimal threshold: -0.4431\n",
      "Epoch 75 train loss: 1.1619, eval loss 1.1716604232788086\n",
      "optimal threshold: -0.4519\n",
      "Epoch 76 train loss: 1.1468, eval loss 1.167581558227539\n",
      "optimal threshold: -0.4609\n",
      "Epoch 77 train loss: 1.1523, eval loss 1.1635246276855469\n",
      "optimal threshold: -0.4658\n",
      "Epoch 78 train loss: 1.1362, eval loss 1.1594891548156738\n",
      "optimal threshold: -0.4741\n",
      "Epoch 79 train loss: 1.1407, eval loss 1.1554691791534424\n",
      "optimal threshold: -0.4822\n",
      "Epoch 80 train loss: 1.1443, eval loss 1.151473045349121\n",
      "optimal threshold: -0.4899\n",
      "Epoch 81 train loss: 1.1336, eval loss 1.1475058794021606\n",
      "optimal threshold: -0.4963\n",
      "Epoch 82 train loss: 1.1289, eval loss 1.143554925918579\n",
      "optimal threshold: -0.5043\n",
      "Epoch 83 train loss: 1.1171, eval loss 1.1396304368972778\n",
      "optimal threshold: -0.5255\n",
      "Epoch 84 train loss: 1.1127, eval loss 1.1357215642929077\n",
      "optimal threshold: -0.5172\n",
      "Epoch 85 train loss: 1.1053, eval loss 1.131836175918579\n",
      "optimal threshold: -0.5249\n",
      "Epoch 86 train loss: 1.1060, eval loss 1.1279780864715576\n",
      "optimal threshold: -0.5325\n",
      "Epoch 87 train loss: 1.1182, eval loss 1.1241382360458374\n",
      "optimal threshold: -0.5398\n",
      "Epoch 88 train loss: 1.0962, eval loss 1.1203383207321167\n",
      "optimal threshold: -0.5515\n",
      "Epoch 89 train loss: 1.0912, eval loss 1.116564154624939\n",
      "optimal threshold: -0.5567\n",
      "Epoch 90 train loss: 1.0952, eval loss 1.1128058433532715\n",
      "optimal threshold: -0.5648\n",
      "Epoch 91 train loss: 1.0964, eval loss 1.1090755462646484\n",
      "optimal threshold: -0.5943\n",
      "Epoch 92 train loss: 1.0872, eval loss 1.1053720712661743\n",
      "optimal threshold: -0.6003\n",
      "Epoch 93 train loss: 1.0761, eval loss 1.1016992330551147\n",
      "optimal threshold: -0.6023\n",
      "Epoch 94 train loss: 1.0847, eval loss 1.0980451107025146\n",
      "optimal threshold: -0.6022\n",
      "Epoch 95 train loss: 1.0751, eval loss 1.0944234132766724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6112\n",
      "Epoch 96 train loss: 1.0779, eval loss 1.090822458267212\n",
      "optimal threshold: -0.6170\n",
      "Epoch 97 train loss: 1.0687, eval loss 1.0872573852539062\n",
      "optimal threshold: -0.6249\n",
      "Epoch 98 train loss: 1.0581, eval loss 1.0837054252624512\n",
      "optimal threshold: -0.6329\n",
      "Epoch 99 train loss: 1.0548, eval loss 1.0802018642425537\n",
      "optimal threshold: -0.6473\n",
      "Epoch 100 train loss: 1.0691, eval loss 1.0767170190811157\n",
      "optimal threshold: -0.6563\n",
      "Epoch 101 train loss: 1.0630, eval loss 1.0732516050338745\n",
      "optimal threshold: -0.6650\n",
      "Epoch 102 train loss: 1.0512, eval loss 1.0698108673095703\n",
      "optimal threshold: -0.6713\n",
      "Epoch 103 train loss: 1.0370, eval loss 1.0664036273956299\n",
      "optimal threshold: -0.6783\n",
      "Epoch 104 train loss: 1.0513, eval loss 1.0630207061767578\n",
      "optimal threshold: -0.6888\n",
      "Epoch 105 train loss: 1.0570, eval loss 1.0596686601638794\n",
      "optimal threshold: -0.6977\n",
      "Epoch 106 train loss: 1.0468, eval loss 1.0563490390777588\n",
      "optimal threshold: -0.7045\n",
      "Epoch 107 train loss: 1.0225, eval loss 1.053043007850647\n",
      "optimal threshold: -0.7116\n",
      "Epoch 108 train loss: 1.0377, eval loss 1.0497695207595825\n",
      "optimal threshold: -0.7189\n",
      "Epoch 109 train loss: 1.0222, eval loss 1.0465271472930908\n",
      "optimal threshold: -0.7261\n",
      "Epoch 110 train loss: 1.0235, eval loss 1.0433015823364258\n",
      "optimal threshold: -0.7372\n",
      "Epoch 111 train loss: 1.0118, eval loss 1.0401042699813843\n",
      "optimal threshold: -0.7399\n",
      "Epoch 112 train loss: 1.0136, eval loss 1.0369430780410767\n",
      "optimal threshold: -0.7453\n",
      "Epoch 113 train loss: 1.0215, eval loss 1.0338044166564941\n",
      "optimal threshold: -0.7528\n",
      "Epoch 114 train loss: 0.9972, eval loss 1.0306860208511353\n",
      "optimal threshold: -0.7593\n",
      "Epoch 115 train loss: 0.9985, eval loss 1.0275897979736328\n",
      "optimal threshold: -0.7672\n",
      "Epoch 116 train loss: 0.9969, eval loss 1.0245187282562256\n",
      "optimal threshold: -0.7819\n",
      "Epoch 117 train loss: 1.0017, eval loss 1.0214778184890747\n",
      "optimal threshold: -0.7881\n",
      "Epoch 118 train loss: 1.0095, eval loss 1.018471598625183\n",
      "optimal threshold: -0.7940\n",
      "Epoch 119 train loss: 0.9850, eval loss 1.0154743194580078\n",
      "optimal threshold: -0.7986\n",
      "Epoch 120 train loss: 0.9966, eval loss 1.0125057697296143\n",
      "optimal threshold: -0.8067\n",
      "Epoch 121 train loss: 0.9922, eval loss 1.0095611810684204\n",
      "optimal threshold: -0.8154\n",
      "Epoch 122 train loss: 0.9709, eval loss 1.0066452026367188\n",
      "optimal threshold: -0.8227\n",
      "Epoch 123 train loss: 0.9840, eval loss 1.0037522315979004\n",
      "optimal threshold: -0.8253\n",
      "Epoch 124 train loss: 0.9646, eval loss 1.0008809566497803\n",
      "optimal threshold: -0.8304\n",
      "Epoch 125 train loss: 0.9694, eval loss 0.9980342984199524\n",
      "optimal threshold: -0.8427\n",
      "Epoch 126 train loss: 0.9595, eval loss 0.9951974153518677\n",
      "optimal threshold: -0.8415\n",
      "Epoch 127 train loss: 0.9912, eval loss 0.9923839569091797\n",
      "optimal threshold: -0.8476\n",
      "Epoch 128 train loss: 0.9722, eval loss 0.9896042943000793\n",
      "optimal threshold: -0.8497\n",
      "Epoch 129 train loss: 0.9685, eval loss 0.9868422746658325\n",
      "optimal threshold: -0.8542\n",
      "Epoch 130 train loss: 0.9585, eval loss 0.9840989112854004\n",
      "optimal threshold: -0.8600\n",
      "Epoch 131 train loss: 0.9724, eval loss 0.9813770651817322\n",
      "optimal threshold: -0.8633\n",
      "Epoch 132 train loss: 0.9489, eval loss 0.9786756038665771\n",
      "optimal threshold: -0.8690\n",
      "Epoch 133 train loss: 0.9581, eval loss 0.9759948253631592\n",
      "optimal threshold: -0.8733\n",
      "Epoch 134 train loss: 0.9550, eval loss 0.97333824634552\n",
      "optimal threshold: -0.8778\n",
      "Epoch 135 train loss: 0.9446, eval loss 0.9706913232803345\n",
      "optimal threshold: -0.8838\n",
      "Epoch 136 train loss: 0.9477, eval loss 0.9680635929107666\n",
      "optimal threshold: -0.8859\n",
      "Epoch 137 train loss: 0.9513, eval loss 0.9654517769813538\n",
      "optimal threshold: -0.8907\n",
      "Epoch 138 train loss: 0.9258, eval loss 0.9628568887710571\n",
      "optimal threshold: -0.8949\n",
      "Epoch 139 train loss: 0.9369, eval loss 0.9602956175804138\n",
      "optimal threshold: -0.8995\n",
      "Epoch 140 train loss: 0.9313, eval loss 0.9577463269233704\n",
      "optimal threshold: -0.9046\n",
      "Epoch 141 train loss: 0.9331, eval loss 0.9552048444747925\n",
      "optimal threshold: -0.9088\n",
      "Epoch 142 train loss: 0.9239, eval loss 0.9526981711387634\n",
      "optimal threshold: -0.9134\n",
      "Epoch 143 train loss: 0.9278, eval loss 0.9502087831497192\n",
      "optimal threshold: -0.9214\n",
      "Epoch 144 train loss: 0.9188, eval loss 0.9477136135101318\n",
      "optimal threshold: -0.9262\n",
      "Epoch 145 train loss: 0.9266, eval loss 0.9452477097511292\n",
      "optimal threshold: -0.9346\n",
      "Epoch 146 train loss: 0.9025, eval loss 0.9427990913391113\n",
      "optimal threshold: -0.9380\n",
      "Epoch 147 train loss: 0.9318, eval loss 0.9403709173202515\n",
      "optimal threshold: -0.9416\n",
      "Epoch 148 train loss: 0.9071, eval loss 0.937946081161499\n",
      "optimal threshold: -0.9412\n",
      "Epoch 149 train loss: 0.9105, eval loss 0.9355411529541016\n",
      "optimal threshold: -0.9451\n",
      "Epoch 150 train loss: 0.9331, eval loss 0.9331498742103577\n",
      "optimal threshold: -0.9477\n",
      "Epoch 151 train loss: 0.9039, eval loss 0.93077552318573\n",
      "optimal threshold: -0.9511\n",
      "Epoch 152 train loss: 0.9213, eval loss 0.9284180402755737\n",
      "optimal threshold: -0.9520\n",
      "Epoch 153 train loss: 0.9174, eval loss 0.9260728359222412\n",
      "optimal threshold: -0.9590\n",
      "Epoch 154 train loss: 0.9040, eval loss 0.9237396717071533\n",
      "optimal threshold: -0.9620\n",
      "Epoch 155 train loss: 0.8877, eval loss 0.9214129447937012\n",
      "optimal threshold: -0.9642\n",
      "Epoch 156 train loss: 0.9113, eval loss 0.9191093444824219\n",
      "optimal threshold: -0.9675\n",
      "Epoch 157 train loss: 0.8903, eval loss 0.9168190956115723\n",
      "optimal threshold: -0.9799\n",
      "Epoch 158 train loss: 0.8965, eval loss 0.9145328998565674\n",
      "optimal threshold: -0.9738\n",
      "Epoch 159 train loss: 0.9007, eval loss 0.9122544527053833\n",
      "optimal threshold: -0.9974\n",
      "Epoch 160 train loss: 0.9089, eval loss 0.910007119178772\n",
      "optimal threshold: -0.9958\n",
      "Epoch 161 train loss: 0.8982, eval loss 0.9077673554420471\n",
      "optimal threshold: -1.0019\n",
      "Epoch 162 train loss: 0.8843, eval loss 0.9055339694023132\n",
      "optimal threshold: -1.0032\n",
      "Epoch 163 train loss: 0.9099, eval loss 0.9033154249191284\n",
      "optimal threshold: -1.0040\n",
      "Epoch 164 train loss: 0.8756, eval loss 0.9011166095733643\n",
      "optimal threshold: -1.0048\n",
      "Epoch 165 train loss: 0.8803, eval loss 0.8989217281341553\n",
      "optimal threshold: -0.9924\n",
      "Epoch 166 train loss: 0.8698, eval loss 0.8967415690422058\n",
      "optimal threshold: -0.9941\n",
      "Epoch 167 train loss: 0.8602, eval loss 0.8945809006690979\n",
      "optimal threshold: -0.9968\n",
      "Epoch 168 train loss: 0.8641, eval loss 0.8924300670623779\n",
      "optimal threshold: -1.0202\n",
      "Epoch 169 train loss: 0.8755, eval loss 0.8902876973152161\n",
      "optimal threshold: -0.9871\n",
      "Epoch 170 train loss: 0.8756, eval loss 0.8881585597991943\n",
      "optimal threshold: -0.9888\n",
      "Epoch 171 train loss: 0.8715, eval loss 0.8860547542572021\n",
      "optimal threshold: -0.9908\n",
      "Epoch 172 train loss: 0.8730, eval loss 0.8839526772499084\n",
      "optimal threshold: -0.9919\n",
      "Epoch 173 train loss: 0.8605, eval loss 0.8818542957305908\n",
      "optimal threshold: -0.9889\n",
      "Epoch 174 train loss: 0.8609, eval loss 0.8797720074653625\n",
      "optimal threshold: -0.9904\n",
      "Epoch 175 train loss: 0.8702, eval loss 0.877712607383728\n",
      "optimal threshold: -0.9916\n",
      "Epoch 176 train loss: 0.8448, eval loss 0.8756493926048279\n",
      "optimal threshold: -0.9946\n",
      "Epoch 177 train loss: 0.8612, eval loss 0.8736099600791931\n",
      "optimal threshold: -0.9941\n",
      "Epoch 178 train loss: 0.8733, eval loss 0.8715795874595642\n",
      "optimal threshold: -0.9948\n",
      "Epoch 179 train loss: 0.8548, eval loss 0.8695610761642456\n",
      "optimal threshold: -1.0029\n",
      "Epoch 180 train loss: 0.8733, eval loss 0.8675574064254761\n",
      "optimal threshold: -1.0039\n",
      "Epoch 181 train loss: 0.8473, eval loss 0.8655607104301453\n",
      "optimal threshold: -1.0048\n",
      "Epoch 182 train loss: 0.8581, eval loss 0.8635798692703247\n",
      "optimal threshold: -1.0058\n",
      "Epoch 183 train loss: 0.8439, eval loss 0.8616190552711487\n",
      "optimal threshold: -1.0049\n",
      "Epoch 184 train loss: 0.8436, eval loss 0.85965895652771\n",
      "optimal threshold: -1.0055\n",
      "Epoch 185 train loss: 0.8552, eval loss 0.8577264547348022\n",
      "optimal threshold: -1.0101\n",
      "Epoch 186 train loss: 0.8210, eval loss 0.8557969331741333\n",
      "optimal threshold: -1.0098\n",
      "Epoch 187 train loss: 0.8477, eval loss 0.8538914322853088\n",
      "optimal threshold: -1.0087\n",
      "Epoch 188 train loss: 0.8373, eval loss 0.8519952893257141\n",
      "optimal threshold: -1.0119\n",
      "Epoch 189 train loss: 0.8328, eval loss 0.8501042127609253\n",
      "optimal threshold: -1.0695\n",
      "Epoch 190 train loss: 0.8515, eval loss 0.8482328057289124\n",
      "optimal threshold: -0.9842\n",
      "Epoch 191 train loss: 0.8428, eval loss 0.8463746309280396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.9841\n",
      "Epoch 192 train loss: 0.8262, eval loss 0.8445358276367188\n",
      "optimal threshold: -0.9836\n",
      "Epoch 193 train loss: 0.8396, eval loss 0.8427014350891113\n",
      "optimal threshold: -0.9831\n",
      "Epoch 194 train loss: 0.8415, eval loss 0.8408911824226379\n",
      "optimal threshold: -0.9826\n",
      "Epoch 195 train loss: 0.8334, eval loss 0.8390921354293823\n",
      "optimal threshold: -0.9821\n",
      "Epoch 196 train loss: 0.8214, eval loss 0.8373191356658936\n",
      "optimal threshold: -0.9831\n",
      "Epoch 197 train loss: 0.8521, eval loss 0.8355460166931152\n",
      "optimal threshold: -0.9832\n",
      "Epoch 198 train loss: 0.8254, eval loss 0.8337895274162292\n",
      "optimal threshold: -0.9819\n",
      "Epoch 199 train loss: 0.8255, eval loss 0.8320350050926208\n",
      "optimal threshold: -0.9830\n",
      "Epoch 200 train loss: 0.8199, eval loss 0.8302943110466003\n",
      "optimal threshold: -0.9839\n",
      "Epoch 201 train loss: 0.8183, eval loss 0.8285714983940125\n",
      "optimal threshold: -0.9853\n",
      "Epoch 202 train loss: 0.8442, eval loss 0.8268884420394897\n",
      "optimal threshold: -0.9848\n",
      "Epoch 203 train loss: 0.8245, eval loss 0.8252055048942566\n",
      "optimal threshold: -0.9827\n",
      "Epoch 204 train loss: 0.8097, eval loss 0.8235353231430054\n",
      "optimal threshold: -0.9827\n",
      "Epoch 205 train loss: 0.7942, eval loss 0.8218798637390137\n",
      "optimal threshold: -0.9826\n",
      "Epoch 206 train loss: 0.8182, eval loss 0.8202448487281799\n",
      "optimal threshold: -1.0781\n",
      "Epoch 207 train loss: 0.8166, eval loss 0.8186136484146118\n",
      "optimal threshold: -1.0785\n",
      "Epoch 208 train loss: 0.8008, eval loss 0.8170046806335449\n",
      "optimal threshold: -1.0682\n",
      "Epoch 209 train loss: 0.8162, eval loss 0.8154093623161316\n",
      "optimal threshold: -1.0667\n",
      "Epoch 210 train loss: 0.8083, eval loss 0.8138289451599121\n",
      "optimal threshold: -0.8966\n",
      "Epoch 211 train loss: 0.8057, eval loss 0.8122663497924805\n",
      "optimal threshold: -0.8221\n",
      "Epoch 212 train loss: 0.7994, eval loss 0.8107106685638428\n",
      "optimal threshold: -0.8189\n",
      "Epoch 213 train loss: 0.8016, eval loss 0.8091744184494019\n",
      "optimal threshold: -0.8160\n",
      "Epoch 214 train loss: 0.7932, eval loss 0.8076561689376831\n",
      "optimal threshold: -0.8142\n",
      "Epoch 215 train loss: 0.8049, eval loss 0.8061519861221313\n",
      "optimal threshold: -0.8122\n",
      "Epoch 216 train loss: 0.7917, eval loss 0.8046614527702332\n",
      "optimal threshold: -0.8068\n",
      "Epoch 217 train loss: 0.7846, eval loss 0.8031837344169617\n",
      "optimal threshold: -0.8041\n",
      "Epoch 218 train loss: 0.8102, eval loss 0.8017238974571228\n",
      "optimal threshold: -0.8064\n",
      "Epoch 219 train loss: 0.8069, eval loss 0.8002704977989197\n",
      "optimal threshold: -0.8030\n",
      "Epoch 220 train loss: 0.7973, eval loss 0.7988502383232117\n",
      "optimal threshold: -0.8100\n",
      "Epoch 221 train loss: 0.8035, eval loss 0.7974418997764587\n",
      "optimal threshold: -0.8057\n",
      "Epoch 222 train loss: 0.7838, eval loss 0.7960387468338013\n",
      "optimal threshold: -0.8034\n",
      "Epoch 223 train loss: 0.8025, eval loss 0.794684648513794\n",
      "optimal threshold: -0.8009\n",
      "Epoch 224 train loss: 0.7949, eval loss 0.793324887752533\n",
      "optimal threshold: -0.7985\n",
      "Epoch 225 train loss: 0.8016, eval loss 0.7919837236404419\n",
      "optimal threshold: -0.7959\n",
      "Epoch 226 train loss: 0.7873, eval loss 0.790658175945282\n",
      "optimal threshold: -0.7877\n",
      "Epoch 227 train loss: 0.8042, eval loss 0.7893224954605103\n",
      "optimal threshold: -0.7859\n",
      "Epoch 228 train loss: 0.7966, eval loss 0.7880302667617798\n",
      "optimal threshold: -0.7879\n",
      "Epoch 229 train loss: 0.7872, eval loss 0.7867476940155029\n",
      "optimal threshold: -0.7856\n",
      "Epoch 230 train loss: 0.7948, eval loss 0.7854725122451782\n",
      "optimal threshold: -0.7834\n",
      "Epoch 231 train loss: 0.8018, eval loss 0.7842152714729309\n",
      "optimal threshold: -0.7808\n",
      "Epoch 232 train loss: 0.7877, eval loss 0.7829715609550476\n",
      "optimal threshold: -0.7805\n",
      "Epoch 233 train loss: 0.7756, eval loss 0.7817352414131165\n",
      "optimal threshold: -0.7810\n",
      "Epoch 234 train loss: 0.7963, eval loss 0.7805212140083313\n",
      "optimal threshold: -0.7780\n",
      "Epoch 235 train loss: 0.7787, eval loss 0.7793338298797607\n",
      "optimal threshold: -0.7836\n",
      "Epoch 236 train loss: 0.7801, eval loss 0.7781529426574707\n",
      "optimal threshold: -0.7821\n",
      "Epoch 237 train loss: 0.7801, eval loss 0.7769936323165894\n",
      "optimal threshold: -0.7618\n",
      "Epoch 238 train loss: 0.7630, eval loss 0.7758500576019287\n",
      "optimal threshold: -0.7596\n",
      "Epoch 239 train loss: 0.7959, eval loss 0.7747122049331665\n",
      "optimal threshold: -0.7653\n",
      "Epoch 240 train loss: 0.7506, eval loss 0.7735872864723206\n",
      "optimal threshold: -0.7628\n",
      "Epoch 241 train loss: 0.7792, eval loss 0.7724807262420654\n",
      "optimal threshold: -0.7609\n",
      "Epoch 242 train loss: 0.7674, eval loss 0.7713910937309265\n",
      "optimal threshold: -1.1005\n",
      "Epoch 243 train loss: 0.7691, eval loss 0.7703126668930054\n",
      "optimal threshold: -1.0974\n",
      "Epoch 244 train loss: 0.7681, eval loss 0.7692481279373169\n",
      "optimal threshold: -1.0965\n",
      "Epoch 245 train loss: 0.8084, eval loss 0.7682077884674072\n",
      "optimal threshold: -0.7761\n",
      "Epoch 246 train loss: 0.7833, eval loss 0.7671692967414856\n",
      "optimal threshold: -0.7754\n",
      "Epoch 247 train loss: 0.7535, eval loss 0.7661460638046265\n",
      "optimal threshold: -0.7703\n",
      "Epoch 248 train loss: 0.7530, eval loss 0.7651447057723999\n",
      "optimal threshold: -0.7704\n",
      "Epoch 249 train loss: 0.7642, eval loss 0.7641646265983582\n",
      "optimal threshold: -0.7725\n",
      "Epoch 250 train loss: 0.7834, eval loss 0.7632005214691162\n",
      "optimal threshold: -0.7702\n",
      "Epoch 251 train loss: 0.7784, eval loss 0.7622343301773071\n",
      "optimal threshold: -0.8783\n",
      "Epoch 252 train loss: 0.7738, eval loss 0.7612899541854858\n",
      "optimal threshold: -0.7564\n",
      "Epoch 253 train loss: 0.7646, eval loss 0.7603543996810913\n",
      "optimal threshold: -0.8669\n",
      "Epoch 254 train loss: 0.7633, eval loss 0.7594262957572937\n",
      "optimal threshold: -0.8652\n",
      "Epoch 255 train loss: 0.7486, eval loss 0.758514940738678\n",
      "optimal threshold: -0.7518\n",
      "Epoch 256 train loss: 0.7698, eval loss 0.7576155662536621\n",
      "optimal threshold: -0.7505\n",
      "Epoch 257 train loss: 0.7517, eval loss 0.7567340135574341\n",
      "optimal threshold: -0.7490\n",
      "Epoch 258 train loss: 0.7716, eval loss 0.7558687925338745\n",
      "optimal threshold: -0.7473\n",
      "Epoch 259 train loss: 0.7705, eval loss 0.7550017237663269\n",
      "optimal threshold: -1.0550\n",
      "Epoch 260 train loss: 0.7985, eval loss 0.7541614770889282\n",
      "optimal threshold: -1.0635\n",
      "Epoch 261 train loss: 0.7626, eval loss 0.7533196806907654\n",
      "optimal threshold: -1.0663\n",
      "Epoch 262 train loss: 0.7910, eval loss 0.752495527267456\n",
      "optimal threshold: -1.0640\n",
      "Epoch 263 train loss: 0.7557, eval loss 0.7516847848892212\n",
      "optimal threshold: -1.0615\n",
      "Epoch 264 train loss: 0.7651, eval loss 0.7508780360221863\n",
      "optimal threshold: -1.0589\n",
      "Epoch 265 train loss: 0.7677, eval loss 0.7500914931297302\n",
      "optimal threshold: -1.0584\n",
      "Epoch 266 train loss: 0.7743, eval loss 0.7493095397949219\n",
      "optimal threshold: -0.7212\n",
      "Epoch 267 train loss: 0.7536, eval loss 0.7485422492027283\n",
      "optimal threshold: -1.0528\n",
      "Epoch 268 train loss: 0.7834, eval loss 0.747783362865448\n",
      "optimal threshold: -1.0512\n",
      "Epoch 269 train loss: 0.7615, eval loss 0.7470397353172302\n",
      "optimal threshold: -1.0498\n",
      "Epoch 270 train loss: 0.7531, eval loss 0.7463030219078064\n",
      "optimal threshold: -1.0495\n",
      "Epoch 271 train loss: 0.7397, eval loss 0.7455731630325317\n",
      "optimal threshold: -1.0471\n",
      "Epoch 272 train loss: 0.7484, eval loss 0.7448650598526001\n",
      "optimal threshold: -1.0462\n",
      "Epoch 273 train loss: 0.7661, eval loss 0.7441697716712952\n",
      "optimal threshold: -1.0373\n",
      "Epoch 274 train loss: 0.7551, eval loss 0.7434855103492737\n",
      "optimal threshold: -1.0422\n",
      "Epoch 275 train loss: 0.7737, eval loss 0.742812991142273\n",
      "optimal threshold: -1.0433\n",
      "Epoch 276 train loss: 0.7773, eval loss 0.7421505451202393\n",
      "optimal threshold: -1.0426\n",
      "Epoch 277 train loss: 0.7560, eval loss 0.7414835691452026\n",
      "optimal threshold: -1.0197\n",
      "Epoch 278 train loss: 0.7598, eval loss 0.7408390641212463\n",
      "optimal threshold: -1.0192\n",
      "Epoch 279 train loss: 0.7466, eval loss 0.7402064204216003\n",
      "optimal threshold: -1.0206\n",
      "Epoch 280 train loss: 0.7483, eval loss 0.7395760416984558\n",
      "optimal threshold: -1.0217\n",
      "Epoch 281 train loss: 0.7535, eval loss 0.7389561533927917\n",
      "optimal threshold: -0.7257\n",
      "Epoch 282 train loss: 0.7795, eval loss 0.7383488416671753\n",
      "optimal threshold: -0.7308\n",
      "Epoch 283 train loss: 0.7895, eval loss 0.7377448081970215\n",
      "optimal threshold: -0.7304\n",
      "Epoch 284 train loss: 0.7490, eval loss 0.7371535897254944\n",
      "optimal threshold: -0.6976\n",
      "Epoch 285 train loss: 0.7653, eval loss 0.7365541458129883\n",
      "optimal threshold: -0.6954\n",
      "Epoch 286 train loss: 0.7542, eval loss 0.735985279083252\n",
      "optimal threshold: -1.0113\n",
      "Epoch 287 train loss: 0.7701, eval loss 0.7354251742362976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7288\n",
      "Epoch 288 train loss: 0.7323, eval loss 0.7348719239234924\n",
      "optimal threshold: -0.7253\n",
      "Epoch 289 train loss: 0.7553, eval loss 0.7343236207962036\n",
      "optimal threshold: -0.7217\n",
      "Epoch 290 train loss: 0.7597, eval loss 0.7337828874588013\n",
      "optimal threshold: -0.7227\n",
      "Epoch 291 train loss: 0.7272, eval loss 0.7332431674003601\n",
      "optimal threshold: -0.7198\n",
      "Epoch 292 train loss: 0.7541, eval loss 0.7327187657356262\n",
      "optimal threshold: -0.7190\n",
      "Epoch 293 train loss: 0.7757, eval loss 0.7321955561637878\n",
      "optimal threshold: -0.6753\n",
      "Epoch 294 train loss: 0.7448, eval loss 0.7316863536834717\n",
      "optimal threshold: -0.7259\n",
      "Epoch 295 train loss: 0.7587, eval loss 0.7311795353889465\n",
      "optimal threshold: -0.6740\n",
      "Epoch 296 train loss: 0.7518, eval loss 0.7306807041168213\n",
      "optimal threshold: -0.7184\n",
      "Epoch 297 train loss: 0.7390, eval loss 0.7301839590072632\n",
      "optimal threshold: -0.7174\n",
      "Epoch 298 train loss: 0.7319, eval loss 0.7297073006629944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:41:33,698] Trial 20 finished with value: 0.7894734144210815 and parameters: {'learning_rate_exp': -5.806051625443795, 'dropout_p': 0.32190182852966853, 'l2_reg_exp': -4.585231432467018, 'batch_size': 300, 'N': 212}. Best is trial 16 with value: 0.2552145719528198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7117\n",
      "Epoch 299 train loss: 0.7895, eval loss 0.729235053062439\n",
      "optimal threshold: -0.2788\n",
      "Epoch 0 train loss: 1.2627, eval loss 1.2786622047424316\n",
      "optimal threshold: -0.6002\n",
      "Epoch 1 train loss: 1.1023, eval loss 1.113495945930481\n",
      "optimal threshold: -0.8226\n",
      "Epoch 2 train loss: 0.9603, eval loss 0.9794813990592957\n",
      "optimal threshold: -0.9381\n",
      "Epoch 3 train loss: 0.8584, eval loss 0.8729106783866882\n",
      "optimal threshold: -0.6805\n",
      "Epoch 4 train loss: 0.7895, eval loss 0.7991425395011902\n",
      "optimal threshold: -0.6724\n",
      "Epoch 5 train loss: 0.7598, eval loss 0.7562375068664551\n",
      "optimal threshold: -0.7179\n",
      "Epoch 6 train loss: 0.7483, eval loss 0.7325267195701599\n",
      "optimal threshold: -0.8570\n",
      "Epoch 7 train loss: 0.7368, eval loss 0.718846321105957\n",
      "optimal threshold: -0.4697\n",
      "Epoch 8 train loss: 0.7007, eval loss 0.7098273634910583\n",
      "optimal threshold: -0.4750\n",
      "Epoch 9 train loss: 0.7111, eval loss 0.7030707597732544\n",
      "optimal threshold: -0.6952\n",
      "Epoch 10 train loss: 0.7117, eval loss 0.6975773572921753\n",
      "optimal threshold: -0.7469\n",
      "Epoch 11 train loss: 0.6878, eval loss 0.692890465259552\n",
      "optimal threshold: -0.7566\n",
      "Epoch 12 train loss: 0.6809, eval loss 0.6888235807418823\n",
      "optimal threshold: -0.8632\n",
      "Epoch 13 train loss: 0.7147, eval loss 0.6852808594703674\n",
      "optimal threshold: -0.6867\n",
      "Epoch 14 train loss: 0.6816, eval loss 0.682041585445404\n",
      "optimal threshold: -0.5769\n",
      "Epoch 15 train loss: 0.6952, eval loss 0.6792933344841003\n",
      "optimal threshold: -0.6627\n",
      "Epoch 16 train loss: 0.6682, eval loss 0.6768003702163696\n",
      "optimal threshold: -0.8754\n",
      "Epoch 17 train loss: 0.6842, eval loss 0.6746490597724915\n",
      "optimal threshold: -0.6771\n",
      "Epoch 18 train loss: 0.6740, eval loss 0.6727080941200256\n",
      "optimal threshold: -0.6942\n",
      "Epoch 19 train loss: 0.6960, eval loss 0.6709632277488708\n",
      "optimal threshold: -0.5212\n",
      "Epoch 20 train loss: 0.6739, eval loss 0.6694625616073608\n",
      "optimal threshold: -0.5669\n",
      "Epoch 21 train loss: 0.6602, eval loss 0.6680740714073181\n",
      "optimal threshold: -0.5666\n",
      "Epoch 22 train loss: 0.6723, eval loss 0.6668363809585571\n",
      "optimal threshold: -0.5144\n",
      "Epoch 23 train loss: 0.6760, eval loss 0.6657874584197998\n",
      "optimal threshold: -0.5075\n",
      "Epoch 24 train loss: 0.6805, eval loss 0.6648927927017212\n",
      "optimal threshold: -0.5623\n",
      "Epoch 25 train loss: 0.6643, eval loss 0.6640205383300781\n",
      "optimal threshold: -0.6810\n",
      "Epoch 26 train loss: 0.6436, eval loss 0.6632416248321533\n",
      "optimal threshold: -0.6821\n",
      "Epoch 27 train loss: 0.6591, eval loss 0.662644624710083\n",
      "optimal threshold: -0.6664\n",
      "Epoch 28 train loss: 0.6925, eval loss 0.6619033217430115\n",
      "optimal threshold: -0.5888\n",
      "Epoch 29 train loss: 0.6657, eval loss 0.6613278388977051\n",
      "optimal threshold: -0.5858\n",
      "Epoch 30 train loss: 0.6490, eval loss 0.6608507037162781\n",
      "optimal threshold: -0.5789\n",
      "Epoch 31 train loss: 0.6549, eval loss 0.6602729558944702\n",
      "optimal threshold: -0.5753\n",
      "Epoch 32 train loss: 0.6500, eval loss 0.6598849892616272\n",
      "optimal threshold: -0.5568\n",
      "Epoch 33 train loss: 0.6804, eval loss 0.6594721674919128\n",
      "optimal threshold: -0.5628\n",
      "Epoch 34 train loss: 0.6570, eval loss 0.659092903137207\n",
      "optimal threshold: -0.5606\n",
      "Epoch 35 train loss: 0.6545, eval loss 0.6588329076766968\n",
      "optimal threshold: -0.5687\n",
      "Epoch 36 train loss: 0.6599, eval loss 0.6584341526031494\n",
      "optimal threshold: -0.3651\n",
      "Epoch 37 train loss: 0.6437, eval loss 0.6581786870956421\n",
      "optimal threshold: -0.3610\n",
      "Epoch 38 train loss: 0.6501, eval loss 0.6579152941703796\n",
      "optimal threshold: -0.5404\n",
      "Epoch 39 train loss: 0.6533, eval loss 0.6576340794563293\n",
      "optimal threshold: -0.5529\n",
      "Epoch 40 train loss: 0.6235, eval loss 0.6574747562408447\n",
      "optimal threshold: -0.5569\n",
      "Epoch 41 train loss: 0.6597, eval loss 0.6572471857070923\n",
      "optimal threshold: -0.4044\n",
      "Epoch 42 train loss: 0.6429, eval loss 0.6570651531219482\n",
      "optimal threshold: -0.4132\n",
      "Epoch 43 train loss: 0.6349, eval loss 0.6567431688308716\n",
      "optimal threshold: -0.6778\n",
      "Epoch 44 train loss: 0.6611, eval loss 0.6566483378410339\n",
      "optimal threshold: -0.6657\n",
      "Epoch 45 train loss: 0.6389, eval loss 0.6564645767211914\n",
      "optimal threshold: -0.6586\n",
      "Epoch 46 train loss: 0.6337, eval loss 0.6563242077827454\n",
      "optimal threshold: -0.6799\n",
      "Epoch 47 train loss: 0.6388, eval loss 0.6561313271522522\n",
      "optimal threshold: -0.6670\n",
      "Epoch 48 train loss: 0.6301, eval loss 0.6559625864028931\n",
      "optimal threshold: -0.6594\n",
      "Epoch 49 train loss: 0.6732, eval loss 0.655874490737915\n",
      "optimal threshold: -0.6714\n",
      "Epoch 50 train loss: 0.6746, eval loss 0.6556528806686401\n",
      "optimal threshold: -0.5554\n",
      "Epoch 51 train loss: 0.6279, eval loss 0.6555549502372742\n",
      "optimal threshold: -0.5459\n",
      "Epoch 52 train loss: 0.6246, eval loss 0.6554509997367859\n",
      "optimal threshold: -0.5519\n",
      "Epoch 53 train loss: 0.6083, eval loss 0.6553013920783997\n",
      "optimal threshold: -0.5542\n",
      "Epoch 54 train loss: 0.6445, eval loss 0.6553243398666382\n",
      "optimal threshold: -0.6377\n",
      "Epoch 55 train loss: 0.6413, eval loss 0.6552480459213257\n",
      "optimal threshold: -0.5631\n",
      "Epoch 56 train loss: 0.6073, eval loss 0.6551469564437866\n",
      "optimal threshold: -0.4053\n",
      "Epoch 57 train loss: 0.6565, eval loss 0.6550062894821167\n",
      "optimal threshold: -0.5667\n",
      "Epoch 58 train loss: 0.6224, eval loss 0.6549848914146423\n",
      "optimal threshold: -0.3899\n",
      "Epoch 59 train loss: 0.6237, eval loss 0.6548898816108704\n",
      "optimal threshold: -0.5424\n",
      "Epoch 60 train loss: 0.6349, eval loss 0.6547828912734985\n",
      "optimal threshold: -0.3969\n",
      "Epoch 61 train loss: 0.6229, eval loss 0.654737114906311\n",
      "optimal threshold: -0.3884\n",
      "Epoch 62 train loss: 0.6263, eval loss 0.6546779274940491\n",
      "optimal threshold: -0.4185\n",
      "Epoch 63 train loss: 0.6622, eval loss 0.6546496152877808\n",
      "optimal threshold: -0.4162\n",
      "Epoch 64 train loss: 0.6166, eval loss 0.6546421647071838\n",
      "optimal threshold: -0.4160\n",
      "Epoch 65 train loss: 0.6276, eval loss 0.6546250581741333\n",
      "optimal threshold: -0.6091\n",
      "Epoch 66 train loss: 0.6314, eval loss 0.654586136341095\n",
      "optimal threshold: -0.3796\n",
      "Epoch 67 train loss: 0.6175, eval loss 0.6545838117599487\n",
      "optimal threshold: -0.3805\n",
      "Epoch 68 train loss: 0.6310, eval loss 0.6545670032501221\n",
      "optimal threshold: -0.3935\n",
      "Epoch 69 train loss: 0.6110, eval loss 0.6544795632362366\n",
      "optimal threshold: -0.3770\n",
      "Epoch 70 train loss: 0.6359, eval loss 0.6545568704605103\n",
      "optimal threshold: -0.3827\n",
      "Epoch 71 train loss: 0.6347, eval loss 0.6545706987380981\n",
      "optimal threshold: -0.3870\n",
      "Epoch 72 train loss: 0.6196, eval loss 0.6544188857078552\n",
      "optimal threshold: -0.3799\n",
      "Epoch 73 train loss: 0.6304, eval loss 0.6545899510383606\n",
      "optimal threshold: -0.3775\n",
      "Epoch 74 train loss: 0.6042, eval loss 0.6545717120170593\n",
      "optimal threshold: -0.3692\n",
      "Epoch 75 train loss: 0.6153, eval loss 0.6545512080192566\n",
      "optimal threshold: -0.3570\n",
      "Epoch 76 train loss: 0.6175, eval loss 0.6544294357299805\n",
      "optimal threshold: -0.3583\n",
      "Epoch 77 train loss: 0.6217, eval loss 0.6544663310050964\n",
      "optimal threshold: -0.3636\n",
      "Epoch 78 train loss: 0.6402, eval loss 0.6545076370239258\n",
      "optimal threshold: -0.3642\n",
      "Epoch 79 train loss: 0.6372, eval loss 0.6546141505241394\n",
      "optimal threshold: -0.3622\n",
      "Epoch 80 train loss: 0.6150, eval loss 0.6544760465621948\n",
      "optimal threshold: -0.3549\n",
      "Epoch 81 train loss: 0.6325, eval loss 0.654478907585144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:42:47,907] Trial 21 finished with value: 0.5970991849899292 and parameters: {'learning_rate_exp': -4.612558574285801, 'dropout_p': 0.13548523737582419, 'l2_reg_exp': -6.214720686667134, 'batch_size': 259, 'N': 489}. Best is trial 16 with value: 0.2552145719528198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.3273\n",
      "optimal threshold: -0.0893\n",
      "Epoch 0 train loss: 1.4169, eval loss 1.3955610990524292\n",
      "optimal threshold: -0.1882\n",
      "Epoch 1 train loss: 1.3703, eval loss 1.333511233329773\n",
      "optimal threshold: -0.3310\n",
      "Epoch 2 train loss: 1.2916, eval loss 1.2447600364685059\n",
      "optimal threshold: -0.5156\n",
      "Epoch 3 train loss: 1.1908, eval loss 1.1416078805923462\n",
      "optimal threshold: -0.7059\n",
      "Epoch 4 train loss: 1.0960, eval loss 1.0486375093460083\n",
      "optimal threshold: -0.8690\n",
      "Epoch 5 train loss: 1.1195, eval loss 0.9804518818855286\n",
      "optimal threshold: -0.9339\n",
      "Epoch 6 train loss: 1.0177, eval loss 0.9299028515815735\n",
      "optimal threshold: -1.0193\n",
      "Epoch 7 train loss: 0.9265, eval loss 0.8887859582901001\n",
      "optimal threshold: -0.9614\n",
      "Epoch 8 train loss: 0.8603, eval loss 0.8538627624511719\n",
      "optimal threshold: -0.9962\n",
      "Epoch 9 train loss: 0.7968, eval loss 0.8233563303947449\n",
      "optimal threshold: -0.9626\n",
      "Epoch 10 train loss: 0.8160, eval loss 0.7975788116455078\n",
      "optimal threshold: -0.9169\n",
      "Epoch 11 train loss: 0.7148, eval loss 0.7765610814094543\n",
      "optimal threshold: -0.9147\n",
      "Epoch 12 train loss: 0.7885, eval loss 0.7597333192825317\n",
      "optimal threshold: -0.8475\n",
      "Epoch 13 train loss: 0.7250, eval loss 0.7465469837188721\n",
      "optimal threshold: -0.8693\n",
      "Epoch 14 train loss: 0.6653, eval loss 0.7363536357879639\n",
      "optimal threshold: -0.8358\n",
      "Epoch 15 train loss: 0.6443, eval loss 0.7282664775848389\n",
      "optimal threshold: -0.7984\n",
      "Epoch 16 train loss: 0.6489, eval loss 0.7220487594604492\n",
      "optimal threshold: -0.7808\n",
      "Epoch 17 train loss: 0.6532, eval loss 0.716791033744812\n",
      "optimal threshold: -0.7883\n",
      "Epoch 18 train loss: 0.6335, eval loss 0.7126584649085999\n",
      "optimal threshold: -0.7713\n",
      "Epoch 19 train loss: 0.5667, eval loss 0.7090083956718445\n",
      "optimal threshold: -0.7596\n",
      "Epoch 20 train loss: 0.5902, eval loss 0.7058653831481934\n",
      "optimal threshold: -0.7932\n",
      "Epoch 21 train loss: 0.6356, eval loss 0.703233540058136\n",
      "optimal threshold: -0.7793\n",
      "Epoch 22 train loss: 0.6026, eval loss 0.7008492946624756\n",
      "optimal threshold: -0.7818\n",
      "Epoch 23 train loss: 0.6125, eval loss 0.6985458731651306\n",
      "optimal threshold: -0.7149\n",
      "Epoch 24 train loss: 0.5387, eval loss 0.6965857744216919\n",
      "optimal threshold: -0.7269\n",
      "Epoch 25 train loss: 0.6550, eval loss 0.694583535194397\n",
      "optimal threshold: -0.7473\n",
      "Epoch 26 train loss: 0.6029, eval loss 0.6928267478942871\n",
      "optimal threshold: -0.7970\n",
      "Epoch 27 train loss: 0.7354, eval loss 0.691268265247345\n",
      "optimal threshold: -0.8131\n",
      "Epoch 28 train loss: 0.6594, eval loss 0.6897053718566895\n",
      "optimal threshold: -0.8082\n",
      "Epoch 29 train loss: 0.6210, eval loss 0.6882502436637878\n",
      "optimal threshold: -0.7915\n",
      "Epoch 30 train loss: 0.5149, eval loss 0.6868936419487\n",
      "optimal threshold: -0.9345\n",
      "Epoch 31 train loss: 0.6390, eval loss 0.6856406927108765\n",
      "optimal threshold: -0.6912\n",
      "Epoch 32 train loss: 0.6938, eval loss 0.6842965483665466\n",
      "optimal threshold: -0.9414\n",
      "Epoch 33 train loss: 0.5171, eval loss 0.6832583546638489\n",
      "optimal threshold: -0.6901\n",
      "Epoch 34 train loss: 0.5906, eval loss 0.6821019053459167\n",
      "optimal threshold: -0.7026\n",
      "Epoch 35 train loss: 0.5810, eval loss 0.6812029480934143\n",
      "optimal threshold: -0.9286\n",
      "Epoch 36 train loss: 0.6274, eval loss 0.6802714467048645\n",
      "optimal threshold: -0.9318\n",
      "Epoch 37 train loss: 0.7458, eval loss 0.6793563365936279\n",
      "optimal threshold: -0.6853\n",
      "Epoch 38 train loss: 0.6027, eval loss 0.6784501075744629\n",
      "optimal threshold: -0.6891\n",
      "Epoch 39 train loss: 0.5849, eval loss 0.677620530128479\n",
      "optimal threshold: -0.6876\n",
      "Epoch 40 train loss: 0.5794, eval loss 0.6768277883529663\n",
      "optimal threshold: -0.6807\n",
      "Epoch 41 train loss: 0.6006, eval loss 0.6760084629058838\n",
      "optimal threshold: -0.6810\n",
      "Epoch 42 train loss: 0.5417, eval loss 0.6753748059272766\n",
      "optimal threshold: -0.6797\n",
      "Epoch 43 train loss: 0.6446, eval loss 0.6746648550033569\n",
      "optimal threshold: -0.5316\n",
      "Epoch 44 train loss: 0.5876, eval loss 0.6740584373474121\n",
      "optimal threshold: -0.5750\n",
      "Epoch 45 train loss: 0.5320, eval loss 0.6734508275985718\n",
      "optimal threshold: -0.5537\n",
      "Epoch 46 train loss: 0.5792, eval loss 0.6729171872138977\n",
      "optimal threshold: -0.6122\n",
      "Epoch 47 train loss: 0.6674, eval loss 0.6723434925079346\n",
      "optimal threshold: -0.6148\n",
      "Epoch 48 train loss: 0.5840, eval loss 0.6718788146972656\n",
      "optimal threshold: -0.6224\n",
      "Epoch 49 train loss: 0.4843, eval loss 0.6714888215065002\n",
      "optimal threshold: -0.6085\n",
      "Epoch 50 train loss: 0.6016, eval loss 0.6710547804832458\n",
      "optimal threshold: -0.6074\n",
      "Epoch 51 train loss: 0.6253, eval loss 0.670512855052948\n",
      "optimal threshold: -0.5969\n",
      "Epoch 52 train loss: 0.5825, eval loss 0.6701214909553528\n",
      "optimal threshold: -0.5919\n",
      "Epoch 53 train loss: 0.5705, eval loss 0.6696955561637878\n",
      "optimal threshold: -0.5951\n",
      "Epoch 54 train loss: 0.5351, eval loss 0.6693783402442932\n",
      "optimal threshold: -0.6002\n",
      "Epoch 55 train loss: 0.5369, eval loss 0.66902095079422\n",
      "optimal threshold: -0.5882\n",
      "Epoch 56 train loss: 0.5540, eval loss 0.6686676740646362\n",
      "optimal threshold: -0.5955\n",
      "Epoch 57 train loss: 0.5667, eval loss 0.6682921051979065\n",
      "optimal threshold: -0.6021\n",
      "Epoch 58 train loss: 0.6368, eval loss 0.667990505695343\n",
      "optimal threshold: -0.5599\n",
      "Epoch 59 train loss: 0.5782, eval loss 0.6676700115203857\n",
      "optimal threshold: -0.6124\n",
      "Epoch 60 train loss: 0.5247, eval loss 0.6674202084541321\n",
      "optimal threshold: -0.6161\n",
      "Epoch 61 train loss: 0.6347, eval loss 0.6671456694602966\n",
      "optimal threshold: -0.6092\n",
      "Epoch 62 train loss: 0.5428, eval loss 0.666772723197937\n",
      "optimal threshold: -0.5190\n",
      "Epoch 63 train loss: 0.5662, eval loss 0.666588306427002\n",
      "optimal threshold: -0.5185\n",
      "Epoch 64 train loss: 0.5689, eval loss 0.666353166103363\n",
      "optimal threshold: -0.5086\n",
      "Epoch 65 train loss: 0.4713, eval loss 0.6659181118011475\n",
      "optimal threshold: -0.5069\n",
      "Epoch 66 train loss: 0.4986, eval loss 0.6657614707946777\n",
      "optimal threshold: -0.5057\n",
      "Epoch 67 train loss: 0.5124, eval loss 0.6655361652374268\n",
      "optimal threshold: -0.4954\n",
      "Epoch 68 train loss: 0.5011, eval loss 0.6652252674102783\n",
      "optimal threshold: -0.4862\n",
      "Epoch 69 train loss: 0.6329, eval loss 0.6649343967437744\n",
      "optimal threshold: -0.4992\n",
      "Epoch 70 train loss: 0.5336, eval loss 0.6648969054222107\n",
      "optimal threshold: -0.4795\n",
      "Epoch 71 train loss: 0.5362, eval loss 0.6646876335144043\n",
      "optimal threshold: -0.4829\n",
      "Epoch 72 train loss: 0.6243, eval loss 0.6645238995552063\n",
      "optimal threshold: -0.4857\n",
      "Epoch 73 train loss: 0.6554, eval loss 0.6644250750541687\n",
      "optimal threshold: -0.4687\n",
      "Epoch 74 train loss: 0.4662, eval loss 0.6641680598258972\n",
      "optimal threshold: -0.4786\n",
      "Epoch 75 train loss: 0.6333, eval loss 0.6638643145561218\n",
      "optimal threshold: -0.4800\n",
      "Epoch 76 train loss: 0.5512, eval loss 0.6637577414512634\n",
      "optimal threshold: -0.4807\n",
      "Epoch 77 train loss: 0.6359, eval loss 0.6636032462120056\n",
      "optimal threshold: -0.5035\n",
      "Epoch 78 train loss: 0.5565, eval loss 0.663547933101654\n",
      "optimal threshold: -0.5004\n",
      "Epoch 79 train loss: 0.6190, eval loss 0.663369357585907\n",
      "optimal threshold: -0.5751\n",
      "Epoch 80 train loss: 0.6053, eval loss 0.6631101965904236\n",
      "optimal threshold: -0.5324\n",
      "Epoch 81 train loss: 0.5928, eval loss 0.6629689335823059\n",
      "optimal threshold: -0.5768\n",
      "Epoch 82 train loss: 0.4666, eval loss 0.6627946496009827\n",
      "optimal threshold: -0.5799\n",
      "Epoch 83 train loss: 0.5817, eval loss 0.6626924276351929\n",
      "optimal threshold: -0.5811\n",
      "Epoch 84 train loss: 0.5946, eval loss 0.6625335812568665\n",
      "optimal threshold: -0.5845\n",
      "Epoch 85 train loss: 0.6009, eval loss 0.6624311804771423\n",
      "optimal threshold: -0.5780\n",
      "Epoch 86 train loss: 0.6351, eval loss 0.6622432470321655\n",
      "optimal threshold: -0.5884\n",
      "Epoch 87 train loss: 0.5809, eval loss 0.6622531414031982\n",
      "optimal threshold: -0.5321\n",
      "Epoch 88 train loss: 0.5711, eval loss 0.661986768245697\n",
      "optimal threshold: -0.5381\n",
      "Epoch 89 train loss: 0.5532, eval loss 0.6619490385055542\n",
      "optimal threshold: -0.5358\n",
      "Epoch 90 train loss: 0.5353, eval loss 0.6617847084999084\n",
      "optimal threshold: -0.5304\n",
      "Epoch 91 train loss: 0.5140, eval loss 0.661697268486023\n",
      "optimal threshold: -0.5433\n",
      "Epoch 92 train loss: 0.6235, eval loss 0.6615433692932129\n",
      "optimal threshold: -0.5434\n",
      "Epoch 93 train loss: 0.5051, eval loss 0.6614254713058472\n",
      "optimal threshold: -0.4755\n",
      "Epoch 94 train loss: 0.4907, eval loss 0.6613460779190063\n",
      "optimal threshold: -0.4686\n",
      "Epoch 95 train loss: 0.5970, eval loss 0.6612132787704468\n",
      "optimal threshold: -0.4701\n",
      "Epoch 96 train loss: 0.6016, eval loss 0.6611859798431396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4641\n",
      "Epoch 97 train loss: 0.5414, eval loss 0.6610449552536011\n",
      "optimal threshold: -0.4737\n",
      "Epoch 98 train loss: 0.5438, eval loss 0.6609635949134827\n",
      "optimal threshold: -0.4759\n",
      "Epoch 99 train loss: 0.6330, eval loss 0.6609196066856384\n",
      "optimal threshold: -0.5416\n",
      "Epoch 100 train loss: 0.5275, eval loss 0.6607823967933655\n",
      "optimal threshold: -0.5395\n",
      "Epoch 101 train loss: 0.5035, eval loss 0.6606765985488892\n",
      "optimal threshold: -0.5381\n",
      "Epoch 102 train loss: 0.5006, eval loss 0.6606227159500122\n",
      "optimal threshold: -0.5423\n",
      "Epoch 103 train loss: 0.4781, eval loss 0.6605770587921143\n",
      "optimal threshold: -0.5402\n",
      "Epoch 104 train loss: 0.5820, eval loss 0.6603926420211792\n",
      "optimal threshold: -0.5397\n",
      "Epoch 105 train loss: 0.5477, eval loss 0.6602133512496948\n",
      "optimal threshold: -0.5371\n",
      "Epoch 106 train loss: 0.6344, eval loss 0.6601784825325012\n",
      "optimal threshold: -0.5498\n",
      "Epoch 107 train loss: 0.5417, eval loss 0.6601158380508423\n",
      "optimal threshold: -0.5457\n",
      "Epoch 108 train loss: 0.6298, eval loss 0.6601245999336243\n",
      "optimal threshold: -0.4687\n",
      "Epoch 109 train loss: 0.6145, eval loss 0.6600440144538879\n",
      "optimal threshold: -0.5461\n",
      "Epoch 110 train loss: 0.4674, eval loss 0.6600053310394287\n",
      "optimal threshold: -0.4696\n",
      "Epoch 111 train loss: 0.5387, eval loss 0.6598551869392395\n",
      "optimal threshold: -0.4711\n",
      "Epoch 112 train loss: 0.5379, eval loss 0.6599099040031433\n",
      "optimal threshold: -0.4646\n",
      "Epoch 113 train loss: 0.5514, eval loss 0.6598063111305237\n",
      "optimal threshold: -0.4182\n",
      "Epoch 114 train loss: 0.5410, eval loss 0.6597248911857605\n",
      "optimal threshold: -0.4154\n",
      "Epoch 115 train loss: 0.5380, eval loss 0.6596660614013672\n",
      "optimal threshold: -0.4390\n",
      "Epoch 116 train loss: 0.6709, eval loss 0.6595797538757324\n",
      "optimal threshold: -0.4360\n",
      "Epoch 117 train loss: 0.5641, eval loss 0.6595453023910522\n",
      "optimal threshold: -0.4358\n",
      "Epoch 118 train loss: 0.5453, eval loss 0.6594627499580383\n",
      "optimal threshold: -0.4363\n",
      "Epoch 119 train loss: 0.5713, eval loss 0.65936678647995\n",
      "optimal threshold: -0.4416\n",
      "Epoch 120 train loss: 0.5344, eval loss 0.6592216491699219\n",
      "optimal threshold: -0.4266\n",
      "Epoch 121 train loss: 0.5557, eval loss 0.6591026782989502\n",
      "optimal threshold: -0.4267\n",
      "Epoch 122 train loss: 0.4725, eval loss 0.6591182947158813\n",
      "optimal threshold: -0.4365\n",
      "Epoch 123 train loss: 0.5456, eval loss 0.658983588218689\n",
      "optimal threshold: -0.4824\n",
      "Epoch 124 train loss: 0.5244, eval loss 0.6589646339416504\n",
      "optimal threshold: -0.4828\n",
      "Epoch 125 train loss: 0.5152, eval loss 0.6589422225952148\n",
      "optimal threshold: -0.4722\n",
      "Epoch 126 train loss: 0.5425, eval loss 0.6587654948234558\n",
      "optimal threshold: -0.4732\n",
      "Epoch 127 train loss: 0.5442, eval loss 0.6587466597557068\n",
      "optimal threshold: -0.4720\n",
      "Epoch 128 train loss: 0.4833, eval loss 0.6586763858795166\n",
      "optimal threshold: -0.4808\n",
      "Epoch 129 train loss: 0.5224, eval loss 0.6587100625038147\n",
      "optimal threshold: -0.4790\n",
      "Epoch 130 train loss: 0.4716, eval loss 0.658677875995636\n",
      "optimal threshold: -0.4770\n",
      "Epoch 131 train loss: 0.5131, eval loss 0.658587634563446\n",
      "optimal threshold: -0.4762\n",
      "Epoch 132 train loss: 0.5957, eval loss 0.6584566235542297\n",
      "optimal threshold: -0.4437\n",
      "Epoch 133 train loss: 0.5086, eval loss 0.6585378646850586\n",
      "optimal threshold: -0.4480\n",
      "Epoch 134 train loss: 0.5571, eval loss 0.658518373966217\n",
      "optimal threshold: -0.4651\n",
      "Epoch 135 train loss: 0.5144, eval loss 0.6583167314529419\n",
      "optimal threshold: -0.4531\n",
      "Epoch 136 train loss: 0.5373, eval loss 0.658290445804596\n",
      "optimal threshold: -0.4564\n",
      "Epoch 137 train loss: 0.5598, eval loss 0.6583698391914368\n",
      "optimal threshold: -0.4436\n",
      "Epoch 138 train loss: 0.5967, eval loss 0.6582309603691101\n",
      "optimal threshold: -0.4487\n",
      "Epoch 139 train loss: 0.5080, eval loss 0.6581642031669617\n",
      "optimal threshold: -0.4470\n",
      "Epoch 140 train loss: 0.5960, eval loss 0.6581052541732788\n",
      "optimal threshold: -0.4349\n",
      "Epoch 141 train loss: 0.5869, eval loss 0.658077597618103\n",
      "optimal threshold: -0.4367\n",
      "Epoch 142 train loss: 0.5504, eval loss 0.6580600142478943\n",
      "optimal threshold: -0.4436\n",
      "Epoch 143 train loss: 0.5520, eval loss 0.6580201387405396\n",
      "optimal threshold: -0.4425\n",
      "Epoch 144 train loss: 0.5636, eval loss 0.6580146551132202\n",
      "optimal threshold: -0.4463\n",
      "Epoch 145 train loss: 0.5614, eval loss 0.6579687595367432\n",
      "optimal threshold: -0.4423\n",
      "Epoch 146 train loss: 0.4638, eval loss 0.657910168170929\n",
      "optimal threshold: -0.4446\n",
      "Epoch 147 train loss: 0.5153, eval loss 0.6579077839851379\n",
      "optimal threshold: -0.4395\n",
      "Epoch 148 train loss: 0.4723, eval loss 0.6578071117401123\n",
      "optimal threshold: -0.4451\n",
      "Epoch 149 train loss: 0.5930, eval loss 0.6578481793403625\n",
      "optimal threshold: -0.4459\n",
      "Epoch 150 train loss: 0.4465, eval loss 0.6578145027160645\n",
      "optimal threshold: -0.4411\n",
      "Epoch 151 train loss: 0.5854, eval loss 0.6577919721603394\n",
      "optimal threshold: -0.4296\n",
      "Epoch 152 train loss: 0.5550, eval loss 0.6576825380325317\n",
      "optimal threshold: -0.4433\n",
      "Epoch 153 train loss: 0.5603, eval loss 0.6576560735702515\n",
      "optimal threshold: -0.4378\n",
      "Epoch 154 train loss: 0.4992, eval loss 0.6576233506202698\n",
      "optimal threshold: -0.4394\n",
      "Epoch 155 train loss: 0.4992, eval loss 0.6576200723648071\n",
      "optimal threshold: -0.4318\n",
      "Epoch 156 train loss: 0.5560, eval loss 0.6576328873634338\n",
      "optimal threshold: -0.4380\n",
      "Epoch 157 train loss: 0.5391, eval loss 0.6576116681098938\n",
      "optimal threshold: -0.4598\n",
      "Epoch 158 train loss: 0.4717, eval loss 0.6575775146484375\n",
      "optimal threshold: -0.4661\n",
      "Epoch 159 train loss: 0.5455, eval loss 0.6576135754585266\n",
      "optimal threshold: -0.4651\n",
      "Epoch 160 train loss: 0.5294, eval loss 0.6575067639350891\n",
      "optimal threshold: -0.4606\n",
      "Epoch 161 train loss: 0.6200, eval loss 0.6575538516044617\n",
      "optimal threshold: -0.4616\n",
      "Epoch 162 train loss: 0.5100, eval loss 0.6574584245681763\n",
      "optimal threshold: -0.4596\n",
      "Epoch 163 train loss: 0.5192, eval loss 0.6574026346206665\n",
      "optimal threshold: -0.4587\n",
      "Epoch 164 train loss: 0.4914, eval loss 0.657505214214325\n",
      "optimal threshold: -0.4594\n",
      "Epoch 165 train loss: 0.4626, eval loss 0.6574445366859436\n",
      "optimal threshold: -0.4597\n",
      "Epoch 166 train loss: 0.5808, eval loss 0.6574591994285583\n",
      "optimal threshold: -0.4656\n",
      "Epoch 167 train loss: 0.6757, eval loss 0.6573870778083801\n",
      "optimal threshold: -0.4720\n",
      "Epoch 168 train loss: 0.5523, eval loss 0.6573445200920105\n",
      "optimal threshold: -0.4711\n",
      "Epoch 169 train loss: 0.5753, eval loss 0.657252311706543\n",
      "optimal threshold: -0.4691\n",
      "Epoch 170 train loss: 0.5354, eval loss 0.6572924852371216\n",
      "optimal threshold: -0.4628\n",
      "Epoch 171 train loss: 0.6684, eval loss 0.6571671366691589\n",
      "optimal threshold: -0.4690\n",
      "Epoch 172 train loss: 0.6113, eval loss 0.6571855545043945\n",
      "optimal threshold: -0.4618\n",
      "Epoch 173 train loss: 0.5102, eval loss 0.6571006774902344\n",
      "optimal threshold: -0.4927\n",
      "Epoch 174 train loss: 0.5125, eval loss 0.657109260559082\n",
      "optimal threshold: -0.4632\n",
      "Epoch 175 train loss: 0.5292, eval loss 0.6572081446647644\n",
      "optimal threshold: -0.4559\n",
      "Epoch 176 train loss: 0.5192, eval loss 0.6571425795555115\n",
      "optimal threshold: -0.4537\n",
      "Epoch 177 train loss: 0.5039, eval loss 0.6571738719940186\n",
      "optimal threshold: -0.4459\n",
      "Epoch 178 train loss: 0.5229, eval loss 0.6570579409599304\n",
      "optimal threshold: -0.4691\n",
      "Epoch 179 train loss: 0.5761, eval loss 0.6571401357650757\n",
      "optimal threshold: -0.4666\n",
      "Epoch 180 train loss: 0.5017, eval loss 0.657176673412323\n",
      "optimal threshold: -0.4584\n",
      "Epoch 181 train loss: 0.5880, eval loss 0.6570946574211121\n",
      "optimal threshold: -0.4934\n",
      "Epoch 182 train loss: 0.5674, eval loss 0.6569664478302002\n",
      "optimal threshold: -0.4882\n",
      "Epoch 183 train loss: 0.5618, eval loss 0.6571354269981384\n",
      "optimal threshold: -0.4473\n",
      "Epoch 184 train loss: 0.4614, eval loss 0.6570921540260315\n",
      "optimal threshold: -0.4555\n",
      "Epoch 185 train loss: 0.4693, eval loss 0.6571915745735168\n",
      "optimal threshold: -0.5101\n",
      "Epoch 186 train loss: 0.5887, eval loss 0.6571989059448242\n",
      "optimal threshold: -0.4904\n",
      "Epoch 187 train loss: 0.5756, eval loss 0.6571723818778992\n",
      "optimal threshold: -0.4893\n",
      "Epoch 188 train loss: 0.5468, eval loss 0.6571291089057922\n",
      "optimal threshold: -0.4894\n",
      "Epoch 189 train loss: 0.4972, eval loss 0.6571578979492188\n",
      "optimal threshold: -0.4814\n",
      "Epoch 190 train loss: 0.5719, eval loss 0.6570806503295898\n",
      "optimal threshold: -0.4821\n",
      "Epoch 191 train loss: 0.5027, eval loss 0.6571255326271057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:48:37,419] Trial 22 finished with value: 0.45559161901474 and parameters: {'learning_rate_exp': -5.0144542061374295, 'dropout_p': 0.2356188580668877, 'l2_reg_exp': -3.2752735323801705, 'batch_size': 35, 'N': 87}. Best is trial 16 with value: 0.2552145719528198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5186\n",
      "optimal threshold: -0.9802\n",
      "Epoch 0 train loss: 0.9608, eval loss 0.9355086088180542\n",
      "optimal threshold: -1.0052\n",
      "Epoch 1 train loss: 0.6095, eval loss 0.7421994805335999\n",
      "optimal threshold: -1.0284\n",
      "Epoch 2 train loss: 0.5075, eval loss 0.7096469402313232\n",
      "optimal threshold: -1.0051\n",
      "Epoch 3 train loss: 0.5704, eval loss 0.6952019929885864\n",
      "optimal threshold: -0.7626\n",
      "Epoch 4 train loss: 0.6156, eval loss 0.6855341196060181\n",
      "optimal threshold: -0.7609\n",
      "Epoch 5 train loss: 0.5763, eval loss 0.6787901520729065\n",
      "optimal threshold: -0.6472\n",
      "Epoch 6 train loss: 0.5821, eval loss 0.6738849878311157\n",
      "optimal threshold: -0.6406\n",
      "Epoch 7 train loss: 0.5262, eval loss 0.6706817150115967\n",
      "optimal threshold: -0.4868\n",
      "Epoch 8 train loss: 0.5669, eval loss 0.6680670380592346\n",
      "optimal threshold: -0.6490\n",
      "Epoch 9 train loss: 0.5433, eval loss 0.6662498712539673\n",
      "optimal threshold: -0.5236\n",
      "Epoch 10 train loss: 0.5097, eval loss 0.6646054983139038\n",
      "optimal threshold: -0.5508\n",
      "Epoch 11 train loss: 0.5072, eval loss 0.6635252237319946\n",
      "optimal threshold: -0.4117\n",
      "Epoch 12 train loss: 0.5187, eval loss 0.6627171635627747\n",
      "optimal threshold: -0.4487\n",
      "Epoch 13 train loss: 0.5261, eval loss 0.661933422088623\n",
      "optimal threshold: -0.4206\n",
      "Epoch 14 train loss: 0.5286, eval loss 0.6611343026161194\n",
      "optimal threshold: -0.3801\n",
      "Epoch 15 train loss: 0.5565, eval loss 0.6602982878684998\n",
      "optimal threshold: -0.3974\n",
      "Epoch 16 train loss: 0.4516, eval loss 0.6597261428833008\n",
      "optimal threshold: -0.3873\n",
      "Epoch 17 train loss: 0.4566, eval loss 0.6591982245445251\n",
      "optimal threshold: -0.4809\n",
      "Epoch 18 train loss: 0.4833, eval loss 0.6586838364601135\n",
      "optimal threshold: -0.4753\n",
      "Epoch 19 train loss: 0.4900, eval loss 0.658284068107605\n",
      "optimal threshold: -0.4581\n",
      "Epoch 20 train loss: 0.4698, eval loss 0.6577244400978088\n",
      "optimal threshold: -0.4718\n",
      "Epoch 21 train loss: 0.4774, eval loss 0.6576045751571655\n",
      "optimal threshold: -0.4529\n",
      "Epoch 22 train loss: 0.5040, eval loss 0.657044529914856\n",
      "optimal threshold: -0.4831\n",
      "Epoch 23 train loss: 0.4255, eval loss 0.6568688750267029\n",
      "optimal threshold: -0.4514\n",
      "Epoch 24 train loss: 0.4842, eval loss 0.656414806842804\n",
      "optimal threshold: -0.4632\n",
      "Epoch 25 train loss: 0.5335, eval loss 0.6562944054603577\n",
      "optimal threshold: -0.4499\n",
      "Epoch 26 train loss: 0.4282, eval loss 0.6563403010368347\n",
      "optimal threshold: -0.4158\n",
      "Epoch 27 train loss: 0.4692, eval loss 0.6561495065689087\n",
      "optimal threshold: -0.4411\n",
      "Epoch 28 train loss: 0.5057, eval loss 0.6560004949569702\n",
      "optimal threshold: -0.4121\n",
      "Epoch 29 train loss: 0.5126, eval loss 0.6557980179786682\n",
      "optimal threshold: -0.4217\n",
      "Epoch 30 train loss: 0.4431, eval loss 0.6553553342819214\n",
      "optimal threshold: -0.4264\n",
      "Epoch 31 train loss: 0.5666, eval loss 0.6554797887802124\n",
      "optimal threshold: -0.3406\n",
      "Epoch 32 train loss: 0.4529, eval loss 0.6554578542709351\n",
      "optimal threshold: -0.3444\n",
      "Epoch 33 train loss: 0.4900, eval loss 0.6553726196289062\n",
      "optimal threshold: -0.3612\n",
      "Epoch 34 train loss: 0.5275, eval loss 0.6551244854927063\n",
      "optimal threshold: -0.3556\n",
      "Epoch 35 train loss: 0.4342, eval loss 0.6550624966621399\n",
      "optimal threshold: -0.3725\n",
      "Epoch 36 train loss: 0.5095, eval loss 0.6549798846244812\n",
      "optimal threshold: -0.3360\n",
      "Epoch 37 train loss: 0.4393, eval loss 0.6547819375991821\n",
      "optimal threshold: -0.5578\n",
      "Epoch 38 train loss: 0.5269, eval loss 0.6549474596977234\n",
      "optimal threshold: -0.5495\n",
      "Epoch 39 train loss: 0.4750, eval loss 0.6549888849258423\n",
      "optimal threshold: -0.3301\n",
      "Epoch 40 train loss: 0.4951, eval loss 0.6548593640327454\n",
      "optimal threshold: -0.3215\n",
      "Epoch 41 train loss: 0.4609, eval loss 0.6548656225204468\n",
      "optimal threshold: -0.3542\n",
      "Epoch 42 train loss: 0.4931, eval loss 0.6549233198165894\n",
      "optimal threshold: -0.5804\n",
      "Epoch 43 train loss: 0.5267, eval loss 0.6548928618431091\n",
      "optimal threshold: -0.5559\n",
      "Epoch 44 train loss: 0.5124, eval loss 0.6549456715583801\n",
      "optimal threshold: -0.5831\n",
      "Epoch 45 train loss: 0.4986, eval loss 0.655291736125946\n",
      "optimal threshold: -0.5617\n",
      "Epoch 46 train loss: 0.5253, eval loss 0.6552354693412781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:50:32,958] Trial 23 finished with value: 0.4818389117717743 and parameters: {'learning_rate_exp': -4.513857953718126, 'dropout_p': 0.19157540210362162, 'l2_reg_exp': -2.3492109251519784, 'batch_size': 33, 'N': 232}. Best is trial 16 with value: 0.2552145719528198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5740\n",
      "optimal threshold: -0.3364\n",
      "Epoch 0 train loss: 1.2455, eval loss 1.2551641464233398\n",
      "optimal threshold: -0.7691\n",
      "Epoch 1 train loss: 1.0317, eval loss 1.0227478742599487\n",
      "optimal threshold: -0.9534\n",
      "Epoch 2 train loss: 0.8830, eval loss 0.8660361766815186\n",
      "optimal threshold: -1.0222\n",
      "Epoch 3 train loss: 0.8420, eval loss 0.791572630405426\n",
      "optimal threshold: -0.6501\n",
      "Epoch 4 train loss: 0.7646, eval loss 0.7395362257957458\n",
      "optimal threshold: -0.5907\n",
      "Epoch 5 train loss: 0.7966, eval loss 0.7154216170310974\n",
      "optimal threshold: -0.6386\n",
      "Epoch 6 train loss: 0.7395, eval loss 0.7031012177467346\n",
      "optimal threshold: -0.5893\n",
      "Epoch 7 train loss: 0.7392, eval loss 0.6944361925125122\n",
      "optimal threshold: -0.5737\n",
      "Epoch 8 train loss: 0.6941, eval loss 0.689389705657959\n",
      "optimal threshold: -0.6811\n",
      "Epoch 9 train loss: 0.7892, eval loss 0.6846043467521667\n",
      "optimal threshold: -0.6584\n",
      "Epoch 10 train loss: 0.7607, eval loss 0.6813989281654358\n",
      "optimal threshold: -0.7073\n",
      "Epoch 11 train loss: 0.7301, eval loss 0.6788959503173828\n",
      "optimal threshold: -0.6682\n",
      "Epoch 12 train loss: 0.7094, eval loss 0.6761159896850586\n",
      "optimal threshold: -0.7007\n",
      "Epoch 13 train loss: 0.7471, eval loss 0.6746717691421509\n",
      "optimal threshold: -0.6034\n",
      "Epoch 14 train loss: 0.7096, eval loss 0.6735899448394775\n",
      "optimal threshold: -0.4535\n",
      "Epoch 15 train loss: 0.7375, eval loss 0.6718189120292664\n",
      "optimal threshold: -0.4435\n",
      "Epoch 16 train loss: 0.6813, eval loss 0.6705160737037659\n",
      "optimal threshold: -0.4328\n",
      "Epoch 17 train loss: 0.6983, eval loss 0.6700028777122498\n",
      "optimal threshold: -0.5152\n",
      "Epoch 18 train loss: 0.7245, eval loss 0.6695604920387268\n",
      "optimal threshold: -0.5451\n",
      "Epoch 19 train loss: 0.6996, eval loss 0.6690658926963806\n",
      "optimal threshold: -0.5233\n",
      "Epoch 20 train loss: 0.6630, eval loss 0.6682854890823364\n",
      "optimal threshold: -0.5015\n",
      "Epoch 21 train loss: 0.7193, eval loss 0.668433427810669\n",
      "optimal threshold: -0.5206\n",
      "Epoch 22 train loss: 0.6619, eval loss 0.6682174801826477\n",
      "optimal threshold: -0.4690\n",
      "Epoch 23 train loss: 0.7167, eval loss 0.6670705080032349\n",
      "optimal threshold: -0.5307\n",
      "Epoch 24 train loss: 0.6702, eval loss 0.666306734085083\n",
      "optimal threshold: -0.6835\n",
      "Epoch 25 train loss: 0.6463, eval loss 0.6677947044372559\n",
      "optimal threshold: -0.5628\n",
      "Epoch 26 train loss: 0.6131, eval loss 0.6664231419563293\n",
      "optimal threshold: -0.6161\n",
      "Epoch 27 train loss: 0.7279, eval loss 0.6659796237945557\n",
      "optimal threshold: -0.6399\n",
      "Epoch 28 train loss: 0.6881, eval loss 0.6659335494041443\n",
      "optimal threshold: -0.6372\n",
      "Epoch 29 train loss: 0.6329, eval loss 0.6662797331809998\n",
      "optimal threshold: -0.6338\n",
      "Epoch 30 train loss: 0.6005, eval loss 0.6662138104438782\n",
      "optimal threshold: -0.6019\n",
      "Epoch 31 train loss: 0.7036, eval loss 0.6656126379966736\n",
      "optimal threshold: -0.5988\n",
      "Epoch 32 train loss: 0.7222, eval loss 0.6650698781013489\n",
      "optimal threshold: -0.6859\n",
      "Epoch 33 train loss: 0.6464, eval loss 0.6649112105369568\n",
      "optimal threshold: -0.6394\n",
      "Epoch 34 train loss: 0.7139, eval loss 0.6656596660614014\n",
      "optimal threshold: -0.6401\n",
      "Epoch 35 train loss: 0.6334, eval loss 0.6654006242752075\n",
      "optimal threshold: -0.5816\n",
      "Epoch 36 train loss: 0.6316, eval loss 0.6654148101806641\n",
      "optimal threshold: -0.6073\n",
      "Epoch 37 train loss: 0.6276, eval loss 0.6652319431304932\n",
      "optimal threshold: -0.5978\n",
      "Epoch 38 train loss: 0.6781, eval loss 0.6652540564537048\n",
      "optimal threshold: -0.5962\n",
      "Epoch 39 train loss: 0.6398, eval loss 0.664454996585846\n",
      "optimal threshold: -0.6189\n",
      "Epoch 40 train loss: 0.6898, eval loss 0.6648278832435608\n",
      "optimal threshold: -0.6386\n",
      "Epoch 41 train loss: 0.6618, eval loss 0.664406955242157\n",
      "optimal threshold: -0.6354\n",
      "Epoch 42 train loss: 0.7154, eval loss 0.6646865606307983\n",
      "optimal threshold: -0.6516\n",
      "Epoch 43 train loss: 0.6454, eval loss 0.6650713086128235\n",
      "optimal threshold: -0.5997\n",
      "Epoch 44 train loss: 0.6583, eval loss 0.6644541025161743\n",
      "optimal threshold: -0.6040\n",
      "Epoch 45 train loss: 0.6623, eval loss 0.6646837592124939\n",
      "optimal threshold: -0.6039\n",
      "Epoch 46 train loss: 0.6923, eval loss 0.664121687412262\n",
      "optimal threshold: -0.6123\n",
      "Epoch 47 train loss: 0.6475, eval loss 0.6646609306335449\n",
      "optimal threshold: -0.5763\n",
      "Epoch 48 train loss: 0.6972, eval loss 0.6645807027816772\n",
      "optimal threshold: -0.5925\n",
      "Epoch 49 train loss: 0.6740, eval loss 0.6649999618530273\n",
      "optimal threshold: -0.6084\n",
      "Epoch 50 train loss: 0.6515, eval loss 0.6647137403488159\n",
      "optimal threshold: -0.6081\n",
      "Epoch 51 train loss: 0.6446, eval loss 0.6648364663124084\n",
      "optimal threshold: -0.6235\n",
      "Epoch 52 train loss: 0.6618, eval loss 0.6646145582199097\n",
      "optimal threshold: -0.5907\n",
      "Epoch 53 train loss: 0.6433, eval loss 0.6644460558891296\n",
      "optimal threshold: -0.6241\n",
      "Epoch 54 train loss: 0.6944, eval loss 0.6641943454742432\n",
      "optimal threshold: -0.7196\n",
      "Epoch 55 train loss: 0.6696, eval loss 0.664758026599884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:51:01,052] Trial 24 finished with value: 0.6430070996284485 and parameters: {'learning_rate_exp': -3.5500762922338964, 'dropout_p': 0.5455293044573746, 'l2_reg_exp': -5.195708118202406, 'batch_size': 246, 'N': 43}. Best is trial 16 with value: 0.2552145719528198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5917\n",
      "optimal threshold: -0.2152\n",
      "Epoch 0 train loss: 1.4532, eval loss 1.452929973602295\n",
      "optimal threshold: -0.2167\n",
      "Epoch 1 train loss: 1.4501, eval loss 1.4520236253738403\n",
      "optimal threshold: -0.2180\n",
      "Epoch 2 train loss: 1.4480, eval loss 1.4511255025863647\n",
      "optimal threshold: -0.2194\n",
      "Epoch 3 train loss: 1.4523, eval loss 1.4502259492874146\n",
      "optimal threshold: -0.2207\n",
      "Epoch 4 train loss: 1.4457, eval loss 1.4493308067321777\n",
      "optimal threshold: -0.2220\n",
      "Epoch 5 train loss: 1.4436, eval loss 1.448436975479126\n",
      "optimal threshold: -0.2234\n",
      "Epoch 6 train loss: 1.4444, eval loss 1.4475432634353638\n",
      "optimal threshold: -0.2248\n",
      "Epoch 7 train loss: 1.4387, eval loss 1.446655511856079\n",
      "optimal threshold: -0.2262\n",
      "Epoch 8 train loss: 1.4490, eval loss 1.4457714557647705\n",
      "optimal threshold: -0.2275\n",
      "Epoch 9 train loss: 1.4598, eval loss 1.4448906183242798\n",
      "optimal threshold: -0.2289\n",
      "Epoch 10 train loss: 1.4466, eval loss 1.4440051317214966\n",
      "optimal threshold: -0.2304\n",
      "Epoch 11 train loss: 1.4423, eval loss 1.4431225061416626\n",
      "optimal threshold: -0.2318\n",
      "Epoch 12 train loss: 1.4491, eval loss 1.4422420263290405\n",
      "optimal threshold: -0.2331\n",
      "Epoch 13 train loss: 1.4453, eval loss 1.4413634538650513\n",
      "optimal threshold: -0.2345\n",
      "Epoch 14 train loss: 1.4420, eval loss 1.4404902458190918\n",
      "optimal threshold: -0.2359\n",
      "Epoch 15 train loss: 1.4359, eval loss 1.4396142959594727\n",
      "optimal threshold: -0.2373\n",
      "Epoch 16 train loss: 1.4423, eval loss 1.438738226890564\n",
      "optimal threshold: -0.2386\n",
      "Epoch 17 train loss: 1.4439, eval loss 1.4378622770309448\n",
      "optimal threshold: -0.2400\n",
      "Epoch 18 train loss: 1.4507, eval loss 1.4369934797286987\n",
      "optimal threshold: -0.2414\n",
      "Epoch 19 train loss: 1.4417, eval loss 1.4361209869384766\n",
      "optimal threshold: -0.2428\n",
      "Epoch 20 train loss: 1.4281, eval loss 1.4352526664733887\n",
      "optimal threshold: -0.2443\n",
      "Epoch 21 train loss: 1.4329, eval loss 1.4343860149383545\n",
      "optimal threshold: -0.2457\n",
      "Epoch 22 train loss: 1.4317, eval loss 1.4335180521011353\n",
      "optimal threshold: -0.2471\n",
      "Epoch 23 train loss: 1.4358, eval loss 1.432652473449707\n",
      "optimal threshold: -0.2486\n",
      "Epoch 24 train loss: 1.4367, eval loss 1.4317885637283325\n",
      "optimal threshold: -0.2500\n",
      "Epoch 25 train loss: 1.4352, eval loss 1.4309228658676147\n",
      "optimal threshold: -0.2514\n",
      "Epoch 26 train loss: 1.4316, eval loss 1.430056095123291\n",
      "optimal threshold: -0.2529\n",
      "Epoch 27 train loss: 1.4379, eval loss 1.4291880130767822\n",
      "optimal threshold: -0.2544\n",
      "Epoch 28 train loss: 1.4280, eval loss 1.4283231496810913\n",
      "optimal threshold: -0.2559\n",
      "Epoch 29 train loss: 1.4273, eval loss 1.4274582862854004\n",
      "optimal threshold: -0.2574\n",
      "Epoch 30 train loss: 1.4327, eval loss 1.4265910387039185\n",
      "optimal threshold: -0.2589\n",
      "Epoch 31 train loss: 1.4287, eval loss 1.4257299900054932\n",
      "optimal threshold: -0.2603\n",
      "Epoch 32 train loss: 1.4228, eval loss 1.4248684644699097\n",
      "optimal threshold: -0.2620\n",
      "Epoch 33 train loss: 1.4218, eval loss 1.423997163772583\n",
      "optimal threshold: -0.2634\n",
      "Epoch 34 train loss: 1.4229, eval loss 1.423132300376892\n",
      "optimal threshold: -0.2649\n",
      "Epoch 35 train loss: 1.4181, eval loss 1.4222602844238281\n",
      "optimal threshold: -0.2665\n",
      "Epoch 36 train loss: 1.4194, eval loss 1.421386480331421\n",
      "optimal threshold: -0.2680\n",
      "Epoch 37 train loss: 1.4249, eval loss 1.4205188751220703\n",
      "optimal threshold: -0.2695\n",
      "Epoch 38 train loss: 1.4202, eval loss 1.4196470975875854\n",
      "optimal threshold: -0.2710\n",
      "Epoch 39 train loss: 1.4140, eval loss 1.418777346611023\n",
      "optimal threshold: -0.2726\n",
      "Epoch 40 train loss: 1.4151, eval loss 1.4179044961929321\n",
      "optimal threshold: -0.2742\n",
      "Epoch 41 train loss: 1.4166, eval loss 1.4170303344726562\n",
      "optimal threshold: -0.2759\n",
      "Epoch 42 train loss: 1.4169, eval loss 1.4161570072174072\n",
      "optimal threshold: -0.2775\n",
      "Epoch 43 train loss: 1.4185, eval loss 1.4152843952178955\n",
      "optimal threshold: -0.2790\n",
      "Epoch 44 train loss: 1.4205, eval loss 1.4144079685211182\n",
      "optimal threshold: -0.2805\n",
      "Epoch 45 train loss: 1.4161, eval loss 1.4135310649871826\n",
      "optimal threshold: -0.2821\n",
      "Epoch 46 train loss: 1.4157, eval loss 1.412654995918274\n",
      "optimal threshold: -0.2837\n",
      "Epoch 47 train loss: 1.4179, eval loss 1.4117765426635742\n",
      "optimal threshold: -0.2852\n",
      "Epoch 48 train loss: 1.4166, eval loss 1.410899043083191\n",
      "optimal threshold: -0.2868\n",
      "Epoch 49 train loss: 1.4161, eval loss 1.4100173711776733\n",
      "optimal threshold: -0.2883\n",
      "Epoch 50 train loss: 1.4120, eval loss 1.4091355800628662\n",
      "optimal threshold: -0.2900\n",
      "Epoch 51 train loss: 1.4099, eval loss 1.4082517623901367\n",
      "optimal threshold: -0.2915\n",
      "Epoch 52 train loss: 1.4046, eval loss 1.4073631763458252\n",
      "optimal threshold: -0.2931\n",
      "Epoch 53 train loss: 1.4137, eval loss 1.4064747095108032\n",
      "optimal threshold: -0.2946\n",
      "Epoch 54 train loss: 1.4103, eval loss 1.4055852890014648\n",
      "optimal threshold: -0.2961\n",
      "Epoch 55 train loss: 1.4051, eval loss 1.404702067375183\n",
      "optimal threshold: -0.2975\n",
      "Epoch 56 train loss: 1.4131, eval loss 1.4038115739822388\n",
      "optimal threshold: -0.2990\n",
      "Epoch 57 train loss: 1.4106, eval loss 1.4029247760772705\n",
      "optimal threshold: -0.3006\n",
      "Epoch 58 train loss: 1.3953, eval loss 1.402025580406189\n",
      "optimal threshold: -0.3021\n",
      "Epoch 59 train loss: 1.3983, eval loss 1.4011335372924805\n",
      "optimal threshold: -0.3036\n",
      "Epoch 60 train loss: 1.4035, eval loss 1.4002363681793213\n",
      "optimal threshold: -0.3052\n",
      "Epoch 61 train loss: 1.4058, eval loss 1.3993396759033203\n",
      "optimal threshold: -0.3067\n",
      "Epoch 62 train loss: 1.4003, eval loss 1.3984445333480835\n",
      "optimal threshold: -0.3083\n",
      "Epoch 63 train loss: 1.4037, eval loss 1.397547721862793\n",
      "optimal threshold: -0.1057\n",
      "Epoch 64 train loss: 1.4008, eval loss 1.3966495990753174\n",
      "optimal threshold: -0.1072\n",
      "Epoch 65 train loss: 1.3902, eval loss 1.3957420587539673\n",
      "optimal threshold: -0.1089\n",
      "Epoch 66 train loss: 1.3909, eval loss 1.3948357105255127\n",
      "optimal threshold: -0.1108\n",
      "Epoch 67 train loss: 1.3854, eval loss 1.3939296007156372\n",
      "optimal threshold: -0.1124\n",
      "Epoch 68 train loss: 1.4009, eval loss 1.3930164575576782\n",
      "optimal threshold: -0.1134\n",
      "Epoch 69 train loss: 1.3914, eval loss 1.392107605934143\n",
      "optimal threshold: -0.1125\n",
      "Epoch 70 train loss: 1.3878, eval loss 1.3911913633346558\n",
      "optimal threshold: -0.1147\n",
      "Epoch 71 train loss: 1.3941, eval loss 1.390279769897461\n",
      "optimal threshold: -0.1154\n",
      "Epoch 72 train loss: 1.3900, eval loss 1.3893581628799438\n",
      "optimal threshold: -0.1175\n",
      "Epoch 73 train loss: 1.3838, eval loss 1.3884367942810059\n",
      "optimal threshold: -0.1076\n",
      "Epoch 74 train loss: 1.3875, eval loss 1.3875150680541992\n",
      "optimal threshold: -0.1093\n",
      "Epoch 75 train loss: 1.3876, eval loss 1.3865875005722046\n",
      "optimal threshold: -0.1103\n",
      "Epoch 76 train loss: 1.3850, eval loss 1.3856613636016846\n",
      "optimal threshold: -0.1059\n",
      "Epoch 77 train loss: 1.3922, eval loss 1.3847311735153198\n",
      "optimal threshold: -0.1073\n",
      "Epoch 78 train loss: 1.3815, eval loss 1.3837997913360596\n",
      "optimal threshold: -0.1088\n",
      "Epoch 79 train loss: 1.3825, eval loss 1.3828667402267456\n",
      "optimal threshold: -0.1102\n",
      "Epoch 80 train loss: 1.3938, eval loss 1.3819305896759033\n",
      "optimal threshold: -0.1115\n",
      "Epoch 81 train loss: 1.3785, eval loss 1.3809890747070312\n",
      "optimal threshold: -0.1135\n",
      "Epoch 82 train loss: 1.3865, eval loss 1.3800485134124756\n",
      "optimal threshold: -0.1214\n",
      "Epoch 83 train loss: 1.3831, eval loss 1.3791062831878662\n",
      "optimal threshold: -0.1224\n",
      "Epoch 84 train loss: 1.3810, eval loss 1.378157138824463\n",
      "optimal threshold: -0.1243\n",
      "Epoch 85 train loss: 1.3822, eval loss 1.3772066831588745\n",
      "optimal threshold: -0.1259\n",
      "Epoch 86 train loss: 1.3778, eval loss 1.3762520551681519\n",
      "optimal threshold: -0.1272\n",
      "Epoch 87 train loss: 1.3780, eval loss 1.3753013610839844\n",
      "optimal threshold: -0.1291\n",
      "Epoch 88 train loss: 1.3760, eval loss 1.3743467330932617\n",
      "optimal threshold: -0.1308\n",
      "Epoch 89 train loss: 1.3673, eval loss 1.3733898401260376\n",
      "optimal threshold: -0.1329\n",
      "Epoch 90 train loss: 1.3699, eval loss 1.3724254369735718\n",
      "optimal threshold: -0.1342\n",
      "Epoch 91 train loss: 1.3689, eval loss 1.3714638948440552\n",
      "optimal threshold: -0.1357\n",
      "Epoch 92 train loss: 1.3765, eval loss 1.3705013990402222\n",
      "optimal threshold: -0.1295\n",
      "Epoch 93 train loss: 1.3705, eval loss 1.36953604221344\n",
      "optimal threshold: -0.1301\n",
      "Epoch 94 train loss: 1.3749, eval loss 1.368559718132019\n",
      "optimal threshold: -0.1316\n",
      "Epoch 95 train loss: 1.3632, eval loss 1.3675823211669922\n",
      "optimal threshold: -0.1325\n",
      "Epoch 96 train loss: 1.3597, eval loss 1.3666095733642578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.1338\n",
      "Epoch 97 train loss: 1.3742, eval loss 1.3656318187713623\n",
      "optimal threshold: -0.1353\n",
      "Epoch 98 train loss: 1.3642, eval loss 1.3646507263183594\n",
      "optimal threshold: -0.1368\n",
      "Epoch 99 train loss: 1.3638, eval loss 1.3636623620986938\n",
      "optimal threshold: -0.1389\n",
      "Epoch 100 train loss: 1.3633, eval loss 1.3626717329025269\n",
      "optimal threshold: -0.1461\n",
      "Epoch 101 train loss: 1.3677, eval loss 1.3616797924041748\n",
      "optimal threshold: -0.1471\n",
      "Epoch 102 train loss: 1.3638, eval loss 1.3606865406036377\n",
      "optimal threshold: -0.1487\n",
      "Epoch 103 train loss: 1.3715, eval loss 1.3596923351287842\n",
      "optimal threshold: -0.1503\n",
      "Epoch 104 train loss: 1.3616, eval loss 1.358696699142456\n",
      "optimal threshold: -0.1518\n",
      "Epoch 105 train loss: 1.3533, eval loss 1.3576937913894653\n",
      "optimal threshold: -0.1531\n",
      "Epoch 106 train loss: 1.3545, eval loss 1.3566808700561523\n",
      "optimal threshold: -0.1549\n",
      "Epoch 107 train loss: 1.3587, eval loss 1.3556722402572632\n",
      "optimal threshold: -0.1579\n",
      "Epoch 108 train loss: 1.3493, eval loss 1.3546578884124756\n",
      "optimal threshold: -0.1594\n",
      "Epoch 109 train loss: 1.3582, eval loss 1.353644609451294\n",
      "optimal threshold: -0.1610\n",
      "Epoch 110 train loss: 1.3580, eval loss 1.3526266813278198\n",
      "optimal threshold: -0.1623\n",
      "Epoch 111 train loss: 1.3519, eval loss 1.351605772972107\n",
      "optimal threshold: -0.1640\n",
      "Epoch 112 train loss: 1.3502, eval loss 1.3505842685699463\n",
      "optimal threshold: -0.1658\n",
      "Epoch 113 train loss: 1.3480, eval loss 1.3495523929595947\n",
      "optimal threshold: -0.1526\n",
      "Epoch 114 train loss: 1.3469, eval loss 1.3485186100006104\n",
      "optimal threshold: -0.1541\n",
      "Epoch 115 train loss: 1.3393, eval loss 1.3474856615066528\n",
      "optimal threshold: -0.1551\n",
      "Epoch 116 train loss: 1.3415, eval loss 1.3464455604553223\n",
      "optimal threshold: -0.1565\n",
      "Epoch 117 train loss: 1.3522, eval loss 1.3454002141952515\n",
      "optimal threshold: -0.1581\n",
      "Epoch 118 train loss: 1.3499, eval loss 1.3443562984466553\n",
      "optimal threshold: -0.1595\n",
      "Epoch 119 train loss: 1.3436, eval loss 1.3433085680007935\n",
      "optimal threshold: -0.1611\n",
      "Epoch 120 train loss: 1.3454, eval loss 1.3422523736953735\n",
      "optimal threshold: -0.1628\n",
      "Epoch 121 train loss: 1.3439, eval loss 1.3411991596221924\n",
      "optimal threshold: -0.1667\n",
      "Epoch 122 train loss: 1.3434, eval loss 1.3401365280151367\n",
      "optimal threshold: -0.1683\n",
      "Epoch 123 train loss: 1.3385, eval loss 1.3390663862228394\n",
      "optimal threshold: -0.1698\n",
      "Epoch 124 train loss: 1.3325, eval loss 1.337993860244751\n",
      "optimal threshold: -0.1716\n",
      "Epoch 125 train loss: 1.3336, eval loss 1.3369197845458984\n",
      "optimal threshold: -0.1731\n",
      "Epoch 126 train loss: 1.3271, eval loss 1.3358464241027832\n",
      "optimal threshold: -0.1747\n",
      "Epoch 127 train loss: 1.3290, eval loss 1.3347647190093994\n",
      "optimal threshold: -0.1758\n",
      "Epoch 128 train loss: 1.3423, eval loss 1.3336808681488037\n",
      "optimal threshold: -0.1776\n",
      "Epoch 129 train loss: 1.3347, eval loss 1.3325947523117065\n",
      "optimal threshold: -0.1797\n",
      "Epoch 130 train loss: 1.3265, eval loss 1.331503987312317\n",
      "optimal threshold: -0.1823\n",
      "Epoch 131 train loss: 1.3303, eval loss 1.330413579940796\n",
      "optimal threshold: -0.1840\n",
      "Epoch 132 train loss: 1.3376, eval loss 1.3293156623840332\n",
      "optimal threshold: -0.1850\n",
      "Epoch 133 train loss: 1.3246, eval loss 1.3282135725021362\n",
      "optimal threshold: -0.1875\n",
      "Epoch 134 train loss: 1.3360, eval loss 1.3271074295043945\n",
      "optimal threshold: -0.1892\n",
      "Epoch 135 train loss: 1.3135, eval loss 1.3260024785995483\n",
      "optimal threshold: -0.1911\n",
      "Epoch 136 train loss: 1.3301, eval loss 1.3248897790908813\n",
      "optimal threshold: -0.1930\n",
      "Epoch 137 train loss: 1.3210, eval loss 1.3237735033035278\n",
      "optimal threshold: -0.1948\n",
      "Epoch 138 train loss: 1.3237, eval loss 1.3226534128189087\n",
      "optimal threshold: -0.1954\n",
      "Epoch 139 train loss: 1.3258, eval loss 1.3215287923812866\n",
      "optimal threshold: -0.2009\n",
      "Epoch 140 train loss: 1.3189, eval loss 1.3203973770141602\n",
      "optimal threshold: -0.2030\n",
      "Epoch 141 train loss: 1.3268, eval loss 1.319260835647583\n",
      "optimal threshold: -0.2049\n",
      "Epoch 142 train loss: 1.3112, eval loss 1.3181264400482178\n",
      "optimal threshold: -0.2110\n",
      "Epoch 143 train loss: 1.3174, eval loss 1.3169894218444824\n",
      "optimal threshold: -0.2129\n",
      "Epoch 144 train loss: 1.3167, eval loss 1.3158419132232666\n",
      "optimal threshold: -0.2148\n",
      "Epoch 145 train loss: 1.3088, eval loss 1.3146934509277344\n",
      "optimal threshold: -0.2164\n",
      "Epoch 146 train loss: 1.3117, eval loss 1.313537836074829\n",
      "optimal threshold: -0.2185\n",
      "Epoch 147 train loss: 1.3011, eval loss 1.3123818635940552\n",
      "optimal threshold: -0.2202\n",
      "Epoch 148 train loss: 1.3148, eval loss 1.3112152814865112\n",
      "optimal threshold: -0.2214\n",
      "Epoch 149 train loss: 1.3055, eval loss 1.3100545406341553\n",
      "optimal threshold: -0.2234\n",
      "Epoch 150 train loss: 1.3064, eval loss 1.3088902235031128\n",
      "optimal threshold: -0.2255\n",
      "Epoch 151 train loss: 1.3016, eval loss 1.3077147006988525\n",
      "optimal threshold: -0.2271\n",
      "Epoch 152 train loss: 1.3141, eval loss 1.306538701057434\n",
      "optimal threshold: -0.2288\n",
      "Epoch 153 train loss: 1.3075, eval loss 1.3053593635559082\n",
      "optimal threshold: -0.2277\n",
      "Epoch 154 train loss: 1.3120, eval loss 1.3041796684265137\n",
      "optimal threshold: -0.2298\n",
      "Epoch 155 train loss: 1.3040, eval loss 1.3029980659484863\n",
      "optimal threshold: -0.2327\n",
      "Epoch 156 train loss: 1.2967, eval loss 1.301811933517456\n",
      "optimal threshold: -0.2336\n",
      "Epoch 157 train loss: 1.3049, eval loss 1.3006139993667603\n",
      "optimal threshold: -0.2331\n",
      "Epoch 158 train loss: 1.2981, eval loss 1.2994149923324585\n",
      "optimal threshold: -0.2389\n",
      "Epoch 159 train loss: 1.2927, eval loss 1.2982069253921509\n",
      "optimal threshold: -0.2407\n",
      "Epoch 160 train loss: 1.2946, eval loss 1.296999216079712\n",
      "optimal threshold: -0.2426\n",
      "Epoch 161 train loss: 1.2876, eval loss 1.2957860231399536\n",
      "optimal threshold: -0.2453\n",
      "Epoch 162 train loss: 1.2908, eval loss 1.2945669889450073\n",
      "optimal threshold: -0.2472\n",
      "Epoch 163 train loss: 1.2843, eval loss 1.2933475971221924\n",
      "optimal threshold: -0.2489\n",
      "Epoch 164 train loss: 1.2897, eval loss 1.2921264171600342\n",
      "optimal threshold: -0.2510\n",
      "Epoch 165 train loss: 1.2934, eval loss 1.2909033298492432\n",
      "optimal threshold: -0.2530\n",
      "Epoch 166 train loss: 1.2924, eval loss 1.2896794080734253\n",
      "optimal threshold: -0.2550\n",
      "Epoch 167 train loss: 1.2906, eval loss 1.2884445190429688\n",
      "optimal threshold: -0.2564\n",
      "Epoch 168 train loss: 1.2912, eval loss 1.2872053384780884\n",
      "optimal threshold: -0.2585\n",
      "Epoch 169 train loss: 1.2839, eval loss 1.285955786705017\n",
      "optimal threshold: -0.2605\n",
      "Epoch 170 train loss: 1.2955, eval loss 1.2847107648849487\n",
      "optimal threshold: -0.2632\n",
      "Epoch 171 train loss: 1.2800, eval loss 1.2834668159484863\n",
      "optimal threshold: -0.2654\n",
      "Epoch 172 train loss: 1.2902, eval loss 1.2822120189666748\n",
      "optimal threshold: -0.2666\n",
      "Epoch 173 train loss: 1.2788, eval loss 1.280957818031311\n",
      "optimal threshold: -0.2687\n",
      "Epoch 174 train loss: 1.2792, eval loss 1.279703974723816\n",
      "optimal threshold: -0.2712\n",
      "Epoch 175 train loss: 1.2883, eval loss 1.2784374952316284\n",
      "optimal threshold: -0.2731\n",
      "Epoch 176 train loss: 1.2834, eval loss 1.2771774530410767\n",
      "optimal threshold: -0.2746\n",
      "Epoch 177 train loss: 1.2723, eval loss 1.2759084701538086\n",
      "optimal threshold: -0.2766\n",
      "Epoch 178 train loss: 1.2641, eval loss 1.27463960647583\n",
      "optimal threshold: -0.2758\n",
      "Epoch 179 train loss: 1.2694, eval loss 1.273366928100586\n",
      "optimal threshold: -0.2776\n",
      "Epoch 180 train loss: 1.2713, eval loss 1.2720847129821777\n",
      "optimal threshold: -0.2797\n",
      "Epoch 181 train loss: 1.2737, eval loss 1.270802617073059\n",
      "optimal threshold: -0.2816\n",
      "Epoch 182 train loss: 1.2697, eval loss 1.269512414932251\n",
      "optimal threshold: -0.2840\n",
      "Epoch 183 train loss: 1.2719, eval loss 1.2682223320007324\n",
      "optimal threshold: -0.2857\n",
      "Epoch 184 train loss: 1.2811, eval loss 1.2669296264648438\n",
      "optimal threshold: -0.2877\n",
      "Epoch 185 train loss: 1.2679, eval loss 1.2656389474868774\n",
      "optimal threshold: -0.2894\n",
      "Epoch 186 train loss: 1.2727, eval loss 1.2643507719039917\n",
      "optimal threshold: -0.2914\n",
      "Epoch 187 train loss: 1.2757, eval loss 1.2630540132522583\n",
      "optimal threshold: -0.2934\n",
      "Epoch 188 train loss: 1.2574, eval loss 1.261752724647522\n",
      "optimal threshold: -0.2956\n",
      "Epoch 189 train loss: 1.2635, eval loss 1.2604519128799438\n",
      "optimal threshold: -0.2979\n",
      "Epoch 190 train loss: 1.2471, eval loss 1.2591395378112793\n",
      "optimal threshold: -0.3004\n",
      "Epoch 191 train loss: 1.2559, eval loss 1.257822036743164\n",
      "optimal threshold: -0.3026\n",
      "Epoch 192 train loss: 1.2512, eval loss 1.2565078735351562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.3053\n",
      "Epoch 193 train loss: 1.2450, eval loss 1.255192518234253\n",
      "optimal threshold: -0.3070\n",
      "Epoch 194 train loss: 1.2637, eval loss 1.253873586654663\n",
      "optimal threshold: -0.3093\n",
      "Epoch 195 train loss: 1.2465, eval loss 1.2525516748428345\n",
      "optimal threshold: -0.3114\n",
      "Epoch 196 train loss: 1.2568, eval loss 1.2512348890304565\n",
      "optimal threshold: -0.3137\n",
      "Epoch 197 train loss: 1.2457, eval loss 1.2499096393585205\n",
      "optimal threshold: -0.3149\n",
      "Epoch 198 train loss: 1.2503, eval loss 1.2485804557800293\n",
      "optimal threshold: -0.3169\n",
      "Epoch 199 train loss: 1.2483, eval loss 1.2472528219223022\n",
      "optimal threshold: -0.3182\n",
      "Epoch 200 train loss: 1.2467, eval loss 1.2459256649017334\n",
      "optimal threshold: -0.3209\n",
      "Epoch 201 train loss: 1.2435, eval loss 1.2445863485336304\n",
      "optimal threshold: -0.3229\n",
      "Epoch 202 train loss: 1.2384, eval loss 1.2432491779327393\n",
      "optimal threshold: -0.3212\n",
      "Epoch 203 train loss: 1.2445, eval loss 1.2419058084487915\n",
      "optimal threshold: -0.3277\n",
      "Epoch 204 train loss: 1.2379, eval loss 1.240564227104187\n",
      "optimal threshold: -0.3293\n",
      "Epoch 205 train loss: 1.2415, eval loss 1.2392228841781616\n",
      "optimal threshold: -0.3312\n",
      "Epoch 206 train loss: 1.2376, eval loss 1.237871766090393\n",
      "optimal threshold: -0.3334\n",
      "Epoch 207 train loss: 1.2403, eval loss 1.2365254163742065\n",
      "optimal threshold: -0.3356\n",
      "Epoch 208 train loss: 1.2389, eval loss 1.2351727485656738\n",
      "optimal threshold: -0.3361\n",
      "Epoch 209 train loss: 1.2394, eval loss 1.2338182926177979\n",
      "optimal threshold: -0.3381\n",
      "Epoch 210 train loss: 1.2376, eval loss 1.2324637174606323\n",
      "optimal threshold: -0.3400\n",
      "Epoch 211 train loss: 1.2358, eval loss 1.2311069965362549\n",
      "optimal threshold: -0.3422\n",
      "Epoch 212 train loss: 1.2239, eval loss 1.2297512292861938\n",
      "optimal threshold: -0.3446\n",
      "Epoch 213 train loss: 1.2303, eval loss 1.228392243385315\n",
      "optimal threshold: -0.3466\n",
      "Epoch 214 train loss: 1.2184, eval loss 1.227028727531433\n",
      "optimal threshold: -0.3492\n",
      "Epoch 215 train loss: 1.2336, eval loss 1.2256680727005005\n",
      "optimal threshold: -0.3513\n",
      "Epoch 216 train loss: 1.2213, eval loss 1.2243036031723022\n",
      "optimal threshold: -0.3682\n",
      "Epoch 217 train loss: 1.2320, eval loss 1.2229342460632324\n",
      "optimal threshold: -0.3667\n",
      "Epoch 218 train loss: 1.2208, eval loss 1.221567988395691\n",
      "optimal threshold: -0.3701\n",
      "Epoch 219 train loss: 1.2170, eval loss 1.2201952934265137\n",
      "optimal threshold: -0.3708\n",
      "Epoch 220 train loss: 1.2191, eval loss 1.2188208103179932\n",
      "optimal threshold: -0.3739\n",
      "Epoch 221 train loss: 1.2242, eval loss 1.217446208000183\n",
      "optimal threshold: -0.3773\n",
      "Epoch 222 train loss: 1.2135, eval loss 1.2160640954971313\n",
      "optimal threshold: -0.3796\n",
      "Epoch 223 train loss: 1.2279, eval loss 1.214682698249817\n",
      "optimal threshold: -0.3813\n",
      "Epoch 224 train loss: 1.2218, eval loss 1.2133032083511353\n",
      "optimal threshold: -0.3823\n",
      "Epoch 225 train loss: 1.2164, eval loss 1.2119250297546387\n",
      "optimal threshold: -0.3849\n",
      "Epoch 226 train loss: 1.2116, eval loss 1.210539698600769\n",
      "optimal threshold: -0.3870\n",
      "Epoch 227 train loss: 1.2087, eval loss 1.2091559171676636\n",
      "optimal threshold: -0.3895\n",
      "Epoch 228 train loss: 1.2119, eval loss 1.2077648639678955\n",
      "optimal threshold: -0.3928\n",
      "Epoch 229 train loss: 1.2050, eval loss 1.2063703536987305\n",
      "optimal threshold: -0.3950\n",
      "Epoch 230 train loss: 1.2207, eval loss 1.2049819231033325\n",
      "optimal threshold: -0.3968\n",
      "Epoch 231 train loss: 1.2147, eval loss 1.203589677810669\n",
      "optimal threshold: -0.3997\n",
      "Epoch 232 train loss: 1.2001, eval loss 1.2021960020065308\n",
      "optimal threshold: -0.3969\n",
      "Epoch 233 train loss: 1.1872, eval loss 1.200796365737915\n",
      "optimal threshold: -0.3993\n",
      "Epoch 234 train loss: 1.2116, eval loss 1.1993894577026367\n",
      "optimal threshold: -0.4009\n",
      "Epoch 235 train loss: 1.2047, eval loss 1.1979917287826538\n",
      "optimal threshold: -0.4043\n",
      "Epoch 236 train loss: 1.1842, eval loss 1.1965906620025635\n",
      "optimal threshold: -0.4068\n",
      "Epoch 237 train loss: 1.2074, eval loss 1.1951875686645508\n",
      "optimal threshold: -0.4091\n",
      "Epoch 238 train loss: 1.1881, eval loss 1.1937869787216187\n",
      "optimal threshold: -0.4108\n",
      "Epoch 239 train loss: 1.1877, eval loss 1.192383885383606\n",
      "optimal threshold: -0.4145\n",
      "Epoch 240 train loss: 1.1966, eval loss 1.1909749507904053\n",
      "optimal threshold: -0.4165\n",
      "Epoch 241 train loss: 1.1792, eval loss 1.1895673274993896\n",
      "optimal threshold: -0.4191\n",
      "Epoch 242 train loss: 1.1958, eval loss 1.1881542205810547\n",
      "optimal threshold: -0.4214\n",
      "Epoch 243 train loss: 1.1957, eval loss 1.1867401599884033\n",
      "optimal threshold: -0.4305\n",
      "Epoch 244 train loss: 1.1948, eval loss 1.1853314638137817\n",
      "optimal threshold: -0.4329\n",
      "Epoch 245 train loss: 1.1669, eval loss 1.1839183568954468\n",
      "optimal threshold: -0.4291\n",
      "Epoch 246 train loss: 1.1718, eval loss 1.182501196861267\n",
      "optimal threshold: -0.4365\n",
      "Epoch 247 train loss: 1.1700, eval loss 1.1810866594314575\n",
      "optimal threshold: -0.4390\n",
      "Epoch 248 train loss: 1.1776, eval loss 1.1796746253967285\n",
      "optimal threshold: -0.4389\n",
      "Epoch 249 train loss: 1.1688, eval loss 1.178262710571289\n",
      "optimal threshold: -0.4419\n",
      "Epoch 250 train loss: 1.1885, eval loss 1.176842212677002\n",
      "optimal threshold: -0.4438\n",
      "Epoch 251 train loss: 1.1848, eval loss 1.175424337387085\n",
      "optimal threshold: -0.4465\n",
      "Epoch 252 train loss: 1.1774, eval loss 1.1740022897720337\n",
      "optimal threshold: -0.4489\n",
      "Epoch 253 train loss: 1.1740, eval loss 1.1725813150405884\n",
      "optimal threshold: -0.4515\n",
      "Epoch 254 train loss: 1.1566, eval loss 1.171161413192749\n",
      "optimal threshold: -0.4541\n",
      "Epoch 255 train loss: 1.1778, eval loss 1.1697410345077515\n",
      "optimal threshold: -0.4562\n",
      "Epoch 256 train loss: 1.1740, eval loss 1.1683145761489868\n",
      "optimal threshold: -0.4577\n",
      "Epoch 257 train loss: 1.1754, eval loss 1.166886806488037\n",
      "optimal threshold: -0.4660\n",
      "Epoch 258 train loss: 1.1633, eval loss 1.1654577255249023\n",
      "optimal threshold: -0.4691\n",
      "Epoch 259 train loss: 1.1637, eval loss 1.1640310287475586\n",
      "optimal threshold: -0.4651\n",
      "Epoch 260 train loss: 1.1569, eval loss 1.1626064777374268\n",
      "optimal threshold: -0.4675\n",
      "Epoch 261 train loss: 1.1640, eval loss 1.1611844301223755\n",
      "optimal threshold: -0.4730\n",
      "Epoch 262 train loss: 1.1457, eval loss 1.1597609519958496\n",
      "optimal threshold: -0.4776\n",
      "Epoch 263 train loss: 1.1532, eval loss 1.1583356857299805\n",
      "optimal threshold: -0.4788\n",
      "Epoch 264 train loss: 1.1509, eval loss 1.156907081604004\n",
      "optimal threshold: -0.4824\n",
      "Epoch 265 train loss: 1.1534, eval loss 1.1554733514785767\n",
      "optimal threshold: -0.4842\n",
      "Epoch 266 train loss: 1.1611, eval loss 1.1540424823760986\n",
      "optimal threshold: -0.4864\n",
      "Epoch 267 train loss: 1.1604, eval loss 1.152604579925537\n",
      "optimal threshold: -0.4888\n",
      "Epoch 268 train loss: 1.1464, eval loss 1.1511753797531128\n",
      "optimal threshold: -0.4927\n",
      "Epoch 269 train loss: 1.1467, eval loss 1.1497479677200317\n",
      "optimal threshold: -0.4954\n",
      "Epoch 270 train loss: 1.1602, eval loss 1.148317575454712\n",
      "optimal threshold: -0.4980\n",
      "Epoch 271 train loss: 1.1472, eval loss 1.1468827724456787\n",
      "optimal threshold: -0.5016\n",
      "Epoch 272 train loss: 1.1441, eval loss 1.145457148551941\n",
      "optimal threshold: -0.5037\n",
      "Epoch 273 train loss: 1.1351, eval loss 1.1440327167510986\n",
      "optimal threshold: -0.5062\n",
      "Epoch 274 train loss: 1.1429, eval loss 1.1426002979278564\n",
      "optimal threshold: -0.5087\n",
      "Epoch 275 train loss: 1.1552, eval loss 1.141167402267456\n",
      "optimal threshold: -0.5121\n",
      "Epoch 276 train loss: 1.1362, eval loss 1.1397404670715332\n",
      "optimal threshold: -0.5149\n",
      "Epoch 277 train loss: 1.1527, eval loss 1.138310432434082\n",
      "optimal threshold: -0.5173\n",
      "Epoch 278 train loss: 1.1561, eval loss 1.1368768215179443\n",
      "optimal threshold: -0.5242\n",
      "Epoch 279 train loss: 1.1602, eval loss 1.1354444026947021\n",
      "optimal threshold: -0.5262\n",
      "Epoch 280 train loss: 1.1381, eval loss 1.1340197324752808\n",
      "optimal threshold: -0.5294\n",
      "Epoch 281 train loss: 1.1263, eval loss 1.1325892210006714\n",
      "optimal threshold: -0.5176\n",
      "Epoch 282 train loss: 1.1425, eval loss 1.1311626434326172\n",
      "optimal threshold: -0.5201\n",
      "Epoch 283 train loss: 1.1186, eval loss 1.1297378540039062\n",
      "optimal threshold: -0.5334\n",
      "Epoch 284 train loss: 1.1348, eval loss 1.128309965133667\n",
      "optimal threshold: -0.5252\n",
      "Epoch 285 train loss: 1.1342, eval loss 1.1268821954727173\n",
      "optimal threshold: -0.5271\n",
      "Epoch 286 train loss: 1.1064, eval loss 1.1254523992538452\n",
      "optimal threshold: -0.5403\n",
      "Epoch 287 train loss: 1.1345, eval loss 1.1240218877792358\n",
      "optimal threshold: -0.5417\n",
      "Epoch 288 train loss: 1.1064, eval loss 1.1225990056991577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5436\n",
      "Epoch 289 train loss: 1.1055, eval loss 1.1211670637130737\n",
      "optimal threshold: -0.5459\n",
      "Epoch 290 train loss: 1.1226, eval loss 1.1197396516799927\n",
      "optimal threshold: -0.5492\n",
      "Epoch 291 train loss: 1.1209, eval loss 1.1183173656463623\n",
      "optimal threshold: -0.5515\n",
      "Epoch 292 train loss: 1.1268, eval loss 1.1168936491012573\n",
      "optimal threshold: -0.5540\n",
      "Epoch 293 train loss: 1.1123, eval loss 1.1154675483703613\n",
      "optimal threshold: -0.5566\n",
      "Epoch 294 train loss: 1.1007, eval loss 1.1140413284301758\n",
      "optimal threshold: -0.5593\n",
      "Epoch 295 train loss: 1.1220, eval loss 1.1126227378845215\n",
      "optimal threshold: -0.5620\n",
      "Epoch 296 train loss: 1.1242, eval loss 1.111199140548706\n",
      "optimal threshold: -0.5647\n",
      "Epoch 297 train loss: 1.1073, eval loss 1.1097692251205444\n",
      "optimal threshold: -0.5675\n",
      "Epoch 298 train loss: 1.1103, eval loss 1.1083506345748901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:53:17,139] Trial 25 finished with value: 1.1062806844711304 and parameters: {'learning_rate_exp': -5.8377173458711855, 'dropout_p': 0.45442776997884454, 'l2_reg_exp': -4.193707812852139, 'batch_size': 412, 'N': 98}. Best is trial 16 with value: 0.2552145719528198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5629\n",
      "Epoch 299 train loss: 1.1063, eval loss 1.1069345474243164\n",
      "optimal threshold: -0.1838\n",
      "Epoch 0 train loss: 1.3333, eval loss 1.3329137563705444\n",
      "optimal threshold: -0.4052\n",
      "Epoch 1 train loss: 1.1972, eval loss 1.1930128335952759\n",
      "optimal threshold: -0.7704\n",
      "Epoch 2 train loss: 0.9831, eval loss 0.9676210284233093\n",
      "optimal threshold: -0.9029\n",
      "Epoch 3 train loss: 0.8245, eval loss 0.8082664608955383\n",
      "optimal threshold: -0.8051\n",
      "Epoch 4 train loss: 0.7707, eval loss 0.7388967275619507\n",
      "optimal threshold: -0.7548\n",
      "Epoch 5 train loss: 0.7434, eval loss 0.7128286957740784\n",
      "optimal threshold: -0.6693\n",
      "Epoch 6 train loss: 0.7325, eval loss 0.7008044123649597\n",
      "optimal threshold: -0.6451\n",
      "Epoch 7 train loss: 0.7121, eval loss 0.6934648752212524\n",
      "optimal threshold: -0.5578\n",
      "Epoch 8 train loss: 0.7240, eval loss 0.688090980052948\n",
      "optimal threshold: -0.5930\n",
      "Epoch 9 train loss: 0.7123, eval loss 0.6838011741638184\n",
      "optimal threshold: -0.6016\n",
      "Epoch 10 train loss: 0.7122, eval loss 0.6802821755409241\n",
      "optimal threshold: -0.5460\n",
      "Epoch 11 train loss: 0.6955, eval loss 0.6774168610572815\n",
      "optimal threshold: -0.5880\n",
      "Epoch 12 train loss: 0.7048, eval loss 0.6750958561897278\n",
      "optimal threshold: -0.5978\n",
      "Epoch 13 train loss: 0.7008, eval loss 0.6730625033378601\n",
      "optimal threshold: -0.6306\n",
      "Epoch 14 train loss: 0.6897, eval loss 0.6714784502983093\n",
      "optimal threshold: -0.5979\n",
      "Epoch 15 train loss: 0.6735, eval loss 0.6699405312538147\n",
      "optimal threshold: -0.4272\n",
      "Epoch 16 train loss: 0.6945, eval loss 0.668590784072876\n",
      "optimal threshold: -0.4102\n",
      "Epoch 17 train loss: 0.6931, eval loss 0.6675601601600647\n",
      "optimal threshold: -0.4074\n",
      "Epoch 18 train loss: 0.6725, eval loss 0.6666277050971985\n",
      "optimal threshold: -0.4211\n",
      "Epoch 19 train loss: 0.7017, eval loss 0.6655923128128052\n",
      "optimal threshold: -0.4129\n",
      "Epoch 20 train loss: 0.6960, eval loss 0.6647335886955261\n",
      "optimal threshold: -0.3951\n",
      "Epoch 21 train loss: 0.6816, eval loss 0.6639742851257324\n",
      "optimal threshold: -0.3099\n",
      "Epoch 22 train loss: 0.6709, eval loss 0.6631410121917725\n",
      "optimal threshold: -0.3787\n",
      "Epoch 23 train loss: 0.6757, eval loss 0.662578821182251\n",
      "optimal threshold: -0.3421\n",
      "Epoch 24 train loss: 0.6602, eval loss 0.6620128154754639\n",
      "optimal threshold: -0.3847\n",
      "Epoch 25 train loss: 0.6698, eval loss 0.6616851091384888\n",
      "optimal threshold: -0.3560\n",
      "Epoch 26 train loss: 0.6551, eval loss 0.6611477136611938\n",
      "optimal threshold: -0.3662\n",
      "Epoch 27 train loss: 0.6700, eval loss 0.6607249975204468\n",
      "optimal threshold: -0.4061\n",
      "Epoch 28 train loss: 0.6722, eval loss 0.6604700684547424\n",
      "optimal threshold: -0.3918\n",
      "Epoch 29 train loss: 0.6576, eval loss 0.6601645946502686\n",
      "optimal threshold: -0.4032\n",
      "Epoch 30 train loss: 0.6530, eval loss 0.6597522497177124\n",
      "optimal threshold: -0.4019\n",
      "Epoch 31 train loss: 0.6683, eval loss 0.6595003008842468\n",
      "optimal threshold: -0.3940\n",
      "Epoch 32 train loss: 0.6573, eval loss 0.659257173538208\n",
      "optimal threshold: -0.4024\n",
      "Epoch 33 train loss: 0.6698, eval loss 0.6590555906295776\n",
      "optimal threshold: -0.3590\n",
      "Epoch 34 train loss: 0.6635, eval loss 0.6586844325065613\n",
      "optimal threshold: -0.3538\n",
      "Epoch 35 train loss: 0.6650, eval loss 0.6587219834327698\n",
      "optimal threshold: -0.3583\n",
      "Epoch 36 train loss: 0.6518, eval loss 0.6583989262580872\n",
      "optimal threshold: -0.3714\n",
      "Epoch 37 train loss: 0.6553, eval loss 0.6582083702087402\n",
      "optimal threshold: -0.2822\n",
      "Epoch 38 train loss: 0.6759, eval loss 0.6581661701202393\n",
      "optimal threshold: -0.2787\n",
      "Epoch 39 train loss: 0.6613, eval loss 0.6581025719642639\n",
      "optimal threshold: -0.4686\n",
      "Epoch 40 train loss: 0.6716, eval loss 0.6579639315605164\n",
      "optimal threshold: -0.3596\n",
      "Epoch 41 train loss: 0.6973, eval loss 0.6577877998352051\n",
      "optimal threshold: -0.4738\n",
      "Epoch 42 train loss: 0.6733, eval loss 0.6577580571174622\n",
      "optimal threshold: -0.3587\n",
      "Epoch 43 train loss: 0.6834, eval loss 0.6576455235481262\n",
      "optimal threshold: -0.3860\n",
      "Epoch 44 train loss: 0.6441, eval loss 0.6576785445213318\n",
      "optimal threshold: -0.2918\n",
      "Epoch 45 train loss: 0.6565, eval loss 0.6577039361000061\n",
      "optimal threshold: -0.2948\n",
      "Epoch 46 train loss: 0.6825, eval loss 0.6575809121131897\n",
      "optimal threshold: -0.2956\n",
      "Epoch 47 train loss: 0.6668, eval loss 0.6576511263847351\n",
      "optimal threshold: -0.3017\n",
      "Epoch 48 train loss: 0.6553, eval loss 0.6574471592903137\n",
      "optimal threshold: -0.3021\n",
      "Epoch 49 train loss: 0.6573, eval loss 0.6576887369155884\n",
      "optimal threshold: -0.2615\n",
      "Epoch 50 train loss: 0.6652, eval loss 0.657453715801239\n",
      "optimal threshold: -0.2675\n",
      "Epoch 51 train loss: 0.6757, eval loss 0.657436192035675\n",
      "optimal threshold: -0.2778\n",
      "Epoch 52 train loss: 0.6531, eval loss 0.6574556231498718\n",
      "optimal threshold: -0.2731\n",
      "Epoch 53 train loss: 0.6611, eval loss 0.6575143337249756\n",
      "optimal threshold: -0.2610\n",
      "Epoch 54 train loss: 0.6577, eval loss 0.657396137714386\n",
      "optimal threshold: -0.2771\n",
      "Epoch 55 train loss: 0.6699, eval loss 0.657474935054779\n",
      "optimal threshold: -0.2861\n",
      "Epoch 56 train loss: 0.6463, eval loss 0.6573971509933472\n",
      "optimal threshold: -0.2778\n",
      "Epoch 57 train loss: 0.6765, eval loss 0.6573379635810852\n",
      "optimal threshold: -0.3078\n",
      "Epoch 58 train loss: 0.6540, eval loss 0.6573822498321533\n",
      "optimal threshold: -0.2819\n",
      "Epoch 59 train loss: 0.6363, eval loss 0.6572721600532532\n",
      "optimal threshold: -0.2854\n",
      "Epoch 60 train loss: 0.6503, eval loss 0.657267689704895\n",
      "optimal threshold: -0.2682\n",
      "Epoch 61 train loss: 0.6504, eval loss 0.6573989391326904\n",
      "optimal threshold: -0.2737\n",
      "Epoch 62 train loss: 0.6616, eval loss 0.6572269201278687\n",
      "optimal threshold: -0.2630\n",
      "Epoch 63 train loss: 0.6463, eval loss 0.6574553847312927\n",
      "optimal threshold: -0.2998\n",
      "Epoch 64 train loss: 0.6625, eval loss 0.6575338244438171\n",
      "optimal threshold: -0.2593\n",
      "Epoch 65 train loss: 0.6717, eval loss 0.6577203869819641\n",
      "optimal threshold: -0.2785\n",
      "Epoch 66 train loss: 0.6386, eval loss 0.6575871109962463\n",
      "optimal threshold: -0.2917\n",
      "Epoch 67 train loss: 0.6604, eval loss 0.6572511792182922\n",
      "optimal threshold: -0.2754\n",
      "Epoch 68 train loss: 0.6585, eval loss 0.6571227312088013\n",
      "optimal threshold: -0.2954\n",
      "Epoch 69 train loss: 0.6424, eval loss 0.6574164628982544\n",
      "optimal threshold: -0.3394\n",
      "Epoch 70 train loss: 0.6549, eval loss 0.6575145125389099\n",
      "optimal threshold: -0.3494\n",
      "Epoch 71 train loss: 0.6632, eval loss 0.657490611076355\n",
      "optimal threshold: -0.3731\n",
      "Epoch 72 train loss: 0.6221, eval loss 0.6572808027267456\n",
      "optimal threshold: -0.3892\n",
      "Epoch 73 train loss: 0.6770, eval loss 0.6574234962463379\n",
      "optimal threshold: -0.3640\n",
      "Epoch 74 train loss: 0.6443, eval loss 0.6577701568603516\n",
      "optimal threshold: -0.3788\n",
      "Epoch 75 train loss: 0.6473, eval loss 0.6577232480049133\n",
      "optimal threshold: -0.3525\n",
      "Epoch 76 train loss: 0.6365, eval loss 0.6579328179359436\n",
      "optimal threshold: -0.5770\n",
      "Epoch 77 train loss: 0.6426, eval loss 0.6577839851379395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:53:53,106] Trial 26 finished with value: 0.6381198763847351 and parameters: {'learning_rate_exp': -3.8167273739707737, 'dropout_p': 0.15451530867414293, 'l2_reg_exp': -3.7647338892883155, 'batch_size': 394, 'N': 90}. Best is trial 16 with value: 0.2552145719528198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.3918\n",
      "optimal threshold: -0.4083\n",
      "Epoch 0 train loss: 0.7474, eval loss 0.6667942404747009\n",
      "optimal threshold: -0.3680\n",
      "Epoch 1 train loss: 0.6797, eval loss 0.6615830659866333\n",
      "optimal threshold: -0.4741\n",
      "Epoch 2 train loss: 0.6689, eval loss 0.6651472449302673\n",
      "optimal threshold: -0.4999\n",
      "Epoch 3 train loss: 0.6816, eval loss 0.6643032431602478\n",
      "optimal threshold: -0.5034\n",
      "Epoch 4 train loss: 0.6554, eval loss 0.6643617749214172\n",
      "optimal threshold: -0.5125\n",
      "Epoch 5 train loss: 0.6742, eval loss 0.66073077917099\n",
      "optimal threshold: -0.6787\n",
      "Epoch 6 train loss: 0.6379, eval loss 0.670745849609375\n",
      "optimal threshold: -0.4801\n",
      "Epoch 7 train loss: 0.6523, eval loss 0.6764196157455444\n",
      "optimal threshold: -0.5659\n",
      "Epoch 8 train loss: 0.6160, eval loss 0.6744080781936646\n",
      "optimal threshold: -0.4957\n",
      "Epoch 9 train loss: 0.6408, eval loss 0.6718577742576599\n",
      "optimal threshold: -0.5820\n",
      "Epoch 10 train loss: 0.6707, eval loss 0.6777241826057434\n",
      "optimal threshold: -0.5972\n",
      "Epoch 11 train loss: 0.6039, eval loss 0.6806628704071045\n",
      "optimal threshold: -0.3891\n",
      "Epoch 12 train loss: 0.6156, eval loss 0.6778941750526428\n",
      "optimal threshold: -0.6122\n",
      "Epoch 13 train loss: 0.6362, eval loss 0.6836835741996765\n",
      "optimal threshold: -0.3599\n",
      "Epoch 14 train loss: 0.6108, eval loss 0.6860582828521729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:54:01,857] Trial 27 finished with value: 0.5986214280128479 and parameters: {'learning_rate_exp': -2.1412374336156774, 'dropout_p': 0.2823976785354183, 'l2_reg_exp': -4.746766627346993, 'batch_size': 225, 'N': 111}. Best is trial 16 with value: 0.2552145719528198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4700\n",
      "optimal threshold: -0.6247\n",
      "Epoch 0 train loss: 0.6699, eval loss 0.6669756174087524\n",
      "optimal threshold: -0.5905\n",
      "Epoch 1 train loss: 0.6712, eval loss 0.6623003482818604\n",
      "optimal threshold: -0.6140\n",
      "Epoch 2 train loss: 0.6368, eval loss 0.6611074209213257\n",
      "optimal threshold: -0.5585\n",
      "Epoch 3 train loss: 0.6180, eval loss 0.6603827476501465\n",
      "optimal threshold: -0.6208\n",
      "Epoch 4 train loss: 0.6386, eval loss 0.6659618020057678\n",
      "optimal threshold: -0.5356\n",
      "Epoch 5 train loss: 0.5992, eval loss 0.6693997979164124\n",
      "optimal threshold: -0.4919\n",
      "Epoch 6 train loss: 0.6113, eval loss 0.6766005754470825\n",
      "optimal threshold: -0.5117\n",
      "Epoch 7 train loss: 0.5961, eval loss 0.6779166460037231\n",
      "optimal threshold: -0.4009\n",
      "Epoch 8 train loss: 0.5673, eval loss 0.6843284368515015\n",
      "optimal threshold: -0.5116\n",
      "Epoch 9 train loss: 0.5840, eval loss 0.6910669803619385\n",
      "optimal threshold: -0.4274\n",
      "Epoch 10 train loss: 0.5270, eval loss 0.7069171667098999\n",
      "optimal threshold: -0.4611\n",
      "Epoch 11 train loss: 0.5411, eval loss 0.7098985910415649\n",
      "optimal threshold: -0.3348\n",
      "Epoch 12 train loss: 0.5443, eval loss 0.7258718013763428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:54:09,956] Trial 28 finished with value: 0.556196928024292 and parameters: {'learning_rate_exp': -2.293829158473805, 'dropout_p': 0.2130935371242404, 'l2_reg_exp': -3.7949975909330607, 'batch_size': 381, 'N': 316}. Best is trial 16 with value: 0.2552145719528198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4799\n",
      "optimal threshold: -0.1191\n",
      "Epoch 0 train loss: 1.3770, eval loss 1.3821855783462524\n",
      "optimal threshold: -0.1229\n",
      "Epoch 1 train loss: 1.3802, eval loss 1.3808529376983643\n",
      "optimal threshold: -0.1224\n",
      "Epoch 2 train loss: 1.3818, eval loss 1.3795281648635864\n",
      "optimal threshold: -0.1227\n",
      "Epoch 3 train loss: 1.3764, eval loss 1.3782074451446533\n",
      "optimal threshold: -0.1247\n",
      "Epoch 4 train loss: 1.3807, eval loss 1.3768898248672485\n",
      "optimal threshold: -0.1266\n",
      "Epoch 5 train loss: 1.3792, eval loss 1.3755767345428467\n",
      "optimal threshold: -0.1332\n",
      "Epoch 6 train loss: 1.3639, eval loss 1.374267339706421\n",
      "optimal threshold: -0.1270\n",
      "Epoch 7 train loss: 1.3777, eval loss 1.3729597330093384\n",
      "optimal threshold: -0.1301\n",
      "Epoch 8 train loss: 1.3768, eval loss 1.3716539144515991\n",
      "optimal threshold: -0.1311\n",
      "Epoch 9 train loss: 1.3661, eval loss 1.3703492879867554\n",
      "optimal threshold: -0.1333\n",
      "Epoch 10 train loss: 1.3632, eval loss 1.3690457344055176\n",
      "optimal threshold: -0.1357\n",
      "Epoch 11 train loss: 1.3718, eval loss 1.3677427768707275\n",
      "optimal threshold: -0.1388\n",
      "Epoch 12 train loss: 1.3694, eval loss 1.3664424419403076\n",
      "optimal threshold: -0.1413\n",
      "Epoch 13 train loss: 1.3671, eval loss 1.3651410341262817\n",
      "optimal threshold: -0.1434\n",
      "Epoch 14 train loss: 1.3605, eval loss 1.3638410568237305\n",
      "optimal threshold: -0.1454\n",
      "Epoch 15 train loss: 1.3638, eval loss 1.3625410795211792\n",
      "optimal threshold: -0.1472\n",
      "Epoch 16 train loss: 1.3582, eval loss 1.3612357378005981\n",
      "optimal threshold: -0.1524\n",
      "Epoch 17 train loss: 1.3598, eval loss 1.3599275350570679\n",
      "optimal threshold: -0.1547\n",
      "Epoch 18 train loss: 1.3614, eval loss 1.3586227893829346\n",
      "optimal threshold: -0.1558\n",
      "Epoch 19 train loss: 1.3523, eval loss 1.3573148250579834\n",
      "optimal threshold: -0.1583\n",
      "Epoch 20 train loss: 1.3521, eval loss 1.356005072593689\n",
      "optimal threshold: -0.1607\n",
      "Epoch 21 train loss: 1.3555, eval loss 1.3546940088272095\n",
      "optimal threshold: -0.1627\n",
      "Epoch 22 train loss: 1.3453, eval loss 1.353380799293518\n",
      "optimal threshold: -0.1608\n",
      "Epoch 23 train loss: 1.3467, eval loss 1.3520601987838745\n",
      "optimal threshold: -0.1626\n",
      "Epoch 24 train loss: 1.3492, eval loss 1.3507336378097534\n",
      "optimal threshold: -0.1710\n",
      "Epoch 25 train loss: 1.3469, eval loss 1.3494094610214233\n",
      "optimal threshold: -0.1693\n",
      "Epoch 26 train loss: 1.3438, eval loss 1.3480820655822754\n",
      "optimal threshold: -0.1685\n",
      "Epoch 27 train loss: 1.3423, eval loss 1.3467504978179932\n",
      "optimal threshold: -0.1711\n",
      "Epoch 28 train loss: 1.3428, eval loss 1.3454142808914185\n",
      "optimal threshold: -0.1757\n",
      "Epoch 29 train loss: 1.3372, eval loss 1.3440736532211304\n",
      "optimal threshold: -0.1752\n",
      "Epoch 30 train loss: 1.3414, eval loss 1.3427289724349976\n",
      "optimal threshold: -0.1788\n",
      "Epoch 31 train loss: 1.3327, eval loss 1.341376543045044\n",
      "optimal threshold: -0.1789\n",
      "Epoch 32 train loss: 1.3324, eval loss 1.3400201797485352\n",
      "optimal threshold: -0.1807\n",
      "Epoch 33 train loss: 1.3449, eval loss 1.3386592864990234\n",
      "optimal threshold: -0.1831\n",
      "Epoch 34 train loss: 1.3336, eval loss 1.3372929096221924\n",
      "optimal threshold: -0.1858\n",
      "Epoch 35 train loss: 1.3284, eval loss 1.3359185457229614\n",
      "optimal threshold: -0.1874\n",
      "Epoch 36 train loss: 1.3282, eval loss 1.334538459777832\n",
      "optimal threshold: -0.1901\n",
      "Epoch 37 train loss: 1.3330, eval loss 1.3331588506698608\n",
      "optimal threshold: -0.1928\n",
      "Epoch 38 train loss: 1.3279, eval loss 1.331769347190857\n",
      "optimal threshold: -0.1958\n",
      "Epoch 39 train loss: 1.3283, eval loss 1.3303717374801636\n",
      "optimal threshold: -0.1983\n",
      "Epoch 40 train loss: 1.3231, eval loss 1.3289685249328613\n",
      "optimal threshold: -0.2001\n",
      "Epoch 41 train loss: 1.3146, eval loss 1.32755708694458\n",
      "optimal threshold: -0.2028\n",
      "Epoch 42 train loss: 1.3191, eval loss 1.3261370658874512\n",
      "optimal threshold: -0.2052\n",
      "Epoch 43 train loss: 1.3182, eval loss 1.324714183807373\n",
      "optimal threshold: -0.2035\n",
      "Epoch 44 train loss: 1.3201, eval loss 1.3232818841934204\n",
      "optimal threshold: -0.2110\n",
      "Epoch 45 train loss: 1.3227, eval loss 1.3218393325805664\n",
      "optimal threshold: -0.2057\n",
      "Epoch 46 train loss: 1.3119, eval loss 1.32039213180542\n",
      "optimal threshold: -0.2079\n",
      "Epoch 47 train loss: 1.3126, eval loss 1.3189377784729004\n",
      "optimal threshold: -0.2106\n",
      "Epoch 48 train loss: 1.3151, eval loss 1.3174799680709839\n",
      "optimal threshold: -0.2133\n",
      "Epoch 49 train loss: 1.3121, eval loss 1.3160120248794556\n",
      "optimal threshold: -0.2160\n",
      "Epoch 50 train loss: 1.3178, eval loss 1.314538836479187\n",
      "optimal threshold: -0.2231\n",
      "Epoch 51 train loss: 1.3008, eval loss 1.3130559921264648\n",
      "optimal threshold: -0.2264\n",
      "Epoch 52 train loss: 1.3156, eval loss 1.3115652799606323\n",
      "optimal threshold: -0.2292\n",
      "Epoch 53 train loss: 1.3043, eval loss 1.3100700378417969\n",
      "optimal threshold: -0.2322\n",
      "Epoch 54 train loss: 1.3022, eval loss 1.3085719347000122\n",
      "optimal threshold: -0.2352\n",
      "Epoch 55 train loss: 1.3055, eval loss 1.3070628643035889\n",
      "optimal threshold: -0.2387\n",
      "Epoch 56 train loss: 1.2903, eval loss 1.3055516481399536\n",
      "optimal threshold: -0.2401\n",
      "Epoch 57 train loss: 1.2945, eval loss 1.304028034210205\n",
      "optimal threshold: -0.2385\n",
      "Epoch 58 train loss: 1.2916, eval loss 1.3025071620941162\n",
      "optimal threshold: -0.2407\n",
      "Epoch 59 train loss: 1.2930, eval loss 1.3009731769561768\n",
      "optimal threshold: -0.2434\n",
      "Epoch 60 train loss: 1.2810, eval loss 1.2994282245635986\n",
      "optimal threshold: -0.2455\n",
      "Epoch 61 train loss: 1.2868, eval loss 1.2978758811950684\n",
      "optimal threshold: -0.2489\n",
      "Epoch 62 train loss: 1.2940, eval loss 1.2963247299194336\n",
      "optimal threshold: -0.2523\n",
      "Epoch 63 train loss: 1.2856, eval loss 1.2947638034820557\n",
      "optimal threshold: -0.2547\n",
      "Epoch 64 train loss: 1.2761, eval loss 1.2931928634643555\n",
      "optimal threshold: -0.2577\n",
      "Epoch 65 train loss: 1.2820, eval loss 1.2916181087493896\n",
      "optimal threshold: -0.2612\n",
      "Epoch 66 train loss: 1.2931, eval loss 1.2900406122207642\n",
      "optimal threshold: -0.2640\n",
      "Epoch 67 train loss: 1.2868, eval loss 1.2884504795074463\n",
      "optimal threshold: -0.2656\n",
      "Epoch 68 train loss: 1.2783, eval loss 1.2868527173995972\n",
      "optimal threshold: -0.2679\n",
      "Epoch 69 train loss: 1.2796, eval loss 1.2852532863616943\n",
      "optimal threshold: -0.2707\n",
      "Epoch 70 train loss: 1.2778, eval loss 1.2836490869522095\n",
      "optimal threshold: -0.2737\n",
      "Epoch 71 train loss: 1.2674, eval loss 1.2820348739624023\n",
      "optimal threshold: -0.2731\n",
      "Epoch 72 train loss: 1.2636, eval loss 1.2804148197174072\n",
      "optimal threshold: -0.2761\n",
      "Epoch 73 train loss: 1.2659, eval loss 1.2787917852401733\n",
      "optimal threshold: -0.2812\n",
      "Epoch 74 train loss: 1.2593, eval loss 1.2771652936935425\n",
      "optimal threshold: -0.2830\n",
      "Epoch 75 train loss: 1.2634, eval loss 1.275527834892273\n",
      "optimal threshold: -0.2859\n",
      "Epoch 76 train loss: 1.2695, eval loss 1.273887276649475\n",
      "optimal threshold: -0.2892\n",
      "Epoch 77 train loss: 1.2673, eval loss 1.272239089012146\n",
      "optimal threshold: -0.2920\n",
      "Epoch 78 train loss: 1.2495, eval loss 1.2705882787704468\n",
      "optimal threshold: -0.2952\n",
      "Epoch 79 train loss: 1.2525, eval loss 1.2689297199249268\n",
      "optimal threshold: -0.2968\n",
      "Epoch 80 train loss: 1.2609, eval loss 1.2672618627548218\n",
      "optimal threshold: -0.2997\n",
      "Epoch 81 train loss: 1.2572, eval loss 1.2655919790267944\n",
      "optimal threshold: -0.3023\n",
      "Epoch 82 train loss: 1.2450, eval loss 1.263917088508606\n",
      "optimal threshold: -0.3061\n",
      "Epoch 83 train loss: 1.2441, eval loss 1.2622345685958862\n",
      "optimal threshold: -0.3137\n",
      "Epoch 84 train loss: 1.2491, eval loss 1.260553002357483\n",
      "optimal threshold: -0.3127\n",
      "Epoch 85 train loss: 1.2456, eval loss 1.258866310119629\n",
      "optimal threshold: -0.3191\n",
      "Epoch 86 train loss: 1.2392, eval loss 1.2571693658828735\n",
      "optimal threshold: -0.3221\n",
      "Epoch 87 train loss: 1.2563, eval loss 1.2554699182510376\n",
      "optimal threshold: -0.3249\n",
      "Epoch 88 train loss: 1.2468, eval loss 1.253765344619751\n",
      "optimal threshold: -0.3283\n",
      "Epoch 89 train loss: 1.2406, eval loss 1.2520536184310913\n",
      "optimal threshold: -0.3301\n",
      "Epoch 90 train loss: 1.2436, eval loss 1.2503379583358765\n",
      "optimal threshold: -0.3349\n",
      "Epoch 91 train loss: 1.2460, eval loss 1.2486261129379272\n",
      "optimal threshold: -0.3381\n",
      "Epoch 92 train loss: 1.2351, eval loss 1.2469005584716797\n",
      "optimal threshold: -0.3425\n",
      "Epoch 93 train loss: 1.2348, eval loss 1.2451751232147217\n",
      "optimal threshold: -0.3457\n",
      "Epoch 94 train loss: 1.2260, eval loss 1.2434409856796265\n",
      "optimal threshold: -0.3496\n",
      "Epoch 95 train loss: 1.2188, eval loss 1.2417038679122925\n",
      "optimal threshold: -0.3531\n",
      "Epoch 96 train loss: 1.2095, eval loss 1.2399641275405884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.3566\n",
      "Epoch 97 train loss: 1.2226, eval loss 1.2382196187973022\n",
      "optimal threshold: -0.3601\n",
      "Epoch 98 train loss: 1.2169, eval loss 1.2364752292633057\n",
      "optimal threshold: -0.3632\n",
      "Epoch 99 train loss: 1.2166, eval loss 1.234727144241333\n",
      "optimal threshold: -0.3662\n",
      "Epoch 100 train loss: 1.2219, eval loss 1.2329661846160889\n",
      "optimal threshold: -0.3695\n",
      "Epoch 101 train loss: 1.2221, eval loss 1.231204628944397\n",
      "optimal threshold: -0.3724\n",
      "Epoch 102 train loss: 1.2169, eval loss 1.2294448614120483\n",
      "optimal threshold: -0.3761\n",
      "Epoch 103 train loss: 1.2181, eval loss 1.2276794910430908\n",
      "optimal threshold: -0.3796\n",
      "Epoch 104 train loss: 1.2209, eval loss 1.2259091138839722\n",
      "optimal threshold: -0.3844\n",
      "Epoch 105 train loss: 1.2113, eval loss 1.224137544631958\n",
      "optimal threshold: -0.3881\n",
      "Epoch 106 train loss: 1.2111, eval loss 1.222357153892517\n",
      "optimal threshold: -0.3927\n",
      "Epoch 107 train loss: 1.2054, eval loss 1.2205761671066284\n",
      "optimal threshold: -0.3964\n",
      "Epoch 108 train loss: 1.1951, eval loss 1.2187923192977905\n",
      "optimal threshold: -0.3985\n",
      "Epoch 109 train loss: 1.2126, eval loss 1.2170072793960571\n",
      "optimal threshold: -0.4010\n",
      "Epoch 110 train loss: 1.1922, eval loss 1.2152230739593506\n",
      "optimal threshold: -0.4091\n",
      "Epoch 111 train loss: 1.1935, eval loss 1.2134281396865845\n",
      "optimal threshold: -0.4123\n",
      "Epoch 112 train loss: 1.1959, eval loss 1.211627721786499\n",
      "optimal threshold: -0.4159\n",
      "Epoch 113 train loss: 1.1968, eval loss 1.2098274230957031\n",
      "optimal threshold: -0.4144\n",
      "Epoch 114 train loss: 1.1871, eval loss 1.2080267667770386\n",
      "optimal threshold: -0.4177\n",
      "Epoch 115 train loss: 1.1944, eval loss 1.2062206268310547\n",
      "optimal threshold: -0.4211\n",
      "Epoch 116 train loss: 1.1913, eval loss 1.204412579536438\n",
      "optimal threshold: -0.4248\n",
      "Epoch 117 train loss: 1.1809, eval loss 1.2026036977767944\n",
      "optimal threshold: -0.4278\n",
      "Epoch 118 train loss: 1.1857, eval loss 1.2007883787155151\n",
      "optimal threshold: -0.4325\n",
      "Epoch 119 train loss: 1.1793, eval loss 1.1989768743515015\n",
      "optimal threshold: -0.4364\n",
      "Epoch 120 train loss: 1.1856, eval loss 1.1971606016159058\n",
      "optimal threshold: -0.4455\n",
      "Epoch 121 train loss: 1.1700, eval loss 1.1953394412994385\n",
      "optimal threshold: -0.4486\n",
      "Epoch 122 train loss: 1.1850, eval loss 1.1935189962387085\n",
      "optimal threshold: -0.4523\n",
      "Epoch 123 train loss: 1.1845, eval loss 1.1916978359222412\n",
      "optimal threshold: -0.4555\n",
      "Epoch 124 train loss: 1.1802, eval loss 1.189879298210144\n",
      "optimal threshold: -0.4594\n",
      "Epoch 125 train loss: 1.1740, eval loss 1.1880518198013306\n",
      "optimal threshold: -0.4630\n",
      "Epoch 126 train loss: 1.1754, eval loss 1.1862186193466187\n",
      "optimal threshold: -0.4665\n",
      "Epoch 127 train loss: 1.1624, eval loss 1.184387445449829\n",
      "optimal threshold: -0.4692\n",
      "Epoch 128 train loss: 1.1653, eval loss 1.1825560331344604\n",
      "optimal threshold: -0.4726\n",
      "Epoch 129 train loss: 1.1706, eval loss 1.1807210445404053\n",
      "optimal threshold: -0.4762\n",
      "Epoch 130 train loss: 1.1658, eval loss 1.1788908243179321\n",
      "optimal threshold: -0.4806\n",
      "Epoch 131 train loss: 1.1662, eval loss 1.1770533323287964\n",
      "optimal threshold: -0.4832\n",
      "Epoch 132 train loss: 1.1591, eval loss 1.175209403038025\n",
      "optimal threshold: -0.4856\n",
      "Epoch 133 train loss: 1.1614, eval loss 1.1733665466308594\n",
      "optimal threshold: -0.4886\n",
      "Epoch 134 train loss: 1.1724, eval loss 1.1715253591537476\n",
      "optimal threshold: -0.4917\n",
      "Epoch 135 train loss: 1.1496, eval loss 1.1696842908859253\n",
      "optimal threshold: -0.4953\n",
      "Epoch 136 train loss: 1.1539, eval loss 1.1678398847579956\n",
      "optimal threshold: -0.4999\n",
      "Epoch 137 train loss: 1.1429, eval loss 1.1659924983978271\n",
      "optimal threshold: -0.5017\n",
      "Epoch 138 train loss: 1.1363, eval loss 1.1641420125961304\n",
      "optimal threshold: -0.5052\n",
      "Epoch 139 train loss: 1.1394, eval loss 1.162291407585144\n",
      "optimal threshold: -0.5091\n",
      "Epoch 140 train loss: 1.1292, eval loss 1.1604381799697876\n",
      "optimal threshold: -0.5128\n",
      "Epoch 141 train loss: 1.1466, eval loss 1.1585875749588013\n",
      "optimal threshold: -0.5164\n",
      "Epoch 142 train loss: 1.1362, eval loss 1.156744122505188\n",
      "optimal threshold: -0.5079\n",
      "Epoch 143 train loss: 1.1260, eval loss 1.1548928022384644\n",
      "optimal threshold: -0.5106\n",
      "Epoch 144 train loss: 1.1482, eval loss 1.1530369520187378\n",
      "optimal threshold: -0.5143\n",
      "Epoch 145 train loss: 1.1273, eval loss 1.1511807441711426\n",
      "optimal threshold: -0.5342\n",
      "Epoch 146 train loss: 1.1244, eval loss 1.149329662322998\n",
      "optimal threshold: -0.5208\n",
      "Epoch 147 train loss: 1.1223, eval loss 1.1474812030792236\n",
      "optimal threshold: -0.5265\n",
      "Epoch 148 train loss: 1.1256, eval loss 1.1456289291381836\n",
      "optimal threshold: -0.5296\n",
      "Epoch 149 train loss: 1.1139, eval loss 1.1437774896621704\n",
      "optimal threshold: -0.5333\n",
      "Epoch 150 train loss: 1.1186, eval loss 1.1419204473495483\n",
      "optimal threshold: -0.5532\n",
      "Epoch 151 train loss: 1.1083, eval loss 1.1400681734085083\n",
      "optimal threshold: -0.5410\n",
      "Epoch 152 train loss: 1.1297, eval loss 1.138217806816101\n",
      "optimal threshold: -0.5436\n",
      "Epoch 153 train loss: 1.1092, eval loss 1.1363637447357178\n",
      "optimal threshold: -0.5638\n",
      "Epoch 154 train loss: 1.1205, eval loss 1.1345106363296509\n",
      "optimal threshold: -0.5491\n",
      "Epoch 155 train loss: 1.1042, eval loss 1.1326576471328735\n",
      "optimal threshold: -0.5683\n",
      "Epoch 156 train loss: 1.1071, eval loss 1.1308103799819946\n",
      "optimal threshold: -0.5591\n",
      "Epoch 157 train loss: 1.0942, eval loss 1.1289559602737427\n",
      "optimal threshold: -0.5622\n",
      "Epoch 158 train loss: 1.0977, eval loss 1.1270989179611206\n",
      "optimal threshold: -0.5655\n",
      "Epoch 159 train loss: 1.1054, eval loss 1.1252506971359253\n",
      "optimal threshold: -0.5683\n",
      "Epoch 160 train loss: 1.0881, eval loss 1.1233998537063599\n",
      "optimal threshold: -0.5714\n",
      "Epoch 161 train loss: 1.0906, eval loss 1.1215459108352661\n",
      "optimal threshold: -0.5750\n",
      "Epoch 162 train loss: 1.0923, eval loss 1.1196954250335693\n",
      "optimal threshold: -0.5964\n",
      "Epoch 163 train loss: 1.1078, eval loss 1.1178441047668457\n",
      "optimal threshold: -0.6004\n",
      "Epoch 164 train loss: 1.0759, eval loss 1.1159958839416504\n",
      "optimal threshold: -0.6044\n",
      "Epoch 165 train loss: 1.1124, eval loss 1.1141481399536133\n",
      "optimal threshold: -0.6084\n",
      "Epoch 166 train loss: 1.0865, eval loss 1.112301230430603\n",
      "optimal threshold: -0.6165\n",
      "Epoch 167 train loss: 1.0766, eval loss 1.1104488372802734\n",
      "optimal threshold: -0.6187\n",
      "Epoch 168 train loss: 1.0814, eval loss 1.1085997819900513\n",
      "optimal threshold: -0.6169\n",
      "Epoch 169 train loss: 1.0863, eval loss 1.1067537069320679\n",
      "optimal threshold: -0.6203\n",
      "Epoch 170 train loss: 1.0934, eval loss 1.1049113273620605\n",
      "optimal threshold: -0.6240\n",
      "Epoch 171 train loss: 1.0900, eval loss 1.1030704975128174\n",
      "optimal threshold: -0.6348\n",
      "Epoch 172 train loss: 1.0891, eval loss 1.1012275218963623\n",
      "optimal threshold: -0.6382\n",
      "Epoch 173 train loss: 1.0938, eval loss 1.0993834733963013\n",
      "optimal threshold: -0.6430\n",
      "Epoch 174 train loss: 1.0685, eval loss 1.0975464582443237\n",
      "optimal threshold: -0.6462\n",
      "Epoch 175 train loss: 1.0834, eval loss 1.095711588859558\n",
      "optimal threshold: -0.6503\n",
      "Epoch 176 train loss: 1.0813, eval loss 1.0938780307769775\n",
      "optimal threshold: -0.6529\n",
      "Epoch 177 train loss: 1.0708, eval loss 1.0920460224151611\n",
      "optimal threshold: -0.6476\n",
      "Epoch 178 train loss: 1.0667, eval loss 1.0902111530303955\n",
      "optimal threshold: -0.6604\n",
      "Epoch 179 train loss: 1.0666, eval loss 1.0883768796920776\n",
      "optimal threshold: -0.6552\n",
      "Epoch 180 train loss: 1.0511, eval loss 1.0865423679351807\n",
      "optimal threshold: -0.6587\n",
      "Epoch 181 train loss: 1.0562, eval loss 1.0847055912017822\n",
      "optimal threshold: -0.6566\n",
      "Epoch 182 train loss: 1.0563, eval loss 1.082872986793518\n",
      "optimal threshold: -0.6595\n",
      "Epoch 183 train loss: 1.0609, eval loss 1.0810455083847046\n",
      "optimal threshold: -0.6627\n",
      "Epoch 184 train loss: 1.0512, eval loss 1.0792245864868164\n",
      "optimal threshold: -0.6627\n",
      "Epoch 185 train loss: 1.0596, eval loss 1.0774059295654297\n",
      "optimal threshold: -0.6798\n",
      "Epoch 186 train loss: 1.0485, eval loss 1.0755860805511475\n",
      "optimal threshold: -0.6837\n",
      "Epoch 187 train loss: 1.0433, eval loss 1.0737636089324951\n",
      "optimal threshold: -0.6877\n",
      "Epoch 188 train loss: 1.0368, eval loss 1.0719510316848755\n",
      "optimal threshold: -0.6906\n",
      "Epoch 189 train loss: 1.0540, eval loss 1.070135235786438\n",
      "optimal threshold: -0.6930\n",
      "Epoch 190 train loss: 1.0409, eval loss 1.0683224201202393\n",
      "optimal threshold: -0.6864\n",
      "Epoch 191 train loss: 1.0420, eval loss 1.0665090084075928\n",
      "optimal threshold: -0.6898\n",
      "Epoch 192 train loss: 1.0617, eval loss 1.0647047758102417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6922\n",
      "Epoch 193 train loss: 1.0413, eval loss 1.0629023313522339\n",
      "optimal threshold: -0.7006\n",
      "Epoch 194 train loss: 1.0491, eval loss 1.0610994100570679\n",
      "optimal threshold: -0.7028\n",
      "Epoch 195 train loss: 1.0261, eval loss 1.0593029260635376\n",
      "optimal threshold: -0.7088\n",
      "Epoch 196 train loss: 1.0218, eval loss 1.0575045347213745\n",
      "optimal threshold: -0.7117\n",
      "Epoch 197 train loss: 1.0253, eval loss 1.0557078123092651\n",
      "optimal threshold: -0.7140\n",
      "Epoch 198 train loss: 1.0341, eval loss 1.0539120435714722\n",
      "optimal threshold: -0.7175\n",
      "Epoch 199 train loss: 1.0389, eval loss 1.0521200895309448\n",
      "optimal threshold: -0.7197\n",
      "Epoch 200 train loss: 1.0159, eval loss 1.0503300428390503\n",
      "optimal threshold: -0.7225\n",
      "Epoch 201 train loss: 1.0319, eval loss 1.0485390424728394\n",
      "optimal threshold: -0.7252\n",
      "Epoch 202 train loss: 1.0229, eval loss 1.0467567443847656\n",
      "optimal threshold: -0.7279\n",
      "Epoch 203 train loss: 1.0162, eval loss 1.0449714660644531\n",
      "optimal threshold: -0.7308\n",
      "Epoch 204 train loss: 1.0321, eval loss 1.0431863069534302\n",
      "optimal threshold: -0.7339\n",
      "Epoch 205 train loss: 1.0150, eval loss 1.0414063930511475\n",
      "optimal threshold: -0.7372\n",
      "Epoch 206 train loss: 1.0051, eval loss 1.039628505706787\n",
      "optimal threshold: -0.7408\n",
      "Epoch 207 train loss: 1.0111, eval loss 1.0378546714782715\n",
      "optimal threshold: -0.7416\n",
      "Epoch 208 train loss: 0.9970, eval loss 1.0360816717147827\n",
      "optimal threshold: -0.7507\n",
      "Epoch 209 train loss: 1.0062, eval loss 1.0343148708343506\n",
      "optimal threshold: -0.7418\n",
      "Epoch 210 train loss: 1.0016, eval loss 1.0325504541397095\n",
      "optimal threshold: -0.7491\n",
      "Epoch 211 train loss: 1.0038, eval loss 1.0307917594909668\n",
      "optimal threshold: -0.7520\n",
      "Epoch 212 train loss: 0.9993, eval loss 1.029036283493042\n",
      "optimal threshold: -0.7551\n",
      "Epoch 213 train loss: 1.0048, eval loss 1.027280569076538\n",
      "optimal threshold: -0.7590\n",
      "Epoch 214 train loss: 1.0048, eval loss 1.0255316495895386\n",
      "optimal threshold: -0.7593\n",
      "Epoch 215 train loss: 0.9993, eval loss 1.0237913131713867\n",
      "optimal threshold: -0.7620\n",
      "Epoch 216 train loss: 0.9798, eval loss 1.022045373916626\n",
      "optimal threshold: -0.7703\n",
      "Epoch 217 train loss: 0.9867, eval loss 1.0203057527542114\n",
      "optimal threshold: -0.7741\n",
      "Epoch 218 train loss: 0.9747, eval loss 1.0185672044754028\n",
      "optimal threshold: -0.7777\n",
      "Epoch 219 train loss: 0.9700, eval loss 1.0168330669403076\n",
      "optimal threshold: -0.7798\n",
      "Epoch 220 train loss: 0.9675, eval loss 1.015110731124878\n",
      "optimal threshold: -0.7840\n",
      "Epoch 221 train loss: 0.9880, eval loss 1.0133781433105469\n",
      "optimal threshold: -0.7872\n",
      "Epoch 222 train loss: 0.9527, eval loss 1.0116506814956665\n",
      "optimal threshold: -0.7856\n",
      "Epoch 223 train loss: 0.9816, eval loss 1.0099331140518188\n",
      "optimal threshold: -0.7927\n",
      "Epoch 224 train loss: 0.9631, eval loss 1.0082181692123413\n",
      "optimal threshold: -0.7917\n",
      "Epoch 225 train loss: 0.9470, eval loss 1.006503939628601\n",
      "optimal threshold: -0.7948\n",
      "Epoch 226 train loss: 0.9679, eval loss 1.0047969818115234\n",
      "optimal threshold: -0.8047\n",
      "Epoch 227 train loss: 0.9561, eval loss 1.0030903816223145\n",
      "optimal threshold: -0.8065\n",
      "Epoch 228 train loss: 0.9526, eval loss 1.0013904571533203\n",
      "optimal threshold: -0.8088\n",
      "Epoch 229 train loss: 0.9726, eval loss 0.9996910095214844\n",
      "optimal threshold: -0.8122\n",
      "Epoch 230 train loss: 0.9716, eval loss 0.9980007410049438\n",
      "optimal threshold: -0.8161\n",
      "Epoch 231 train loss: 0.9806, eval loss 0.9963095188140869\n",
      "optimal threshold: -0.8184\n",
      "Epoch 232 train loss: 0.9535, eval loss 0.994621217250824\n",
      "optimal threshold: -0.8024\n",
      "Epoch 233 train loss: 0.9470, eval loss 0.9929412007331848\n",
      "optimal threshold: -0.8039\n",
      "Epoch 234 train loss: 0.9649, eval loss 0.9912624955177307\n",
      "optimal threshold: -0.8060\n",
      "Epoch 235 train loss: 0.9491, eval loss 0.9895921349525452\n",
      "optimal threshold: -0.8096\n",
      "Epoch 236 train loss: 0.9609, eval loss 0.9879176020622253\n",
      "optimal threshold: -0.8120\n",
      "Epoch 237 train loss: 0.9550, eval loss 0.9862502217292786\n",
      "optimal threshold: -0.8283\n",
      "Epoch 238 train loss: 0.9679, eval loss 0.984587550163269\n",
      "optimal threshold: -0.8185\n",
      "Epoch 239 train loss: 0.9767, eval loss 0.9829348921775818\n",
      "optimal threshold: -0.8209\n",
      "Epoch 240 train loss: 0.9461, eval loss 0.9812839031219482\n",
      "optimal threshold: -0.8235\n",
      "Epoch 241 train loss: 0.9326, eval loss 0.9796333312988281\n",
      "optimal threshold: -0.8256\n",
      "Epoch 242 train loss: 0.9403, eval loss 0.9779818058013916\n",
      "optimal threshold: -0.8369\n",
      "Epoch 243 train loss: 0.9256, eval loss 0.9763424396514893\n",
      "optimal threshold: -0.8494\n",
      "Epoch 244 train loss: 0.9275, eval loss 0.9747049808502197\n",
      "optimal threshold: -0.8526\n",
      "Epoch 245 train loss: 0.9417, eval loss 0.9730668663978577\n",
      "optimal threshold: -0.8393\n",
      "Epoch 246 train loss: 0.9291, eval loss 0.9714406728744507\n",
      "optimal threshold: -0.8421\n",
      "Epoch 247 train loss: 0.9342, eval loss 0.9698154330253601\n",
      "optimal threshold: -0.8444\n",
      "Epoch 248 train loss: 0.9191, eval loss 0.9681973457336426\n",
      "optimal threshold: -0.8473\n",
      "Epoch 249 train loss: 0.9294, eval loss 0.9665830731391907\n",
      "optimal threshold: -0.8509\n",
      "Epoch 250 train loss: 0.9240, eval loss 0.9649693369865417\n",
      "optimal threshold: -0.8529\n",
      "Epoch 251 train loss: 0.9321, eval loss 0.9633597731590271\n",
      "optimal threshold: -0.8548\n",
      "Epoch 252 train loss: 0.9329, eval loss 0.9617607593536377\n",
      "optimal threshold: -0.8748\n",
      "Epoch 253 train loss: 0.9164, eval loss 0.9601672291755676\n",
      "optimal threshold: -0.8774\n",
      "Epoch 254 train loss: 0.9226, eval loss 0.9585791230201721\n",
      "optimal threshold: -0.8808\n",
      "Epoch 255 train loss: 0.9228, eval loss 0.9569886922836304\n",
      "optimal threshold: -0.8661\n",
      "Epoch 256 train loss: 0.9212, eval loss 0.9554059505462646\n",
      "optimal threshold: -0.8867\n",
      "Epoch 257 train loss: 0.8998, eval loss 0.9538280963897705\n",
      "optimal threshold: -0.8906\n",
      "Epoch 258 train loss: 0.9062, eval loss 0.9522515535354614\n",
      "optimal threshold: -0.8922\n",
      "Epoch 259 train loss: 0.9172, eval loss 0.9506850242614746\n",
      "optimal threshold: -0.8933\n",
      "Epoch 260 train loss: 0.9367, eval loss 0.9491267204284668\n",
      "optimal threshold: -0.8957\n",
      "Epoch 261 train loss: 0.9239, eval loss 0.9475734829902649\n",
      "optimal threshold: -0.8983\n",
      "Epoch 262 train loss: 0.9295, eval loss 0.9460231065750122\n",
      "optimal threshold: -0.9010\n",
      "Epoch 263 train loss: 0.9212, eval loss 0.9444795250892639\n",
      "optimal threshold: -0.9031\n",
      "Epoch 264 train loss: 0.8793, eval loss 0.9429371953010559\n",
      "optimal threshold: -0.9056\n",
      "Epoch 265 train loss: 0.8801, eval loss 0.9413983821868896\n",
      "optimal threshold: -0.9052\n",
      "Epoch 266 train loss: 0.8864, eval loss 0.9398613572120667\n",
      "optimal threshold: -0.9076\n",
      "Epoch 267 train loss: 0.9168, eval loss 0.9383320212364197\n",
      "optimal threshold: -0.9101\n",
      "Epoch 268 train loss: 0.8976, eval loss 0.936815083026886\n",
      "optimal threshold: -0.9065\n",
      "Epoch 269 train loss: 0.8947, eval loss 0.9352927207946777\n",
      "optimal threshold: -0.9077\n",
      "Epoch 270 train loss: 0.8947, eval loss 0.9337771534919739\n",
      "optimal threshold: -0.8988\n",
      "Epoch 271 train loss: 0.8673, eval loss 0.9322672486305237\n",
      "optimal threshold: -0.9004\n",
      "Epoch 272 train loss: 0.8761, eval loss 0.9307630062103271\n",
      "optimal threshold: -0.9024\n",
      "Epoch 273 train loss: 0.9090, eval loss 0.9292619228363037\n",
      "optimal threshold: -0.9047\n",
      "Epoch 274 train loss: 0.9046, eval loss 0.9277661442756653\n",
      "optimal threshold: -0.9067\n",
      "Epoch 275 train loss: 0.8779, eval loss 0.9262742400169373\n",
      "optimal threshold: -0.9072\n",
      "Epoch 276 train loss: 0.8929, eval loss 0.9247912168502808\n",
      "optimal threshold: -0.9085\n",
      "Epoch 277 train loss: 0.8841, eval loss 0.9233152866363525\n",
      "optimal threshold: -0.8962\n",
      "Epoch 278 train loss: 0.9040, eval loss 0.9218447208404541\n",
      "optimal threshold: -0.8960\n",
      "Epoch 279 train loss: 0.8929, eval loss 0.9203748106956482\n",
      "optimal threshold: -0.8982\n",
      "Epoch 280 train loss: 0.8713, eval loss 0.9189134240150452\n",
      "optimal threshold: -0.9091\n",
      "Epoch 281 train loss: 0.8640, eval loss 0.9174547791481018\n",
      "optimal threshold: -0.9096\n",
      "Epoch 282 train loss: 0.8915, eval loss 0.9159983992576599\n",
      "optimal threshold: -0.9101\n",
      "Epoch 283 train loss: 0.8624, eval loss 0.9145540595054626\n",
      "optimal threshold: -0.9114\n",
      "Epoch 284 train loss: 0.8839, eval loss 0.913118839263916\n",
      "optimal threshold: -0.9126\n",
      "Epoch 285 train loss: 0.8722, eval loss 0.9116864204406738\n",
      "optimal threshold: -0.9139\n",
      "Epoch 286 train loss: 0.8923, eval loss 0.9102551937103271\n",
      "optimal threshold: -0.9155\n",
      "Epoch 287 train loss: 0.8621, eval loss 0.908830463886261\n",
      "optimal threshold: -0.9174\n",
      "Epoch 288 train loss: 0.8891, eval loss 0.9074140191078186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.9162\n",
      "Epoch 289 train loss: 0.8637, eval loss 0.9060040712356567\n",
      "optimal threshold: -0.9229\n",
      "Epoch 290 train loss: 0.8564, eval loss 0.9045971632003784\n",
      "optimal threshold: -0.9238\n",
      "Epoch 291 train loss: 0.8524, eval loss 0.9031926393508911\n",
      "optimal threshold: -0.9248\n",
      "Epoch 292 train loss: 0.8393, eval loss 0.9018012881278992\n",
      "optimal threshold: -0.9276\n",
      "Epoch 293 train loss: 0.8544, eval loss 0.9004045128822327\n",
      "optimal threshold: -0.9282\n",
      "Epoch 294 train loss: 0.8530, eval loss 0.8990207314491272\n",
      "optimal threshold: -0.9300\n",
      "Epoch 295 train loss: 0.8444, eval loss 0.8976420164108276\n",
      "optimal threshold: -0.9303\n",
      "Epoch 296 train loss: 0.8514, eval loss 0.8962677717208862\n",
      "optimal threshold: -0.9318\n",
      "Epoch 297 train loss: 0.8515, eval loss 0.8949038982391357\n",
      "optimal threshold: -0.9372\n",
      "Epoch 298 train loss: 0.8585, eval loss 0.8935458660125732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:56:44,257] Trial 29 finished with value: 0.8397979140281677 and parameters: {'learning_rate_exp': -5.925603251355529, 'dropout_p': 0.24975698587382617, 'l2_reg_exp': -2.128857899777074, 'batch_size': 424, 'N': 195}. Best is trial 16 with value: 0.2552145719528198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.9307\n",
      "Epoch 299 train loss: 0.8398, eval loss 0.8921859860420227\n",
      "optimal threshold: -0.7805\n",
      "Epoch 0 train loss: 0.7857, eval loss 0.7575544118881226\n",
      "optimal threshold: -0.8902\n",
      "Epoch 1 train loss: 0.7273, eval loss 0.6904507279396057\n",
      "optimal threshold: -0.8633\n",
      "Epoch 2 train loss: 0.7276, eval loss 0.6737356185913086\n",
      "optimal threshold: -0.7474\n",
      "Epoch 3 train loss: 0.7017, eval loss 0.6666428446769714\n",
      "optimal threshold: -0.6440\n",
      "Epoch 4 train loss: 0.7199, eval loss 0.6627585887908936\n",
      "optimal threshold: -0.7860\n",
      "Epoch 5 train loss: 0.7203, eval loss 0.6608931422233582\n",
      "optimal threshold: -0.7534\n",
      "Epoch 6 train loss: 0.7220, eval loss 0.6595521569252014\n",
      "optimal threshold: -0.7736\n",
      "Epoch 7 train loss: 0.6945, eval loss 0.6588941216468811\n",
      "optimal threshold: -0.4670\n",
      "Epoch 8 train loss: 0.6858, eval loss 0.6581977605819702\n",
      "optimal threshold: -0.5075\n",
      "Epoch 9 train loss: 0.6657, eval loss 0.6579232811927795\n",
      "optimal threshold: -0.5332\n",
      "Epoch 10 train loss: 0.6605, eval loss 0.6581047177314758\n",
      "optimal threshold: -0.6997\n",
      "Epoch 11 train loss: 0.6799, eval loss 0.6576599478721619\n",
      "optimal threshold: -0.5959\n",
      "Epoch 12 train loss: 0.6732, eval loss 0.6576684713363647\n",
      "optimal threshold: -0.6774\n",
      "Epoch 13 train loss: 0.6583, eval loss 0.6574579477310181\n",
      "optimal threshold: -0.6879\n",
      "Epoch 14 train loss: 0.7057, eval loss 0.6578960418701172\n",
      "optimal threshold: -0.7142\n",
      "Epoch 15 train loss: 0.6497, eval loss 0.6579017043113708\n",
      "optimal threshold: -0.7023\n",
      "Epoch 16 train loss: 0.6699, eval loss 0.6579242944717407\n",
      "optimal threshold: -0.6881\n",
      "Epoch 17 train loss: 0.6666, eval loss 0.6580844521522522\n",
      "optimal threshold: -0.6883\n",
      "Epoch 18 train loss: 0.6517, eval loss 0.658298909664154\n",
      "optimal threshold: -0.5032\n",
      "Epoch 19 train loss: 0.6584, eval loss 0.6584660410881042\n",
      "optimal threshold: -0.4543\n",
      "Epoch 20 train loss: 0.6881, eval loss 0.6583064794540405\n",
      "optimal threshold: -0.5075\n",
      "Epoch 21 train loss: 0.6847, eval loss 0.6588462591171265\n",
      "optimal threshold: -0.4908\n",
      "Epoch 22 train loss: 0.6492, eval loss 0.659604012966156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 14:57:09,038] Trial 30 finished with value: 0.6653580069541931 and parameters: {'learning_rate_exp': -3.892634491757124, 'dropout_p': 0.18342300919418952, 'l2_reg_exp': -6.941052856234309, 'batch_size': 180, 'N': 429}. Best is trial 16 with value: 0.2552145719528198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4839\n",
      "optimal threshold: -0.2972\n",
      "Epoch 0 train loss: 1.3468, eval loss 1.2635984420776367\n",
      "optimal threshold: -0.6906\n",
      "Epoch 1 train loss: 1.2604, eval loss 1.0983010530471802\n",
      "optimal threshold: -0.9112\n",
      "Epoch 2 train loss: 1.1484, eval loss 0.9934476613998413\n",
      "optimal threshold: -0.9674\n",
      "Epoch 3 train loss: 1.0650, eval loss 0.9144423604011536\n",
      "optimal threshold: -0.9718\n",
      "Epoch 4 train loss: 0.8621, eval loss 0.8497177362442017\n",
      "optimal threshold: -1.0909\n",
      "Epoch 5 train loss: 0.7451, eval loss 0.8009341359138489\n",
      "optimal threshold: -1.1285\n",
      "Epoch 6 train loss: 0.7482, eval loss 0.7683132290840149\n",
      "optimal threshold: -1.0032\n",
      "Epoch 7 train loss: 0.5672, eval loss 0.748309850692749\n",
      "optimal threshold: -1.0376\n",
      "Epoch 8 train loss: 0.6213, eval loss 0.7352098226547241\n",
      "optimal threshold: -1.0778\n",
      "Epoch 9 train loss: 0.4235, eval loss 0.7267581224441528\n",
      "optimal threshold: -1.1058\n",
      "Epoch 10 train loss: 0.4431, eval loss 0.720530092716217\n",
      "optimal threshold: -1.0536\n",
      "Epoch 11 train loss: 0.4614, eval loss 0.7158379554748535\n",
      "optimal threshold: -1.0753\n",
      "Epoch 12 train loss: 0.4816, eval loss 0.7117730975151062\n",
      "optimal threshold: -1.0606\n",
      "Epoch 13 train loss: 0.4300, eval loss 0.7083120942115784\n",
      "optimal threshold: -1.1535\n",
      "Epoch 14 train loss: 0.4040, eval loss 0.7051273584365845\n",
      "optimal threshold: -1.1718\n",
      "Epoch 15 train loss: 0.3846, eval loss 0.7024401426315308\n",
      "optimal threshold: -1.1055\n",
      "Epoch 16 train loss: 0.4791, eval loss 0.6998267769813538\n",
      "optimal threshold: -1.0837\n",
      "Epoch 17 train loss: 0.3723, eval loss 0.6975390911102295\n",
      "optimal threshold: -1.1380\n",
      "Epoch 18 train loss: 0.5589, eval loss 0.6953450441360474\n",
      "optimal threshold: -1.0094\n",
      "Epoch 19 train loss: 0.3890, eval loss 0.6931911706924438\n",
      "optimal threshold: -1.0257\n",
      "Epoch 20 train loss: 0.3893, eval loss 0.6913901567459106\n",
      "optimal threshold: -0.6115\n",
      "Epoch 21 train loss: 0.3752, eval loss 0.6894375085830688\n",
      "optimal threshold: -0.6260\n",
      "Epoch 22 train loss: 0.3782, eval loss 0.6879510283470154\n",
      "optimal threshold: -0.6251\n",
      "Epoch 23 train loss: 0.5228, eval loss 0.686442494392395\n",
      "optimal threshold: -0.6329\n",
      "Epoch 24 train loss: 0.3288, eval loss 0.6850439310073853\n",
      "optimal threshold: -0.9257\n",
      "Epoch 25 train loss: 0.4488, eval loss 0.6837358474731445\n",
      "optimal threshold: -0.6525\n",
      "Epoch 26 train loss: 0.5218, eval loss 0.6822383403778076\n",
      "optimal threshold: -0.6812\n",
      "Epoch 27 train loss: 0.3737, eval loss 0.6811929941177368\n",
      "optimal threshold: -0.9457\n",
      "Epoch 28 train loss: 0.3868, eval loss 0.6801314353942871\n",
      "optimal threshold: -0.9382\n",
      "Epoch 29 train loss: 0.3897, eval loss 0.6790798902511597\n",
      "optimal threshold: -0.9554\n",
      "Epoch 30 train loss: 0.4557, eval loss 0.6782698631286621\n",
      "optimal threshold: -0.7181\n",
      "Epoch 31 train loss: 0.3701, eval loss 0.6773604154586792\n",
      "optimal threshold: -0.6233\n",
      "Epoch 32 train loss: 0.3699, eval loss 0.6764959692955017\n",
      "optimal threshold: -0.6284\n",
      "Epoch 33 train loss: 0.4889, eval loss 0.6758040189743042\n",
      "optimal threshold: -0.4816\n",
      "Epoch 34 train loss: 0.4826, eval loss 0.6749656796455383\n",
      "optimal threshold: -0.6058\n",
      "Epoch 35 train loss: 0.3908, eval loss 0.6743082404136658\n",
      "optimal threshold: -0.6083\n",
      "Epoch 36 train loss: 0.3807, eval loss 0.6737618446350098\n",
      "optimal threshold: -0.6018\n",
      "Epoch 37 train loss: 0.4414, eval loss 0.6732068657875061\n",
      "optimal threshold: -0.5966\n",
      "Epoch 38 train loss: 0.3555, eval loss 0.6726860404014587\n",
      "optimal threshold: -0.5288\n",
      "Epoch 39 train loss: 0.3510, eval loss 0.6721541881561279\n",
      "optimal threshold: -0.5557\n",
      "Epoch 40 train loss: 0.3603, eval loss 0.671696662902832\n",
      "optimal threshold: -0.5207\n",
      "Epoch 41 train loss: 0.3135, eval loss 0.6711261868476868\n",
      "optimal threshold: -0.5244\n",
      "Epoch 42 train loss: 0.4421, eval loss 0.6707931160926819\n",
      "optimal threshold: -0.5309\n",
      "Epoch 43 train loss: 0.3600, eval loss 0.67036372423172\n",
      "optimal threshold: -0.5225\n",
      "Epoch 44 train loss: 0.3267, eval loss 0.6699419021606445\n",
      "optimal threshold: -0.5331\n",
      "Epoch 45 train loss: 0.4390, eval loss 0.669606626033783\n",
      "optimal threshold: -0.4829\n",
      "Epoch 46 train loss: 0.3595, eval loss 0.6691686511039734\n",
      "optimal threshold: -0.4725\n",
      "Epoch 47 train loss: 0.3819, eval loss 0.6687906980514526\n",
      "optimal threshold: -0.4865\n",
      "Epoch 48 train loss: 0.3285, eval loss 0.6686157584190369\n",
      "optimal threshold: -0.4652\n",
      "Epoch 49 train loss: 0.3663, eval loss 0.66817307472229\n",
      "optimal threshold: -0.5730\n",
      "Epoch 50 train loss: 0.3714, eval loss 0.6680209636688232\n",
      "optimal threshold: -0.4615\n",
      "Epoch 51 train loss: 0.3818, eval loss 0.6676000356674194\n",
      "optimal threshold: -0.4576\n",
      "Epoch 52 train loss: 0.2755, eval loss 0.667374312877655\n",
      "optimal threshold: -0.5598\n",
      "Epoch 53 train loss: 0.3995, eval loss 0.6671155691146851\n",
      "optimal threshold: -0.6154\n",
      "Epoch 54 train loss: 0.4191, eval loss 0.6668795347213745\n",
      "optimal threshold: -0.3968\n",
      "Epoch 55 train loss: 0.3374, eval loss 0.6666704416275024\n",
      "optimal threshold: -0.6044\n",
      "Epoch 56 train loss: 0.2877, eval loss 0.6662499904632568\n",
      "optimal threshold: -0.4028\n",
      "Epoch 57 train loss: 0.3026, eval loss 0.6659960150718689\n",
      "optimal threshold: -0.4109\n",
      "Epoch 58 train loss: 0.3913, eval loss 0.6656346917152405\n",
      "optimal threshold: -0.3835\n",
      "Epoch 59 train loss: 0.3649, eval loss 0.6654896140098572\n",
      "optimal threshold: -0.5762\n",
      "Epoch 60 train loss: 0.4640, eval loss 0.6653938889503479\n",
      "optimal threshold: -0.5679\n",
      "Epoch 61 train loss: 0.3179, eval loss 0.6651554107666016\n",
      "optimal threshold: -0.5732\n",
      "Epoch 62 train loss: 0.3645, eval loss 0.6651082634925842\n",
      "optimal threshold: -0.5650\n",
      "Epoch 63 train loss: 0.3082, eval loss 0.6648242473602295\n",
      "optimal threshold: -0.5768\n",
      "Epoch 64 train loss: 0.2810, eval loss 0.6646697521209717\n",
      "optimal threshold: -0.5689\n",
      "Epoch 65 train loss: 0.3177, eval loss 0.6645026803016663\n",
      "optimal threshold: -0.5595\n",
      "Epoch 66 train loss: 0.4690, eval loss 0.6641697883605957\n",
      "optimal threshold: -0.5603\n",
      "Epoch 67 train loss: 0.4029, eval loss 0.66397625207901\n",
      "optimal threshold: -0.5644\n",
      "Epoch 68 train loss: 0.4179, eval loss 0.6639489531517029\n",
      "optimal threshold: -0.5616\n",
      "Epoch 69 train loss: 0.3325, eval loss 0.6637058258056641\n",
      "optimal threshold: -0.5812\n",
      "Epoch 70 train loss: 0.4039, eval loss 0.6637383699417114\n",
      "optimal threshold: -0.5717\n",
      "Epoch 71 train loss: 0.3969, eval loss 0.6634597182273865\n",
      "optimal threshold: -0.5750\n",
      "Epoch 72 train loss: 0.4172, eval loss 0.6633562445640564\n",
      "optimal threshold: -0.5754\n",
      "Epoch 73 train loss: 0.3168, eval loss 0.6631803512573242\n",
      "optimal threshold: -0.5727\n",
      "Epoch 74 train loss: 0.3007, eval loss 0.6630706191062927\n",
      "optimal threshold: -0.5701\n",
      "Epoch 75 train loss: 0.3227, eval loss 0.6629378199577332\n",
      "optimal threshold: -0.5761\n",
      "Epoch 76 train loss: 0.4060, eval loss 0.6629892587661743\n",
      "optimal threshold: -0.5771\n",
      "Epoch 77 train loss: 0.3688, eval loss 0.6627579927444458\n",
      "optimal threshold: -0.5857\n",
      "Epoch 78 train loss: 0.2783, eval loss 0.662753164768219\n",
      "optimal threshold: -0.5819\n",
      "Epoch 79 train loss: 0.2918, eval loss 0.6626702547073364\n",
      "optimal threshold: -0.5590\n",
      "Epoch 80 train loss: 0.3820, eval loss 0.6625745296478271\n",
      "optimal threshold: -0.5661\n",
      "Epoch 81 train loss: 0.3208, eval loss 0.6622727513313293\n",
      "optimal threshold: -0.5596\n",
      "Epoch 82 train loss: 0.3389, eval loss 0.6623035073280334\n",
      "optimal threshold: -0.5731\n",
      "Epoch 83 train loss: 0.3324, eval loss 0.6622701287269592\n",
      "optimal threshold: -0.5560\n",
      "Epoch 84 train loss: 0.3484, eval loss 0.6620026230812073\n",
      "optimal threshold: -0.5675\n",
      "Epoch 85 train loss: 0.4380, eval loss 0.6620211601257324\n",
      "optimal threshold: -0.5927\n",
      "Epoch 86 train loss: 0.3486, eval loss 0.6619556546211243\n",
      "optimal threshold: -0.5856\n",
      "Epoch 87 train loss: 0.3703, eval loss 0.6617916822433472\n",
      "optimal threshold: -0.5367\n",
      "Epoch 88 train loss: 0.4226, eval loss 0.6618751287460327\n",
      "optimal threshold: -0.5340\n",
      "Epoch 89 train loss: 0.4190, eval loss 0.6617562770843506\n",
      "optimal threshold: -0.5285\n",
      "Epoch 90 train loss: 0.3913, eval loss 0.6615620255470276\n",
      "optimal threshold: -0.5271\n",
      "Epoch 91 train loss: 0.4273, eval loss 0.6614218950271606\n",
      "optimal threshold: -0.5150\n",
      "Epoch 92 train loss: 0.3430, eval loss 0.6613041162490845\n",
      "optimal threshold: -0.5210\n",
      "Epoch 93 train loss: 0.3970, eval loss 0.6612004041671753\n",
      "optimal threshold: -0.5268\n",
      "Epoch 94 train loss: 0.4052, eval loss 0.6611803770065308\n",
      "optimal threshold: -0.5218\n",
      "Epoch 95 train loss: 0.3216, eval loss 0.6611738801002502\n",
      "optimal threshold: -0.5222\n",
      "Epoch 96 train loss: 0.3552, eval loss 0.6610974669456482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4972\n",
      "Epoch 97 train loss: 0.2953, eval loss 0.6609145998954773\n",
      "optimal threshold: -0.6699\n",
      "Epoch 98 train loss: 0.3166, eval loss 0.6607246398925781\n",
      "optimal threshold: -0.4893\n",
      "Epoch 99 train loss: 0.3034, eval loss 0.6605939269065857\n",
      "optimal threshold: -0.4893\n",
      "Epoch 100 train loss: 0.3806, eval loss 0.6606371998786926\n",
      "optimal threshold: -0.5071\n",
      "Epoch 101 train loss: 0.4109, eval loss 0.6604487299919128\n",
      "optimal threshold: -0.6314\n",
      "Epoch 102 train loss: 0.4561, eval loss 0.6604344248771667\n",
      "optimal threshold: -0.6278\n",
      "Epoch 103 train loss: 0.3068, eval loss 0.6603007912635803\n",
      "optimal threshold: -0.6229\n",
      "Epoch 104 train loss: 0.3376, eval loss 0.6602222323417664\n",
      "optimal threshold: -0.6313\n",
      "Epoch 105 train loss: 0.3892, eval loss 0.6602197885513306\n",
      "optimal threshold: -0.6785\n",
      "Epoch 106 train loss: 0.2929, eval loss 0.6601511836051941\n",
      "optimal threshold: -0.6776\n",
      "Epoch 107 train loss: 0.4581, eval loss 0.6599563360214233\n",
      "optimal threshold: -0.6816\n",
      "Epoch 108 train loss: 0.3704, eval loss 0.6600009799003601\n",
      "optimal threshold: -0.5151\n",
      "Epoch 109 train loss: 0.3601, eval loss 0.660017192363739\n",
      "optimal threshold: -0.5129\n",
      "Epoch 110 train loss: 0.2220, eval loss 0.6599414348602295\n",
      "optimal threshold: -0.5229\n",
      "Epoch 111 train loss: 0.2889, eval loss 0.6599628925323486\n",
      "optimal threshold: -0.5213\n",
      "Epoch 112 train loss: 0.3780, eval loss 0.659877359867096\n",
      "optimal threshold: -0.6345\n",
      "Epoch 113 train loss: 0.3200, eval loss 0.6596192121505737\n",
      "optimal threshold: -0.6367\n",
      "Epoch 114 train loss: 0.3371, eval loss 0.6596234440803528\n",
      "optimal threshold: -0.5317\n",
      "Epoch 115 train loss: 0.3203, eval loss 0.6596569418907166\n",
      "optimal threshold: -0.6451\n",
      "Epoch 116 train loss: 0.2522, eval loss 0.6594635844230652\n",
      "optimal threshold: -0.5218\n",
      "Epoch 117 train loss: 0.2448, eval loss 0.6595173478126526\n",
      "optimal threshold: -0.7569\n",
      "Epoch 118 train loss: 0.4041, eval loss 0.6594955921173096\n",
      "optimal threshold: -0.5410\n",
      "Epoch 119 train loss: 0.3272, eval loss 0.6595414280891418\n",
      "optimal threshold: -0.5339\n",
      "Epoch 120 train loss: 0.3920, eval loss 0.6594629287719727\n",
      "optimal threshold: -0.5250\n",
      "Epoch 121 train loss: 0.3387, eval loss 0.6593366861343384\n",
      "optimal threshold: -0.6399\n",
      "Epoch 122 train loss: 0.4078, eval loss 0.6592771410942078\n",
      "optimal threshold: -0.6559\n",
      "Epoch 123 train loss: 0.3429, eval loss 0.6593059301376343\n",
      "optimal threshold: -0.6417\n",
      "Epoch 124 train loss: 0.3208, eval loss 0.6592978835105896\n",
      "optimal threshold: -0.6664\n",
      "Epoch 125 train loss: 0.3970, eval loss 0.6593138575553894\n",
      "optimal threshold: -0.6672\n",
      "Epoch 126 train loss: 0.3959, eval loss 0.6592846512794495\n",
      "optimal threshold: -0.6640\n",
      "Epoch 127 train loss: 0.3460, eval loss 0.6591522097587585\n",
      "optimal threshold: -0.6743\n",
      "Epoch 128 train loss: 0.3331, eval loss 0.6592035889625549\n",
      "optimal threshold: -0.6757\n",
      "Epoch 129 train loss: 0.2963, eval loss 0.6591096520423889\n",
      "optimal threshold: -0.6669\n",
      "Epoch 130 train loss: 0.3702, eval loss 0.659008800983429\n",
      "optimal threshold: -0.6593\n",
      "Epoch 131 train loss: 0.3150, eval loss 0.6589915156364441\n",
      "optimal threshold: -0.6743\n",
      "Epoch 132 train loss: 0.3408, eval loss 0.6590276956558228\n",
      "optimal threshold: -0.6615\n",
      "Epoch 133 train loss: 0.2452, eval loss 0.6589502096176147\n",
      "optimal threshold: -0.6725\n",
      "Epoch 134 train loss: 0.3816, eval loss 0.6589629650115967\n",
      "optimal threshold: -0.6691\n",
      "Epoch 135 train loss: 0.3903, eval loss 0.65892094373703\n",
      "optimal threshold: -0.6767\n",
      "Epoch 136 train loss: 0.3102, eval loss 0.658994197845459\n",
      "optimal threshold: -0.6726\n",
      "Epoch 137 train loss: 0.3669, eval loss 0.6589921116828918\n",
      "optimal threshold: -0.6680\n",
      "Epoch 138 train loss: 0.3535, eval loss 0.6586776971817017\n",
      "optimal threshold: -0.6505\n",
      "Epoch 139 train loss: 0.2487, eval loss 0.6586912274360657\n",
      "optimal threshold: -0.6549\n",
      "Epoch 140 train loss: 0.2904, eval loss 0.6586164236068726\n",
      "optimal threshold: -0.6535\n",
      "Epoch 141 train loss: 0.3783, eval loss 0.6586542129516602\n",
      "optimal threshold: -0.6542\n",
      "Epoch 142 train loss: 0.3935, eval loss 0.6584692597389221\n",
      "optimal threshold: -0.6614\n",
      "Epoch 143 train loss: 0.3919, eval loss 0.6586470603942871\n",
      "optimal threshold: -0.6506\n",
      "Epoch 144 train loss: 0.3150, eval loss 0.6586052179336548\n",
      "optimal threshold: -0.6597\n",
      "Epoch 145 train loss: 0.3255, eval loss 0.6586374640464783\n",
      "optimal threshold: -0.6551\n",
      "Epoch 146 train loss: 0.5096, eval loss 0.6587031483650208\n",
      "optimal threshold: -0.6579\n",
      "Epoch 147 train loss: 0.3819, eval loss 0.6587730646133423\n",
      "optimal threshold: -0.6499\n",
      "Epoch 148 train loss: 0.3475, eval loss 0.6586395502090454\n",
      "optimal threshold: -0.6594\n",
      "Epoch 149 train loss: 0.3733, eval loss 0.6586019992828369\n",
      "optimal threshold: -0.6743\n",
      "Epoch 150 train loss: 0.3167, eval loss 0.6585668325424194\n",
      "optimal threshold: -0.6770\n",
      "Epoch 151 train loss: 0.3233, eval loss 0.6584461331367493\n",
      "optimal threshold: -0.6552\n",
      "Epoch 152 train loss: 0.2615, eval loss 0.6581680774688721\n",
      "optimal threshold: -0.6698\n",
      "Epoch 153 train loss: 0.3289, eval loss 0.6583240032196045\n",
      "optimal threshold: -0.6392\n",
      "Epoch 154 train loss: 0.3078, eval loss 0.6581730246543884\n",
      "optimal threshold: -0.6529\n",
      "Epoch 155 train loss: 0.3577, eval loss 0.6580280661582947\n",
      "optimal threshold: -0.6595\n",
      "Epoch 156 train loss: 0.3667, eval loss 0.6581787467002869\n",
      "optimal threshold: -0.6570\n",
      "Epoch 157 train loss: 0.3224, eval loss 0.6580844521522522\n",
      "optimal threshold: -0.6347\n",
      "Epoch 158 train loss: 0.3360, eval loss 0.6579018235206604\n",
      "optimal threshold: -0.7028\n",
      "Epoch 159 train loss: 0.3387, eval loss 0.6579549908638\n",
      "optimal threshold: -0.6962\n",
      "Epoch 160 train loss: 0.4564, eval loss 0.6579702496528625\n",
      "optimal threshold: -0.7150\n",
      "Epoch 161 train loss: 0.2968, eval loss 0.6581456065177917\n",
      "optimal threshold: -0.7056\n",
      "Epoch 162 train loss: 0.2704, eval loss 0.6579686999320984\n",
      "optimal threshold: -0.6578\n",
      "Epoch 163 train loss: 0.3375, eval loss 0.6579316258430481\n",
      "optimal threshold: -0.6549\n",
      "Epoch 164 train loss: 0.3337, eval loss 0.6578823328018188\n",
      "optimal threshold: -0.6631\n",
      "Epoch 165 train loss: 0.3741, eval loss 0.6581146121025085\n",
      "optimal threshold: -0.6582\n",
      "Epoch 166 train loss: 0.3266, eval loss 0.6579270958900452\n",
      "optimal threshold: -0.6577\n",
      "Epoch 167 train loss: 0.3099, eval loss 0.6577719449996948\n",
      "optimal threshold: -0.6625\n",
      "Epoch 168 train loss: 0.3118, eval loss 0.657783031463623\n",
      "optimal threshold: -0.6634\n",
      "Epoch 169 train loss: 0.3142, eval loss 0.657619297504425\n",
      "optimal threshold: -0.6501\n",
      "Epoch 170 train loss: 0.3245, eval loss 0.6574446558952332\n",
      "optimal threshold: -0.7004\n",
      "Epoch 171 train loss: 0.3122, eval loss 0.6575536727905273\n",
      "optimal threshold: -0.6896\n",
      "Epoch 172 train loss: 0.2774, eval loss 0.6575424671173096\n",
      "optimal threshold: -0.6963\n",
      "Epoch 173 train loss: 0.2792, eval loss 0.6576246619224548\n",
      "optimal threshold: -0.7044\n",
      "Epoch 174 train loss: 0.2860, eval loss 0.6578133702278137\n",
      "optimal threshold: -0.6963\n",
      "Epoch 175 train loss: 0.3426, eval loss 0.657710611820221\n",
      "optimal threshold: -0.6929\n",
      "Epoch 176 train loss: 0.3056, eval loss 0.6577329039573669\n",
      "optimal threshold: -0.7145\n",
      "Epoch 177 train loss: 0.2831, eval loss 0.6577155590057373\n",
      "optimal threshold: -0.7049\n",
      "Epoch 178 train loss: 0.2741, eval loss 0.657856822013855\n",
      "optimal threshold: -0.6979\n",
      "Epoch 179 train loss: 0.3298, eval loss 0.6579642295837402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 15:05:26,384] Trial 31 finished with value: 0.25941312313079834 and parameters: {'learning_rate_exp': -5.144239615593831, 'dropout_p': 0.54472287316771, 'l2_reg_exp': -3.1893879643469156, 'batch_size': 39, 'N': 470}. Best is trial 16 with value: 0.2552145719528198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7183\n",
      "optimal threshold: -0.1229\n",
      "Epoch 0 train loss: 1.3584, eval loss 1.3517721891403198\n",
      "optimal threshold: -0.3913\n",
      "Epoch 1 train loss: 1.2208, eval loss 1.1942638158798218\n",
      "optimal threshold: -0.7370\n",
      "Epoch 2 train loss: 1.0519, eval loss 1.0187710523605347\n",
      "optimal threshold: -0.9825\n",
      "Epoch 3 train loss: 0.9182, eval loss 0.8829734921455383\n",
      "optimal threshold: -0.8841\n",
      "Epoch 4 train loss: 0.8343, eval loss 0.7931370735168457\n",
      "optimal threshold: -0.7845\n",
      "Epoch 5 train loss: 0.7689, eval loss 0.7441151142120361\n",
      "optimal threshold: -0.7476\n",
      "Epoch 6 train loss: 0.7434, eval loss 0.7198242545127869\n",
      "optimal threshold: -0.7531\n",
      "Epoch 7 train loss: 0.7446, eval loss 0.707159698009491\n",
      "optimal threshold: -0.6807\n",
      "Epoch 8 train loss: 0.7459, eval loss 0.6988744735717773\n",
      "optimal threshold: -0.6668\n",
      "Epoch 9 train loss: 0.7324, eval loss 0.6926716566085815\n",
      "optimal threshold: -0.6462\n",
      "Epoch 10 train loss: 0.7152, eval loss 0.6876308917999268\n",
      "optimal threshold: -0.9130\n",
      "Epoch 11 train loss: 0.7188, eval loss 0.6835484504699707\n",
      "optimal threshold: -0.5598\n",
      "Epoch 12 train loss: 0.7086, eval loss 0.6801689267158508\n",
      "optimal threshold: -0.6436\n",
      "Epoch 13 train loss: 0.7052, eval loss 0.6772807240486145\n",
      "optimal threshold: -0.5760\n",
      "Epoch 14 train loss: 0.6893, eval loss 0.674828290939331\n",
      "optimal threshold: -0.6231\n",
      "Epoch 15 train loss: 0.6902, eval loss 0.6726977825164795\n",
      "optimal threshold: -0.6135\n",
      "Epoch 16 train loss: 0.6843, eval loss 0.670913815498352\n",
      "optimal threshold: -0.6495\n",
      "Epoch 17 train loss: 0.6824, eval loss 0.6693830490112305\n",
      "optimal threshold: -0.6530\n",
      "Epoch 18 train loss: 0.6865, eval loss 0.6679604053497314\n",
      "optimal threshold: -0.6269\n",
      "Epoch 19 train loss: 0.6845, eval loss 0.6667444705963135\n",
      "optimal threshold: -0.6337\n",
      "Epoch 20 train loss: 0.6949, eval loss 0.6656379103660583\n",
      "optimal threshold: -0.5939\n",
      "Epoch 21 train loss: 0.6965, eval loss 0.664752721786499\n",
      "optimal threshold: -0.6199\n",
      "Epoch 22 train loss: 0.6706, eval loss 0.6639175415039062\n",
      "optimal threshold: -0.6140\n",
      "Epoch 23 train loss: 0.6758, eval loss 0.6631032824516296\n",
      "optimal threshold: -0.6214\n",
      "Epoch 24 train loss: 0.6814, eval loss 0.6625574231147766\n",
      "optimal threshold: -0.6238\n",
      "Epoch 25 train loss: 0.6749, eval loss 0.6618642210960388\n",
      "optimal threshold: -0.6239\n",
      "Epoch 26 train loss: 0.6773, eval loss 0.6612958908081055\n",
      "optimal threshold: -0.5988\n",
      "Epoch 27 train loss: 0.6735, eval loss 0.6607387661933899\n",
      "optimal threshold: -0.7634\n",
      "Epoch 28 train loss: 0.6609, eval loss 0.6601529717445374\n",
      "optimal threshold: -0.7611\n",
      "Epoch 29 train loss: 0.6591, eval loss 0.6596361994743347\n",
      "optimal threshold: -0.7653\n",
      "Epoch 30 train loss: 0.6713, eval loss 0.6593568921089172\n",
      "optimal threshold: -0.6919\n",
      "Epoch 31 train loss: 0.6505, eval loss 0.6588760614395142\n",
      "optimal threshold: -0.7595\n",
      "Epoch 32 train loss: 0.6401, eval loss 0.6585853099822998\n",
      "optimal threshold: -0.7485\n",
      "Epoch 33 train loss: 0.6652, eval loss 0.6581858992576599\n",
      "optimal threshold: -0.4909\n",
      "Epoch 34 train loss: 0.6602, eval loss 0.6579597592353821\n",
      "optimal threshold: -0.4387\n",
      "Epoch 35 train loss: 0.6639, eval loss 0.6576718091964722\n",
      "optimal threshold: -0.4649\n",
      "Epoch 36 train loss: 0.6602, eval loss 0.657245934009552\n",
      "optimal threshold: -0.4697\n",
      "Epoch 37 train loss: 0.6548, eval loss 0.6569620370864868\n",
      "optimal threshold: -0.7331\n",
      "Epoch 38 train loss: 0.6456, eval loss 0.6567624807357788\n",
      "optimal threshold: -0.5323\n",
      "Epoch 39 train loss: 0.6531, eval loss 0.6565907597541809\n",
      "optimal threshold: -0.5284\n",
      "Epoch 40 train loss: 0.6541, eval loss 0.6563156247138977\n",
      "optimal threshold: -0.4998\n",
      "Epoch 41 train loss: 0.6204, eval loss 0.6560119986534119\n",
      "optimal threshold: -0.4841\n",
      "Epoch 42 train loss: 0.6294, eval loss 0.6558506488800049\n",
      "optimal threshold: -0.4783\n",
      "Epoch 43 train loss: 0.6426, eval loss 0.6556207537651062\n",
      "optimal threshold: -0.4761\n",
      "Epoch 44 train loss: 0.6291, eval loss 0.6556621193885803\n",
      "optimal threshold: -0.4435\n",
      "Epoch 45 train loss: 0.6297, eval loss 0.6555307507514954\n",
      "optimal threshold: -0.4385\n",
      "Epoch 46 train loss: 0.6504, eval loss 0.6554580926895142\n",
      "optimal threshold: -0.4180\n",
      "Epoch 47 train loss: 0.6345, eval loss 0.6553303599357605\n",
      "optimal threshold: -0.4455\n",
      "Epoch 48 train loss: 0.6441, eval loss 0.6551306247711182\n",
      "optimal threshold: -0.5471\n",
      "Epoch 49 train loss: 0.6519, eval loss 0.6550652980804443\n",
      "optimal threshold: -0.4580\n",
      "Epoch 50 train loss: 0.6390, eval loss 0.65506911277771\n",
      "optimal threshold: -0.5344\n",
      "Epoch 51 train loss: 0.6467, eval loss 0.6549369692802429\n",
      "optimal threshold: -0.5185\n",
      "Epoch 52 train loss: 0.6429, eval loss 0.6549795866012573\n",
      "optimal threshold: -0.4025\n",
      "Epoch 53 train loss: 0.6368, eval loss 0.6548489332199097\n",
      "optimal threshold: -0.4111\n",
      "Epoch 54 train loss: 0.6481, eval loss 0.6547992825508118\n",
      "optimal threshold: -0.3971\n",
      "Epoch 55 train loss: 0.6369, eval loss 0.6547824740409851\n",
      "optimal threshold: -0.3905\n",
      "Epoch 56 train loss: 0.6509, eval loss 0.6548928618431091\n",
      "optimal threshold: -0.3864\n",
      "Epoch 57 train loss: 0.6380, eval loss 0.6547633409500122\n",
      "optimal threshold: -0.4382\n",
      "Epoch 58 train loss: 0.6500, eval loss 0.6546991467475891\n",
      "optimal threshold: -0.3959\n",
      "Epoch 59 train loss: 0.6518, eval loss 0.6548029780387878\n",
      "optimal threshold: -0.4344\n",
      "Epoch 60 train loss: 0.6373, eval loss 0.6546511650085449\n",
      "optimal threshold: -0.5024\n",
      "Epoch 61 train loss: 0.6113, eval loss 0.6547471284866333\n",
      "optimal threshold: -0.4861\n",
      "Epoch 62 train loss: 0.6455, eval loss 0.654841423034668\n",
      "optimal threshold: -0.5047\n",
      "Epoch 63 train loss: 0.6400, eval loss 0.6548318266868591\n",
      "optimal threshold: -0.3802\n",
      "Epoch 64 train loss: 0.6464, eval loss 0.6546788811683655\n",
      "optimal threshold: -0.4898\n",
      "Epoch 65 train loss: 0.6399, eval loss 0.654759407043457\n",
      "optimal threshold: -0.3870\n",
      "Epoch 66 train loss: 0.6377, eval loss 0.6546788811683655\n",
      "optimal threshold: -0.3584\n",
      "Epoch 67 train loss: 0.6505, eval loss 0.6548709273338318\n",
      "optimal threshold: -0.3681\n",
      "Epoch 68 train loss: 0.6300, eval loss 0.6549519896507263\n",
      "optimal threshold: -0.3680\n",
      "Epoch 69 train loss: 0.6198, eval loss 0.6550580859184265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 15:06:00,284] Trial 32 finished with value: 0.639958918094635 and parameters: {'learning_rate_exp': -4.066717710532796, 'dropout_p': 0.19701638100404661, 'l2_reg_exp': -4.621434464540643, 'batch_size': 447, 'N': 196}. Best is trial 16 with value: 0.2552145719528198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.3726\n",
      "optimal threshold: -0.0728\n",
      "Epoch 0 train loss: 1.3839, eval loss 1.3860002756118774\n",
      "optimal threshold: -0.5070\n",
      "Epoch 1 train loss: 1.1251, eval loss 1.1176632642745972\n",
      "optimal threshold: -0.8865\n",
      "Epoch 2 train loss: 1.0033, eval loss 0.9179044365882874\n",
      "optimal threshold: -0.8869\n",
      "Epoch 3 train loss: 0.8975, eval loss 0.8216458559036255\n",
      "optimal threshold: -0.9114\n",
      "Epoch 4 train loss: 0.7637, eval loss 0.7692517638206482\n",
      "optimal threshold: -0.8130\n",
      "Epoch 5 train loss: 0.7431, eval loss 0.7395287752151489\n",
      "optimal threshold: -0.7202\n",
      "Epoch 6 train loss: 0.9219, eval loss 0.7236543893814087\n",
      "optimal threshold: -0.7829\n",
      "Epoch 7 train loss: 0.7450, eval loss 0.7129682898521423\n",
      "optimal threshold: -0.8549\n",
      "Epoch 8 train loss: 0.7974, eval loss 0.7060366272926331\n",
      "optimal threshold: -0.3929\n",
      "Epoch 9 train loss: 0.8308, eval loss 0.7005939483642578\n",
      "optimal threshold: -0.6863\n",
      "Epoch 10 train loss: 0.7332, eval loss 0.6960771083831787\n",
      "optimal threshold: -0.6943\n",
      "Epoch 11 train loss: 0.8099, eval loss 0.6925036311149597\n",
      "optimal threshold: -0.6684\n",
      "Epoch 12 train loss: 0.6756, eval loss 0.6893041729927063\n",
      "optimal threshold: -0.7126\n",
      "Epoch 13 train loss: 0.6732, eval loss 0.6867249608039856\n",
      "optimal threshold: -0.7219\n",
      "Epoch 14 train loss: 0.8546, eval loss 0.6840584874153137\n",
      "optimal threshold: -0.6236\n",
      "Epoch 15 train loss: 0.8087, eval loss 0.6825218796730042\n",
      "optimal threshold: -0.6038\n",
      "Epoch 16 train loss: 0.8278, eval loss 0.6802108883857727\n",
      "optimal threshold: -0.6473\n",
      "Epoch 17 train loss: 0.8605, eval loss 0.6798183917999268\n",
      "optimal threshold: -0.6023\n",
      "Epoch 18 train loss: 0.7875, eval loss 0.677558422088623\n",
      "optimal threshold: -0.6346\n",
      "Epoch 19 train loss: 0.8156, eval loss 0.6768852472305298\n",
      "optimal threshold: -0.5588\n",
      "Epoch 20 train loss: 0.7528, eval loss 0.6765416264533997\n",
      "optimal threshold: -0.5729\n",
      "Epoch 21 train loss: 0.7355, eval loss 0.6752960681915283\n",
      "optimal threshold: -0.6043\n",
      "Epoch 22 train loss: 0.8138, eval loss 0.6752874851226807\n",
      "optimal threshold: -0.5585\n",
      "Epoch 23 train loss: 0.7248, eval loss 0.6737121939659119\n",
      "optimal threshold: -0.5494\n",
      "Epoch 24 train loss: 0.7605, eval loss 0.6736112833023071\n",
      "optimal threshold: -0.3588\n",
      "Epoch 25 train loss: 0.7192, eval loss 0.6732163429260254\n",
      "optimal threshold: -0.4151\n",
      "Epoch 26 train loss: 0.7371, eval loss 0.6728435754776001\n",
      "optimal threshold: -0.4359\n",
      "Epoch 27 train loss: 0.7471, eval loss 0.6731998324394226\n",
      "optimal threshold: -0.3944\n",
      "Epoch 28 train loss: 0.7333, eval loss 0.6722443699836731\n",
      "optimal threshold: -0.3378\n",
      "Epoch 29 train loss: 0.7422, eval loss 0.6720762252807617\n",
      "optimal threshold: -0.3413\n",
      "Epoch 30 train loss: 0.7059, eval loss 0.6719404458999634\n",
      "optimal threshold: -0.3532\n",
      "Epoch 31 train loss: 0.7956, eval loss 0.6716377139091492\n",
      "optimal threshold: -0.4870\n",
      "Epoch 32 train loss: 0.7304, eval loss 0.6717000603675842\n",
      "optimal threshold: -0.4676\n",
      "Epoch 33 train loss: 0.8085, eval loss 0.6707773208618164\n",
      "optimal threshold: -0.4501\n",
      "Epoch 34 train loss: 0.8141, eval loss 0.6705000400543213\n",
      "optimal threshold: -0.3939\n",
      "Epoch 35 train loss: 0.6817, eval loss 0.6705466508865356\n",
      "optimal threshold: -0.4182\n",
      "Epoch 36 train loss: 0.6958, eval loss 0.6699209809303284\n",
      "optimal threshold: -0.3970\n",
      "Epoch 37 train loss: 0.8990, eval loss 0.6698566675186157\n",
      "optimal threshold: -0.4651\n",
      "Epoch 38 train loss: 0.7270, eval loss 0.6699764132499695\n",
      "optimal threshold: -0.4565\n",
      "Epoch 39 train loss: 0.7542, eval loss 0.6690552234649658\n",
      "optimal threshold: -0.4300\n",
      "Epoch 40 train loss: 0.6994, eval loss 0.6687197685241699\n",
      "optimal threshold: -0.4215\n",
      "Epoch 41 train loss: 0.7884, eval loss 0.6690168380737305\n",
      "optimal threshold: -0.4088\n",
      "Epoch 42 train loss: 0.6648, eval loss 0.669424295425415\n",
      "optimal threshold: -0.3907\n",
      "Epoch 43 train loss: 0.7018, eval loss 0.6687138080596924\n",
      "optimal threshold: -0.4988\n",
      "Epoch 44 train loss: 0.7865, eval loss 0.6686112880706787\n",
      "optimal threshold: -0.5606\n",
      "Epoch 45 train loss: 0.6919, eval loss 0.6680353879928589\n",
      "optimal threshold: -0.5287\n",
      "Epoch 46 train loss: 0.6971, eval loss 0.6680335998535156\n",
      "optimal threshold: -0.5305\n",
      "Epoch 47 train loss: 0.7202, eval loss 0.6683377027511597\n",
      "optimal threshold: -0.5678\n",
      "Epoch 48 train loss: 0.8144, eval loss 0.6689183115959167\n",
      "optimal threshold: -0.5492\n",
      "Epoch 49 train loss: 0.8033, eval loss 0.6686857342720032\n",
      "optimal threshold: -0.5288\n",
      "Epoch 50 train loss: 0.6896, eval loss 0.6686037182807922\n",
      "optimal threshold: -0.5087\n",
      "Epoch 51 train loss: 0.7041, eval loss 0.6688428521156311\n",
      "optimal threshold: -0.5124\n",
      "Epoch 52 train loss: 0.6461, eval loss 0.6687872409820557\n",
      "optimal threshold: -0.5039\n",
      "Epoch 53 train loss: 0.8193, eval loss 0.6690967679023743\n",
      "optimal threshold: -0.5065\n",
      "Epoch 54 train loss: 0.7376, eval loss 0.6689579486846924\n",
      "optimal threshold: -0.5217\n",
      "Epoch 55 train loss: 0.7823, eval loss 0.6688295602798462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 15:06:31,547] Trial 33 finished with value: 0.7755488753318787 and parameters: {'learning_rate_exp': -3.7506876510245823, 'dropout_p': 0.5660292483850252, 'l2_reg_exp': -4.701692196065747, 'batch_size': 156, 'N': 32}. Best is trial 16 with value: 0.2552145719528198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5148\n",
      "optimal threshold: -0.2144\n",
      "Epoch 0 train loss: 1.4056, eval loss 1.407853364944458\n",
      "optimal threshold: -0.2359\n",
      "Epoch 1 train loss: 1.3932, eval loss 1.3988951444625854\n",
      "optimal threshold: -0.2595\n",
      "Epoch 2 train loss: 1.3878, eval loss 1.3897215127944946\n",
      "optimal threshold: -0.2847\n",
      "Epoch 3 train loss: 1.3719, eval loss 1.3802627325057983\n",
      "optimal threshold: -0.3092\n",
      "Epoch 4 train loss: 1.3664, eval loss 1.3704054355621338\n",
      "optimal threshold: -0.3350\n",
      "Epoch 5 train loss: 1.3507, eval loss 1.3601527214050293\n",
      "optimal threshold: -0.3644\n",
      "Epoch 6 train loss: 1.3419, eval loss 1.349379062652588\n",
      "optimal threshold: -0.3958\n",
      "Epoch 7 train loss: 1.3388, eval loss 1.3380988836288452\n",
      "optimal threshold: -0.2106\n",
      "Epoch 8 train loss: 1.3248, eval loss 1.3263096809387207\n",
      "optimal threshold: -0.2313\n",
      "Epoch 9 train loss: 1.3084, eval loss 1.3139917850494385\n",
      "optimal threshold: -0.2538\n",
      "Epoch 10 train loss: 1.3003, eval loss 1.3011102676391602\n",
      "optimal threshold: -0.2640\n",
      "Epoch 11 train loss: 1.2793, eval loss 1.2876794338226318\n",
      "optimal threshold: -0.2983\n",
      "Epoch 12 train loss: 1.2679, eval loss 1.273802399635315\n",
      "optimal threshold: -0.3198\n",
      "Epoch 13 train loss: 1.2498, eval loss 1.2595735788345337\n",
      "optimal threshold: -0.3564\n",
      "Epoch 14 train loss: 1.2278, eval loss 1.245019555091858\n",
      "optimal threshold: -0.3846\n",
      "Epoch 15 train loss: 1.2168, eval loss 1.2302170991897583\n",
      "optimal threshold: -0.3977\n",
      "Epoch 16 train loss: 1.2120, eval loss 1.215293288230896\n",
      "optimal threshold: -0.4276\n",
      "Epoch 17 train loss: 1.2024, eval loss 1.200283169746399\n",
      "optimal threshold: -0.4553\n",
      "Epoch 18 train loss: 1.1880, eval loss 1.18523108959198\n",
      "optimal threshold: -0.4752\n",
      "Epoch 19 train loss: 1.1646, eval loss 1.1701886653900146\n",
      "optimal threshold: -0.5012\n",
      "Epoch 20 train loss: 1.1538, eval loss 1.1552196741104126\n",
      "optimal threshold: -0.5213\n",
      "Epoch 21 train loss: 1.1344, eval loss 1.1403735876083374\n",
      "optimal threshold: -0.5475\n",
      "Epoch 22 train loss: 1.1158, eval loss 1.1256849765777588\n",
      "optimal threshold: -0.5746\n",
      "Epoch 23 train loss: 1.1006, eval loss 1.111150860786438\n",
      "optimal threshold: -0.5831\n",
      "Epoch 24 train loss: 1.0956, eval loss 1.0967668294906616\n",
      "optimal threshold: -0.6056\n",
      "Epoch 25 train loss: 1.0694, eval loss 1.0826387405395508\n",
      "optimal threshold: -0.6399\n",
      "Epoch 26 train loss: 1.0699, eval loss 1.0687557458877563\n",
      "optimal threshold: -0.7110\n",
      "Epoch 27 train loss: 1.0558, eval loss 1.0551013946533203\n",
      "optimal threshold: -0.6888\n",
      "Epoch 28 train loss: 1.0329, eval loss 1.041751742362976\n",
      "optimal threshold: -0.7140\n",
      "Epoch 29 train loss: 1.0231, eval loss 1.028716802597046\n",
      "optimal threshold: -0.7329\n",
      "Epoch 30 train loss: 1.0185, eval loss 1.0159697532653809\n",
      "optimal threshold: -0.7587\n",
      "Epoch 31 train loss: 0.9959, eval loss 1.0035334825515747\n",
      "optimal threshold: -0.7813\n",
      "Epoch 32 train loss: 0.9753, eval loss 0.9914205074310303\n",
      "optimal threshold: -0.7910\n",
      "Epoch 33 train loss: 0.9752, eval loss 0.9795974493026733\n",
      "optimal threshold: -0.8503\n",
      "Epoch 34 train loss: 0.9656, eval loss 0.9681077599525452\n",
      "optimal threshold: -0.8633\n",
      "Epoch 35 train loss: 0.9550, eval loss 0.956900954246521\n",
      "optimal threshold: -0.8773\n",
      "Epoch 36 train loss: 0.9314, eval loss 0.9459656476974487\n",
      "optimal threshold: -0.8907\n",
      "Epoch 37 train loss: 0.9283, eval loss 0.935417890548706\n",
      "optimal threshold: -0.8994\n",
      "Epoch 38 train loss: 0.9250, eval loss 0.9251756072044373\n",
      "optimal threshold: -0.9128\n",
      "Epoch 39 train loss: 0.9086, eval loss 0.9152241349220276\n",
      "optimal threshold: -0.9110\n",
      "Epoch 40 train loss: 0.9142, eval loss 0.9056270718574524\n",
      "optimal threshold: -0.9409\n",
      "Epoch 41 train loss: 0.8982, eval loss 0.8963571190834045\n",
      "optimal threshold: -0.9480\n",
      "Epoch 42 train loss: 0.8821, eval loss 0.8873779773712158\n",
      "optimal threshold: -0.8917\n",
      "Epoch 43 train loss: 0.8706, eval loss 0.8787440061569214\n",
      "optimal threshold: -0.8912\n",
      "Epoch 44 train loss: 0.8703, eval loss 0.8704100847244263\n",
      "optimal threshold: -0.8969\n",
      "Epoch 45 train loss: 0.8476, eval loss 0.8623354434967041\n",
      "optimal threshold: -0.8981\n",
      "Epoch 46 train loss: 0.8553, eval loss 0.854594349861145\n",
      "optimal threshold: -0.9012\n",
      "Epoch 47 train loss: 0.8438, eval loss 0.8471314311027527\n",
      "optimal threshold: -0.9032\n",
      "Epoch 48 train loss: 0.8319, eval loss 0.8399709463119507\n",
      "optimal threshold: -0.8860\n",
      "Epoch 49 train loss: 0.8404, eval loss 0.8330649137496948\n",
      "optimal threshold: -0.8941\n",
      "Epoch 50 train loss: 0.8163, eval loss 0.8264610171318054\n",
      "optimal threshold: -0.8954\n",
      "Epoch 51 train loss: 0.8080, eval loss 0.8201220631599426\n",
      "optimal threshold: -0.8829\n",
      "Epoch 52 train loss: 0.8027, eval loss 0.8140377402305603\n",
      "optimal threshold: -0.8894\n",
      "Epoch 53 train loss: 0.7982, eval loss 0.8082297444343567\n",
      "optimal threshold: -0.8645\n",
      "Epoch 54 train loss: 0.7884, eval loss 0.8026427030563354\n",
      "optimal threshold: -0.8504\n",
      "Epoch 55 train loss: 0.7972, eval loss 0.7972773313522339\n",
      "optimal threshold: -0.8463\n",
      "Epoch 56 train loss: 0.7909, eval loss 0.7921485900878906\n",
      "optimal threshold: -0.8407\n",
      "Epoch 57 train loss: 0.7914, eval loss 0.787270188331604\n",
      "optimal threshold: -0.8389\n",
      "Epoch 58 train loss: 0.7903, eval loss 0.7825955748558044\n",
      "optimal threshold: -0.8397\n",
      "Epoch 59 train loss: 0.7854, eval loss 0.7781498432159424\n",
      "optimal threshold: -0.8227\n",
      "Epoch 60 train loss: 0.7691, eval loss 0.7739152908325195\n",
      "optimal threshold: -0.8253\n",
      "Epoch 61 train loss: 0.7638, eval loss 0.76988285779953\n",
      "optimal threshold: -0.7907\n",
      "Epoch 62 train loss: 0.7644, eval loss 0.7660462856292725\n",
      "optimal threshold: -0.7850\n",
      "Epoch 63 train loss: 0.7461, eval loss 0.7624441385269165\n",
      "optimal threshold: -0.7825\n",
      "Epoch 64 train loss: 0.7783, eval loss 0.7590177059173584\n",
      "optimal threshold: -0.7823\n",
      "Epoch 65 train loss: 0.7605, eval loss 0.755831241607666\n",
      "optimal threshold: -0.7733\n",
      "Epoch 66 train loss: 0.7435, eval loss 0.7527106404304504\n",
      "optimal threshold: -0.7966\n",
      "Epoch 67 train loss: 0.7658, eval loss 0.7497676014900208\n",
      "optimal threshold: -0.7765\n",
      "Epoch 68 train loss: 0.7450, eval loss 0.7470029592514038\n",
      "optimal threshold: -0.7874\n",
      "Epoch 69 train loss: 0.7497, eval loss 0.7443861365318298\n",
      "optimal threshold: -0.7540\n",
      "Epoch 70 train loss: 0.7473, eval loss 0.7418942451477051\n",
      "optimal threshold: -0.7872\n",
      "Epoch 71 train loss: 0.7275, eval loss 0.7395517230033875\n",
      "optimal threshold: -0.7798\n",
      "Epoch 72 train loss: 0.7301, eval loss 0.7373577356338501\n",
      "optimal threshold: -0.7729\n",
      "Epoch 73 train loss: 0.7214, eval loss 0.7352502346038818\n",
      "optimal threshold: -0.7704\n",
      "Epoch 74 train loss: 0.7279, eval loss 0.7332898378372192\n",
      "optimal threshold: -0.7563\n",
      "Epoch 75 train loss: 0.7547, eval loss 0.7313920259475708\n",
      "optimal threshold: -0.6720\n",
      "Epoch 76 train loss: 0.7338, eval loss 0.7295805811882019\n",
      "optimal threshold: -0.6665\n",
      "Epoch 77 train loss: 0.7365, eval loss 0.7278749942779541\n",
      "optimal threshold: -0.6693\n",
      "Epoch 78 train loss: 0.7367, eval loss 0.7262852191925049\n",
      "optimal threshold: -0.6618\n",
      "Epoch 79 train loss: 0.7312, eval loss 0.7247592210769653\n",
      "optimal threshold: -0.6656\n",
      "Epoch 80 train loss: 0.7102, eval loss 0.7232919335365295\n",
      "optimal threshold: -0.6557\n",
      "Epoch 81 train loss: 0.7376, eval loss 0.7219138145446777\n",
      "optimal threshold: -0.6758\n",
      "Epoch 82 train loss: 0.6911, eval loss 0.7205672264099121\n",
      "optimal threshold: -0.6741\n",
      "Epoch 83 train loss: 0.7161, eval loss 0.7193139791488647\n",
      "optimal threshold: -0.6429\n",
      "Epoch 84 train loss: 0.7216, eval loss 0.7181215286254883\n",
      "optimal threshold: -0.5913\n",
      "Epoch 85 train loss: 0.7091, eval loss 0.7169720530509949\n",
      "optimal threshold: -0.5796\n",
      "Epoch 86 train loss: 0.7164, eval loss 0.7158781886100769\n",
      "optimal threshold: -0.5843\n",
      "Epoch 87 train loss: 0.7408, eval loss 0.7148300409317017\n",
      "optimal threshold: -0.5753\n",
      "Epoch 88 train loss: 0.6960, eval loss 0.7138482928276062\n",
      "optimal threshold: -0.5714\n",
      "Epoch 89 train loss: 0.7209, eval loss 0.7128775119781494\n",
      "optimal threshold: -0.5855\n",
      "Epoch 90 train loss: 0.7095, eval loss 0.7119376063346863\n",
      "optimal threshold: -0.5803\n",
      "Epoch 91 train loss: 0.6940, eval loss 0.7110432386398315\n",
      "optimal threshold: -0.5815\n",
      "Epoch 92 train loss: 0.7200, eval loss 0.7101722955703735\n",
      "optimal threshold: -0.5786\n",
      "Epoch 93 train loss: 0.7348, eval loss 0.709324300289154\n",
      "optimal threshold: -0.6101\n",
      "Epoch 94 train loss: 0.6971, eval loss 0.7085241675376892\n",
      "optimal threshold: -0.5965\n",
      "Epoch 95 train loss: 0.6885, eval loss 0.7077292799949646\n",
      "optimal threshold: -0.5723\n",
      "Epoch 96 train loss: 0.6967, eval loss 0.706951379776001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6080\n",
      "Epoch 97 train loss: 0.7108, eval loss 0.7062191367149353\n",
      "optimal threshold: -1.0152\n",
      "Epoch 98 train loss: 0.7121, eval loss 0.705506443977356\n",
      "optimal threshold: -1.0119\n",
      "Epoch 99 train loss: 0.6992, eval loss 0.704811692237854\n",
      "optimal threshold: -0.9968\n",
      "Epoch 100 train loss: 0.6995, eval loss 0.7041547298431396\n",
      "optimal threshold: -0.5825\n",
      "Epoch 101 train loss: 0.7177, eval loss 0.7035228610038757\n",
      "optimal threshold: -0.5593\n",
      "Epoch 102 train loss: 0.6991, eval loss 0.702882707118988\n",
      "optimal threshold: -1.1541\n",
      "Epoch 103 train loss: 0.6883, eval loss 0.7022769451141357\n",
      "optimal threshold: -1.1524\n",
      "Epoch 104 train loss: 0.6967, eval loss 0.7016550302505493\n",
      "optimal threshold: -0.8749\n",
      "Epoch 105 train loss: 0.6965, eval loss 0.7010324001312256\n",
      "optimal threshold: -0.8732\n",
      "Epoch 106 train loss: 0.6939, eval loss 0.7004631757736206\n",
      "optimal threshold: -0.8705\n",
      "Epoch 107 train loss: 0.6878, eval loss 0.6998869776725769\n",
      "optimal threshold: -0.8669\n",
      "Epoch 108 train loss: 0.7049, eval loss 0.6993058919906616\n",
      "optimal threshold: -0.8669\n",
      "Epoch 109 train loss: 0.6935, eval loss 0.6987743377685547\n",
      "optimal threshold: -0.8808\n",
      "Epoch 110 train loss: 0.7045, eval loss 0.6982398629188538\n",
      "optimal threshold: -0.8736\n",
      "Epoch 111 train loss: 0.6877, eval loss 0.697744607925415\n",
      "optimal threshold: -0.8663\n",
      "Epoch 112 train loss: 0.6881, eval loss 0.6972274780273438\n",
      "optimal threshold: -0.8669\n",
      "Epoch 113 train loss: 0.6933, eval loss 0.6967229843139648\n",
      "optimal threshold: -0.8774\n",
      "Epoch 114 train loss: 0.6792, eval loss 0.6962184906005859\n",
      "optimal threshold: -0.8620\n",
      "Epoch 115 train loss: 0.6928, eval loss 0.6957253813743591\n",
      "optimal threshold: -0.8522\n",
      "Epoch 116 train loss: 0.6723, eval loss 0.6952378749847412\n",
      "optimal threshold: -0.8373\n",
      "Epoch 117 train loss: 0.7200, eval loss 0.6947359442710876\n",
      "optimal threshold: -0.8523\n",
      "Epoch 118 train loss: 0.6961, eval loss 0.6942601203918457\n",
      "optimal threshold: -0.8208\n",
      "Epoch 119 train loss: 0.6997, eval loss 0.6938449740409851\n",
      "optimal threshold: -0.8636\n",
      "Epoch 120 train loss: 0.6965, eval loss 0.6934037804603577\n",
      "optimal threshold: -0.8627\n",
      "Epoch 121 train loss: 0.7095, eval loss 0.6929551362991333\n",
      "optimal threshold: -0.8698\n",
      "Epoch 122 train loss: 0.6821, eval loss 0.6925297379493713\n",
      "optimal threshold: -0.8724\n",
      "Epoch 123 train loss: 0.6742, eval loss 0.6921183466911316\n",
      "optimal threshold: -0.8712\n",
      "Epoch 124 train loss: 0.7170, eval loss 0.6916715502738953\n",
      "optimal threshold: -0.8689\n",
      "Epoch 125 train loss: 0.6791, eval loss 0.691257119178772\n",
      "optimal threshold: -0.8658\n",
      "Epoch 126 train loss: 0.7138, eval loss 0.6908621788024902\n",
      "optimal threshold: -0.8664\n",
      "Epoch 127 train loss: 0.6771, eval loss 0.690420389175415\n",
      "optimal threshold: -0.8645\n",
      "Epoch 128 train loss: 0.6850, eval loss 0.6900023221969604\n",
      "optimal threshold: -0.8674\n",
      "Epoch 129 train loss: 0.6668, eval loss 0.6895687580108643\n",
      "optimal threshold: -0.8684\n",
      "Epoch 130 train loss: 0.6651, eval loss 0.6892088651657104\n",
      "optimal threshold: -0.8670\n",
      "Epoch 131 train loss: 0.6790, eval loss 0.6888545155525208\n",
      "optimal threshold: -0.8889\n",
      "Epoch 132 train loss: 0.7027, eval loss 0.6884828209877014\n",
      "optimal threshold: -0.8858\n",
      "Epoch 133 train loss: 0.7056, eval loss 0.6880935430526733\n",
      "optimal threshold: -0.8887\n",
      "Epoch 134 train loss: 0.6856, eval loss 0.6877300143241882\n",
      "optimal threshold: -0.8876\n",
      "Epoch 135 train loss: 0.6876, eval loss 0.6873598694801331\n",
      "optimal threshold: -0.8869\n",
      "Epoch 136 train loss: 0.6952, eval loss 0.6869454383850098\n",
      "optimal threshold: -0.8362\n",
      "Epoch 137 train loss: 0.6733, eval loss 0.6866034865379333\n",
      "optimal threshold: -0.8910\n",
      "Epoch 138 train loss: 0.6910, eval loss 0.6862226128578186\n",
      "optimal threshold: -0.8765\n",
      "Epoch 139 train loss: 0.6775, eval loss 0.6858500242233276\n",
      "optimal threshold: -0.8392\n",
      "Epoch 140 train loss: 0.6815, eval loss 0.685493528842926\n",
      "optimal threshold: -0.9091\n",
      "Epoch 141 train loss: 0.6957, eval loss 0.6852016448974609\n",
      "optimal threshold: -0.9114\n",
      "Epoch 142 train loss: 0.6938, eval loss 0.684934675693512\n",
      "optimal threshold: -0.9108\n",
      "Epoch 143 train loss: 0.6650, eval loss 0.6846120357513428\n",
      "optimal threshold: -0.9091\n",
      "Epoch 144 train loss: 0.6634, eval loss 0.6842887997627258\n",
      "optimal threshold: -0.9088\n",
      "Epoch 145 train loss: 0.6809, eval loss 0.6839495301246643\n",
      "optimal threshold: -0.8693\n",
      "Epoch 146 train loss: 0.6714, eval loss 0.6836791634559631\n",
      "optimal threshold: -0.8743\n",
      "Epoch 147 train loss: 0.6653, eval loss 0.6833432912826538\n",
      "optimal threshold: -0.8693\n",
      "Epoch 148 train loss: 0.6683, eval loss 0.6830466389656067\n",
      "optimal threshold: -0.8711\n",
      "Epoch 149 train loss: 0.6594, eval loss 0.6827785968780518\n",
      "optimal threshold: -0.8711\n",
      "Epoch 150 train loss: 0.6600, eval loss 0.6824886202812195\n",
      "optimal threshold: -0.8753\n",
      "Epoch 151 train loss: 0.6626, eval loss 0.6821865439414978\n",
      "optimal threshold: -0.7716\n",
      "Epoch 152 train loss: 0.6717, eval loss 0.6818861961364746\n",
      "optimal threshold: -0.8859\n",
      "Epoch 153 train loss: 0.6720, eval loss 0.6815626621246338\n",
      "optimal threshold: -0.8884\n",
      "Epoch 154 train loss: 0.6227, eval loss 0.681274950504303\n",
      "optimal threshold: -0.8879\n",
      "Epoch 155 train loss: 0.6515, eval loss 0.6809723973274231\n",
      "optimal threshold: -0.8866\n",
      "Epoch 156 train loss: 0.6740, eval loss 0.6807162761688232\n",
      "optimal threshold: -0.8859\n",
      "Epoch 157 train loss: 0.6665, eval loss 0.6804383397102356\n",
      "optimal threshold: -0.8810\n",
      "Epoch 158 train loss: 0.6895, eval loss 0.6801373362541199\n",
      "optimal threshold: -0.7088\n",
      "Epoch 159 train loss: 0.6571, eval loss 0.679905116558075\n",
      "optimal threshold: -0.7237\n",
      "Epoch 160 train loss: 0.6959, eval loss 0.6796059608459473\n",
      "optimal threshold: -0.7186\n",
      "Epoch 161 train loss: 0.6685, eval loss 0.679320216178894\n",
      "optimal threshold: -0.7240\n",
      "Epoch 162 train loss: 0.6815, eval loss 0.6790521740913391\n",
      "optimal threshold: -0.7210\n",
      "Epoch 163 train loss: 0.6845, eval loss 0.6787775754928589\n",
      "optimal threshold: -0.7182\n",
      "Epoch 164 train loss: 0.6262, eval loss 0.6785581111907959\n",
      "optimal threshold: -0.7236\n",
      "Epoch 165 train loss: 0.6781, eval loss 0.6783310770988464\n",
      "optimal threshold: -0.7433\n",
      "Epoch 166 train loss: 0.6744, eval loss 0.6781170964241028\n",
      "optimal threshold: -0.7303\n",
      "Epoch 167 train loss: 0.6531, eval loss 0.6778832674026489\n",
      "optimal threshold: -0.7321\n",
      "Epoch 168 train loss: 0.6505, eval loss 0.6776298880577087\n",
      "optimal threshold: -0.7118\n",
      "Epoch 169 train loss: 0.6520, eval loss 0.6773877143859863\n",
      "optimal threshold: -0.7108\n",
      "Epoch 170 train loss: 0.6587, eval loss 0.6771501898765564\n",
      "optimal threshold: -0.8726\n",
      "Epoch 171 train loss: 0.6698, eval loss 0.6769185662269592\n",
      "optimal threshold: -0.7435\n",
      "Epoch 172 train loss: 0.6396, eval loss 0.676704466342926\n",
      "optimal threshold: -0.7498\n",
      "Epoch 173 train loss: 0.6743, eval loss 0.6764985918998718\n",
      "optimal threshold: -0.7541\n",
      "Epoch 174 train loss: 0.6862, eval loss 0.6762901544570923\n",
      "optimal threshold: -0.7434\n",
      "Epoch 175 train loss: 0.6431, eval loss 0.676078736782074\n",
      "optimal threshold: -0.6962\n",
      "Epoch 176 train loss: 0.6547, eval loss 0.6758255362510681\n",
      "optimal threshold: -0.7101\n",
      "Epoch 177 train loss: 0.6543, eval loss 0.6756099462509155\n",
      "optimal threshold: -0.7079\n",
      "Epoch 178 train loss: 0.6421, eval loss 0.6753972172737122\n",
      "optimal threshold: -0.8737\n",
      "Epoch 179 train loss: 0.6654, eval loss 0.6752138137817383\n",
      "optimal threshold: -0.5208\n",
      "Epoch 180 train loss: 0.6399, eval loss 0.6749948263168335\n",
      "optimal threshold: -0.5223\n",
      "Epoch 181 train loss: 0.6234, eval loss 0.6748239994049072\n",
      "optimal threshold: -0.5224\n",
      "Epoch 182 train loss: 0.6512, eval loss 0.6746204495429993\n",
      "optimal threshold: -0.5242\n",
      "Epoch 183 train loss: 0.6809, eval loss 0.6744473576545715\n",
      "optimal threshold: -0.5372\n",
      "Epoch 184 train loss: 0.6299, eval loss 0.6742585897445679\n",
      "optimal threshold: -0.8382\n",
      "Epoch 185 train loss: 0.6717, eval loss 0.6740604043006897\n",
      "optimal threshold: -0.8383\n",
      "Epoch 186 train loss: 0.6577, eval loss 0.6738449931144714\n",
      "optimal threshold: -0.7407\n",
      "Epoch 187 train loss: 0.6398, eval loss 0.6736671328544617\n",
      "optimal threshold: -0.5911\n",
      "Epoch 188 train loss: 0.6279, eval loss 0.6735461950302124\n",
      "optimal threshold: -0.7415\n",
      "Epoch 189 train loss: 0.6655, eval loss 0.6733599901199341\n",
      "optimal threshold: -0.5877\n",
      "Epoch 190 train loss: 0.6504, eval loss 0.6731669902801514\n",
      "optimal threshold: -0.6134\n",
      "Epoch 191 train loss: 0.6508, eval loss 0.6729728579521179\n",
      "optimal threshold: -0.5358\n",
      "Epoch 192 train loss: 0.6595, eval loss 0.6727985143661499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.5330\n",
      "Epoch 193 train loss: 0.6586, eval loss 0.6726360321044922\n",
      "optimal threshold: -0.6212\n",
      "Epoch 194 train loss: 0.6474, eval loss 0.6724994778633118\n",
      "optimal threshold: -0.6214\n",
      "Epoch 195 train loss: 0.6643, eval loss 0.6723155975341797\n",
      "optimal threshold: -0.6230\n",
      "Epoch 196 train loss: 0.6837, eval loss 0.6721499562263489\n",
      "optimal threshold: -0.6212\n",
      "Epoch 197 train loss: 0.6406, eval loss 0.6719724535942078\n",
      "optimal threshold: -0.6239\n",
      "Epoch 198 train loss: 0.6311, eval loss 0.6718196868896484\n",
      "optimal threshold: -0.6339\n",
      "Epoch 199 train loss: 0.6359, eval loss 0.6716614365577698\n",
      "optimal threshold: -0.6327\n",
      "Epoch 200 train loss: 0.6595, eval loss 0.6715054512023926\n",
      "optimal threshold: -0.6306\n",
      "Epoch 201 train loss: 0.6353, eval loss 0.6713276505470276\n",
      "optimal threshold: -0.6316\n",
      "Epoch 202 train loss: 0.6216, eval loss 0.6712200045585632\n",
      "optimal threshold: -0.6315\n",
      "Epoch 203 train loss: 0.6373, eval loss 0.6710546612739563\n",
      "optimal threshold: -0.5103\n",
      "Epoch 204 train loss: 0.6807, eval loss 0.6708968877792358\n",
      "optimal threshold: -0.6278\n",
      "Epoch 205 train loss: 0.6300, eval loss 0.6706761121749878\n",
      "optimal threshold: -0.5038\n",
      "Epoch 206 train loss: 0.6345, eval loss 0.6704928278923035\n",
      "optimal threshold: -0.5066\n",
      "Epoch 207 train loss: 0.6331, eval loss 0.6703653335571289\n",
      "optimal threshold: -0.5005\n",
      "Epoch 208 train loss: 0.6785, eval loss 0.6702499389648438\n",
      "optimal threshold: -0.5031\n",
      "Epoch 209 train loss: 0.6405, eval loss 0.670161247253418\n",
      "optimal threshold: -0.5057\n",
      "Epoch 210 train loss: 0.6374, eval loss 0.67007976770401\n",
      "optimal threshold: -0.5056\n",
      "Epoch 211 train loss: 0.6422, eval loss 0.669959306716919\n",
      "optimal threshold: -0.5791\n",
      "Epoch 212 train loss: 0.6701, eval loss 0.6698075532913208\n",
      "optimal threshold: -0.5931\n",
      "Epoch 213 train loss: 0.6645, eval loss 0.6696882247924805\n",
      "optimal threshold: -0.5891\n",
      "Epoch 214 train loss: 0.6267, eval loss 0.6695775985717773\n",
      "optimal threshold: -0.5902\n",
      "Epoch 215 train loss: 0.6350, eval loss 0.6694548726081848\n",
      "optimal threshold: -0.5864\n",
      "Epoch 216 train loss: 0.6628, eval loss 0.6692991256713867\n",
      "optimal threshold: -0.5819\n",
      "Epoch 217 train loss: 0.6613, eval loss 0.6692025065422058\n",
      "optimal threshold: -0.5889\n",
      "Epoch 218 train loss: 0.6405, eval loss 0.6689913272857666\n",
      "optimal threshold: -0.5923\n",
      "Epoch 219 train loss: 0.6173, eval loss 0.6688987612724304\n",
      "optimal threshold: -0.5917\n",
      "Epoch 220 train loss: 0.6481, eval loss 0.6687504053115845\n",
      "optimal threshold: -0.5927\n",
      "Epoch 221 train loss: 0.6272, eval loss 0.6686500906944275\n",
      "optimal threshold: -0.5907\n",
      "Epoch 222 train loss: 0.6202, eval loss 0.6685148477554321\n",
      "optimal threshold: -0.5910\n",
      "Epoch 223 train loss: 0.6456, eval loss 0.6684039235115051\n",
      "optimal threshold: -0.5904\n",
      "Epoch 224 train loss: 0.6604, eval loss 0.6683000326156616\n",
      "optimal threshold: -0.5918\n",
      "Epoch 225 train loss: 0.6394, eval loss 0.6681681871414185\n",
      "optimal threshold: -0.5979\n",
      "Epoch 226 train loss: 0.6554, eval loss 0.668129563331604\n",
      "optimal threshold: -0.5723\n",
      "Epoch 227 train loss: 0.6356, eval loss 0.6680294871330261\n",
      "optimal threshold: -0.5687\n",
      "Epoch 228 train loss: 0.6262, eval loss 0.6679143309593201\n",
      "optimal threshold: -0.5682\n",
      "Epoch 229 train loss: 0.6483, eval loss 0.667812168598175\n",
      "optimal threshold: -0.5665\n",
      "Epoch 230 train loss: 0.6438, eval loss 0.6676594018936157\n",
      "optimal threshold: -0.5670\n",
      "Epoch 231 train loss: 0.6243, eval loss 0.6675419211387634\n",
      "optimal threshold: -0.5669\n",
      "Epoch 232 train loss: 0.6494, eval loss 0.6674126982688904\n",
      "optimal threshold: -0.5619\n",
      "Epoch 233 train loss: 0.6015, eval loss 0.6672840118408203\n",
      "optimal threshold: -0.6094\n",
      "Epoch 234 train loss: 0.6522, eval loss 0.6672261357307434\n",
      "optimal threshold: -0.6093\n",
      "Epoch 235 train loss: 0.6142, eval loss 0.6671428084373474\n",
      "optimal threshold: -0.6081\n",
      "Epoch 236 train loss: 0.6429, eval loss 0.6670105457305908\n",
      "optimal threshold: -0.6082\n",
      "Epoch 237 train loss: 0.6093, eval loss 0.6669466495513916\n",
      "optimal threshold: -0.8971\n",
      "Epoch 238 train loss: 0.6222, eval loss 0.6668276786804199\n",
      "optimal threshold: -0.6133\n",
      "Epoch 239 train loss: 0.6333, eval loss 0.6667299866676331\n",
      "optimal threshold: -0.6100\n",
      "Epoch 240 train loss: 0.6216, eval loss 0.6666478514671326\n",
      "optimal threshold: -0.6066\n",
      "Epoch 241 train loss: 0.6563, eval loss 0.666549026966095\n",
      "optimal threshold: -0.6062\n",
      "Epoch 242 train loss: 0.6653, eval loss 0.6664632558822632\n",
      "optimal threshold: -0.6128\n",
      "Epoch 243 train loss: 0.6271, eval loss 0.6664095520973206\n",
      "optimal threshold: -0.6068\n",
      "Epoch 244 train loss: 0.6651, eval loss 0.6662729978561401\n",
      "optimal threshold: -0.6070\n",
      "Epoch 245 train loss: 0.6481, eval loss 0.6661772727966309\n",
      "optimal threshold: -0.6090\n",
      "Epoch 246 train loss: 0.6504, eval loss 0.6661239862442017\n",
      "optimal threshold: -0.6062\n",
      "Epoch 247 train loss: 0.6540, eval loss 0.6659759283065796\n",
      "optimal threshold: -0.6036\n",
      "Epoch 248 train loss: 0.6454, eval loss 0.665850818157196\n",
      "optimal threshold: -0.6029\n",
      "Epoch 249 train loss: 0.6422, eval loss 0.6657565236091614\n",
      "optimal threshold: -0.6050\n",
      "Epoch 250 train loss: 0.6301, eval loss 0.6656832695007324\n",
      "optimal threshold: -0.6061\n",
      "Epoch 251 train loss: 0.6247, eval loss 0.6656624674797058\n",
      "optimal threshold: -0.8781\n",
      "Epoch 252 train loss: 0.6647, eval loss 0.6656079292297363\n",
      "optimal threshold: -0.8789\n",
      "Epoch 253 train loss: 0.6366, eval loss 0.6655175089836121\n",
      "optimal threshold: -0.8805\n",
      "Epoch 254 train loss: 0.6182, eval loss 0.6654235124588013\n",
      "optimal threshold: -0.8805\n",
      "Epoch 255 train loss: 0.6414, eval loss 0.6653333306312561\n",
      "optimal threshold: -0.8792\n",
      "Epoch 256 train loss: 0.6266, eval loss 0.6652346849441528\n",
      "optimal threshold: -0.8795\n",
      "Epoch 257 train loss: 0.6255, eval loss 0.6651641130447388\n",
      "optimal threshold: -0.6941\n",
      "Epoch 258 train loss: 0.6261, eval loss 0.665080726146698\n",
      "optimal threshold: -0.6946\n",
      "Epoch 259 train loss: 0.6034, eval loss 0.6650251746177673\n",
      "optimal threshold: -0.6905\n",
      "Epoch 260 train loss: 0.6309, eval loss 0.6648902297019958\n",
      "optimal threshold: -0.6904\n",
      "Epoch 261 train loss: 0.6052, eval loss 0.6648187637329102\n",
      "optimal threshold: -0.6901\n",
      "Epoch 262 train loss: 0.6324, eval loss 0.6647554636001587\n",
      "optimal threshold: -0.7024\n",
      "Epoch 263 train loss: 0.6088, eval loss 0.6646695137023926\n",
      "optimal threshold: -0.7034\n",
      "Epoch 264 train loss: 0.6448, eval loss 0.6646287441253662\n",
      "optimal threshold: -0.7130\n",
      "Epoch 265 train loss: 0.6719, eval loss 0.6645920276641846\n",
      "optimal threshold: -0.7043\n",
      "Epoch 266 train loss: 0.6389, eval loss 0.6645031571388245\n",
      "optimal threshold: -0.7041\n",
      "Epoch 267 train loss: 0.6039, eval loss 0.6644337773323059\n",
      "optimal threshold: -0.7030\n",
      "Epoch 268 train loss: 0.6255, eval loss 0.6643431186676025\n",
      "optimal threshold: -0.7023\n",
      "Epoch 269 train loss: 0.6507, eval loss 0.664232075214386\n",
      "optimal threshold: -0.7027\n",
      "Epoch 270 train loss: 0.6100, eval loss 0.6641868352890015\n",
      "optimal threshold: -0.7038\n",
      "Epoch 271 train loss: 0.6555, eval loss 0.6641270518302917\n",
      "optimal threshold: -0.7035\n",
      "Epoch 272 train loss: 0.5941, eval loss 0.6640341281890869\n",
      "optimal threshold: -0.7012\n",
      "Epoch 273 train loss: 0.6549, eval loss 0.6639261245727539\n",
      "optimal threshold: -0.7013\n",
      "Epoch 274 train loss: 0.6332, eval loss 0.6638712882995605\n",
      "optimal threshold: -0.7166\n",
      "Epoch 275 train loss: 0.6363, eval loss 0.6638295650482178\n",
      "optimal threshold: -0.7035\n",
      "Epoch 276 train loss: 0.6407, eval loss 0.6637466549873352\n",
      "optimal threshold: -0.7046\n",
      "Epoch 277 train loss: 0.6529, eval loss 0.663689374923706\n",
      "optimal threshold: -0.6998\n",
      "Epoch 278 train loss: 0.6066, eval loss 0.6635780930519104\n",
      "optimal threshold: -0.7029\n",
      "Epoch 279 train loss: 0.5997, eval loss 0.6635585427284241\n",
      "optimal threshold: -0.7048\n",
      "Epoch 280 train loss: 0.6446, eval loss 0.6635162830352783\n",
      "optimal threshold: -0.7080\n",
      "Epoch 281 train loss: 0.6138, eval loss 0.6634534001350403\n",
      "optimal threshold: -0.7071\n",
      "Epoch 282 train loss: 0.6176, eval loss 0.6633787751197815\n",
      "optimal threshold: -0.7053\n",
      "Epoch 283 train loss: 0.6153, eval loss 0.6632907390594482\n",
      "optimal threshold: -0.7071\n",
      "Epoch 284 train loss: 0.6163, eval loss 0.6632500290870667\n",
      "optimal threshold: -0.7020\n",
      "Epoch 285 train loss: 0.6080, eval loss 0.663206160068512\n",
      "optimal threshold: -0.7029\n",
      "Epoch 286 train loss: 0.6415, eval loss 0.6631791591644287\n",
      "optimal threshold: -0.7016\n",
      "Epoch 287 train loss: 0.6099, eval loss 0.6630908846855164\n",
      "optimal threshold: -0.7015\n",
      "Epoch 288 train loss: 0.6178, eval loss 0.6630185842514038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7263\n",
      "Epoch 289 train loss: 0.6494, eval loss 0.6629599332809448\n",
      "optimal threshold: -0.7319\n",
      "Epoch 290 train loss: 0.6518, eval loss 0.6628916263580322\n",
      "optimal threshold: -0.7340\n",
      "Epoch 291 train loss: 0.6229, eval loss 0.662876546382904\n",
      "optimal threshold: -0.7161\n",
      "Epoch 292 train loss: 0.6413, eval loss 0.6627469062805176\n",
      "optimal threshold: -0.7178\n",
      "Epoch 293 train loss: 0.6339, eval loss 0.6627122759819031\n",
      "optimal threshold: -0.7188\n",
      "Epoch 294 train loss: 0.6278, eval loss 0.6626796722412109\n",
      "optimal threshold: -0.7180\n",
      "Epoch 295 train loss: 0.6334, eval loss 0.6626153588294983\n",
      "optimal threshold: -0.7196\n",
      "Epoch 296 train loss: 0.6214, eval loss 0.6625964045524597\n",
      "optimal threshold: -0.7283\n",
      "Epoch 297 train loss: 0.6049, eval loss 0.6625489592552185\n",
      "optimal threshold: -0.7249\n",
      "Epoch 298 train loss: 0.6267, eval loss 0.6625251173973083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 15:09:00,912] Trial 34 finished with value: 0.630776584148407 and parameters: {'learning_rate_exp': -5.0757004155555965, 'dropout_p': 0.2626281724546179, 'l2_reg_exp': -6.966719999437767, 'batch_size': 492, 'N': 190}. Best is trial 16 with value: 0.2552145719528198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7273\n",
      "Epoch 299 train loss: 0.6308, eval loss 0.6625308990478516\n",
      "optimal threshold: -0.4814\n",
      "Epoch 0 train loss: 0.3081, eval loss 0.6630246639251709\n",
      "optimal threshold: -0.5364\n",
      "Epoch 1 train loss: 0.1887, eval loss 0.6578989028930664\n",
      "optimal threshold: -0.4534\n",
      "Epoch 2 train loss: 0.2048, eval loss 0.6577960252761841\n",
      "optimal threshold: -0.7407\n",
      "Epoch 3 train loss: 0.1649, eval loss 0.659246027469635\n",
      "optimal threshold: -0.6280\n",
      "Epoch 4 train loss: 0.1511, eval loss 0.659493088722229\n",
      "optimal threshold: -0.5133\n",
      "Epoch 5 train loss: 0.1643, eval loss 0.6644236445426941\n",
      "optimal threshold: -0.4279\n",
      "Epoch 6 train loss: 0.1297, eval loss 0.6700659990310669\n",
      "optimal threshold: -0.5366\n",
      "Epoch 7 train loss: 0.1402, eval loss 0.6742897033691406\n",
      "optimal threshold: -0.4844\n",
      "Epoch 8 train loss: 0.1018, eval loss 0.6795153617858887\n",
      "optimal threshold: -0.4455\n",
      "Epoch 9 train loss: 0.1025, eval loss 0.6866596937179565\n",
      "optimal threshold: -0.5197\n",
      "Epoch 10 train loss: 0.0985, eval loss 0.6954774260520935\n",
      "optimal threshold: -0.5607\n",
      "Epoch 11 train loss: 0.0821, eval loss 0.7088019251823425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 15:09:14,427] Trial 35 finished with value: 0.0636233314871788 and parameters: {'learning_rate_exp': -2.8889127901537135, 'dropout_p': 0.13286605028603324, 'l2_reg_exp': -5.338978135983149, 'batch_size': 124, 'N': 340}. Best is trial 35 with value: 0.0636233314871788.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4523\n",
      "optimal threshold: -0.0019\n",
      "Epoch 0 train loss: 1.4908, eval loss 1.4846267700195312\n",
      "optimal threshold: -0.0088\n",
      "Epoch 1 train loss: 1.4836, eval loss 1.4826102256774902\n",
      "optimal threshold: -0.0157\n",
      "Epoch 2 train loss: 1.4846, eval loss 1.4806095361709595\n",
      "optimal threshold: -0.0226\n",
      "Epoch 3 train loss: 1.4858, eval loss 1.4786220788955688\n",
      "optimal threshold: -0.0295\n",
      "Epoch 4 train loss: 1.4841, eval loss 1.4766403436660767\n",
      "optimal threshold: -0.0365\n",
      "Epoch 5 train loss: 1.4758, eval loss 1.47466242313385\n",
      "optimal threshold: -0.0433\n",
      "Epoch 6 train loss: 1.4796, eval loss 1.4726967811584473\n",
      "optimal threshold: 0.0330\n",
      "Epoch 7 train loss: 1.4766, eval loss 1.470739483833313\n",
      "optimal threshold: 0.0284\n",
      "Epoch 8 train loss: 1.4685, eval loss 1.4687913656234741\n",
      "optimal threshold: 0.0276\n",
      "Epoch 9 train loss: 1.4693, eval loss 1.4668446779251099\n",
      "optimal threshold: 0.0249\n",
      "Epoch 10 train loss: 1.4657, eval loss 1.4649008512496948\n",
      "optimal threshold: 0.0240\n",
      "Epoch 11 train loss: 1.4747, eval loss 1.4629565477371216\n",
      "optimal threshold: 0.0214\n",
      "Epoch 12 train loss: 1.4632, eval loss 1.4610140323638916\n",
      "optimal threshold: 0.0158\n",
      "Epoch 13 train loss: 1.4676, eval loss 1.4590675830841064\n",
      "optimal threshold: 0.0132\n",
      "Epoch 14 train loss: 1.4630, eval loss 1.4571213722229004\n",
      "optimal threshold: 0.0100\n",
      "Epoch 15 train loss: 1.4419, eval loss 1.4551812410354614\n",
      "optimal threshold: 0.0083\n",
      "Epoch 16 train loss: 1.4503, eval loss 1.4532328844070435\n",
      "optimal threshold: 0.0056\n",
      "Epoch 17 train loss: 1.4590, eval loss 1.4512816667556763\n",
      "optimal threshold: 0.0027\n",
      "Epoch 18 train loss: 1.4548, eval loss 1.4493342638015747\n",
      "optimal threshold: 0.0062\n",
      "Epoch 19 train loss: 1.4498, eval loss 1.4473685026168823\n",
      "optimal threshold: -0.0054\n",
      "Epoch 20 train loss: 1.4459, eval loss 1.4454009532928467\n",
      "optimal threshold: -0.0033\n",
      "Epoch 21 train loss: 1.4326, eval loss 1.4434293508529663\n",
      "optimal threshold: -0.0106\n",
      "Epoch 22 train loss: 1.4417, eval loss 1.4414492845535278\n",
      "optimal threshold: -0.0082\n",
      "Epoch 23 train loss: 1.4479, eval loss 1.4394690990447998\n",
      "optimal threshold: -0.0121\n",
      "Epoch 24 train loss: 1.4431, eval loss 1.4374817609786987\n",
      "optimal threshold: -0.0168\n",
      "Epoch 25 train loss: 1.4357, eval loss 1.4354804754257202\n",
      "optimal threshold: -0.0188\n",
      "Epoch 26 train loss: 1.4355, eval loss 1.433466911315918\n",
      "optimal threshold: -0.0228\n",
      "Epoch 27 train loss: 1.4264, eval loss 1.431444525718689\n",
      "optimal threshold: -0.0273\n",
      "Epoch 28 train loss: 1.4320, eval loss 1.4294133186340332\n",
      "optimal threshold: -0.0311\n",
      "Epoch 29 train loss: 1.4187, eval loss 1.4273808002471924\n",
      "optimal threshold: -0.0336\n",
      "Epoch 30 train loss: 1.4113, eval loss 1.4253356456756592\n",
      "optimal threshold: -0.0373\n",
      "Epoch 31 train loss: 1.4206, eval loss 1.4232776165008545\n",
      "optimal threshold: -0.0423\n",
      "Epoch 32 train loss: 1.4053, eval loss 1.4212158918380737\n",
      "optimal threshold: -0.0460\n",
      "Epoch 33 train loss: 1.4163, eval loss 1.4191348552703857\n",
      "optimal threshold: -0.0493\n",
      "Epoch 34 train loss: 1.4183, eval loss 1.417044997215271\n",
      "optimal threshold: -0.0535\n",
      "Epoch 35 train loss: 1.4219, eval loss 1.4149487018585205\n",
      "optimal threshold: -0.0581\n",
      "Epoch 36 train loss: 1.4041, eval loss 1.4128464460372925\n",
      "optimal threshold: -0.0611\n",
      "Epoch 37 train loss: 1.4080, eval loss 1.410733699798584\n",
      "optimal threshold: -0.0657\n",
      "Epoch 38 train loss: 1.4158, eval loss 1.4086037874221802\n",
      "optimal threshold: -0.0692\n",
      "Epoch 39 train loss: 1.3952, eval loss 1.4064693450927734\n",
      "optimal threshold: -0.0736\n",
      "Epoch 40 train loss: 1.3947, eval loss 1.4043158292770386\n",
      "optimal threshold: -0.0778\n",
      "Epoch 41 train loss: 1.3944, eval loss 1.402158260345459\n",
      "optimal threshold: -0.0834\n",
      "Epoch 42 train loss: 1.3960, eval loss 1.3999937772750854\n",
      "optimal threshold: -0.0867\n",
      "Epoch 43 train loss: 1.3939, eval loss 1.3978123664855957\n",
      "optimal threshold: -0.0904\n",
      "Epoch 44 train loss: 1.3987, eval loss 1.395623803138733\n",
      "optimal threshold: -0.0949\n",
      "Epoch 45 train loss: 1.3903, eval loss 1.3934260606765747\n",
      "optimal threshold: -0.0900\n",
      "Epoch 46 train loss: 1.3731, eval loss 1.3912169933319092\n",
      "optimal threshold: -0.0831\n",
      "Epoch 47 train loss: 1.3815, eval loss 1.3890011310577393\n",
      "optimal threshold: -0.0872\n",
      "Epoch 48 train loss: 1.3818, eval loss 1.3867740631103516\n",
      "optimal threshold: -0.0910\n",
      "Epoch 49 train loss: 1.3891, eval loss 1.3845453262329102\n",
      "optimal threshold: -0.0950\n",
      "Epoch 50 train loss: 1.3593, eval loss 1.3823095560073853\n",
      "optimal threshold: -0.0984\n",
      "Epoch 51 train loss: 1.3721, eval loss 1.3800625801086426\n",
      "optimal threshold: -0.1021\n",
      "Epoch 52 train loss: 1.3628, eval loss 1.3778094053268433\n",
      "optimal threshold: -0.1101\n",
      "Epoch 53 train loss: 1.3631, eval loss 1.3755450248718262\n",
      "optimal threshold: -0.1141\n",
      "Epoch 54 train loss: 1.3648, eval loss 1.3732837438583374\n",
      "optimal threshold: -0.1174\n",
      "Epoch 55 train loss: 1.3621, eval loss 1.3710100650787354\n",
      "optimal threshold: -0.1224\n",
      "Epoch 56 train loss: 1.3697, eval loss 1.3687258958816528\n",
      "optimal threshold: -0.1263\n",
      "Epoch 57 train loss: 1.3627, eval loss 1.3664331436157227\n",
      "optimal threshold: -0.1304\n",
      "Epoch 58 train loss: 1.3525, eval loss 1.3641349077224731\n",
      "optimal threshold: -0.1312\n",
      "Epoch 59 train loss: 1.3542, eval loss 1.3618286848068237\n",
      "optimal threshold: -0.1320\n",
      "Epoch 60 train loss: 1.3316, eval loss 1.359525203704834\n",
      "optimal threshold: -0.1398\n",
      "Epoch 61 train loss: 1.3486, eval loss 1.3572041988372803\n",
      "optimal threshold: -0.1454\n",
      "Epoch 62 train loss: 1.3341, eval loss 1.3548864126205444\n",
      "optimal threshold: -0.1432\n",
      "Epoch 63 train loss: 1.3292, eval loss 1.3525621891021729\n",
      "optimal threshold: -0.1475\n",
      "Epoch 64 train loss: 1.3366, eval loss 1.3502302169799805\n",
      "optimal threshold: -0.1518\n",
      "Epoch 65 train loss: 1.3405, eval loss 1.3478834629058838\n",
      "optimal threshold: -0.1565\n",
      "Epoch 66 train loss: 1.3295, eval loss 1.3455361127853394\n",
      "optimal threshold: -0.1609\n",
      "Epoch 67 train loss: 1.3296, eval loss 1.3431884050369263\n",
      "optimal threshold: -0.1701\n",
      "Epoch 68 train loss: 1.3213, eval loss 1.3408324718475342\n",
      "optimal threshold: -0.1701\n",
      "Epoch 69 train loss: 1.3275, eval loss 1.3384709358215332\n",
      "optimal threshold: -0.1746\n",
      "Epoch 70 train loss: 1.3230, eval loss 1.336111307144165\n",
      "optimal threshold: -0.1801\n",
      "Epoch 71 train loss: 1.3275, eval loss 1.3337517976760864\n",
      "optimal threshold: -0.1871\n",
      "Epoch 72 train loss: 1.3174, eval loss 1.3313788175582886\n",
      "optimal threshold: -0.1890\n",
      "Epoch 73 train loss: 1.2979, eval loss 1.3289986848831177\n",
      "optimal threshold: -0.1927\n",
      "Epoch 74 train loss: 1.3172, eval loss 1.3266204595565796\n",
      "optimal threshold: -0.2008\n",
      "Epoch 75 train loss: 1.3076, eval loss 1.324239730834961\n",
      "optimal threshold: -0.2045\n",
      "Epoch 76 train loss: 1.3033, eval loss 1.3218533992767334\n",
      "optimal threshold: -0.2088\n",
      "Epoch 77 train loss: 1.3009, eval loss 1.3194597959518433\n",
      "optimal threshold: -0.2051\n",
      "Epoch 78 train loss: 1.3078, eval loss 1.3170714378356934\n",
      "optimal threshold: -0.2058\n",
      "Epoch 79 train loss: 1.3033, eval loss 1.3146839141845703\n",
      "optimal threshold: -0.2110\n",
      "Epoch 80 train loss: 1.2842, eval loss 1.3122857809066772\n",
      "optimal threshold: -0.2149\n",
      "Epoch 81 train loss: 1.2893, eval loss 1.309883713722229\n",
      "optimal threshold: -0.2214\n",
      "Epoch 82 train loss: 1.3010, eval loss 1.3074842691421509\n",
      "optimal threshold: -0.2257\n",
      "Epoch 83 train loss: 1.3003, eval loss 1.3050854206085205\n",
      "optimal threshold: -0.2305\n",
      "Epoch 84 train loss: 1.3027, eval loss 1.3026793003082275\n",
      "optimal threshold: -0.2345\n",
      "Epoch 85 train loss: 1.2808, eval loss 1.3002713918685913\n",
      "optimal threshold: -0.2386\n",
      "Epoch 86 train loss: 1.2866, eval loss 1.2978609800338745\n",
      "optimal threshold: -0.2428\n",
      "Epoch 87 train loss: 1.2576, eval loss 1.295452356338501\n",
      "optimal threshold: -0.2471\n",
      "Epoch 88 train loss: 1.2660, eval loss 1.293042778968811\n",
      "optimal threshold: -0.2518\n",
      "Epoch 89 train loss: 1.2545, eval loss 1.2906253337860107\n",
      "optimal threshold: -0.2579\n",
      "Epoch 90 train loss: 1.2792, eval loss 1.2882080078125\n",
      "optimal threshold: -0.2665\n",
      "Epoch 91 train loss: 1.2559, eval loss 1.2857905626296997\n",
      "optimal threshold: -0.2707\n",
      "Epoch 92 train loss: 1.2485, eval loss 1.2833683490753174\n",
      "optimal threshold: -0.2745\n",
      "Epoch 93 train loss: 1.2671, eval loss 1.2809422016143799\n",
      "optimal threshold: -0.2794\n",
      "Epoch 94 train loss: 1.2441, eval loss 1.278512954711914\n",
      "optimal threshold: -0.2822\n",
      "Epoch 95 train loss: 1.2581, eval loss 1.2760902643203735\n",
      "optimal threshold: -0.2855\n",
      "Epoch 96 train loss: 1.2575, eval loss 1.2736663818359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.2898\n",
      "Epoch 97 train loss: 1.2443, eval loss 1.2712434530258179\n",
      "optimal threshold: -0.2938\n",
      "Epoch 98 train loss: 1.2597, eval loss 1.2688168287277222\n",
      "optimal threshold: -0.2984\n",
      "Epoch 99 train loss: 1.2333, eval loss 1.2663923501968384\n",
      "optimal threshold: -0.3014\n",
      "Epoch 100 train loss: 1.2447, eval loss 1.2639646530151367\n",
      "optimal threshold: -0.3069\n",
      "Epoch 101 train loss: 1.2251, eval loss 1.261536955833435\n",
      "optimal threshold: -0.3118\n",
      "Epoch 102 train loss: 1.2289, eval loss 1.2591055631637573\n",
      "optimal threshold: -0.3170\n",
      "Epoch 103 train loss: 1.2200, eval loss 1.256675362586975\n",
      "optimal threshold: -0.3221\n",
      "Epoch 104 train loss: 1.2416, eval loss 1.254246473312378\n",
      "optimal threshold: -0.3277\n",
      "Epoch 105 train loss: 1.2198, eval loss 1.2518229484558105\n",
      "optimal threshold: -0.3320\n",
      "Epoch 106 train loss: 1.2202, eval loss 1.2493940591812134\n",
      "optimal threshold: -0.3365\n",
      "Epoch 107 train loss: 1.2416, eval loss 1.2469699382781982\n",
      "optimal threshold: -0.3412\n",
      "Epoch 108 train loss: 1.2098, eval loss 1.2445392608642578\n",
      "optimal threshold: -0.3462\n",
      "Epoch 109 train loss: 1.1997, eval loss 1.2421153783798218\n",
      "optimal threshold: -0.3516\n",
      "Epoch 110 train loss: 1.1908, eval loss 1.2396924495697021\n",
      "optimal threshold: -0.3614\n",
      "Epoch 111 train loss: 1.2100, eval loss 1.2372649908065796\n",
      "optimal threshold: -0.3608\n",
      "Epoch 112 train loss: 1.1937, eval loss 1.2348461151123047\n",
      "optimal threshold: -0.3662\n",
      "Epoch 113 train loss: 1.1993, eval loss 1.2324206829071045\n",
      "optimal threshold: -0.3761\n",
      "Epoch 114 train loss: 1.1991, eval loss 1.23000168800354\n",
      "optimal threshold: -0.3800\n",
      "Epoch 115 train loss: 1.1895, eval loss 1.2275755405426025\n",
      "optimal threshold: -0.3861\n",
      "Epoch 116 train loss: 1.1700, eval loss 1.2251553535461426\n",
      "optimal threshold: -0.3915\n",
      "Epoch 117 train loss: 1.1896, eval loss 1.2227377891540527\n",
      "optimal threshold: -0.3963\n",
      "Epoch 118 train loss: 1.1745, eval loss 1.220321536064148\n",
      "optimal threshold: -0.3999\n",
      "Epoch 119 train loss: 1.1972, eval loss 1.217912197113037\n",
      "optimal threshold: -0.4027\n",
      "Epoch 120 train loss: 1.1725, eval loss 1.2154978513717651\n",
      "optimal threshold: -0.4087\n",
      "Epoch 121 train loss: 1.1738, eval loss 1.213085412979126\n",
      "optimal threshold: -0.4132\n",
      "Epoch 122 train loss: 1.1776, eval loss 1.2106752395629883\n",
      "optimal threshold: -0.4177\n",
      "Epoch 123 train loss: 1.1727, eval loss 1.2082732915878296\n",
      "optimal threshold: -0.4221\n",
      "Epoch 124 train loss: 1.1654, eval loss 1.2058703899383545\n",
      "optimal threshold: -0.4267\n",
      "Epoch 125 train loss: 1.1765, eval loss 1.20347261428833\n",
      "optimal threshold: -0.4328\n",
      "Epoch 126 train loss: 1.1536, eval loss 1.201078176498413\n",
      "optimal threshold: -0.4373\n",
      "Epoch 127 train loss: 1.1646, eval loss 1.1986855268478394\n",
      "optimal threshold: -0.4420\n",
      "Epoch 128 train loss: 1.1681, eval loss 1.1963002681732178\n",
      "optimal threshold: -0.4355\n",
      "Epoch 129 train loss: 1.1580, eval loss 1.193912148475647\n",
      "optimal threshold: -0.4510\n",
      "Epoch 130 train loss: 1.1485, eval loss 1.1915309429168701\n",
      "optimal threshold: -0.4563\n",
      "Epoch 131 train loss: 1.1541, eval loss 1.1891541481018066\n",
      "optimal threshold: -0.4594\n",
      "Epoch 132 train loss: 1.1479, eval loss 1.1867828369140625\n",
      "optimal threshold: -0.4668\n",
      "Epoch 133 train loss: 1.1483, eval loss 1.1844183206558228\n",
      "optimal threshold: -0.4708\n",
      "Epoch 134 train loss: 1.1391, eval loss 1.1820536851882935\n",
      "optimal threshold: -0.4773\n",
      "Epoch 135 train loss: 1.1372, eval loss 1.1796941757202148\n",
      "optimal threshold: -0.4750\n",
      "Epoch 136 train loss: 1.1270, eval loss 1.1773338317871094\n",
      "optimal threshold: -0.4775\n",
      "Epoch 137 train loss: 1.1371, eval loss 1.17497980594635\n",
      "optimal threshold: -0.4825\n",
      "Epoch 138 train loss: 1.1307, eval loss 1.1726335287094116\n",
      "optimal threshold: -0.4850\n",
      "Epoch 139 train loss: 1.1300, eval loss 1.1702909469604492\n",
      "optimal threshold: -0.4906\n",
      "Epoch 140 train loss: 1.1273, eval loss 1.1679558753967285\n",
      "optimal threshold: -0.4943\n",
      "Epoch 141 train loss: 1.1200, eval loss 1.165623426437378\n",
      "optimal threshold: -0.5107\n",
      "Epoch 142 train loss: 1.1163, eval loss 1.1632903814315796\n",
      "optimal threshold: -0.5136\n",
      "Epoch 143 train loss: 1.1153, eval loss 1.1609704494476318\n",
      "optimal threshold: -0.5169\n",
      "Epoch 144 train loss: 1.1139, eval loss 1.158657193183899\n",
      "optimal threshold: -0.5182\n",
      "Epoch 145 train loss: 1.1129, eval loss 1.1563464403152466\n",
      "optimal threshold: -0.5225\n",
      "Epoch 146 train loss: 1.0917, eval loss 1.1540474891662598\n",
      "optimal threshold: -0.5283\n",
      "Epoch 147 train loss: 1.0986, eval loss 1.1517482995986938\n",
      "optimal threshold: -0.5311\n",
      "Epoch 148 train loss: 1.0989, eval loss 1.1494556665420532\n",
      "optimal threshold: -0.5358\n",
      "Epoch 149 train loss: 1.1207, eval loss 1.1471713781356812\n",
      "optimal threshold: -0.5408\n",
      "Epoch 150 train loss: 1.1089, eval loss 1.1448907852172852\n",
      "optimal threshold: -0.5452\n",
      "Epoch 151 train loss: 1.0921, eval loss 1.142613410949707\n",
      "optimal threshold: -0.5491\n",
      "Epoch 152 train loss: 1.1099, eval loss 1.1403378248214722\n",
      "optimal threshold: -0.5628\n",
      "Epoch 153 train loss: 1.1193, eval loss 1.1380771398544312\n",
      "optimal threshold: -0.5653\n",
      "Epoch 154 train loss: 1.1075, eval loss 1.1358253955841064\n",
      "optimal threshold: -0.5676\n",
      "Epoch 155 train loss: 1.0754, eval loss 1.1335722208023071\n",
      "optimal threshold: -0.5744\n",
      "Epoch 156 train loss: 1.1083, eval loss 1.1313246488571167\n",
      "optimal threshold: -0.5795\n",
      "Epoch 157 train loss: 1.0936, eval loss 1.1290823221206665\n",
      "optimal threshold: -0.5774\n",
      "Epoch 158 train loss: 1.0526, eval loss 1.126848578453064\n",
      "optimal threshold: -0.5819\n",
      "Epoch 159 train loss: 1.0747, eval loss 1.1246176958084106\n",
      "optimal threshold: -0.5877\n",
      "Epoch 160 train loss: 1.0627, eval loss 1.1223965883255005\n",
      "optimal threshold: -0.5881\n",
      "Epoch 161 train loss: 1.0844, eval loss 1.1201814413070679\n",
      "optimal threshold: -0.5923\n",
      "Epoch 162 train loss: 1.0701, eval loss 1.1179741621017456\n",
      "optimal threshold: -0.6000\n",
      "Epoch 163 train loss: 1.0937, eval loss 1.115767002105713\n",
      "optimal threshold: -0.6053\n",
      "Epoch 164 train loss: 1.0551, eval loss 1.1135667562484741\n",
      "optimal threshold: -0.6081\n",
      "Epoch 165 train loss: 1.0579, eval loss 1.1113731861114502\n",
      "optimal threshold: -0.6113\n",
      "Epoch 166 train loss: 1.0883, eval loss 1.1091898679733276\n",
      "optimal threshold: -0.6165\n",
      "Epoch 167 train loss: 1.0744, eval loss 1.1070144176483154\n",
      "optimal threshold: -0.6288\n",
      "Epoch 168 train loss: 1.0587, eval loss 1.1048393249511719\n",
      "optimal threshold: -0.6226\n",
      "Epoch 169 train loss: 1.0770, eval loss 1.1026746034622192\n",
      "optimal threshold: -0.6329\n",
      "Epoch 170 train loss: 1.0548, eval loss 1.1005171537399292\n",
      "optimal threshold: -0.6458\n",
      "Epoch 171 train loss: 1.0696, eval loss 1.0983597040176392\n",
      "optimal threshold: -0.6536\n",
      "Epoch 172 train loss: 1.0465, eval loss 1.0962109565734863\n",
      "optimal threshold: -0.6502\n",
      "Epoch 173 train loss: 1.0681, eval loss 1.0940684080123901\n",
      "optimal threshold: -0.6501\n",
      "Epoch 174 train loss: 1.0195, eval loss 1.091932773590088\n",
      "optimal threshold: -0.6550\n",
      "Epoch 175 train loss: 1.0183, eval loss 1.089799165725708\n",
      "optimal threshold: -0.6568\n",
      "Epoch 176 train loss: 1.0490, eval loss 1.0876818895339966\n",
      "optimal threshold: -0.6675\n",
      "Epoch 177 train loss: 1.0278, eval loss 1.0855677127838135\n",
      "optimal threshold: -0.6726\n",
      "Epoch 178 train loss: 1.0479, eval loss 1.0834599733352661\n",
      "optimal threshold: -0.6730\n",
      "Epoch 179 train loss: 1.0224, eval loss 1.0813580751419067\n",
      "optimal threshold: -0.6779\n",
      "Epoch 180 train loss: 1.0279, eval loss 1.0792591571807861\n",
      "optimal threshold: -0.6814\n",
      "Epoch 181 train loss: 1.0380, eval loss 1.0771688222885132\n",
      "optimal threshold: -0.6852\n",
      "Epoch 182 train loss: 1.0059, eval loss 1.0750929117202759\n",
      "optimal threshold: -0.6859\n",
      "Epoch 183 train loss: 1.0276, eval loss 1.07300865650177\n",
      "optimal threshold: -0.6890\n",
      "Epoch 184 train loss: 1.0117, eval loss 1.070929765701294\n",
      "optimal threshold: -0.6928\n",
      "Epoch 185 train loss: 1.0191, eval loss 1.068866491317749\n",
      "optimal threshold: -0.6990\n",
      "Epoch 186 train loss: 1.0191, eval loss 1.0668070316314697\n",
      "optimal threshold: -0.7027\n",
      "Epoch 187 train loss: 1.0158, eval loss 1.064753532409668\n",
      "optimal threshold: -0.7077\n",
      "Epoch 188 train loss: 0.9940, eval loss 1.062699794769287\n",
      "optimal threshold: -0.7092\n",
      "Epoch 189 train loss: 1.0190, eval loss 1.0606513023376465\n",
      "optimal threshold: -0.7135\n",
      "Epoch 190 train loss: 0.9964, eval loss 1.058614730834961\n",
      "optimal threshold: -0.7186\n",
      "Epoch 191 train loss: 1.0095, eval loss 1.0565834045410156\n",
      "optimal threshold: -0.7221\n",
      "Epoch 192 train loss: 1.0077, eval loss 1.0545544624328613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7258\n",
      "Epoch 193 train loss: 1.0265, eval loss 1.0525333881378174\n",
      "optimal threshold: -0.7302\n",
      "Epoch 194 train loss: 1.0256, eval loss 1.0505242347717285\n",
      "optimal threshold: -0.7341\n",
      "Epoch 195 train loss: 1.0061, eval loss 1.0485111474990845\n",
      "optimal threshold: -0.7399\n",
      "Epoch 196 train loss: 1.0206, eval loss 1.046505331993103\n",
      "optimal threshold: -0.7465\n",
      "Epoch 197 train loss: 0.9842, eval loss 1.0445067882537842\n",
      "optimal threshold: -0.7508\n",
      "Epoch 198 train loss: 1.0047, eval loss 1.0425148010253906\n",
      "optimal threshold: -0.7562\n",
      "Epoch 199 train loss: 0.9708, eval loss 1.0405247211456299\n",
      "optimal threshold: -0.7574\n",
      "Epoch 200 train loss: 0.9785, eval loss 1.0385386943817139\n",
      "optimal threshold: -0.7609\n",
      "Epoch 201 train loss: 0.9883, eval loss 1.0365636348724365\n",
      "optimal threshold: -0.7647\n",
      "Epoch 202 train loss: 0.9841, eval loss 1.0345945358276367\n",
      "optimal threshold: -0.7679\n",
      "Epoch 203 train loss: 0.9774, eval loss 1.0326319932937622\n",
      "optimal threshold: -0.7713\n",
      "Epoch 204 train loss: 0.9749, eval loss 1.0306745767593384\n",
      "optimal threshold: -0.7760\n",
      "Epoch 205 train loss: 0.9843, eval loss 1.0287230014801025\n",
      "optimal threshold: -0.7801\n",
      "Epoch 206 train loss: 0.9803, eval loss 1.0267729759216309\n",
      "optimal threshold: -0.7835\n",
      "Epoch 207 train loss: 0.9810, eval loss 1.0248332023620605\n",
      "optimal threshold: -0.7872\n",
      "Epoch 208 train loss: 0.9529, eval loss 1.0229028463363647\n",
      "optimal threshold: -0.7920\n",
      "Epoch 209 train loss: 0.9672, eval loss 1.0209711790084839\n",
      "optimal threshold: -0.7955\n",
      "Epoch 210 train loss: 0.9362, eval loss 1.0190463066101074\n",
      "optimal threshold: -0.7947\n",
      "Epoch 211 train loss: 0.9558, eval loss 1.0171257257461548\n",
      "optimal threshold: -0.7981\n",
      "Epoch 212 train loss: 0.9432, eval loss 1.0152156352996826\n",
      "optimal threshold: -0.8079\n",
      "Epoch 213 train loss: 0.9338, eval loss 1.013310432434082\n",
      "optimal threshold: -0.8043\n",
      "Epoch 214 train loss: 0.9596, eval loss 1.0114115476608276\n",
      "optimal threshold: -0.8075\n",
      "Epoch 215 train loss: 0.9553, eval loss 1.0095149278640747\n",
      "optimal threshold: -0.8124\n",
      "Epoch 216 train loss: 0.9479, eval loss 1.007627010345459\n",
      "optimal threshold: -0.8162\n",
      "Epoch 217 train loss: 0.9535, eval loss 1.0057413578033447\n",
      "optimal threshold: -0.8199\n",
      "Epoch 218 train loss: 0.9289, eval loss 1.0038604736328125\n",
      "optimal threshold: -0.7894\n",
      "Epoch 219 train loss: 0.9365, eval loss 1.0019830465316772\n",
      "optimal threshold: -0.8289\n",
      "Epoch 220 train loss: 0.9396, eval loss 1.000113606452942\n",
      "optimal threshold: -0.8297\n",
      "Epoch 221 train loss: 0.9301, eval loss 0.9982519149780273\n",
      "optimal threshold: -0.8324\n",
      "Epoch 222 train loss: 0.9467, eval loss 0.996407687664032\n",
      "optimal threshold: -0.7993\n",
      "Epoch 223 train loss: 0.9322, eval loss 0.9945597648620605\n",
      "optimal threshold: -0.8146\n",
      "Epoch 224 train loss: 0.9552, eval loss 0.99271559715271\n",
      "optimal threshold: -0.8206\n",
      "Epoch 225 train loss: 0.9291, eval loss 0.9908754229545593\n",
      "optimal threshold: -0.8219\n",
      "Epoch 226 train loss: 0.9417, eval loss 0.9890426993370056\n",
      "optimal threshold: -0.8492\n",
      "Epoch 227 train loss: 0.9055, eval loss 0.9872189164161682\n",
      "optimal threshold: -0.8269\n",
      "Epoch 228 train loss: 0.9154, eval loss 0.985399067401886\n",
      "optimal threshold: -0.8309\n",
      "Epoch 229 train loss: 0.8937, eval loss 0.983584463596344\n",
      "optimal threshold: -0.8337\n",
      "Epoch 230 train loss: 0.9398, eval loss 0.9817811846733093\n",
      "optimal threshold: -0.8515\n",
      "Epoch 231 train loss: 0.9240, eval loss 0.9799808263778687\n",
      "optimal threshold: -0.8539\n",
      "Epoch 232 train loss: 0.9107, eval loss 0.9781850576400757\n",
      "optimal threshold: -0.8562\n",
      "Epoch 233 train loss: 0.8906, eval loss 0.9763928651809692\n",
      "optimal threshold: -0.8526\n",
      "Epoch 234 train loss: 0.8910, eval loss 0.9746123552322388\n",
      "optimal threshold: -0.8549\n",
      "Epoch 235 train loss: 0.9237, eval loss 0.9728376269340515\n",
      "optimal threshold: -0.8578\n",
      "Epoch 236 train loss: 0.9023, eval loss 0.9710635542869568\n",
      "optimal threshold: -0.8612\n",
      "Epoch 237 train loss: 0.8969, eval loss 0.9692988395690918\n",
      "optimal threshold: -0.8698\n",
      "Epoch 238 train loss: 0.8763, eval loss 0.9675363302230835\n",
      "optimal threshold: -0.8725\n",
      "Epoch 239 train loss: 0.8944, eval loss 0.9657806754112244\n",
      "optimal threshold: -0.8739\n",
      "Epoch 240 train loss: 0.9062, eval loss 0.9640263319015503\n",
      "optimal threshold: -0.8759\n",
      "Epoch 241 train loss: 0.8826, eval loss 0.9622848033905029\n",
      "optimal threshold: -0.8790\n",
      "Epoch 242 train loss: 0.8697, eval loss 0.9605501294136047\n",
      "optimal threshold: -0.8816\n",
      "Epoch 243 train loss: 0.8895, eval loss 0.9588205814361572\n",
      "optimal threshold: -0.8829\n",
      "Epoch 244 train loss: 0.9429, eval loss 0.9570953249931335\n",
      "optimal threshold: -0.8832\n",
      "Epoch 245 train loss: 0.9458, eval loss 0.9553802609443665\n",
      "optimal threshold: -0.8807\n",
      "Epoch 246 train loss: 0.8388, eval loss 0.953667163848877\n",
      "optimal threshold: -0.8683\n",
      "Epoch 247 train loss: 0.9179, eval loss 0.9519557356834412\n",
      "optimal threshold: -0.8869\n",
      "Epoch 248 train loss: 0.8765, eval loss 0.9502524137496948\n",
      "optimal threshold: -0.8876\n",
      "Epoch 249 train loss: 0.9007, eval loss 0.9485626220703125\n",
      "optimal threshold: -0.8916\n",
      "Epoch 250 train loss: 0.8910, eval loss 0.9468721747398376\n",
      "optimal threshold: -0.8920\n",
      "Epoch 251 train loss: 0.9372, eval loss 0.9451878666877747\n",
      "optimal threshold: -0.8946\n",
      "Epoch 252 train loss: 0.8440, eval loss 0.9435149431228638\n",
      "optimal threshold: -0.8971\n",
      "Epoch 253 train loss: 0.9053, eval loss 0.9418469071388245\n",
      "optimal threshold: -0.8793\n",
      "Epoch 254 train loss: 0.8751, eval loss 0.9401832818984985\n",
      "optimal threshold: -0.8802\n",
      "Epoch 255 train loss: 0.8745, eval loss 0.9385263323783875\n",
      "optimal threshold: -0.8819\n",
      "Epoch 256 train loss: 0.8723, eval loss 0.9368728995323181\n",
      "optimal threshold: -0.8840\n",
      "Epoch 257 train loss: 0.8774, eval loss 0.9352304935455322\n",
      "optimal threshold: -0.8859\n",
      "Epoch 258 train loss: 0.8871, eval loss 0.933594286441803\n",
      "optimal threshold: -0.8877\n",
      "Epoch 259 train loss: 0.8500, eval loss 0.9319658279418945\n",
      "optimal threshold: -0.8794\n",
      "Epoch 260 train loss: 0.8341, eval loss 0.9303433299064636\n",
      "optimal threshold: -0.8792\n",
      "Epoch 261 train loss: 0.8505, eval loss 0.9287251830101013\n",
      "optimal threshold: -0.8788\n",
      "Epoch 262 train loss: 0.8672, eval loss 0.9271105527877808\n",
      "optimal threshold: -0.8794\n",
      "Epoch 263 train loss: 0.8687, eval loss 0.9255019426345825\n",
      "optimal threshold: -0.8807\n",
      "Epoch 264 train loss: 0.8493, eval loss 0.9239023327827454\n",
      "optimal threshold: -0.8819\n",
      "Epoch 265 train loss: 0.8387, eval loss 0.9223077297210693\n",
      "optimal threshold: -0.8832\n",
      "Epoch 266 train loss: 0.8356, eval loss 0.9207186698913574\n",
      "optimal threshold: -0.8833\n",
      "Epoch 267 train loss: 0.8169, eval loss 0.9191314578056335\n",
      "optimal threshold: -0.8856\n",
      "Epoch 268 train loss: 0.8803, eval loss 0.9175562858581543\n",
      "optimal threshold: -0.8872\n",
      "Epoch 269 train loss: 0.8536, eval loss 0.9159850478172302\n",
      "optimal threshold: -0.8806\n",
      "Epoch 270 train loss: 0.8468, eval loss 0.9144231081008911\n",
      "optimal threshold: -0.8810\n",
      "Epoch 271 train loss: 0.8238, eval loss 0.9128735661506653\n",
      "optimal threshold: -0.8814\n",
      "Epoch 272 train loss: 0.8019, eval loss 0.911320149898529\n",
      "optimal threshold: -0.8830\n",
      "Epoch 273 train loss: 0.8610, eval loss 0.9097775220870972\n",
      "optimal threshold: -0.8829\n",
      "Epoch 274 train loss: 0.8579, eval loss 0.9082433581352234\n",
      "optimal threshold: -0.8880\n",
      "Epoch 275 train loss: 0.7941, eval loss 0.9067140817642212\n",
      "optimal threshold: -0.8948\n",
      "Epoch 276 train loss: 0.8015, eval loss 0.9051819443702698\n",
      "optimal threshold: -0.8818\n",
      "Epoch 277 train loss: 0.8601, eval loss 0.903666615486145\n",
      "optimal threshold: -0.8822\n",
      "Epoch 278 train loss: 0.8210, eval loss 0.9021565914154053\n",
      "optimal threshold: -0.8838\n",
      "Epoch 279 train loss: 0.8236, eval loss 0.9006545543670654\n",
      "optimal threshold: -0.8858\n",
      "Epoch 280 train loss: 0.8414, eval loss 0.8991525769233704\n",
      "optimal threshold: -0.8834\n",
      "Epoch 281 train loss: 0.8159, eval loss 0.8976513743400574\n",
      "optimal threshold: -0.8830\n",
      "Epoch 282 train loss: 0.8013, eval loss 0.8961628079414368\n",
      "optimal threshold: -0.9064\n",
      "Epoch 283 train loss: 0.8257, eval loss 0.8946744799613953\n",
      "optimal threshold: -0.8920\n",
      "Epoch 284 train loss: 0.8204, eval loss 0.8931974172592163\n",
      "optimal threshold: -0.9010\n",
      "Epoch 285 train loss: 0.7959, eval loss 0.891724705696106\n",
      "optimal threshold: -0.9021\n",
      "Epoch 286 train loss: 0.8280, eval loss 0.8902649283409119\n",
      "optimal threshold: -0.8961\n",
      "Epoch 287 train loss: 0.8113, eval loss 0.8888065218925476\n",
      "optimal threshold: -0.8993\n",
      "Epoch 288 train loss: 0.7868, eval loss 0.8873555064201355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.8991\n",
      "Epoch 289 train loss: 0.7826, eval loss 0.8859165906906128\n",
      "optimal threshold: -0.8895\n",
      "Epoch 290 train loss: 0.8415, eval loss 0.8844733834266663\n",
      "optimal threshold: -0.8954\n",
      "Epoch 291 train loss: 0.8062, eval loss 0.8830412030220032\n",
      "optimal threshold: -0.8900\n",
      "Epoch 292 train loss: 0.7890, eval loss 0.8816227316856384\n",
      "optimal threshold: -0.8907\n",
      "Epoch 293 train loss: 0.8546, eval loss 0.8802042603492737\n",
      "optimal threshold: -0.8913\n",
      "Epoch 294 train loss: 0.8261, eval loss 0.8787952065467834\n",
      "optimal threshold: -0.8891\n",
      "Epoch 295 train loss: 0.8103, eval loss 0.8773926496505737\n",
      "optimal threshold: -0.8906\n",
      "Epoch 296 train loss: 0.8176, eval loss 0.875997006893158\n",
      "optimal threshold: -0.8912\n",
      "Epoch 297 train loss: 0.7905, eval loss 0.8746042847633362\n",
      "optimal threshold: -0.8935\n",
      "Epoch 298 train loss: 0.7904, eval loss 0.8732104897499084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 15:12:01,394] Trial 36 finished with value: 0.7693045735359192 and parameters: {'learning_rate_exp': -5.933891558864904, 'dropout_p': 0.2572926656831473, 'l2_reg_exp': -3.3992306846144045, 'batch_size': 473, 'N': 289}. Best is trial 35 with value: 0.0636233314871788.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.8939\n",
      "Epoch 299 train loss: 0.7693, eval loss 0.8718283176422119\n",
      "optimal threshold: -0.0477\n",
      "Epoch 0 train loss: 1.4102, eval loss 1.4085359573364258\n",
      "optimal threshold: -0.0526\n",
      "Epoch 1 train loss: 1.3848, eval loss 1.4062883853912354\n",
      "optimal threshold: -0.0484\n",
      "Epoch 2 train loss: 1.3747, eval loss 1.4040369987487793\n",
      "optimal threshold: -0.0507\n",
      "Epoch 3 train loss: 1.4178, eval loss 1.401804804801941\n",
      "optimal threshold: -0.0544\n",
      "Epoch 4 train loss: 1.3920, eval loss 1.3995869159698486\n",
      "optimal threshold: -0.0581\n",
      "Epoch 5 train loss: 1.4211, eval loss 1.3973476886749268\n",
      "optimal threshold: -0.0590\n",
      "Epoch 6 train loss: 1.4045, eval loss 1.3951232433319092\n",
      "optimal threshold: -0.0620\n",
      "Epoch 7 train loss: 1.3851, eval loss 1.3928768634796143\n",
      "optimal threshold: -0.0650\n",
      "Epoch 8 train loss: 1.4000, eval loss 1.390627145767212\n",
      "optimal threshold: -0.0685\n",
      "Epoch 9 train loss: 1.3943, eval loss 1.388404130935669\n",
      "optimal threshold: -0.0713\n",
      "Epoch 10 train loss: 1.3834, eval loss 1.3861773014068604\n",
      "optimal threshold: -0.0733\n",
      "Epoch 11 train loss: 1.3972, eval loss 1.3839333057403564\n",
      "optimal threshold: -0.0755\n",
      "Epoch 12 train loss: 1.3859, eval loss 1.3817236423492432\n",
      "optimal threshold: -0.0777\n",
      "Epoch 13 train loss: 1.4089, eval loss 1.3794913291931152\n",
      "optimal threshold: -0.0791\n",
      "Epoch 14 train loss: 1.3600, eval loss 1.3772549629211426\n",
      "optimal threshold: -0.0818\n",
      "Epoch 15 train loss: 1.3982, eval loss 1.375016450881958\n",
      "optimal threshold: -0.0848\n",
      "Epoch 16 train loss: 1.3886, eval loss 1.3727777004241943\n",
      "optimal threshold: -0.0887\n",
      "Epoch 17 train loss: 1.3366, eval loss 1.3705123662948608\n",
      "optimal threshold: -0.0915\n",
      "Epoch 18 train loss: 1.3521, eval loss 1.3682523965835571\n",
      "optimal threshold: -0.0904\n",
      "Epoch 19 train loss: 1.3468, eval loss 1.3660038709640503\n",
      "optimal threshold: -0.0932\n",
      "Epoch 20 train loss: 1.3408, eval loss 1.3637267351150513\n",
      "optimal threshold: -0.0983\n",
      "Epoch 21 train loss: 1.3765, eval loss 1.3614472150802612\n",
      "optimal threshold: -0.1008\n",
      "Epoch 22 train loss: 1.3317, eval loss 1.359167218208313\n",
      "optimal threshold: -0.1032\n",
      "Epoch 23 train loss: 1.3297, eval loss 1.3569000959396362\n",
      "optimal threshold: -0.1063\n",
      "Epoch 24 train loss: 1.3458, eval loss 1.3546167612075806\n",
      "optimal threshold: -0.1074\n",
      "Epoch 25 train loss: 1.3412, eval loss 1.352309226989746\n",
      "optimal threshold: -0.1099\n",
      "Epoch 26 train loss: 1.3398, eval loss 1.3500382900238037\n",
      "optimal threshold: -0.1126\n",
      "Epoch 27 train loss: 1.3578, eval loss 1.3477247953414917\n",
      "optimal threshold: -0.1151\n",
      "Epoch 28 train loss: 1.3444, eval loss 1.3454328775405884\n",
      "optimal threshold: -0.1182\n",
      "Epoch 29 train loss: 1.3384, eval loss 1.3431193828582764\n",
      "optimal threshold: -0.1214\n",
      "Epoch 30 train loss: 1.3419, eval loss 1.3407963514328003\n",
      "optimal threshold: -0.1242\n",
      "Epoch 31 train loss: 1.3209, eval loss 1.3384708166122437\n",
      "optimal threshold: -0.1305\n",
      "Epoch 32 train loss: 1.3375, eval loss 1.3361209630966187\n",
      "optimal threshold: -0.1337\n",
      "Epoch 33 train loss: 1.3553, eval loss 1.3337767124176025\n",
      "optimal threshold: -0.1338\n",
      "Epoch 34 train loss: 1.3411, eval loss 1.3314300775527954\n",
      "optimal threshold: -0.1373\n",
      "Epoch 35 train loss: 1.3519, eval loss 1.329060673713684\n",
      "optimal threshold: -0.1432\n",
      "Epoch 36 train loss: 1.3358, eval loss 1.326710820198059\n",
      "optimal threshold: -0.1464\n",
      "Epoch 37 train loss: 1.3357, eval loss 1.324338436126709\n",
      "optimal threshold: -0.1493\n",
      "Epoch 38 train loss: 1.3342, eval loss 1.3219753503799438\n",
      "optimal threshold: -0.1542\n",
      "Epoch 39 train loss: 1.3368, eval loss 1.3195831775665283\n",
      "optimal threshold: -0.1556\n",
      "Epoch 40 train loss: 1.3522, eval loss 1.3171977996826172\n",
      "optimal threshold: -0.1598\n",
      "Epoch 41 train loss: 1.2977, eval loss 1.314799189567566\n",
      "optimal threshold: -0.1609\n",
      "Epoch 42 train loss: 1.3037, eval loss 1.3123770952224731\n",
      "optimal threshold: -0.1642\n",
      "Epoch 43 train loss: 1.2927, eval loss 1.3099397420883179\n",
      "optimal threshold: -0.1667\n",
      "Epoch 44 train loss: 1.2859, eval loss 1.3075065612792969\n",
      "optimal threshold: -0.1702\n",
      "Epoch 45 train loss: 1.2988, eval loss 1.3050739765167236\n",
      "optimal threshold: -0.1730\n",
      "Epoch 46 train loss: 1.3200, eval loss 1.3026167154312134\n",
      "optimal threshold: -0.1750\n",
      "Epoch 47 train loss: 1.3298, eval loss 1.3001573085784912\n",
      "optimal threshold: -0.1793\n",
      "Epoch 48 train loss: 1.2942, eval loss 1.2976956367492676\n",
      "optimal threshold: -0.1837\n",
      "Epoch 49 train loss: 1.3042, eval loss 1.2952122688293457\n",
      "optimal threshold: -0.1873\n",
      "Epoch 50 train loss: 1.2723, eval loss 1.2927541732788086\n",
      "optimal threshold: -0.1908\n",
      "Epoch 51 train loss: 1.2636, eval loss 1.2902745008468628\n",
      "optimal threshold: -0.1910\n",
      "Epoch 52 train loss: 1.3113, eval loss 1.287790060043335\n",
      "optimal threshold: -0.1947\n",
      "Epoch 53 train loss: 1.2938, eval loss 1.2853028774261475\n",
      "optimal threshold: -0.1968\n",
      "Epoch 54 train loss: 1.2885, eval loss 1.2828172445297241\n",
      "optimal threshold: -0.1912\n",
      "Epoch 55 train loss: 1.3225, eval loss 1.280283808708191\n",
      "optimal threshold: -0.2034\n",
      "Epoch 56 train loss: 1.3051, eval loss 1.277780532836914\n",
      "optimal threshold: -0.1996\n",
      "Epoch 57 train loss: 1.2821, eval loss 1.275233507156372\n",
      "optimal threshold: -0.2031\n",
      "Epoch 58 train loss: 1.2861, eval loss 1.2726713418960571\n",
      "optimal threshold: -0.2170\n",
      "Epoch 59 train loss: 1.2930, eval loss 1.2701021432876587\n",
      "optimal threshold: -0.2124\n",
      "Epoch 60 train loss: 1.2988, eval loss 1.2675279378890991\n",
      "optimal threshold: -0.2161\n",
      "Epoch 61 train loss: 1.2548, eval loss 1.2649836540222168\n",
      "optimal threshold: -0.2210\n",
      "Epoch 62 train loss: 1.2878, eval loss 1.2623741626739502\n",
      "optimal threshold: -0.2247\n",
      "Epoch 63 train loss: 1.2744, eval loss 1.2597715854644775\n",
      "optimal threshold: -0.2278\n",
      "Epoch 64 train loss: 1.2472, eval loss 1.257146954536438\n",
      "optimal threshold: -0.2310\n",
      "Epoch 65 train loss: 1.2328, eval loss 1.2545182704925537\n",
      "optimal threshold: -0.2350\n",
      "Epoch 66 train loss: 1.2714, eval loss 1.2519081830978394\n",
      "optimal threshold: -0.2390\n",
      "Epoch 67 train loss: 1.2429, eval loss 1.2492921352386475\n",
      "optimal threshold: -0.2422\n",
      "Epoch 68 train loss: 1.3084, eval loss 1.2466685771942139\n",
      "optimal threshold: -0.2470\n",
      "Epoch 69 train loss: 1.2461, eval loss 1.2440422773361206\n",
      "optimal threshold: -0.2513\n",
      "Epoch 70 train loss: 1.2589, eval loss 1.2413588762283325\n",
      "optimal threshold: -0.2518\n",
      "Epoch 71 train loss: 1.2555, eval loss 1.2386474609375\n",
      "optimal threshold: -0.2554\n",
      "Epoch 72 train loss: 1.2560, eval loss 1.235982060432434\n",
      "optimal threshold: -0.2595\n",
      "Epoch 73 train loss: 1.2456, eval loss 1.2332895994186401\n",
      "optimal threshold: -0.2638\n",
      "Epoch 74 train loss: 1.2471, eval loss 1.2305898666381836\n",
      "optimal threshold: -0.2684\n",
      "Epoch 75 train loss: 1.2139, eval loss 1.2278989553451538\n",
      "optimal threshold: -0.2728\n",
      "Epoch 76 train loss: 1.2104, eval loss 1.2251927852630615\n",
      "optimal threshold: -0.2769\n",
      "Epoch 77 train loss: 1.2056, eval loss 1.2224630117416382\n",
      "optimal threshold: -0.2816\n",
      "Epoch 78 train loss: 1.2515, eval loss 1.2197147607803345\n",
      "optimal threshold: -0.2848\n",
      "Epoch 79 train loss: 1.2453, eval loss 1.2169908285140991\n",
      "optimal threshold: -0.2933\n",
      "Epoch 80 train loss: 1.2492, eval loss 1.2142360210418701\n",
      "optimal threshold: -0.2961\n",
      "Epoch 81 train loss: 1.2153, eval loss 1.2114877700805664\n",
      "optimal threshold: -0.3000\n",
      "Epoch 82 train loss: 1.2177, eval loss 1.2087546586990356\n",
      "optimal threshold: -0.3165\n",
      "Epoch 83 train loss: 1.2559, eval loss 1.206006646156311\n",
      "optimal threshold: -0.3056\n",
      "Epoch 84 train loss: 1.2301, eval loss 1.203268051147461\n",
      "optimal threshold: -0.3251\n",
      "Epoch 85 train loss: 1.1773, eval loss 1.2005507946014404\n",
      "optimal threshold: -0.3142\n",
      "Epoch 86 train loss: 1.2149, eval loss 1.197792887687683\n",
      "optimal threshold: -0.3326\n",
      "Epoch 87 train loss: 1.1927, eval loss 1.195028305053711\n",
      "optimal threshold: -0.3358\n",
      "Epoch 88 train loss: 1.2230, eval loss 1.1922690868377686\n",
      "optimal threshold: -0.3350\n",
      "Epoch 89 train loss: 1.2256, eval loss 1.1895225048065186\n",
      "optimal threshold: -0.3392\n",
      "Epoch 90 train loss: 1.1922, eval loss 1.186767816543579\n",
      "optimal threshold: -0.3427\n",
      "Epoch 91 train loss: 1.1852, eval loss 1.184003472328186\n",
      "optimal threshold: -0.3491\n",
      "Epoch 92 train loss: 1.2249, eval loss 1.1812493801116943\n",
      "optimal threshold: -0.3529\n",
      "Epoch 93 train loss: 1.1774, eval loss 1.1785029172897339\n",
      "optimal threshold: -0.3570\n",
      "Epoch 94 train loss: 1.2228, eval loss 1.175728440284729\n",
      "optimal threshold: -0.3614\n",
      "Epoch 95 train loss: 1.1914, eval loss 1.1729940176010132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.3658\n",
      "Epoch 96 train loss: 1.2105, eval loss 1.170238971710205\n",
      "optimal threshold: -0.3705\n",
      "Epoch 97 train loss: 1.1905, eval loss 1.1674749851226807\n",
      "optimal threshold: -0.3750\n",
      "Epoch 98 train loss: 1.1310, eval loss 1.1647249460220337\n",
      "optimal threshold: -0.3800\n",
      "Epoch 99 train loss: 1.1483, eval loss 1.161963701248169\n",
      "optimal threshold: -0.3855\n",
      "Epoch 100 train loss: 1.1035, eval loss 1.1592129468917847\n",
      "optimal threshold: -0.3893\n",
      "Epoch 101 train loss: 1.1887, eval loss 1.1564606428146362\n",
      "optimal threshold: -0.3942\n",
      "Epoch 102 train loss: 1.1522, eval loss 1.153725266456604\n",
      "optimal threshold: -0.4009\n",
      "Epoch 103 train loss: 1.1524, eval loss 1.1509791612625122\n",
      "optimal threshold: -0.4044\n",
      "Epoch 104 train loss: 1.1985, eval loss 1.1482378244400024\n",
      "optimal threshold: -0.4071\n",
      "Epoch 105 train loss: 1.1085, eval loss 1.1455239057540894\n",
      "optimal threshold: -0.4135\n",
      "Epoch 106 train loss: 1.1795, eval loss 1.142781138420105\n",
      "optimal threshold: -0.4176\n",
      "Epoch 107 train loss: 1.1678, eval loss 1.1400773525238037\n",
      "optimal threshold: -0.4235\n",
      "Epoch 108 train loss: 1.1183, eval loss 1.1373273134231567\n",
      "optimal threshold: -0.4273\n",
      "Epoch 109 train loss: 1.1203, eval loss 1.1346089839935303\n",
      "optimal threshold: -0.4327\n",
      "Epoch 110 train loss: 1.1522, eval loss 1.1318773031234741\n",
      "optimal threshold: -0.4366\n",
      "Epoch 111 train loss: 1.1284, eval loss 1.1291786432266235\n",
      "optimal threshold: -0.4419\n",
      "Epoch 112 train loss: 1.1396, eval loss 1.1264699697494507\n",
      "optimal threshold: -0.4464\n",
      "Epoch 113 train loss: 1.1585, eval loss 1.1237680912017822\n",
      "optimal threshold: -0.4509\n",
      "Epoch 114 train loss: 1.1207, eval loss 1.1210863590240479\n",
      "optimal threshold: -0.4581\n",
      "Epoch 115 train loss: 1.1720, eval loss 1.11838960647583\n",
      "optimal threshold: -0.4621\n",
      "Epoch 116 train loss: 1.1064, eval loss 1.1157090663909912\n",
      "optimal threshold: -0.4667\n",
      "Epoch 117 train loss: 1.1459, eval loss 1.112999677658081\n",
      "optimal threshold: -0.4733\n",
      "Epoch 118 train loss: 1.1857, eval loss 1.1103280782699585\n",
      "optimal threshold: -0.4748\n",
      "Epoch 119 train loss: 1.1320, eval loss 1.1076710224151611\n",
      "optimal threshold: -0.4790\n",
      "Epoch 120 train loss: 1.1091, eval loss 1.1050249338150024\n",
      "optimal threshold: -0.4839\n",
      "Epoch 121 train loss: 1.1066, eval loss 1.1023671627044678\n",
      "optimal threshold: -0.4890\n",
      "Epoch 122 train loss: 1.1059, eval loss 1.0997313261032104\n",
      "optimal threshold: -0.4932\n",
      "Epoch 123 train loss: 1.0441, eval loss 1.0970901250839233\n",
      "optimal threshold: -0.4991\n",
      "Epoch 124 train loss: 1.1485, eval loss 1.0944315195083618\n",
      "optimal threshold: -0.5037\n",
      "Epoch 125 train loss: 1.1285, eval loss 1.0918177366256714\n",
      "optimal threshold: -0.5111\n",
      "Epoch 126 train loss: 1.0941, eval loss 1.0891908407211304\n",
      "optimal threshold: -0.5092\n",
      "Epoch 127 train loss: 1.1024, eval loss 1.0865991115570068\n",
      "optimal threshold: -0.5147\n",
      "Epoch 128 train loss: 1.0903, eval loss 1.0839972496032715\n",
      "optimal threshold: -0.5235\n",
      "Epoch 129 train loss: 1.1266, eval loss 1.0814176797866821\n",
      "optimal threshold: -0.5277\n",
      "Epoch 130 train loss: 1.0578, eval loss 1.0788637399673462\n",
      "optimal threshold: -0.5356\n",
      "Epoch 131 train loss: 1.0669, eval loss 1.0762970447540283\n",
      "optimal threshold: -0.5354\n",
      "Epoch 132 train loss: 1.0699, eval loss 1.0737557411193848\n",
      "optimal threshold: -0.5400\n",
      "Epoch 133 train loss: 1.0603, eval loss 1.0712324380874634\n",
      "optimal threshold: -0.5434\n",
      "Epoch 134 train loss: 1.0588, eval loss 1.0687025785446167\n",
      "optimal threshold: -0.5479\n",
      "Epoch 135 train loss: 1.1288, eval loss 1.0661813020706177\n",
      "optimal threshold: -0.5516\n",
      "Epoch 136 train loss: 1.0614, eval loss 1.0637054443359375\n",
      "optimal threshold: -0.5561\n",
      "Epoch 137 train loss: 1.0761, eval loss 1.061211109161377\n",
      "optimal threshold: -0.5593\n",
      "Epoch 138 train loss: 1.0566, eval loss 1.0587443113327026\n",
      "optimal threshold: -0.5638\n",
      "Epoch 139 train loss: 1.0794, eval loss 1.056276559829712\n",
      "optimal threshold: -0.5683\n",
      "Epoch 140 train loss: 1.0110, eval loss 1.0538326501846313\n",
      "optimal threshold: -0.5717\n",
      "Epoch 141 train loss: 1.0110, eval loss 1.0513862371444702\n",
      "optimal threshold: -0.5761\n",
      "Epoch 142 train loss: 1.0705, eval loss 1.0489497184753418\n",
      "optimal threshold: -0.5822\n",
      "Epoch 143 train loss: 1.0456, eval loss 1.0465179681777954\n",
      "optimal threshold: -0.5872\n",
      "Epoch 144 train loss: 1.0873, eval loss 1.0441011190414429\n",
      "optimal threshold: -0.5920\n",
      "Epoch 145 train loss: 1.0923, eval loss 1.0416877269744873\n",
      "optimal threshold: -0.5918\n",
      "Epoch 146 train loss: 1.0519, eval loss 1.0393048524856567\n",
      "optimal threshold: -0.6011\n",
      "Epoch 147 train loss: 1.0273, eval loss 1.0369014739990234\n",
      "optimal threshold: -0.6057\n",
      "Epoch 148 train loss: 1.0965, eval loss 1.034537434577942\n",
      "optimal threshold: -0.6028\n",
      "Epoch 149 train loss: 1.0132, eval loss 1.0321632623672485\n",
      "optimal threshold: -0.6083\n",
      "Epoch 150 train loss: 1.1070, eval loss 1.0297996997833252\n",
      "optimal threshold: -0.6121\n",
      "Epoch 151 train loss: 1.0609, eval loss 1.0274702310562134\n",
      "optimal threshold: -0.6159\n",
      "Epoch 152 train loss: 1.0456, eval loss 1.0251634120941162\n",
      "optimal threshold: -0.6198\n",
      "Epoch 153 train loss: 1.0530, eval loss 1.0228782892227173\n",
      "optimal threshold: -0.6237\n",
      "Epoch 154 train loss: 1.0155, eval loss 1.0205720663070679\n",
      "optimal threshold: -0.6280\n",
      "Epoch 155 train loss: 0.9803, eval loss 1.0182899236679077\n",
      "optimal threshold: -0.6317\n",
      "Epoch 156 train loss: 1.0121, eval loss 1.0159906148910522\n",
      "optimal threshold: -0.6356\n",
      "Epoch 157 train loss: 1.0089, eval loss 1.013741374015808\n",
      "optimal threshold: -0.6396\n",
      "Epoch 158 train loss: 1.0127, eval loss 1.0114824771881104\n",
      "optimal threshold: -0.6434\n",
      "Epoch 159 train loss: 1.0145, eval loss 1.0092508792877197\n",
      "optimal threshold: -0.6504\n",
      "Epoch 160 train loss: 1.0394, eval loss 1.00701105594635\n",
      "optimal threshold: -0.6538\n",
      "Epoch 161 train loss: 1.0436, eval loss 1.0047920942306519\n",
      "optimal threshold: -0.6570\n",
      "Epoch 162 train loss: 1.0279, eval loss 1.0026068687438965\n",
      "optimal threshold: -0.6603\n",
      "Epoch 163 train loss: 1.0432, eval loss 1.0004065036773682\n",
      "optimal threshold: -0.6647\n",
      "Epoch 164 train loss: 1.0375, eval loss 0.9982417225837708\n",
      "optimal threshold: -0.6667\n",
      "Epoch 165 train loss: 0.9748, eval loss 0.9960875511169434\n",
      "optimal threshold: -0.6704\n",
      "Epoch 166 train loss: 0.9372, eval loss 0.9939452409744263\n",
      "optimal threshold: -0.6753\n",
      "Epoch 167 train loss: 0.9915, eval loss 0.9918124079704285\n",
      "optimal threshold: -0.6801\n",
      "Epoch 168 train loss: 1.0505, eval loss 0.9896975755691528\n",
      "optimal threshold: -0.6901\n",
      "Epoch 169 train loss: 1.0058, eval loss 0.9875875115394592\n",
      "optimal threshold: -0.6931\n",
      "Epoch 170 train loss: 0.9579, eval loss 0.9854697585105896\n",
      "optimal threshold: -0.6969\n",
      "Epoch 171 train loss: 0.9955, eval loss 0.9833496809005737\n",
      "optimal threshold: -0.6980\n",
      "Epoch 172 train loss: 0.9841, eval loss 0.9812659621238708\n",
      "optimal threshold: -0.7036\n",
      "Epoch 173 train loss: 0.9714, eval loss 0.9791797995567322\n",
      "optimal threshold: -0.7050\n",
      "Epoch 174 train loss: 0.9592, eval loss 0.977093517780304\n",
      "optimal threshold: -0.7080\n",
      "Epoch 175 train loss: 0.9853, eval loss 0.9750455617904663\n",
      "optimal threshold: -0.7121\n",
      "Epoch 176 train loss: 0.9881, eval loss 0.9730280637741089\n",
      "optimal threshold: -0.7170\n",
      "Epoch 177 train loss: 0.9860, eval loss 0.9710001945495605\n",
      "optimal threshold: -0.7164\n",
      "Epoch 178 train loss: 1.0037, eval loss 0.9689803719520569\n",
      "optimal threshold: -0.7204\n",
      "Epoch 179 train loss: 0.9876, eval loss 0.9669902324676514\n",
      "optimal threshold: -0.7235\n",
      "Epoch 180 train loss: 0.9464, eval loss 0.9650286436080933\n",
      "optimal threshold: -0.7269\n",
      "Epoch 181 train loss: 0.8881, eval loss 0.9630692601203918\n",
      "optimal threshold: -0.7303\n",
      "Epoch 182 train loss: 0.9809, eval loss 0.9611107110977173\n",
      "optimal threshold: -0.7341\n",
      "Epoch 183 train loss: 1.0399, eval loss 0.9591584205627441\n",
      "optimal threshold: -0.7377\n",
      "Epoch 184 train loss: 1.0399, eval loss 0.9572375416755676\n",
      "optimal threshold: -0.7381\n",
      "Epoch 185 train loss: 0.9762, eval loss 0.9553340077400208\n",
      "optimal threshold: -0.7415\n",
      "Epoch 186 train loss: 0.9831, eval loss 0.9534232020378113\n",
      "optimal threshold: -0.7448\n",
      "Epoch 187 train loss: 0.9792, eval loss 0.9515068531036377\n",
      "optimal threshold: -0.7483\n",
      "Epoch 188 train loss: 0.8848, eval loss 0.9496178030967712\n",
      "optimal threshold: -0.7525\n",
      "Epoch 189 train loss: 1.0225, eval loss 0.9477478265762329\n",
      "optimal threshold: -0.7547\n",
      "Epoch 190 train loss: 0.9643, eval loss 0.9458831548690796\n",
      "optimal threshold: -0.7552\n",
      "Epoch 191 train loss: 0.8914, eval loss 0.9440218806266785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7591\n",
      "Epoch 192 train loss: 0.9053, eval loss 0.9421707987785339\n",
      "optimal threshold: -0.7610\n",
      "Epoch 193 train loss: 0.9485, eval loss 0.9403473734855652\n",
      "optimal threshold: -0.7627\n",
      "Epoch 194 train loss: 0.9324, eval loss 0.9385101199150085\n",
      "optimal threshold: -0.7707\n",
      "Epoch 195 train loss: 0.9804, eval loss 0.9366965293884277\n",
      "optimal threshold: -0.7809\n",
      "Epoch 196 train loss: 0.9549, eval loss 0.9348995685577393\n",
      "optimal threshold: -0.7848\n",
      "Epoch 197 train loss: 0.9274, eval loss 0.9330782890319824\n",
      "optimal threshold: -0.7859\n",
      "Epoch 198 train loss: 0.9333, eval loss 0.931284487247467\n",
      "optimal threshold: -0.7909\n",
      "Epoch 199 train loss: 0.9568, eval loss 0.9295036196708679\n",
      "optimal threshold: -0.7939\n",
      "Epoch 200 train loss: 0.9447, eval loss 0.9277619123458862\n",
      "optimal threshold: -0.7971\n",
      "Epoch 201 train loss: 0.9638, eval loss 0.9259898662567139\n",
      "optimal threshold: -0.8004\n",
      "Epoch 202 train loss: 0.8773, eval loss 0.924244225025177\n",
      "optimal threshold: -0.8036\n",
      "Epoch 203 train loss: 0.8965, eval loss 0.9225038886070251\n",
      "optimal threshold: -0.7857\n",
      "Epoch 204 train loss: 0.9457, eval loss 0.9207916855812073\n",
      "optimal threshold: -0.7904\n",
      "Epoch 205 train loss: 0.9414, eval loss 0.919093132019043\n",
      "optimal threshold: -0.7934\n",
      "Epoch 206 train loss: 0.9496, eval loss 0.9174092411994934\n",
      "optimal threshold: -0.7956\n",
      "Epoch 207 train loss: 0.9306, eval loss 0.91573166847229\n",
      "optimal threshold: -0.7989\n",
      "Epoch 208 train loss: 0.8977, eval loss 0.9140453338623047\n",
      "optimal threshold: -0.7836\n",
      "Epoch 209 train loss: 0.9464, eval loss 0.9124003052711487\n",
      "optimal threshold: -0.8016\n",
      "Epoch 210 train loss: 0.8588, eval loss 0.9107227921485901\n",
      "optimal threshold: -0.7981\n",
      "Epoch 211 train loss: 0.9007, eval loss 0.909064531326294\n",
      "optimal threshold: -0.7996\n",
      "Epoch 212 train loss: 0.9300, eval loss 0.9074341654777527\n",
      "optimal threshold: -0.8000\n",
      "Epoch 213 train loss: 0.9302, eval loss 0.9058265686035156\n",
      "optimal threshold: -0.8020\n",
      "Epoch 214 train loss: 0.8846, eval loss 0.9042189121246338\n",
      "optimal threshold: -0.8034\n",
      "Epoch 215 train loss: 0.8806, eval loss 0.9026069045066833\n",
      "optimal threshold: -0.8052\n",
      "Epoch 216 train loss: 0.9010, eval loss 0.9010257720947266\n",
      "optimal threshold: -0.8069\n",
      "Epoch 217 train loss: 0.9236, eval loss 0.899450421333313\n",
      "optimal threshold: -0.8057\n",
      "Epoch 218 train loss: 0.8107, eval loss 0.8978787660598755\n",
      "optimal threshold: -0.8067\n",
      "Epoch 219 train loss: 0.9022, eval loss 0.8963093161582947\n",
      "optimal threshold: -0.8081\n",
      "Epoch 220 train loss: 0.9148, eval loss 0.8947771787643433\n",
      "optimal threshold: -0.8124\n",
      "Epoch 221 train loss: 0.9014, eval loss 0.8932409882545471\n",
      "optimal threshold: -0.8141\n",
      "Epoch 222 train loss: 0.8351, eval loss 0.8917123675346375\n",
      "optimal threshold: -0.8021\n",
      "Epoch 223 train loss: 0.9540, eval loss 0.8901830315589905\n",
      "optimal threshold: -0.8031\n",
      "Epoch 224 train loss: 0.8689, eval loss 0.8886877298355103\n",
      "optimal threshold: -0.8053\n",
      "Epoch 225 train loss: 0.9115, eval loss 0.8872020840644836\n",
      "optimal threshold: -0.8071\n",
      "Epoch 226 train loss: 0.8533, eval loss 0.8857417106628418\n",
      "optimal threshold: -0.8108\n",
      "Epoch 227 train loss: 0.8238, eval loss 0.8842616677284241\n",
      "optimal threshold: -0.8114\n",
      "Epoch 228 train loss: 0.8804, eval loss 0.882794976234436\n",
      "optimal threshold: -0.8118\n",
      "Epoch 229 train loss: 0.8646, eval loss 0.8813498020172119\n",
      "optimal threshold: -0.8124\n",
      "Epoch 230 train loss: 0.8460, eval loss 0.8799161911010742\n",
      "optimal threshold: -0.8318\n",
      "Epoch 231 train loss: 0.8930, eval loss 0.8784656524658203\n",
      "optimal threshold: -0.8334\n",
      "Epoch 232 train loss: 0.9009, eval loss 0.8770454525947571\n",
      "optimal threshold: -0.8352\n",
      "Epoch 233 train loss: 0.8268, eval loss 0.8756384253501892\n",
      "optimal threshold: -0.8364\n",
      "Epoch 234 train loss: 0.8941, eval loss 0.8742058277130127\n",
      "optimal threshold: -0.8131\n",
      "Epoch 235 train loss: 0.8553, eval loss 0.8728172779083252\n",
      "optimal threshold: -0.8132\n",
      "Epoch 236 train loss: 0.8849, eval loss 0.8714438080787659\n",
      "optimal threshold: -0.8133\n",
      "Epoch 237 train loss: 0.9122, eval loss 0.8700528740882874\n",
      "optimal threshold: -0.8143\n",
      "Epoch 238 train loss: 0.8797, eval loss 0.8686769008636475\n",
      "optimal threshold: -0.8325\n",
      "Epoch 239 train loss: 0.9035, eval loss 0.8673184514045715\n",
      "optimal threshold: -0.8356\n",
      "Epoch 240 train loss: 0.8217, eval loss 0.8659678101539612\n",
      "optimal threshold: -0.8362\n",
      "Epoch 241 train loss: 0.8341, eval loss 0.864607036113739\n",
      "optimal threshold: -0.8377\n",
      "Epoch 242 train loss: 0.8597, eval loss 0.8632581830024719\n",
      "optimal threshold: -0.8426\n",
      "Epoch 243 train loss: 0.7970, eval loss 0.861928403377533\n",
      "optimal threshold: -0.8434\n",
      "Epoch 244 train loss: 0.7874, eval loss 0.860614001750946\n",
      "optimal threshold: -0.8442\n",
      "Epoch 245 train loss: 0.9079, eval loss 0.8593069314956665\n",
      "optimal threshold: -0.8459\n",
      "Epoch 246 train loss: 0.8460, eval loss 0.8580101728439331\n",
      "optimal threshold: -0.8452\n",
      "Epoch 247 train loss: 0.7846, eval loss 0.8567348122596741\n",
      "optimal threshold: -0.8456\n",
      "Epoch 248 train loss: 0.9247, eval loss 0.8554509282112122\n",
      "optimal threshold: -0.8394\n",
      "Epoch 249 train loss: 0.8475, eval loss 0.85417240858078\n",
      "optimal threshold: -0.8398\n",
      "Epoch 250 train loss: 0.8391, eval loss 0.8529242277145386\n",
      "optimal threshold: -0.8481\n",
      "Epoch 251 train loss: 0.7880, eval loss 0.8516599535942078\n",
      "optimal threshold: -0.8409\n",
      "Epoch 252 train loss: 0.8284, eval loss 0.8504354953765869\n",
      "optimal threshold: -0.8475\n",
      "Epoch 253 train loss: 0.8185, eval loss 0.8492081761360168\n",
      "optimal threshold: -0.8430\n",
      "Epoch 254 train loss: 0.8907, eval loss 0.8479691743850708\n",
      "optimal threshold: -0.8415\n",
      "Epoch 255 train loss: 0.8378, eval loss 0.8467786312103271\n",
      "optimal threshold: -0.8473\n",
      "Epoch 256 train loss: 0.8373, eval loss 0.8455888032913208\n",
      "optimal threshold: -0.8479\n",
      "Epoch 257 train loss: 0.8534, eval loss 0.8443983793258667\n",
      "optimal threshold: -0.8719\n",
      "Epoch 258 train loss: 0.9447, eval loss 0.8432127833366394\n",
      "optimal threshold: -0.8719\n",
      "Epoch 259 train loss: 0.8160, eval loss 0.8420355916023254\n",
      "optimal threshold: -0.8720\n",
      "Epoch 260 train loss: 0.7987, eval loss 0.8408583402633667\n",
      "optimal threshold: -0.8721\n",
      "Epoch 261 train loss: 0.8333, eval loss 0.8396911025047302\n",
      "optimal threshold: -0.8691\n",
      "Epoch 262 train loss: 0.9176, eval loss 0.8385289311408997\n",
      "optimal threshold: -0.8714\n",
      "Epoch 263 train loss: 0.9472, eval loss 0.8373761177062988\n",
      "optimal threshold: -0.8707\n",
      "Epoch 264 train loss: 0.9736, eval loss 0.8362400531768799\n",
      "optimal threshold: -0.8676\n",
      "Epoch 265 train loss: 0.8450, eval loss 0.8351448178291321\n",
      "optimal threshold: -0.8670\n",
      "Epoch 266 train loss: 0.8453, eval loss 0.8340228796005249\n",
      "optimal threshold: -0.8688\n",
      "Epoch 267 train loss: 0.8269, eval loss 0.8328914046287537\n",
      "optimal threshold: -0.8686\n",
      "Epoch 268 train loss: 0.8581, eval loss 0.8317902088165283\n",
      "optimal threshold: -0.8683\n",
      "Epoch 269 train loss: 0.8045, eval loss 0.8306982517242432\n",
      "optimal threshold: -0.8682\n",
      "Epoch 270 train loss: 0.8571, eval loss 0.8295962810516357\n",
      "optimal threshold: -0.8679\n",
      "Epoch 271 train loss: 0.8332, eval loss 0.8285248279571533\n",
      "optimal threshold: -0.8703\n",
      "Epoch 272 train loss: 0.8472, eval loss 0.8274698257446289\n",
      "optimal threshold: -0.8687\n",
      "Epoch 273 train loss: 0.9386, eval loss 0.8264224529266357\n",
      "optimal threshold: -0.8809\n",
      "Epoch 274 train loss: 0.7796, eval loss 0.82539963722229\n",
      "optimal threshold: -0.8795\n",
      "Epoch 275 train loss: 0.9206, eval loss 0.8243381381034851\n",
      "optimal threshold: -0.8783\n",
      "Epoch 276 train loss: 0.7394, eval loss 0.8233011364936829\n",
      "optimal threshold: -0.8781\n",
      "Epoch 277 train loss: 0.7727, eval loss 0.8222435116767883\n",
      "optimal threshold: -0.8786\n",
      "Epoch 278 train loss: 0.8032, eval loss 0.8212336897850037\n",
      "optimal threshold: -0.8599\n",
      "Epoch 279 train loss: 0.8592, eval loss 0.8202202916145325\n",
      "optimal threshold: -0.8602\n",
      "Epoch 280 train loss: 0.8694, eval loss 0.819197952747345\n",
      "optimal threshold: -0.8586\n",
      "Epoch 281 train loss: 0.8593, eval loss 0.8182023167610168\n",
      "optimal threshold: -0.8603\n",
      "Epoch 282 train loss: 0.7491, eval loss 0.8172146677970886\n",
      "optimal threshold: -0.8606\n",
      "Epoch 283 train loss: 0.8326, eval loss 0.8162468075752258\n",
      "optimal threshold: -0.8575\n",
      "Epoch 284 train loss: 0.7983, eval loss 0.8152500987052917\n",
      "optimal threshold: -0.8558\n",
      "Epoch 285 train loss: 0.8027, eval loss 0.8142660856246948\n",
      "optimal threshold: -0.8550\n",
      "Epoch 286 train loss: 0.8554, eval loss 0.813294529914856\n",
      "optimal threshold: -0.8544\n",
      "Epoch 287 train loss: 0.8080, eval loss 0.812323808670044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.8545\n",
      "Epoch 288 train loss: 0.8395, eval loss 0.8113670349121094\n",
      "optimal threshold: -0.8551\n",
      "Epoch 289 train loss: 0.8357, eval loss 0.8104134202003479\n",
      "optimal threshold: -0.8533\n",
      "Epoch 290 train loss: 0.8709, eval loss 0.8094895482063293\n",
      "optimal threshold: -0.8491\n",
      "Epoch 291 train loss: 0.8420, eval loss 0.8085936307907104\n",
      "optimal threshold: -0.8496\n",
      "Epoch 292 train loss: 0.7213, eval loss 0.8076861500740051\n",
      "optimal threshold: -0.8484\n",
      "Epoch 293 train loss: 0.7500, eval loss 0.8067761659622192\n",
      "optimal threshold: -0.8464\n",
      "Epoch 294 train loss: 0.9056, eval loss 0.8058537244796753\n",
      "optimal threshold: -0.8524\n",
      "Epoch 295 train loss: 0.7979, eval loss 0.8049672245979309\n",
      "optimal threshold: -0.8444\n",
      "Epoch 296 train loss: 0.7557, eval loss 0.8040831685066223\n",
      "optimal threshold: -0.8425\n",
      "Epoch 297 train loss: 0.8086, eval loss 0.8032068014144897\n",
      "optimal threshold: -0.8571\n",
      "Epoch 298 train loss: 0.8283, eval loss 0.8023187518119812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 15:14:00,086] Trial 37 finished with value: 0.7862067222595215 and parameters: {'learning_rate_exp': -5.340270518383931, 'dropout_p': 0.5642465182658767, 'l2_reg_exp': -6.196028720687435, 'batch_size': 392, 'N': 36}. Best is trial 35 with value: 0.0636233314871788.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.8400\n",
      "Epoch 299 train loss: 0.7862, eval loss 0.8014475107192993\n",
      "optimal threshold: 0.0864\n",
      "Epoch 0 train loss: 1.4882, eval loss 1.4868669509887695\n",
      "optimal threshold: 0.0870\n",
      "Epoch 1 train loss: 1.4789, eval loss 1.4830607175827026\n",
      "optimal threshold: 0.0787\n",
      "Epoch 2 train loss: 1.4776, eval loss 1.479278564453125\n",
      "optimal threshold: 0.0743\n",
      "Epoch 3 train loss: 1.4707, eval loss 1.475500226020813\n",
      "optimal threshold: 0.0678\n",
      "Epoch 4 train loss: 1.4711, eval loss 1.4717320203781128\n",
      "optimal threshold: 0.0618\n",
      "Epoch 5 train loss: 1.4671, eval loss 1.4679714441299438\n",
      "optimal threshold: 0.0562\n",
      "Epoch 6 train loss: 1.4661, eval loss 1.4641995429992676\n",
      "optimal threshold: 0.0521\n",
      "Epoch 7 train loss: 1.4602, eval loss 1.4604398012161255\n",
      "optimal threshold: 0.0466\n",
      "Epoch 8 train loss: 1.4525, eval loss 1.4566786289215088\n",
      "optimal threshold: 0.0466\n",
      "Epoch 9 train loss: 1.4559, eval loss 1.4528971910476685\n",
      "optimal threshold: 0.0415\n",
      "Epoch 10 train loss: 1.4564, eval loss 1.4491050243377686\n",
      "optimal threshold: 0.0343\n",
      "Epoch 11 train loss: 1.4509, eval loss 1.445304036140442\n",
      "optimal threshold: 0.0283\n",
      "Epoch 12 train loss: 1.4476, eval loss 1.4414907693862915\n",
      "optimal threshold: 0.0233\n",
      "Epoch 13 train loss: 1.4385, eval loss 1.4376569986343384\n",
      "optimal threshold: 0.0148\n",
      "Epoch 14 train loss: 1.4363, eval loss 1.4337913990020752\n",
      "optimal threshold: 0.0089\n",
      "Epoch 15 train loss: 1.4253, eval loss 1.4299144744873047\n",
      "optimal threshold: 0.0034\n",
      "Epoch 16 train loss: 1.4305, eval loss 1.426037073135376\n",
      "optimal threshold: -0.0030\n",
      "Epoch 17 train loss: 1.4250, eval loss 1.422116994857788\n",
      "optimal threshold: -0.0113\n",
      "Epoch 18 train loss: 1.4242, eval loss 1.4181628227233887\n",
      "optimal threshold: -0.0183\n",
      "Epoch 19 train loss: 1.4209, eval loss 1.4141982793807983\n",
      "optimal threshold: -0.0251\n",
      "Epoch 20 train loss: 1.4129, eval loss 1.4101941585540771\n",
      "optimal threshold: -0.0327\n",
      "Epoch 21 train loss: 1.4082, eval loss 1.4061684608459473\n",
      "optimal threshold: -0.0353\n",
      "Epoch 22 train loss: 1.4098, eval loss 1.4021309614181519\n",
      "optimal threshold: -0.0397\n",
      "Epoch 23 train loss: 1.4033, eval loss 1.3980746269226074\n",
      "optimal threshold: -0.0463\n",
      "Epoch 24 train loss: 1.3960, eval loss 1.3939919471740723\n",
      "optimal threshold: -0.0572\n",
      "Epoch 25 train loss: 1.3847, eval loss 1.3898921012878418\n",
      "optimal threshold: -0.0585\n",
      "Epoch 26 train loss: 1.3916, eval loss 1.3857389688491821\n",
      "optimal threshold: -0.0653\n",
      "Epoch 27 train loss: 1.3892, eval loss 1.3815503120422363\n",
      "optimal threshold: -0.0761\n",
      "Epoch 28 train loss: 1.3752, eval loss 1.3773365020751953\n",
      "optimal threshold: -0.0829\n",
      "Epoch 29 train loss: 1.3788, eval loss 1.373101830482483\n",
      "optimal threshold: -0.0896\n",
      "Epoch 30 train loss: 1.3735, eval loss 1.368830680847168\n",
      "optimal threshold: -0.0975\n",
      "Epoch 31 train loss: 1.3744, eval loss 1.3645228147506714\n",
      "optimal threshold: -0.1051\n",
      "Epoch 32 train loss: 1.3722, eval loss 1.3601746559143066\n",
      "optimal threshold: -0.1070\n",
      "Epoch 33 train loss: 1.3612, eval loss 1.3558049201965332\n",
      "optimal threshold: -0.1196\n",
      "Epoch 34 train loss: 1.3603, eval loss 1.3514131307601929\n",
      "optimal threshold: -0.1170\n",
      "Epoch 35 train loss: 1.3607, eval loss 1.3470009565353394\n",
      "optimal threshold: -0.1291\n",
      "Epoch 36 train loss: 1.3522, eval loss 1.342565894126892\n",
      "optimal threshold: -0.1397\n",
      "Epoch 37 train loss: 1.3555, eval loss 1.3381143808364868\n",
      "optimal threshold: -0.1463\n",
      "Epoch 38 train loss: 1.3516, eval loss 1.3336390256881714\n",
      "optimal threshold: -0.1534\n",
      "Epoch 39 train loss: 1.3348, eval loss 1.3291420936584473\n",
      "optimal threshold: -0.1607\n",
      "Epoch 40 train loss: 1.3340, eval loss 1.324601650238037\n",
      "optimal threshold: -0.1684\n",
      "Epoch 41 train loss: 1.3357, eval loss 1.320041298866272\n",
      "optimal threshold: -0.1754\n",
      "Epoch 42 train loss: 1.3252, eval loss 1.3154650926589966\n",
      "optimal threshold: -0.1755\n",
      "Epoch 43 train loss: 1.3176, eval loss 1.3108720779418945\n",
      "optimal threshold: -0.1847\n",
      "Epoch 44 train loss: 1.3254, eval loss 1.3062744140625\n",
      "optimal threshold: -0.1919\n",
      "Epoch 45 train loss: 1.3148, eval loss 1.3016682863235474\n",
      "optimal threshold: -0.2029\n",
      "Epoch 46 train loss: 1.3104, eval loss 1.297023057937622\n",
      "optimal threshold: -0.2203\n",
      "Epoch 47 train loss: 1.3106, eval loss 1.2923732995986938\n",
      "optimal threshold: -0.2278\n",
      "Epoch 48 train loss: 1.3008, eval loss 1.287720799446106\n",
      "optimal threshold: -0.2290\n",
      "Epoch 49 train loss: 1.3010, eval loss 1.2830512523651123\n",
      "optimal threshold: -0.2365\n",
      "Epoch 50 train loss: 1.2865, eval loss 1.278381109237671\n",
      "optimal threshold: -0.2530\n",
      "Epoch 51 train loss: 1.2980, eval loss 1.2736998796463013\n",
      "optimal threshold: -0.2605\n",
      "Epoch 52 train loss: 1.2848, eval loss 1.2690116167068481\n",
      "optimal threshold: -0.2708\n",
      "Epoch 53 train loss: 1.2765, eval loss 1.2643260955810547\n",
      "optimal threshold: -0.2799\n",
      "Epoch 54 train loss: 1.2822, eval loss 1.2596231698989868\n",
      "optimal threshold: -0.2868\n",
      "Epoch 55 train loss: 1.2714, eval loss 1.2549339532852173\n",
      "optimal threshold: -0.2957\n",
      "Epoch 56 train loss: 1.2604, eval loss 1.250249981880188\n",
      "optimal threshold: -0.3049\n",
      "Epoch 57 train loss: 1.2670, eval loss 1.2455694675445557\n",
      "optimal threshold: -0.3163\n",
      "Epoch 58 train loss: 1.2513, eval loss 1.240887999534607\n",
      "optimal threshold: -0.3259\n",
      "Epoch 59 train loss: 1.2538, eval loss 1.2362138032913208\n",
      "optimal threshold: -0.3339\n",
      "Epoch 60 train loss: 1.2449, eval loss 1.2315698862075806\n",
      "optimal threshold: -0.3424\n",
      "Epoch 61 train loss: 1.2447, eval loss 1.226902723312378\n",
      "optimal threshold: -0.3518\n",
      "Epoch 62 train loss: 1.2470, eval loss 1.2222455739974976\n",
      "optimal threshold: -0.3574\n",
      "Epoch 63 train loss: 1.2343, eval loss 1.2176045179367065\n",
      "optimal threshold: -0.3701\n",
      "Epoch 64 train loss: 1.2319, eval loss 1.212973713874817\n",
      "optimal threshold: -0.3738\n",
      "Epoch 65 train loss: 1.2257, eval loss 1.2083460092544556\n",
      "optimal threshold: -0.3837\n",
      "Epoch 66 train loss: 1.2329, eval loss 1.2037086486816406\n",
      "optimal threshold: -0.3841\n",
      "Epoch 67 train loss: 1.2241, eval loss 1.1991117000579834\n",
      "optimal threshold: -0.3978\n",
      "Epoch 68 train loss: 1.2184, eval loss 1.1945339441299438\n",
      "optimal threshold: -0.4068\n",
      "Epoch 69 train loss: 1.2118, eval loss 1.1899665594100952\n",
      "optimal threshold: -0.4158\n",
      "Epoch 70 train loss: 1.2010, eval loss 1.1853950023651123\n",
      "optimal threshold: -0.4198\n",
      "Epoch 71 train loss: 1.1918, eval loss 1.180835485458374\n",
      "optimal threshold: -0.4328\n",
      "Epoch 72 train loss: 1.1990, eval loss 1.176299810409546\n",
      "optimal threshold: -0.4419\n",
      "Epoch 73 train loss: 1.1967, eval loss 1.1717809438705444\n",
      "optimal threshold: -0.4446\n",
      "Epoch 74 train loss: 1.1898, eval loss 1.1672676801681519\n",
      "optimal threshold: -0.4549\n",
      "Epoch 75 train loss: 1.1865, eval loss 1.16277015209198\n",
      "optimal threshold: -0.4649\n",
      "Epoch 76 train loss: 1.1824, eval loss 1.1582919359207153\n",
      "optimal threshold: -0.4738\n",
      "Epoch 77 train loss: 1.1804, eval loss 1.1538370847702026\n",
      "optimal threshold: -0.4843\n",
      "Epoch 78 train loss: 1.1699, eval loss 1.1493932008743286\n",
      "optimal threshold: -0.4918\n",
      "Epoch 79 train loss: 1.1675, eval loss 1.1449720859527588\n",
      "optimal threshold: -0.4968\n",
      "Epoch 80 train loss: 1.1667, eval loss 1.1405766010284424\n",
      "optimal threshold: -0.5072\n",
      "Epoch 81 train loss: 1.1527, eval loss 1.1362009048461914\n",
      "optimal threshold: -0.5158\n",
      "Epoch 82 train loss: 1.1587, eval loss 1.131833553314209\n",
      "optimal threshold: -0.5174\n",
      "Epoch 83 train loss: 1.1390, eval loss 1.1275084018707275\n",
      "optimal threshold: -0.5408\n",
      "Epoch 84 train loss: 1.1386, eval loss 1.1231989860534668\n",
      "optimal threshold: -0.5323\n",
      "Epoch 85 train loss: 1.1452, eval loss 1.118911623954773\n",
      "optimal threshold: -0.5508\n",
      "Epoch 86 train loss: 1.1423, eval loss 1.1146467924118042\n",
      "optimal threshold: -0.5497\n",
      "Epoch 87 train loss: 1.1459, eval loss 1.1104161739349365\n",
      "optimal threshold: -0.5735\n",
      "Epoch 88 train loss: 1.1366, eval loss 1.1062051057815552\n",
      "optimal threshold: -0.5823\n",
      "Epoch 89 train loss: 1.1227, eval loss 1.102020502090454\n",
      "optimal threshold: -0.5906\n",
      "Epoch 90 train loss: 1.1293, eval loss 1.0978684425354004\n",
      "optimal threshold: -0.5910\n",
      "Epoch 91 train loss: 1.1242, eval loss 1.0937358140945435\n",
      "optimal threshold: -0.5991\n",
      "Epoch 92 train loss: 1.1208, eval loss 1.0896384716033936\n",
      "optimal threshold: -0.6205\n",
      "Epoch 93 train loss: 1.1194, eval loss 1.085566759109497\n",
      "optimal threshold: -0.6286\n",
      "Epoch 94 train loss: 1.1139, eval loss 1.0815309286117554\n",
      "optimal threshold: -0.6367\n",
      "Epoch 95 train loss: 1.0970, eval loss 1.0775173902511597\n",
      "optimal threshold: -0.6291\n",
      "Epoch 96 train loss: 1.1144, eval loss 1.0735195875167847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.6381\n",
      "Epoch 97 train loss: 1.0899, eval loss 1.0695544481277466\n",
      "optimal threshold: -0.6446\n",
      "Epoch 98 train loss: 1.0885, eval loss 1.0656126737594604\n",
      "optimal threshold: -0.6533\n",
      "Epoch 99 train loss: 1.0744, eval loss 1.0617156028747559\n",
      "optimal threshold: -0.6611\n",
      "Epoch 100 train loss: 1.0785, eval loss 1.0578391551971436\n",
      "optimal threshold: -0.6696\n",
      "Epoch 101 train loss: 1.0847, eval loss 1.0539956092834473\n",
      "optimal threshold: -0.7026\n",
      "Epoch 102 train loss: 1.0755, eval loss 1.0501831769943237\n",
      "optimal threshold: -0.6838\n",
      "Epoch 103 train loss: 1.0718, eval loss 1.0464012622833252\n",
      "optimal threshold: -0.6919\n",
      "Epoch 104 train loss: 1.0777, eval loss 1.042648196220398\n",
      "optimal threshold: -0.7451\n",
      "Epoch 105 train loss: 1.0750, eval loss 1.0389082431793213\n",
      "optimal threshold: -0.7053\n",
      "Epoch 106 train loss: 1.0734, eval loss 1.0352004766464233\n",
      "optimal threshold: -0.7090\n",
      "Epoch 107 train loss: 1.0576, eval loss 1.031524896621704\n",
      "optimal threshold: -0.7565\n",
      "Epoch 108 train loss: 1.0673, eval loss 1.0278844833374023\n",
      "optimal threshold: -0.7636\n",
      "Epoch 109 train loss: 1.0605, eval loss 1.0242770910263062\n",
      "optimal threshold: -0.7722\n",
      "Epoch 110 train loss: 1.0444, eval loss 1.0207016468048096\n",
      "optimal threshold: -0.7794\n",
      "Epoch 111 train loss: 1.0487, eval loss 1.017141580581665\n",
      "optimal threshold: -0.8016\n",
      "Epoch 112 train loss: 1.0527, eval loss 1.0136114358901978\n",
      "optimal threshold: -0.8100\n",
      "Epoch 113 train loss: 1.0442, eval loss 1.010115146636963\n",
      "optimal threshold: -0.8175\n",
      "Epoch 114 train loss: 1.0442, eval loss 1.0066441297531128\n",
      "optimal threshold: -0.8271\n",
      "Epoch 115 train loss: 1.0408, eval loss 1.0032117366790771\n",
      "optimal threshold: -0.8154\n",
      "Epoch 116 train loss: 1.0257, eval loss 0.9997997879981995\n",
      "optimal threshold: -0.8233\n",
      "Epoch 117 train loss: 1.0171, eval loss 0.9964095950126648\n",
      "optimal threshold: -0.8335\n",
      "Epoch 118 train loss: 1.0250, eval loss 0.9930671453475952\n",
      "optimal threshold: -0.8369\n",
      "Epoch 119 train loss: 1.0199, eval loss 0.9897438287734985\n",
      "optimal threshold: -0.8323\n",
      "Epoch 120 train loss: 1.0292, eval loss 0.9864363074302673\n",
      "optimal threshold: -0.8384\n",
      "Epoch 121 train loss: 1.0057, eval loss 0.9831544756889343\n",
      "optimal threshold: -0.8450\n",
      "Epoch 122 train loss: 1.0089, eval loss 0.979904294013977\n",
      "optimal threshold: -0.8541\n",
      "Epoch 123 train loss: 1.0186, eval loss 0.976685643196106\n",
      "optimal threshold: -0.8576\n",
      "Epoch 124 train loss: 0.9922, eval loss 0.9734904766082764\n",
      "optimal threshold: -0.8640\n",
      "Epoch 125 train loss: 1.0012, eval loss 0.9703188538551331\n",
      "optimal threshold: -0.8704\n",
      "Epoch 126 train loss: 0.9928, eval loss 0.9671684503555298\n",
      "optimal threshold: -0.8766\n",
      "Epoch 127 train loss: 0.9977, eval loss 0.964044988155365\n",
      "optimal threshold: -0.8840\n",
      "Epoch 128 train loss: 0.9954, eval loss 0.9609417915344238\n",
      "optimal threshold: -0.8939\n",
      "Epoch 129 train loss: 0.9923, eval loss 0.9578598141670227\n",
      "optimal threshold: -0.9016\n",
      "Epoch 130 train loss: 0.9894, eval loss 0.9548105597496033\n",
      "optimal threshold: -0.9074\n",
      "Epoch 131 train loss: 0.9898, eval loss 0.951770544052124\n",
      "optimal threshold: -0.9131\n",
      "Epoch 132 train loss: 0.9859, eval loss 0.948752760887146\n",
      "optimal threshold: -0.9193\n",
      "Epoch 133 train loss: 0.9780, eval loss 0.9457737803459167\n",
      "optimal threshold: -0.9255\n",
      "Epoch 134 train loss: 0.9764, eval loss 0.9428048133850098\n",
      "optimal threshold: -0.9249\n",
      "Epoch 135 train loss: 0.9743, eval loss 0.9398593306541443\n",
      "optimal threshold: -0.9355\n",
      "Epoch 136 train loss: 0.9757, eval loss 0.9369457364082336\n",
      "optimal threshold: -0.9340\n",
      "Epoch 137 train loss: 0.9647, eval loss 0.9340510964393616\n",
      "optimal threshold: -0.9388\n",
      "Epoch 138 train loss: 0.9736, eval loss 0.9311752319335938\n",
      "optimal threshold: -0.9445\n",
      "Epoch 139 train loss: 0.9571, eval loss 0.9283166527748108\n",
      "optimal threshold: -0.9524\n",
      "Epoch 140 train loss: 0.9729, eval loss 0.9254829287528992\n",
      "optimal threshold: -0.9570\n",
      "Epoch 141 train loss: 0.9536, eval loss 0.9226714372634888\n",
      "optimal threshold: -0.9695\n",
      "Epoch 142 train loss: 0.9661, eval loss 0.9198756814002991\n",
      "optimal threshold: -0.9743\n",
      "Epoch 143 train loss: 0.9460, eval loss 0.9171149730682373\n",
      "optimal threshold: -0.9836\n",
      "Epoch 144 train loss: 0.9628, eval loss 0.9143799543380737\n",
      "optimal threshold: -0.9846\n",
      "Epoch 145 train loss: 0.9605, eval loss 0.9116515517234802\n",
      "optimal threshold: -0.9880\n",
      "Epoch 146 train loss: 0.9456, eval loss 0.908953845500946\n",
      "optimal threshold: -0.9930\n",
      "Epoch 147 train loss: 0.9452, eval loss 0.9062755703926086\n",
      "optimal threshold: -1.0005\n",
      "Epoch 148 train loss: 0.9549, eval loss 0.9036147594451904\n",
      "optimal threshold: -0.9613\n",
      "Epoch 149 train loss: 0.9363, eval loss 0.9009761810302734\n",
      "optimal threshold: -1.0093\n",
      "Epoch 150 train loss: 0.9364, eval loss 0.898352861404419\n",
      "optimal threshold: -0.8814\n",
      "Epoch 151 train loss: 0.9609, eval loss 0.8957393765449524\n",
      "optimal threshold: -0.8822\n",
      "Epoch 152 train loss: 0.9255, eval loss 0.8931633830070496\n",
      "optimal threshold: -0.9125\n",
      "Epoch 153 train loss: 0.9318, eval loss 0.8906211853027344\n",
      "optimal threshold: -0.9146\n",
      "Epoch 154 train loss: 0.9095, eval loss 0.8880835771560669\n",
      "optimal threshold: -0.9175\n",
      "Epoch 155 train loss: 0.9318, eval loss 0.8855632543563843\n",
      "optimal threshold: -0.9191\n",
      "Epoch 156 train loss: 0.9332, eval loss 0.8830571174621582\n",
      "optimal threshold: -0.9202\n",
      "Epoch 157 train loss: 0.9133, eval loss 0.8805833458900452\n",
      "optimal threshold: -0.9230\n",
      "Epoch 158 train loss: 0.9144, eval loss 0.8781083822250366\n",
      "optimal threshold: -0.9268\n",
      "Epoch 159 train loss: 0.9129, eval loss 0.8756656646728516\n",
      "optimal threshold: -0.9066\n",
      "Epoch 160 train loss: 0.9263, eval loss 0.8732478022575378\n",
      "optimal threshold: -0.9066\n",
      "Epoch 161 train loss: 0.8944, eval loss 0.8708440065383911\n",
      "optimal threshold: -0.9229\n",
      "Epoch 162 train loss: 0.8996, eval loss 0.8684632182121277\n",
      "optimal threshold: -0.9068\n",
      "Epoch 163 train loss: 0.9000, eval loss 0.8660988211631775\n",
      "optimal threshold: -0.9136\n",
      "Epoch 164 train loss: 0.8945, eval loss 0.8637571334838867\n",
      "optimal threshold: -0.9104\n",
      "Epoch 165 train loss: 0.9150, eval loss 0.8614295125007629\n",
      "optimal threshold: -0.9139\n",
      "Epoch 166 train loss: 0.8996, eval loss 0.8591316342353821\n",
      "optimal threshold: -0.9143\n",
      "Epoch 167 train loss: 0.8799, eval loss 0.8568632006645203\n",
      "optimal threshold: -0.9145\n",
      "Epoch 168 train loss: 0.8990, eval loss 0.8545883297920227\n",
      "optimal threshold: -0.9097\n",
      "Epoch 169 train loss: 0.9039, eval loss 0.8523401021957397\n",
      "optimal threshold: -0.9101\n",
      "Epoch 170 train loss: 0.9049, eval loss 0.8501197099685669\n",
      "optimal threshold: -0.9189\n",
      "Epoch 171 train loss: 0.8746, eval loss 0.8479134440422058\n",
      "optimal threshold: -0.9265\n",
      "Epoch 172 train loss: 0.8789, eval loss 0.845731794834137\n",
      "optimal threshold: -0.9277\n",
      "Epoch 173 train loss: 0.8792, eval loss 0.8435810208320618\n",
      "optimal threshold: -0.9251\n",
      "Epoch 174 train loss: 0.8947, eval loss 0.8414326906204224\n",
      "optimal threshold: -0.9256\n",
      "Epoch 175 train loss: 0.8974, eval loss 0.8393192887306213\n",
      "optimal threshold: -0.9245\n",
      "Epoch 176 train loss: 0.8802, eval loss 0.8372366428375244\n",
      "optimal threshold: -0.9287\n",
      "Epoch 177 train loss: 0.8751, eval loss 0.8351621627807617\n",
      "optimal threshold: -0.9215\n",
      "Epoch 178 train loss: 0.8651, eval loss 0.8330937623977661\n",
      "optimal threshold: -0.9212\n",
      "Epoch 179 train loss: 0.8700, eval loss 0.8310505747795105\n",
      "optimal threshold: -0.9213\n",
      "Epoch 180 train loss: 0.8762, eval loss 0.829023003578186\n",
      "optimal threshold: -0.9186\n",
      "Epoch 181 train loss: 0.8766, eval loss 0.8270071744918823\n",
      "optimal threshold: -0.9175\n",
      "Epoch 182 train loss: 0.8792, eval loss 0.8250288367271423\n",
      "optimal threshold: -0.9192\n",
      "Epoch 183 train loss: 0.8540, eval loss 0.8230687379837036\n",
      "optimal threshold: -0.9164\n",
      "Epoch 184 train loss: 0.8710, eval loss 0.8211449980735779\n",
      "optimal threshold: -0.9156\n",
      "Epoch 185 train loss: 0.8663, eval loss 0.8192204833030701\n",
      "optimal threshold: -0.9156\n",
      "Epoch 186 train loss: 0.8766, eval loss 0.8173236846923828\n",
      "optimal threshold: -0.9166\n",
      "Epoch 187 train loss: 0.8610, eval loss 0.8154473900794983\n",
      "optimal threshold: -0.9158\n",
      "Epoch 188 train loss: 0.8503, eval loss 0.8135836124420166\n",
      "optimal threshold: -0.9137\n",
      "Epoch 189 train loss: 0.8636, eval loss 0.8117559552192688\n",
      "optimal threshold: -0.9132\n",
      "Epoch 190 train loss: 0.8506, eval loss 0.8099302649497986\n",
      "optimal threshold: -0.8884\n",
      "Epoch 191 train loss: 0.8515, eval loss 0.8081373572349548\n",
      "optimal threshold: -0.8872\n",
      "Epoch 192 train loss: 0.8495, eval loss 0.8063679933547974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.8854\n",
      "Epoch 193 train loss: 0.8462, eval loss 0.8046035170555115\n",
      "optimal threshold: -0.8865\n",
      "Epoch 194 train loss: 0.8563, eval loss 0.8028786778450012\n",
      "optimal threshold: -0.8854\n",
      "Epoch 195 train loss: 0.8560, eval loss 0.8011713027954102\n",
      "optimal threshold: -0.8854\n",
      "Epoch 196 train loss: 0.8430, eval loss 0.7994809746742249\n",
      "optimal threshold: -0.8852\n",
      "Epoch 197 train loss: 0.8531, eval loss 0.7977996468544006\n",
      "optimal threshold: -0.8867\n",
      "Epoch 198 train loss: 0.8309, eval loss 0.7961456775665283\n",
      "optimal threshold: -0.8850\n",
      "Epoch 199 train loss: 0.8363, eval loss 0.7945064902305603\n",
      "optimal threshold: -0.8847\n",
      "Epoch 200 train loss: 0.8468, eval loss 0.7928914427757263\n",
      "optimal threshold: -0.9961\n",
      "Epoch 201 train loss: 0.8225, eval loss 0.7912957668304443\n",
      "optimal threshold: -0.8890\n",
      "Epoch 202 train loss: 0.8335, eval loss 0.7897289395332336\n",
      "optimal threshold: -0.8853\n",
      "Epoch 203 train loss: 0.8309, eval loss 0.7881889343261719\n",
      "optimal threshold: -0.9924\n",
      "Epoch 204 train loss: 0.8316, eval loss 0.7866607308387756\n",
      "optimal threshold: -0.9894\n",
      "Epoch 205 train loss: 0.8210, eval loss 0.7851588726043701\n",
      "optimal threshold: -0.9878\n",
      "Epoch 206 train loss: 0.8130, eval loss 0.78367680311203\n",
      "optimal threshold: -0.9862\n",
      "Epoch 207 train loss: 0.8197, eval loss 0.782200038433075\n",
      "optimal threshold: -0.9843\n",
      "Epoch 208 train loss: 0.8361, eval loss 0.7807404398918152\n",
      "optimal threshold: -0.9839\n",
      "Epoch 209 train loss: 0.8530, eval loss 0.7793152928352356\n",
      "optimal threshold: -0.9839\n",
      "Epoch 210 train loss: 0.8279, eval loss 0.7779088020324707\n",
      "optimal threshold: -0.9841\n",
      "Epoch 211 train loss: 0.8185, eval loss 0.7765203714370728\n",
      "optimal threshold: -0.9763\n",
      "Epoch 212 train loss: 0.8134, eval loss 0.775153398513794\n",
      "optimal threshold: -0.9755\n",
      "Epoch 213 train loss: 0.8148, eval loss 0.7737835049629211\n",
      "optimal threshold: -0.9725\n",
      "Epoch 214 train loss: 0.8232, eval loss 0.7724533677101135\n",
      "optimal threshold: -1.0021\n",
      "Epoch 215 train loss: 0.8190, eval loss 0.7711380124092102\n",
      "optimal threshold: -0.9665\n",
      "Epoch 216 train loss: 0.8141, eval loss 0.7698485851287842\n",
      "optimal threshold: -0.9661\n",
      "Epoch 217 train loss: 0.8162, eval loss 0.7685744762420654\n",
      "optimal threshold: -0.9642\n",
      "Epoch 218 train loss: 0.8136, eval loss 0.7673287987709045\n",
      "optimal threshold: -0.9623\n",
      "Epoch 219 train loss: 0.8000, eval loss 0.7661013603210449\n",
      "optimal threshold: -0.9959\n",
      "Epoch 220 train loss: 0.8189, eval loss 0.7648816108703613\n",
      "optimal threshold: -0.9947\n",
      "Epoch 221 train loss: 0.7960, eval loss 0.7636741995811462\n",
      "optimal threshold: -0.9962\n",
      "Epoch 222 train loss: 0.7899, eval loss 0.7624874711036682\n",
      "optimal threshold: -0.9927\n",
      "Epoch 223 train loss: 0.8083, eval loss 0.761324942111969\n",
      "optimal threshold: -0.9921\n",
      "Epoch 224 train loss: 0.8099, eval loss 0.7601820230484009\n",
      "optimal threshold: -0.9489\n",
      "Epoch 225 train loss: 0.7890, eval loss 0.7590487599372864\n",
      "optimal threshold: -0.9469\n",
      "Epoch 226 train loss: 0.8022, eval loss 0.7579309344291687\n",
      "optimal threshold: -0.9450\n",
      "Epoch 227 train loss: 0.8052, eval loss 0.7568342685699463\n",
      "optimal threshold: -0.9477\n",
      "Epoch 228 train loss: 0.8140, eval loss 0.7557421326637268\n",
      "optimal threshold: -0.9454\n",
      "Epoch 229 train loss: 0.8113, eval loss 0.7546626329421997\n",
      "optimal threshold: -0.9399\n",
      "Epoch 230 train loss: 0.8024, eval loss 0.7536172866821289\n",
      "optimal threshold: -0.9361\n",
      "Epoch 231 train loss: 0.8077, eval loss 0.7525978088378906\n",
      "optimal threshold: -0.9333\n",
      "Epoch 232 train loss: 0.7849, eval loss 0.7515944838523865\n",
      "optimal threshold: -0.9313\n",
      "Epoch 233 train loss: 0.8016, eval loss 0.7506062388420105\n",
      "optimal threshold: -0.9287\n",
      "Epoch 234 train loss: 0.7900, eval loss 0.7496147155761719\n",
      "optimal threshold: -0.9348\n",
      "Epoch 235 train loss: 0.7934, eval loss 0.7486636638641357\n",
      "optimal threshold: -0.9308\n",
      "Epoch 236 train loss: 0.8115, eval loss 0.7477148175239563\n",
      "optimal threshold: -0.9270\n",
      "Epoch 237 train loss: 0.7696, eval loss 0.7467904090881348\n",
      "optimal threshold: -0.9230\n",
      "Epoch 238 train loss: 0.7748, eval loss 0.7458615303039551\n",
      "optimal threshold: -0.9198\n",
      "Epoch 239 train loss: 0.7950, eval loss 0.7449662089347839\n",
      "optimal threshold: -0.9183\n",
      "Epoch 240 train loss: 0.8094, eval loss 0.7440816760063171\n",
      "optimal threshold: -0.9169\n",
      "Epoch 241 train loss: 0.7900, eval loss 0.7432130575180054\n",
      "optimal threshold: -0.9252\n",
      "Epoch 242 train loss: 0.7603, eval loss 0.7423462867736816\n",
      "optimal threshold: -0.9197\n",
      "Epoch 243 train loss: 0.7811, eval loss 0.7415012717247009\n",
      "optimal threshold: -0.9217\n",
      "Epoch 244 train loss: 0.7735, eval loss 0.7406710386276245\n",
      "optimal threshold: -0.9192\n",
      "Epoch 245 train loss: 0.7898, eval loss 0.7398534417152405\n",
      "optimal threshold: -0.9196\n",
      "Epoch 246 train loss: 0.7745, eval loss 0.73906010389328\n",
      "optimal threshold: -0.9178\n",
      "Epoch 247 train loss: 0.7897, eval loss 0.7382758855819702\n",
      "optimal threshold: -0.9178\n",
      "Epoch 248 train loss: 0.7810, eval loss 0.7375050783157349\n",
      "optimal threshold: -0.9195\n",
      "Epoch 249 train loss: 0.8026, eval loss 0.7367412447929382\n",
      "optimal threshold: -0.9118\n",
      "Epoch 250 train loss: 0.7996, eval loss 0.7359872460365295\n",
      "optimal threshold: -0.9123\n",
      "Epoch 251 train loss: 0.7729, eval loss 0.735241174697876\n",
      "optimal threshold: -0.9337\n",
      "Epoch 252 train loss: 0.7806, eval loss 0.7345095872879028\n",
      "optimal threshold: -0.9396\n",
      "Epoch 253 train loss: 0.7580, eval loss 0.7338144779205322\n",
      "optimal threshold: -0.9386\n",
      "Epoch 254 train loss: 0.7744, eval loss 0.7331123352050781\n",
      "optimal threshold: -0.9398\n",
      "Epoch 255 train loss: 0.7834, eval loss 0.7324187755584717\n",
      "optimal threshold: -0.9383\n",
      "Epoch 256 train loss: 0.8022, eval loss 0.7317491769790649\n",
      "optimal threshold: -0.9346\n",
      "Epoch 257 train loss: 0.7731, eval loss 0.7310973405838013\n",
      "optimal threshold: -0.9389\n",
      "Epoch 258 train loss: 0.7631, eval loss 0.7304463386535645\n",
      "optimal threshold: -0.9369\n",
      "Epoch 259 train loss: 0.7787, eval loss 0.7298078536987305\n",
      "optimal threshold: -0.9282\n",
      "Epoch 260 train loss: 0.7684, eval loss 0.7291889190673828\n",
      "optimal threshold: -0.9307\n",
      "Epoch 261 train loss: 0.7817, eval loss 0.7285749316215515\n",
      "optimal threshold: -0.9301\n",
      "Epoch 262 train loss: 0.7854, eval loss 0.7279579043388367\n",
      "optimal threshold: -0.9276\n",
      "Epoch 263 train loss: 0.7822, eval loss 0.7273504734039307\n",
      "optimal threshold: -0.9256\n",
      "Epoch 264 train loss: 0.7802, eval loss 0.7267449498176575\n",
      "optimal threshold: -0.9179\n",
      "Epoch 265 train loss: 0.7571, eval loss 0.72616046667099\n",
      "optimal threshold: -0.9124\n",
      "Epoch 266 train loss: 0.7811, eval loss 0.7256090641021729\n",
      "optimal threshold: -0.9112\n",
      "Epoch 267 train loss: 0.7944, eval loss 0.72504723072052\n",
      "optimal threshold: -0.9139\n",
      "Epoch 268 train loss: 0.7493, eval loss 0.7245099544525146\n",
      "optimal threshold: -0.9132\n",
      "Epoch 269 train loss: 0.7773, eval loss 0.7239764928817749\n",
      "optimal threshold: -0.9101\n",
      "Epoch 270 train loss: 0.7663, eval loss 0.7234481573104858\n",
      "optimal threshold: -0.9143\n",
      "Epoch 271 train loss: 0.7709, eval loss 0.7229377031326294\n",
      "optimal threshold: -0.9127\n",
      "Epoch 272 train loss: 0.7732, eval loss 0.7224247455596924\n",
      "optimal threshold: -0.9110\n",
      "Epoch 273 train loss: 0.7770, eval loss 0.7219213843345642\n",
      "optimal threshold: -0.9097\n",
      "Epoch 274 train loss: 0.7695, eval loss 0.7214407324790955\n",
      "optimal threshold: -0.9104\n",
      "Epoch 275 train loss: 0.7518, eval loss 0.7209627032279968\n",
      "optimal threshold: -0.9116\n",
      "Epoch 276 train loss: 0.7582, eval loss 0.720513641834259\n",
      "optimal threshold: -0.9121\n",
      "Epoch 277 train loss: 0.7693, eval loss 0.7200453281402588\n",
      "optimal threshold: -0.9043\n",
      "Epoch 278 train loss: 0.7656, eval loss 0.7196170687675476\n",
      "optimal threshold: -0.9020\n",
      "Epoch 279 train loss: 0.7858, eval loss 0.7191638350486755\n",
      "optimal threshold: -0.9192\n",
      "Epoch 280 train loss: 0.7414, eval loss 0.7187100052833557\n",
      "optimal threshold: -0.9169\n",
      "Epoch 281 train loss: 0.7523, eval loss 0.7182772755622864\n",
      "optimal threshold: -0.9157\n",
      "Epoch 282 train loss: 0.7724, eval loss 0.7178449630737305\n",
      "optimal threshold: -0.9170\n",
      "Epoch 283 train loss: 0.7666, eval loss 0.7174252271652222\n",
      "optimal threshold: -0.9176\n",
      "Epoch 284 train loss: 0.7444, eval loss 0.7170172333717346\n",
      "optimal threshold: -0.9387\n",
      "Epoch 285 train loss: 0.7843, eval loss 0.7165974974632263\n",
      "optimal threshold: -0.9362\n",
      "Epoch 286 train loss: 0.7891, eval loss 0.7161839604377747\n",
      "optimal threshold: -0.9340\n",
      "Epoch 287 train loss: 0.7739, eval loss 0.7157866954803467\n",
      "optimal threshold: -0.9631\n",
      "Epoch 288 train loss: 0.7524, eval loss 0.7154032588005066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.9603\n",
      "Epoch 289 train loss: 0.7523, eval loss 0.7150209546089172\n",
      "optimal threshold: -0.9371\n",
      "Epoch 290 train loss: 0.7540, eval loss 0.7146404981613159\n",
      "optimal threshold: -0.9337\n",
      "Epoch 291 train loss: 0.7461, eval loss 0.714280903339386\n",
      "optimal threshold: -0.9343\n",
      "Epoch 292 train loss: 0.7601, eval loss 0.7139277458190918\n",
      "optimal threshold: -0.9256\n",
      "Epoch 293 train loss: 0.7703, eval loss 0.7135781645774841\n",
      "optimal threshold: -0.9395\n",
      "Epoch 294 train loss: 0.7699, eval loss 0.7132202386856079\n",
      "optimal threshold: -0.9437\n",
      "Epoch 295 train loss: 0.7386, eval loss 0.712885856628418\n",
      "optimal threshold: -0.9422\n",
      "Epoch 296 train loss: 0.7567, eval loss 0.7125593423843384\n",
      "optimal threshold: -0.9399\n",
      "Epoch 297 train loss: 0.7309, eval loss 0.7122186422348022\n",
      "optimal threshold: -0.9414\n",
      "Epoch 298 train loss: 0.7546, eval loss 0.7118852138519287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 15:16:25,476] Trial 38 finished with value: 0.7441816926002502 and parameters: {'learning_rate_exp': -5.595589947618587, 'dropout_p': 0.3584345290971456, 'l2_reg_exp': -3.366915553117826, 'batch_size': 367, 'N': 149}. Best is trial 35 with value: 0.0636233314871788.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.9393\n",
      "Epoch 299 train loss: 0.7442, eval loss 0.7115673422813416\n",
      "optimal threshold: 0.1320\n",
      "Epoch 0 train loss: 1.5200, eval loss 1.5154668092727661\n",
      "optimal threshold: 0.0515\n",
      "Epoch 1 train loss: 1.4626, eval loss 1.4542115926742554\n",
      "optimal threshold: -0.0693\n",
      "Epoch 2 train loss: 1.3660, eval loss 1.3606717586517334\n",
      "optimal threshold: -0.2446\n",
      "Epoch 3 train loss: 1.2697, eval loss 1.2427796125411987\n",
      "optimal threshold: -0.4399\n",
      "Epoch 4 train loss: 1.1252, eval loss 1.1040576696395874\n",
      "optimal threshold: -0.7049\n",
      "Epoch 5 train loss: 1.0055, eval loss 0.9883855581283569\n",
      "optimal threshold: -0.8858\n",
      "Epoch 6 train loss: 0.9205, eval loss 0.9062071442604065\n",
      "optimal threshold: -0.9210\n",
      "Epoch 7 train loss: 0.8639, eval loss 0.8525134921073914\n",
      "optimal threshold: -0.9185\n",
      "Epoch 8 train loss: 0.8348, eval loss 0.8137374520301819\n",
      "optimal threshold: -0.8841\n",
      "Epoch 9 train loss: 0.8028, eval loss 0.7839487791061401\n",
      "optimal threshold: -0.8079\n",
      "Epoch 10 train loss: 0.8081, eval loss 0.7615095376968384\n",
      "optimal threshold: -0.9325\n",
      "Epoch 11 train loss: 0.7988, eval loss 0.7450770735740662\n",
      "optimal threshold: -0.7852\n",
      "Epoch 12 train loss: 0.8122, eval loss 0.7327865958213806\n",
      "optimal threshold: -0.7715\n",
      "Epoch 13 train loss: 0.8141, eval loss 0.7236778736114502\n",
      "optimal threshold: -0.7563\n",
      "Epoch 14 train loss: 0.7862, eval loss 0.7166609168052673\n",
      "optimal threshold: -0.8204\n",
      "Epoch 15 train loss: 0.7981, eval loss 0.7110022902488708\n",
      "optimal threshold: -0.7569\n",
      "Epoch 16 train loss: 0.7794, eval loss 0.7062000632286072\n",
      "optimal threshold: -0.7713\n",
      "Epoch 17 train loss: 0.7707, eval loss 0.7022874355316162\n",
      "optimal threshold: -0.6918\n",
      "Epoch 18 train loss: 0.7824, eval loss 0.6987891793251038\n",
      "optimal threshold: -0.7036\n",
      "Epoch 19 train loss: 0.7476, eval loss 0.6961317658424377\n",
      "optimal threshold: -0.7122\n",
      "Epoch 20 train loss: 0.7963, eval loss 0.693472146987915\n",
      "optimal threshold: -0.7061\n",
      "Epoch 21 train loss: 0.7603, eval loss 0.691114068031311\n",
      "optimal threshold: -0.7227\n",
      "Epoch 22 train loss: 0.7781, eval loss 0.6890429854393005\n",
      "optimal threshold: -0.7027\n",
      "Epoch 23 train loss: 0.7778, eval loss 0.6872724294662476\n",
      "optimal threshold: -0.7103\n",
      "Epoch 24 train loss: 0.7893, eval loss 0.685486912727356\n",
      "optimal threshold: -0.7120\n",
      "Epoch 25 train loss: 0.7553, eval loss 0.6839006543159485\n",
      "optimal threshold: -0.7325\n",
      "Epoch 26 train loss: 0.7790, eval loss 0.6827573180198669\n",
      "optimal threshold: -0.7111\n",
      "Epoch 27 train loss: 0.7158, eval loss 0.6813236474990845\n",
      "optimal threshold: -0.6651\n",
      "Epoch 28 train loss: 0.7798, eval loss 0.6801707148551941\n",
      "optimal threshold: -0.6570\n",
      "Epoch 29 train loss: 0.7602, eval loss 0.6789129376411438\n",
      "optimal threshold: -0.6652\n",
      "Epoch 30 train loss: 0.7522, eval loss 0.6778932809829712\n",
      "optimal threshold: -0.6597\n",
      "Epoch 31 train loss: 0.7344, eval loss 0.6770334243774414\n",
      "optimal threshold: -0.6467\n",
      "Epoch 32 train loss: 0.7645, eval loss 0.6759907603263855\n",
      "optimal threshold: -0.6603\n",
      "Epoch 33 train loss: 0.7628, eval loss 0.6753291487693787\n",
      "optimal threshold: -0.6974\n",
      "Epoch 34 train loss: 0.7438, eval loss 0.6746260523796082\n",
      "optimal threshold: -0.6824\n",
      "Epoch 35 train loss: 0.8087, eval loss 0.6738280653953552\n",
      "optimal threshold: -0.7061\n",
      "Epoch 36 train loss: 0.7343, eval loss 0.6733046770095825\n",
      "optimal threshold: -0.7026\n",
      "Epoch 37 train loss: 0.7550, eval loss 0.672654926776886\n",
      "optimal threshold: -0.7161\n",
      "Epoch 38 train loss: 0.7323, eval loss 0.6722164750099182\n",
      "optimal threshold: -0.7174\n",
      "Epoch 39 train loss: 0.7463, eval loss 0.6717245578765869\n",
      "optimal threshold: -0.5115\n",
      "Epoch 40 train loss: 0.7689, eval loss 0.6711170077323914\n",
      "optimal threshold: -0.5506\n",
      "Epoch 41 train loss: 0.7515, eval loss 0.6707375049591064\n",
      "optimal threshold: -0.7140\n",
      "Epoch 42 train loss: 0.6682, eval loss 0.6699910163879395\n",
      "optimal threshold: -0.5294\n",
      "Epoch 43 train loss: 0.7174, eval loss 0.669429361820221\n",
      "optimal threshold: -0.5237\n",
      "Epoch 44 train loss: 0.7781, eval loss 0.6690770983695984\n",
      "optimal threshold: -0.5257\n",
      "Epoch 45 train loss: 0.7369, eval loss 0.6685704588890076\n",
      "optimal threshold: -0.5019\n",
      "Epoch 46 train loss: 0.7269, eval loss 0.6680852174758911\n",
      "optimal threshold: -0.5217\n",
      "Epoch 47 train loss: 0.6897, eval loss 0.6678495407104492\n",
      "optimal threshold: -0.5112\n",
      "Epoch 48 train loss: 0.7326, eval loss 0.6674480438232422\n",
      "optimal threshold: -0.5328\n",
      "Epoch 49 train loss: 0.7032, eval loss 0.6671599745750427\n",
      "optimal threshold: -0.4299\n",
      "Epoch 50 train loss: 0.7244, eval loss 0.6667895913124084\n",
      "optimal threshold: -0.4788\n",
      "Epoch 51 train loss: 0.6792, eval loss 0.666456401348114\n",
      "optimal threshold: -0.4542\n",
      "Epoch 52 train loss: 0.6967, eval loss 0.666274905204773\n",
      "optimal threshold: -0.4566\n",
      "Epoch 53 train loss: 0.7236, eval loss 0.6659337878227234\n",
      "optimal threshold: -0.4344\n",
      "Epoch 54 train loss: 0.7615, eval loss 0.6655098795890808\n",
      "optimal threshold: -0.4612\n",
      "Epoch 55 train loss: 0.7035, eval loss 0.665341854095459\n",
      "optimal threshold: -0.4459\n",
      "Epoch 56 train loss: 0.7307, eval loss 0.6649869680404663\n",
      "optimal threshold: -0.4501\n",
      "Epoch 57 train loss: 0.7148, eval loss 0.6647836565971375\n",
      "optimal threshold: -0.4590\n",
      "Epoch 58 train loss: 0.7272, eval loss 0.6646420359611511\n",
      "optimal threshold: -0.4422\n",
      "Epoch 59 train loss: 0.7315, eval loss 0.6643495559692383\n",
      "optimal threshold: -0.4399\n",
      "Epoch 60 train loss: 0.7021, eval loss 0.6641093492507935\n",
      "optimal threshold: -0.4601\n",
      "Epoch 61 train loss: 0.7309, eval loss 0.6638970971107483\n",
      "optimal threshold: -0.4632\n",
      "Epoch 62 train loss: 0.7376, eval loss 0.6635941863059998\n",
      "optimal threshold: -0.4892\n",
      "Epoch 63 train loss: 0.7337, eval loss 0.663418173789978\n",
      "optimal threshold: -0.4199\n",
      "Epoch 64 train loss: 0.7366, eval loss 0.6632163524627686\n",
      "optimal threshold: -0.4991\n",
      "Epoch 65 train loss: 0.7292, eval loss 0.6631547212600708\n",
      "optimal threshold: -0.4838\n",
      "Epoch 66 train loss: 0.7255, eval loss 0.663072943687439\n",
      "optimal threshold: -0.4873\n",
      "Epoch 67 train loss: 0.6967, eval loss 0.6629493832588196\n",
      "optimal threshold: -0.4707\n",
      "Epoch 68 train loss: 0.7184, eval loss 0.6626629829406738\n",
      "optimal threshold: -0.4733\n",
      "Epoch 69 train loss: 0.7526, eval loss 0.6624298095703125\n",
      "optimal threshold: -0.4631\n",
      "Epoch 70 train loss: 0.7240, eval loss 0.6623654365539551\n",
      "optimal threshold: -0.4607\n",
      "Epoch 71 train loss: 0.7563, eval loss 0.6621755361557007\n",
      "optimal threshold: -0.4715\n",
      "Epoch 72 train loss: 0.7194, eval loss 0.6621642112731934\n",
      "optimal threshold: -0.4425\n",
      "Epoch 73 train loss: 0.7470, eval loss 0.6617361307144165\n",
      "optimal threshold: -0.4534\n",
      "Epoch 74 train loss: 0.7494, eval loss 0.6615962386131287\n",
      "optimal threshold: -0.4581\n",
      "Epoch 75 train loss: 0.7325, eval loss 0.6614904403686523\n",
      "optimal threshold: -0.4638\n",
      "Epoch 76 train loss: 0.6907, eval loss 0.6614106893539429\n",
      "optimal threshold: -0.4541\n",
      "Epoch 77 train loss: 0.7440, eval loss 0.6611738204956055\n",
      "optimal threshold: -0.4808\n",
      "Epoch 78 train loss: 0.6657, eval loss 0.6610746383666992\n",
      "optimal threshold: -0.4463\n",
      "Epoch 79 train loss: 0.7344, eval loss 0.6611360311508179\n",
      "optimal threshold: -0.4909\n",
      "Epoch 80 train loss: 0.7183, eval loss 0.6608738303184509\n",
      "optimal threshold: -0.4894\n",
      "Epoch 81 train loss: 0.7458, eval loss 0.6607847213745117\n",
      "optimal threshold: -0.4271\n",
      "Epoch 82 train loss: 0.7257, eval loss 0.6607728004455566\n",
      "optimal threshold: -0.4101\n",
      "Epoch 83 train loss: 0.7668, eval loss 0.6606715321540833\n",
      "optimal threshold: -0.4191\n",
      "Epoch 84 train loss: 0.6945, eval loss 0.660518229007721\n",
      "optimal threshold: -0.4294\n",
      "Epoch 85 train loss: 0.7398, eval loss 0.6603376865386963\n",
      "optimal threshold: -0.4294\n",
      "Epoch 86 train loss: 0.7060, eval loss 0.6602630019187927\n",
      "optimal threshold: -0.4270\n",
      "Epoch 87 train loss: 0.7217, eval loss 0.6602347493171692\n",
      "optimal threshold: -0.4122\n",
      "Epoch 88 train loss: 0.7539, eval loss 0.6601866483688354\n",
      "optimal threshold: -0.4053\n",
      "Epoch 89 train loss: 0.7243, eval loss 0.6601775288581848\n",
      "optimal threshold: -0.3891\n",
      "Epoch 90 train loss: 0.7462, eval loss 0.6599122285842896\n",
      "optimal threshold: -0.3963\n",
      "Epoch 91 train loss: 0.7371, eval loss 0.6599223017692566\n",
      "optimal threshold: -0.3850\n",
      "Epoch 92 train loss: 0.7356, eval loss 0.6598271131515503\n",
      "optimal threshold: -0.4391\n",
      "Epoch 93 train loss: 0.7630, eval loss 0.6596373319625854\n",
      "optimal threshold: -0.4327\n",
      "Epoch 94 train loss: 0.7248, eval loss 0.6596930623054504\n",
      "optimal threshold: -0.4445\n",
      "Epoch 95 train loss: 0.6882, eval loss 0.6595861911773682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4485\n",
      "Epoch 96 train loss: 0.7061, eval loss 0.6596529483795166\n",
      "optimal threshold: -0.4437\n",
      "Epoch 97 train loss: 0.7194, eval loss 0.6594359874725342\n",
      "optimal threshold: -0.4444\n",
      "Epoch 98 train loss: 0.7178, eval loss 0.6594053506851196\n",
      "optimal threshold: -0.4426\n",
      "Epoch 99 train loss: 0.7465, eval loss 0.6594688892364502\n",
      "optimal threshold: -0.4424\n",
      "Epoch 100 train loss: 0.6732, eval loss 0.6593799591064453\n",
      "optimal threshold: -0.4380\n",
      "Epoch 101 train loss: 0.6975, eval loss 0.6592943072319031\n",
      "optimal threshold: -0.8314\n",
      "Epoch 102 train loss: 0.7488, eval loss 0.6591641902923584\n",
      "optimal threshold: -0.8363\n",
      "Epoch 103 train loss: 0.7537, eval loss 0.6591712236404419\n",
      "optimal threshold: -0.4294\n",
      "Epoch 104 train loss: 0.7281, eval loss 0.6592199206352234\n",
      "optimal threshold: -0.8430\n",
      "Epoch 105 train loss: 0.7192, eval loss 0.6591770648956299\n",
      "optimal threshold: -0.8478\n",
      "Epoch 106 train loss: 0.6761, eval loss 0.6590042114257812\n",
      "optimal threshold: -0.8453\n",
      "Epoch 107 train loss: 0.6842, eval loss 0.658988893032074\n",
      "optimal threshold: -0.8494\n",
      "Epoch 108 train loss: 0.7033, eval loss 0.6590832471847534\n",
      "optimal threshold: -0.8581\n",
      "Epoch 109 train loss: 0.7086, eval loss 0.6589881777763367\n",
      "optimal threshold: -0.8520\n",
      "Epoch 110 train loss: 0.7425, eval loss 0.6588603854179382\n",
      "optimal threshold: -0.8327\n",
      "Epoch 111 train loss: 0.7039, eval loss 0.6589115262031555\n",
      "optimal threshold: -0.8338\n",
      "Epoch 112 train loss: 0.7192, eval loss 0.6589137315750122\n",
      "optimal threshold: -0.8650\n",
      "Epoch 113 train loss: 0.6940, eval loss 0.6587827205657959\n",
      "optimal threshold: -0.8339\n",
      "Epoch 114 train loss: 0.7065, eval loss 0.6588581204414368\n",
      "optimal threshold: -0.8355\n",
      "Epoch 115 train loss: 0.7005, eval loss 0.6588123440742493\n",
      "optimal threshold: -0.8297\n",
      "Epoch 116 train loss: 0.7758, eval loss 0.6586424112319946\n",
      "optimal threshold: -0.8309\n",
      "Epoch 117 train loss: 0.7160, eval loss 0.6586319208145142\n",
      "optimal threshold: -0.8301\n",
      "Epoch 118 train loss: 0.7498, eval loss 0.6586407423019409\n",
      "optimal threshold: -0.8255\n",
      "Epoch 119 train loss: 0.6821, eval loss 0.6587738990783691\n",
      "optimal threshold: -0.7500\n",
      "Epoch 120 train loss: 0.7039, eval loss 0.658841609954834\n",
      "optimal threshold: -0.7487\n",
      "Epoch 121 train loss: 0.7181, eval loss 0.658864438533783\n",
      "optimal threshold: -0.7522\n",
      "Epoch 122 train loss: 0.7346, eval loss 0.6588523387908936\n",
      "optimal threshold: -0.7589\n",
      "Epoch 123 train loss: 0.6764, eval loss 0.6588307023048401\n",
      "optimal threshold: -0.7489\n",
      "Epoch 124 train loss: 0.6776, eval loss 0.6584512591362\n",
      "optimal threshold: -0.7665\n",
      "Epoch 125 train loss: 0.6955, eval loss 0.6586452722549438\n",
      "optimal threshold: -0.7498\n",
      "Epoch 126 train loss: 0.6976, eval loss 0.6586189270019531\n",
      "optimal threshold: -0.7503\n",
      "Epoch 127 train loss: 0.7289, eval loss 0.6584721207618713\n",
      "optimal threshold: -0.7446\n",
      "Epoch 128 train loss: 0.7203, eval loss 0.6585941910743713\n",
      "optimal threshold: -0.4458\n",
      "Epoch 129 train loss: 0.7478, eval loss 0.6585938334465027\n",
      "optimal threshold: -0.4630\n",
      "Epoch 130 train loss: 0.6776, eval loss 0.6586794853210449\n",
      "optimal threshold: -0.4862\n",
      "Epoch 131 train loss: 0.7021, eval loss 0.6587421298027039\n",
      "optimal threshold: -0.4913\n",
      "Epoch 132 train loss: 0.7169, eval loss 0.6587130427360535\n",
      "optimal threshold: -0.7476\n",
      "Epoch 133 train loss: 0.7521, eval loss 0.6584612131118774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 15:17:35,815] Trial 39 finished with value: 0.6892420649528503 and parameters: {'learning_rate_exp': -4.305756190487817, 'dropout_p': 0.21381221392788663, 'l2_reg_exp': -2.9729853547357834, 'batch_size': 192, 'N': 60}. Best is trial 35 with value: 0.0636233314871788.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.7579\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize',sampler = optuna.samplers.RandomSampler())\n",
    "study.optimize(objective, n_trials=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0636233314871788\n",
      "Best hyperparameters: {'learning_rate_exp': -2.8889127901537135, 'dropout_p': 0.13286605028603324, 'l2_reg_exp': -5.338978135983149, 'batch_size': 124, 'N': 340}\n"
     ]
    }
   ],
   "source": [
    "trial = study.best_trial\n",
    "print('Accuracy: {}'.format(trial.value)) #0.9733333333333333\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))\n",
    "#{'n_estimators': 11, 'max_depth': 27.827767703750034}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold: -0.4738\n",
      "Epoch 0 train loss: 0.2777, eval loss 0.6617574095726013\n",
      "optimal threshold: -0.6907\n",
      "Epoch 1 train loss: 0.2368, eval loss 0.6581960320472717\n",
      "optimal threshold: -0.5227\n",
      "Epoch 2 train loss: 0.1929, eval loss 0.6584044694900513\n",
      "optimal threshold: -0.4840\n",
      "Epoch 3 train loss: 0.1254, eval loss 0.658599853515625\n",
      "optimal threshold: -0.5789\n",
      "Epoch 4 train loss: 0.1714, eval loss 0.6587426066398621\n",
      "optimal threshold: -0.5340\n",
      "Epoch 5 train loss: 0.1708, eval loss 0.6613927483558655\n",
      "optimal threshold: -0.5103\n",
      "Epoch 6 train loss: 0.1622, eval loss 0.666963517665863\n",
      "optimal threshold: -0.5872\n",
      "Epoch 7 train loss: 0.1078, eval loss 0.6697480082511902\n",
      "optimal threshold: -0.5992\n",
      "Epoch 8 train loss: 0.1434, eval loss 0.6801380515098572\n",
      "optimal threshold: -0.4574\n",
      "Epoch 9 train loss: 0.1017, eval loss 0.6824542284011841\n",
      "optimal threshold: -0.5340\n",
      "Epoch 10 train loss: 0.1226, eval loss 0.6877071857452393\n",
      "optimal threshold: -0.6188\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 300\n",
    "early_stopping_patience = 10\n",
    "model = MLP(\n",
    "    input_size=X_train.shape[1], \n",
    "    dropout_p=trial.params['dropout_p'],\n",
    "    N=trial.params['N']\n",
    ")\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=10**trial.params['learning_rate_exp'], \n",
    "    weight_decay=10**trial.params['learning_rate_exp']\n",
    ")\n",
    "positive_weight = torch.tensor(compute_class_weight(class_weight='balanced',classes=np.unique(y_train),y=np.ravel(y_train))[1:])\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(weight = positive_weight)\n",
    "\n",
    "train_dataset = MyDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=trial.params['batch_size'])\n",
    "\n",
    "steps_without_improvement = 0\n",
    "\n",
    "best_val_loss = np.inf\n",
    "best_model = None\n",
    "best_threshold = None\n",
    "\n",
    "for epoch_num in range(max_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # model training\n",
    "\n",
    "    # model evaluation, early stopping\n",
    "    valid_metrics=evaluate_model(\n",
    "    model, \n",
    "    X_valid, \n",
    "    y_valid, \n",
    "    loss_fn)\n",
    "    val_loss = valid_metrics['loss']\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        steps_without_improvement = 0\n",
    "        best_model = deepcopy(model)\n",
    "        best_threshold = valid_metrics['threshold']\n",
    "    else:\n",
    "        steps_without_improvement += 1\n",
    "    if early_stopping_patience == steps_without_improvement:\n",
    "        break\n",
    "    print(f\"Epoch {epoch_num} train loss: {loss.item():.4f}, eval loss {valid_metrics['loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPE Sampler:\n",
      "AUROC: 90.78%\n",
      "F1: 69.57%\n",
      "Precision: 60.97%\n",
      "Recall: 80.99%\n"
     ]
    }
   ],
   "source": [
    "test_metrics = evaluate_model(best_model, X_test, y_test, loss_fn, best_threshold)\n",
    "print(\"Random Sampler:\")\n",
    "print(f\"AUROC: {100 * test_metrics['AUROC']:.2f}%\")\n",
    "print(f\"F1: {100 * test_metrics['F1-score']:.2f}%\")\n",
    "print(f\"Precision: {100 * test_metrics['precision']:.2f}%\")\n",
    "print(f\"Recall: {100 * test_metrics['recall']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wyniki okazują się bardzo podobne, TPE osiąga nieznacznie lepsze wyniki AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a5d7af91182035c53be6efb3f9b18ffc3e259c9c524705249407647c970de949"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
