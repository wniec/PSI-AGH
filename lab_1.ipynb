{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresja liniowa i logistyczna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wstęp\n",
    "\n",
    "Celem tego laboratorium będzie stworzenie modelu uczenia maszynowego do estymacji cen nieruchomości na podstawie danych o jej położeniu, ilości sypialń, roku budowy, typie budynku oraz wielu innych parametrów.\n",
    "\n",
    "![house-price-gif](house-price.gif)\n",
    "\n",
    "W trakcie realizacji tego labratorium zapoznamy się z następującymi zagadnieniami:\n",
    "\n",
    "* przygotowaniem danych:\n",
    "    * ładowaniem danych,\n",
    "    * typami danych,\n",
    "    * czyszczeniem danych,\n",
    "    * rozkładami danych,\n",
    "    * obsługą wartości brakujących,\n",
    "    * zmiennymi kategorycznymi uporządkowanymi i nieuporządkowanymi,\n",
    "    * skalowaniem wartości,\n",
    "    * API biblioteki Scikit-Learn dla transformacji danych;\n",
    "* regresją liniową, w szczególności z:\n",
    "    * podziałem zbioru na część treningową i testową,\n",
    "    * oceną jakości modelu,\n",
    "    * walidacją skrośną,\n",
    "    * wyszukiwaniem hiperparametrów,\n",
    "    * problemem przeuczenia, niedouczenia,\n",
    "    * regularyzacją L1 i L2,\n",
    "    * regresją wielomianową;\n",
    "* regresją logistyczną, w szczególności z:\n",
    "    * różnymi rodzajami błędów klasyfikacji,\n",
    "    * metrykami oceniającymi jakość klasyfikatorów.\n",
    "\n",
    "Na pierwszych zajęciach możesz korzystać ze środowiska Google Colab i zdalnego środowiska obliczeniowego. Jeżeli interesuje Cię skonfigurowanie Pythona  na własnym komputerze, to niezbędne informacje są podane w sekcji \"Konfiguracja własnego komputera\".\n",
    "\n",
    "**Uwaga:** niektóre zadania zamiast kodu wymagają podania pisemnej odpowiedzi w miejscu oznaczonym `// skomentuj tutaj`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykorzystywane biblioteki\n",
    "\n",
    "Na zajęciach korzystać będziesz z kilku popularnych bibliotek Pythona, które umożliwiają klasyfikację danych, ich wizualizację czy preprocessing. Są to:\n",
    "\n",
    "* [numpy](https://numpy.org/) - bibliotek do wykonywania obliczeń macierzowych. Pozwala na efektywne przeprowadzanie obliczeń naukowych. Dobrze współgra z biblioteką pandas.\n",
    "* [pandas](https://pandas.pydata.org/) - narzędzie do analizy danych tabelarycznych, ich strukturyzowania oraz manipulacji na nich.\n",
    "* [sklearn](https://scikit-learn.org/stable/) - narzędzie do tworzenia modeli klasyfikacji, regresji, clusteringu itp. Biblioteka ta jest dość rozbudowana i pozwala także na mapowanie danych czy redukcję wymiarów. Więcej informacji znajdziesz w podanym linku.\n",
    "* [missingno](https://pypi.org/project/missingno/) - narzędzie do wizualizacji kompletności danych (brakujących wartości).\n",
    "* [seaborn](https://seaborn.pydata.org/) - kompleksowe narzędzie do wizualizacji danych jako takich. Pozwala na stworzenie bardzo szerokiej gamy wykresów w zależności od potrzeb.\n",
    "\n",
    "Zostały tutaj pominięte pewne standardowe biblioteki jak np. `os` czy `matplotlib`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykorzystanie Google Colab\n",
    "\n",
    "Korzystanie Google Colab nie jest wymagane. W niektórych laboratorich może być jednak przydatny dostęp do środowiska wyposażonego w kartę GPU.\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/apohllo/sztuczna-inteligencja/blob/master/lab1/lab_1.ipynb)\n",
    "\n",
    "Jeżeli pracujesz na Google Colab, zacznij od przeniesienia dwóch plików CSV, które zostały dołączone do laboratorium ([ames_data.csv](ames_data.csv) oraz [bank_marketing_data.csv](bank_marketing_data.csv)), do folderu `/content`. Nie musisz ich umieszczać w `/content/sample_data` - ważne, aby znalazły się w `/content`. Jeżeli pracujesz lokalnie, to wystarczy, że pliki te będą obok tego notebooka.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konfiguracja własnego komputera\n",
    "\n",
    "Jeżeli korzystasz z własnego komputera, to musisz zainstalować trochę więcej bibliotek (Google Colab ma je już zainstalowane). Najlepiej używać Pythona 3.9 lub nowszej wersji. Laboratorium było testowane z wersją 3.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anaconda\n",
    "\n",
    "Jeżeli korzystasz z Anacondy (możesz uruchomić w terminalu):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T17:35:49.000694662Z",
     "start_time": "2023-09-15T17:35:48.923664169Z"
    }
   },
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge --yes numpy pandas scikit-learn matplotlib missingno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### venv\n",
    "\n",
    "Jeżeli używasz zwykłego venv'a (**zdecydowanie niezalecane, szczególnie na Windowsie**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T17:35:49.476264618Z",
     "start_time": "2023-09-15T17:35:49.441019250Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install --yes numpy pandas scikit-learn matplotlib missingno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przypadku własnego komputera, jeżeli instalowałeś z terminala, pamiętaj, aby zarejestrować aktualne środowisko wirtualne jako kernel (środowisko uruchomieniowe) dla Jupyter Notebooka. Wybierz go jako używany kernel w menu na górze notebooka (nazwa jak w komendzie poniżej)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T17:35:50.025546447Z",
     "start_time": "2023-09-15T17:35:49.999161583Z"
    }
   },
   "outputs": [],
   "source": [
    "#!ipython kernel install --user --name \"PSI\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zbiór danych do regresji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykorzystamy zbiór danych [Ames housing](https://www.openintro.org/book/statdata/?data=ames), w którym zadaniem jest przewidywanie wartości domu na podstawie cech budynku, działki, lokalizacji itp. Jest to więc przewidywanie wartości ciągłej, czyli regresja. Zbiór ten zawiera zmienne numeryczne (floaty i inty), kategoryczne nieuporządkowane (*categorical nominal*) oraz kategoryczne uporządkowane (*categorical ordinal*), więc będzie wymagał wstępnego przetworzenia tak jak większość prawdziwych danych w uczeniu maszynowym.\n",
    "\n",
    "Inne znane, ale gorsze jakościowo zbiory tego typu, to na przykład:\n",
    "- Boston housing - rasistowski, z tego powodu usunięty np. ze Scikit-learn ([wyjaśnienie](https://fairlearn.org/main/user_guide/datasets/boston_housing_data.html), [dyskusja](https://github.com/quantumblacklabs/causalnex/issues/92), [badanie](https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8))\n",
    "- California housing - zbyt prosty (tylko kilka zmiennych numerycznych), użyty np. w książce \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" A. Geron ([opis](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html))\n",
    "\n",
    "Autor zbioru to Dean De Cock, a zbiór został opisany oryginalnie w [tym artykule](https://jse.amstat.org/v19n3/decock.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T18:11:58.167689459Z",
     "start_time": "2023-09-15T18:11:57.378074504Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ładowanie danych tabelarycznych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pliki [ames_data.csv](ames_data.csv) oraz [bank_marketing_data.csv](bank_marketing_data.csv) to dwa zbiory danych, niezależne od siebie. Pierwszy jest wykorzystywany w pierwszej części laboratorium (regresji liniowej), natomiast drugi przyda się przy regresji logistycznej (klasyfikacji). Jego celem jest przewidywanie wartości domu.\n",
    "\n",
    "Wczytajmy dane `ames_data.csv` do zmiennej `df` (takiej nazwy często się używa, żeby oznaczyć obiekt `DataFrame` - zaawansowanej tablicy, dostarczonej nam przez bibliotekę `pandas`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T21:17:54.649270399Z",
     "start_time": "2023-09-15T21:17:54.491837233Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ames_data.csv\")\n",
    "\n",
    "# remove dots from names to match data_description.txt\n",
    "df.columns = [col.replace(\".\", \"\") for col in df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zobaczmy jakie dane znajdują się w naszej tabeli. Wykorzystajmy do tego metodę `info()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T21:17:54.959663968Z",
     "start_time": "2023-09-15T21:17:54.871675198Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2930 entries, 0 to 2929\n",
      "Data columns (total 82 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Order          2930 non-null   int64  \n",
      " 1   PID            2930 non-null   int64  \n",
      " 2   MSSubClass     2930 non-null   int64  \n",
      " 3   MSZoning       2930 non-null   object \n",
      " 4   LotFrontage    2440 non-null   float64\n",
      " 5   LotArea        2930 non-null   int64  \n",
      " 6   Street         2930 non-null   object \n",
      " 7   Alley          198 non-null    object \n",
      " 8   LotShape       2930 non-null   object \n",
      " 9   LandContour    2930 non-null   object \n",
      " 10  Utilities      2930 non-null   object \n",
      " 11  LotConfig      2930 non-null   object \n",
      " 12  LandSlope      2930 non-null   object \n",
      " 13  Neighborhood   2930 non-null   object \n",
      " 14  Condition1     2930 non-null   object \n",
      " 15  Condition2     2930 non-null   object \n",
      " 16  BldgType       2930 non-null   object \n",
      " 17  HouseStyle     2930 non-null   object \n",
      " 18  OverallQual    2930 non-null   int64  \n",
      " 19  OverallCond    2930 non-null   int64  \n",
      " 20  YearBuilt      2930 non-null   int64  \n",
      " 21  YearRemodAdd   2930 non-null   int64  \n",
      " 22  RoofStyle      2930 non-null   object \n",
      " 23  RoofMatl       2930 non-null   object \n",
      " 24  Exterior1st    2930 non-null   object \n",
      " 25  Exterior2nd    2930 non-null   object \n",
      " 26  MasVnrType     2907 non-null   object \n",
      " 27  MasVnrArea     2907 non-null   float64\n",
      " 28  ExterQual      2930 non-null   object \n",
      " 29  ExterCond      2930 non-null   object \n",
      " 30  Foundation     2930 non-null   object \n",
      " 31  BsmtQual       2850 non-null   object \n",
      " 32  BsmtCond       2850 non-null   object \n",
      " 33  BsmtExposure   2847 non-null   object \n",
      " 34  BsmtFinType1   2850 non-null   object \n",
      " 35  BsmtFinSF1     2929 non-null   float64\n",
      " 36  BsmtFinType2   2849 non-null   object \n",
      " 37  BsmtFinSF2     2929 non-null   float64\n",
      " 38  BsmtUnfSF      2929 non-null   float64\n",
      " 39  TotalBsmtSF    2929 non-null   float64\n",
      " 40  Heating        2930 non-null   object \n",
      " 41  HeatingQC      2930 non-null   object \n",
      " 42  CentralAir     2930 non-null   object \n",
      " 43  Electrical     2929 non-null   object \n",
      " 44  X1stFlrSF      2930 non-null   int64  \n",
      " 45  X2ndFlrSF      2930 non-null   int64  \n",
      " 46  LowQualFinSF   2930 non-null   int64  \n",
      " 47  GrLivArea      2930 non-null   int64  \n",
      " 48  BsmtFullBath   2928 non-null   float64\n",
      " 49  BsmtHalfBath   2928 non-null   float64\n",
      " 50  FullBath       2930 non-null   int64  \n",
      " 51  HalfBath       2930 non-null   int64  \n",
      " 52  BedroomAbvGr   2930 non-null   int64  \n",
      " 53  KitchenAbvGr   2930 non-null   int64  \n",
      " 54  KitchenQual    2930 non-null   object \n",
      " 55  TotRmsAbvGrd   2930 non-null   int64  \n",
      " 56  Functional     2930 non-null   object \n",
      " 57  Fireplaces     2930 non-null   int64  \n",
      " 58  FireplaceQu    1508 non-null   object \n",
      " 59  GarageType     2773 non-null   object \n",
      " 60  GarageYrBlt    2771 non-null   float64\n",
      " 61  GarageFinish   2771 non-null   object \n",
      " 62  GarageCars     2929 non-null   float64\n",
      " 63  GarageArea     2929 non-null   float64\n",
      " 64  GarageQual     2771 non-null   object \n",
      " 65  GarageCond     2771 non-null   object \n",
      " 66  PavedDrive     2930 non-null   object \n",
      " 67  WoodDeckSF     2930 non-null   int64  \n",
      " 68  OpenPorchSF    2930 non-null   int64  \n",
      " 69  EnclosedPorch  2930 non-null   int64  \n",
      " 70  X3SsnPorch     2930 non-null   int64  \n",
      " 71  ScreenPorch    2930 non-null   int64  \n",
      " 72  PoolArea       2930 non-null   int64  \n",
      " 73  PoolQC         13 non-null     object \n",
      " 74  Fence          572 non-null    object \n",
      " 75  MiscFeature    106 non-null    object \n",
      " 76  MiscVal        2930 non-null   int64  \n",
      " 77  MoSold         2930 non-null   int64  \n",
      " 78  YrSold         2930 non-null   int64  \n",
      " 79  SaleType       2930 non-null   object \n",
      " 80  SaleCondition  2930 non-null   object \n",
      " 81  SalePrice      2930 non-null   int64  \n",
      "dtypes: float64(11), int64(28), object(43)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mamy naprawdę dużo cech! Ich szczegółowy opis znajdziesz w dołączonym do laboratorium pliku [ames_description.txt](ames_description.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wstępna analiza danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zawsze, zanim zaczniesz robić jakąkolwiek predykcję czy analizę danych, dobrze jest zapoznać się z nimi, z ich kodowaniem i znaczeniem. Kolejnym istotnym aspektem jest typ danych. Nie każdy klasyfikator nadaje się do każdego typu.\n",
    "\n",
    "Wyświetlmy teraz kilka przykładowych rekordów z początku pliku, korzystając z metody `head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T21:17:55.507220376Z",
     "start_time": "2023-09-15T21:17:55.401007891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order</th>\n",
       "      <th>PID</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>526301100</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>141.0</td>\n",
       "      <td>31770</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>526350040</td>\n",
       "      <td>20</td>\n",
       "      <td>RH</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>526351010</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gar2</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>526353030</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>93.0</td>\n",
       "      <td>11160</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>244000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>527105010</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>189900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Order        PID  MSSubClass MSZoning  LotFrontage  LotArea Street Alley  \\\n",
       "0      1  526301100          20       RL        141.0    31770   Pave   NaN   \n",
       "1      2  526350040          20       RH         80.0    11622   Pave   NaN   \n",
       "2      3  526351010          20       RL         81.0    14267   Pave   NaN   \n",
       "3      4  526353030          20       RL         93.0    11160   Pave   NaN   \n",
       "4      5  527105010          60       RL         74.0    13830   Pave   NaN   \n",
       "\n",
       "  LotShape LandContour  ... PoolArea PoolQC  Fence MiscFeature MiscVal MoSold  \\\n",
       "0      IR1         Lvl  ...        0    NaN    NaN         NaN       0      5   \n",
       "1      Reg         Lvl  ...        0    NaN  MnPrv         NaN       0      6   \n",
       "2      IR1         Lvl  ...        0    NaN    NaN        Gar2   12500      6   \n",
       "3      Reg         Lvl  ...        0    NaN    NaN         NaN       0      4   \n",
       "4      IR1         Lvl  ...        0    NaN  MnPrv         NaN       0      3   \n",
       "\n",
       "  YrSold SaleType  SaleCondition  SalePrice  \n",
       "0   2010      WD          Normal     215000  \n",
       "1   2010      WD          Normal     105000  \n",
       "2   2010      WD          Normal     172000  \n",
       "3   2010      WD          Normal     244000  \n",
       "4   2010      WD          Normal     189900  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeżeli potrzebujesz szybko stwierdzić, ile dane zawierają rekordów i kolumn, pomocna jest opcja `shape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T21:17:55.871716894Z",
     "start_time": "2023-09-15T21:17:55.857639555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2930, 82)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eksploracja danych, czyszczenie danych i inżynieria cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usunięcie niepotrzebnych kolumn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Niektóre kolumny są **nieinformatywne (uninformative)**, czyli nie niosą żadnej informacji dla zadania, czyli przewidywania wartości domu. Są pewnym rodzajem metadanych. Przykładowo mamy tutaj kolumny **Order** oraz **PID**.\n",
    "\n",
    "**Order** jest po prostu numerem rekordu w zbiorze danych, moglibyśmy przetasować cały zbiór i to nie powinno w żaden sposób wpłynąć na cokolwiek, a więc możemy spokojnie tę kolumnę usunąć.\n",
    "\n",
    "Formalnie czynimy założenie, że rekordy w naszych danych (próbki / wiersze, poszczególne domy w przypadku tego zbioru) są **niezależne i równomiernie rozłożone** (ang. **independent and identically distributed - i.i.d.**). Innymi słowy, kolejność w danych nie ma znaczenia, bo zbieraliśmy dane taką samą metodą i w identycznych warunkach. Jest to bardzo typowe w ML.\n",
    "\n",
    "**PID** jest po prostu numerem identyfikacyjnym danej nieruchomości w systemie informatycznym, a więc też możemy to usunąć."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T21:17:56.307582736Z",
     "start_time": "2023-09-15T21:17:56.154655657Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop([\"Order\", \"PID\"], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usunięcie słabo reprezentowanych dzielnic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dzielnice *GrnHill* oraz *Landmrk* obejmują w sumie zaledwie 3 domy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T21:17:56.788664312Z",
     "start_time": "2023-09-15T21:17:56.700703449Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.loc[~df[\"Neighborhood\"].isin([\"GrnHill\", \"Landmrk\"]), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usunięcie obserwacji odstających (outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usuniemy budynki, które mają powyżej 4000 stóp kwadratowych (ok. 370 metrów kwadratowych) powierzchni. Możemy zobaczyć je na wykresie poniżej. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T21:17:58.554777139Z",
     "start_time": "2023-09-15T21:17:58.245198347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df[\"GrLivArea\"], df[\"SalePrice\"])\n",
    "plt.title(\"House area vs price\")\n",
    "plt.xlabel(\"GrLivArea\")\n",
    "plt.ylabel(\"SalePrice\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać na wykresie, jest dosłownie kilka domów o tej powierzhcni. Takie skrajne przypadki raczej nas nie interesują - a na pewno stanowią problem dla tak prostego modelu jak regresja logistyczna. Nie chcemy też, żeby nasz model uczył się takich anomalii, więc lepiej je usunąć.\n",
    "\n",
    "Tutaj robimy to ręcznie, ale istnieją też algorytmy do detekcji i usuwania obserwacji odstających."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 1 (0.25 punktu)**\n",
    "\n",
    "Usuń rekordy nieruchomości o powierzchni (**GrLivArea**) ponad (ostra nierówność) 4 tys. stóp kwadratowych.\n",
    "\n",
    "Podpowiedź: w Pandas korzysta się z `.loc[]` do filtrowania wierszy i kolumn. Pierwszy indeks oznacza, które wiersze zostawić, a drugi indeks, które kolumny wybrać. Jeżeli chcemy zostawić wszystko (np. nie usuwać żadnych kolumn), to zadziała standardowy Pythonowy `:`, jak przy indeksowaniu list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T21:17:59.547691643Z",
     "start_time": "2023-09-15T21:17:59.468151807Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "# your_code\n",
    "df = df.loc[df[\"GrLivArea\"]<=4000,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zobaczmy jak teraz wygląda ten sam wykres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T21:18:00.523344768Z",
     "start_time": "2023-09-15T21:18:00.374304098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df[\"GrLivArea\"], df[\"SalePrice\"])\n",
    "plt.title(\"House area vs price, outliers removed\")\n",
    "plt.xlabel(\"GrLivArea\")\n",
    "plt.ylabel(\"SalePrice\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformacja logarytmiczna zmiennej zależnej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zawsze warto też przyjrzeć się rozkładowi zmiennej docelowej, żeby poznać jej typ i skalę. Jak widać poniżej, rozkład jest dość skośny, co ma sens - mało jest bardzo drogich domów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T21:18:01.827131630Z",
     "start_time": "2023-09-15T21:18:01.782774803Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      2922.000000\n",
       "mean     180358.266940\n",
       "std       78536.952287\n",
       "min       12789.000000\n",
       "25%      129425.000000\n",
       "50%      160000.000000\n",
       "75%      213430.000000\n",
       "max      625000.000000\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"SalePrice\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T21:18:03.121984012Z",
     "start_time": "2023-09-15T21:18:02.976688658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"SalePrice\"].hist()\n",
    "plt.title(\"Original sale price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozkład normalny jest zwykle korzystniejszy dla tworzenia modeli, bo daje sensowną \"wartość środkową\" do przewidywania, a także penalizuje tak samo błędy niezależnie od ich znaku (zaniżona i zawyżona predykcja). Dokonamy dlatego **transformacji logarytmicznej (log transform)**, czyli zlogarytmujemy zmienną docelową (zależną). Dla stabilności numerycznej używa się zwykle `np.log1p`, a nie `np.log` (tutaj [wyjaśnienie](https://stackoverflow.com/questions/49538185/purpose-of-numpy-log1p)).\n",
    "\n",
    "Dodatkowa korzyść z takiej transformacji jest taka, że regresja liniowa przewiduje dowolne wartości rzeczywiste. Po przekształceniu logarytmicznym jest to całkowicie ok, natomiast w oryginalnej przestrzeni trzeba by wymusić przewidywanie tylko wartości pozytywnych (negatywne ceny są bez sensu). Da się to zrobić, ale zwiększa to koszt obliczeniowy. Operowanie na tzw. log-price jest bardzo częste w finansach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 2 (0.25 punktu)**\n",
    "\n",
    "Przekształć zmienną **SalePrice** za pomocą funkcji logarytmicznej `np.log1p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T21:29:33.982163197Z",
     "start_time": "2023-09-15T21:29:33.815793944Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply log transform\n",
    "# your_code\n",
    "df[\"SalePrice\"] = np.log1p(df[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy teraz jak rozkład **SalePrice** wygląda po transformacji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T21:29:35.216598401Z",
     "start_time": "2023-09-15T21:29:35.051482952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(df[\"SalePrice\"]).hist()\n",
    "plt.title(\"Log sale price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uzupełnianie wartości brakujących"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy też wartości brakujące. Są zmienne, które mają poniżej 10% wartości - takie zmienne dla modeli regresji liniowej są po prostu bezużyteczne, ponieważ brakujących wartości nie można wprost zamodelować. Znacząca liczba cech ma jednak co najwyżej 10% braków. Z nich będziemy jednak starali się zrobić użytek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T21:37:04.538950013Z",
     "start_time": "2023-09-15T21:37:01.638196543Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of FixedLocator locations (0), usually from a call to set_ticks, does not match the number of ticklabels (80).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmissingno\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmsno\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmsno\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\missingno\\missingno.py:266\u001b[0m, in \u001b[0;36mbar\u001b[1;34m(df, figsize, fontsize, labels, log, color, inline, filter, n, p, sort, ax)\u001b[0m\n\u001b[0;32m    264\u001b[0m ax3\u001b[38;5;241m.\u001b[39mset_xticks(ax1\u001b[38;5;241m.\u001b[39mget_xticks())\n\u001b[0;32m    265\u001b[0m ax3\u001b[38;5;241m.\u001b[39mset_xlim(ax1\u001b[38;5;241m.\u001b[39mget_xlim())\n\u001b[1;32m--> 266\u001b[0m \u001b[43max3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_xticklabels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnullity_counts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfontsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfontsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m45\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m ax3\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m axes:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py:75\u001b[0m, in \u001b[0;36m_axis_method_wrapper.__set_name__.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_method(\u001b[38;5;28mself\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axis.py:1798\u001b[0m, in \u001b[0;36mAxis._set_ticklabels\u001b[1;34m(self, labels, fontdict, minor, **kwargs)\u001b[0m\n\u001b[0;32m   1796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fontdict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1797\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(fontdict)\n\u001b[1;32m-> 1798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_ticklabels(labels, minor\u001b[38;5;241m=\u001b[39mminor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axis.py:1720\u001b[0m, in \u001b[0;36mAxis.set_ticklabels\u001b[1;34m(self, ticklabels, minor, **kwargs)\u001b[0m\n\u001b[0;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(locator, mticker\u001b[38;5;241m.\u001b[39mFixedLocator):\n\u001b[0;32m   1717\u001b[0m     \u001b[38;5;66;03m# Passing [] as a list of ticklabels is often used as a way to\u001b[39;00m\n\u001b[0;32m   1718\u001b[0m     \u001b[38;5;66;03m# remove all tick labels, so only error for > 0 ticklabels\u001b[39;00m\n\u001b[0;32m   1719\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(locator\u001b[38;5;241m.\u001b[39mlocs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ticklabels) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ticklabels) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1720\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1721\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of FixedLocator locations\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1722\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(locator\u001b[38;5;241m.\u001b[39mlocs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), usually from a call to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1723\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m set_ticks, does not match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1724\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the number of ticklabels (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ticklabels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1725\u001b[0m     tickd \u001b[38;5;241m=\u001b[39m {loc: lab \u001b[38;5;28;01mfor\u001b[39;00m loc, lab \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(locator\u001b[38;5;241m.\u001b[39mlocs, ticklabels)}\n\u001b[0;32m   1726\u001b[0m     func \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_with_dict, tickd)\n",
      "\u001b[1;31mValueError\u001b[0m: The number of FixedLocator locations (0), usually from a call to set_ticks, does not match the number of ticklabels (80)."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import missingno as msno\n",
    "\n",
    "msno.bar(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W ramach dalszego czyszczenia danych automatycznie uzupełnimy wartości brakujące. Trzeba tu jednak wziąć pod uwagę:\n",
    "- zmienne kategoryczne - nie można w nich dokonać zastąpienia wartości brakującej średnią, medianą itp.\n",
    "- wiele brakujących wartości - estymacja modą czy medianą byłaby niedokładna,\n",
    "- możliwość wykorzystania wiedzy o innych zmiennych na podstawie opisu cech.\n",
    "\n",
    "Można więc zastosować odpowiednią wiedzę i przyjąć wartości domyślne. Przykładowo, brak informacji o powierzchni piwnicy możemy uznać po prostu za brak piwnicy i wpisać tam odpowiednią wartość. W przypadku niektórych zmiennych może doprowadzić to do stworzenia nowej wartości, która implicite będzie reprezentować wartość brakującą.\n",
    "\n",
    "Znaczna część poniższej analizy została zainspirowana [tym notebookiem na Kaggle](https://www.kaggle.com/code/juliencs/a-study-on-regression-applied-to-the-ames-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T23:24:27.212091582Z",
     "start_time": "2023-09-15T23:24:27.122178104Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace_na(df: pd.DataFrame, col: str, value) -> None:\n",
    "    df.loc[:, col] = df.loc[:, col].fillna(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T23:24:27.599968103Z",
     "start_time": "2023-09-15T23:24:27.445745089Z"
    }
   },
   "outputs": [],
   "source": [
    "# Alley : data description says NA means \"no alley access\"\n",
    "replace_na(df, \"Alley\", value=\"None\")\n",
    "\n",
    "# BedroomAbvGr : NA most likely means 0\n",
    "replace_na(df, \"BedroomAbvGr\", value=0)\n",
    "\n",
    "# BsmtQual etc : data description says NA for basement features is \"no basement\"\n",
    "replace_na(df, \"BsmtQual\", value=\"No\")\n",
    "replace_na(df, \"BsmtCond\", value=\"No\")\n",
    "replace_na(df, \"BsmtExposure\", value=\"No\")\n",
    "replace_na(df, \"BsmtFinType1\", value=\"No\")\n",
    "replace_na(df, \"BsmtFinType2\", value=\"No\")\n",
    "replace_na(df, \"BsmtFullBath\", value=0)\n",
    "replace_na(df, \"BsmtHalfBath\", value=0)\n",
    "replace_na(df, \"BsmtUnfSF\", value=0)\n",
    "\n",
    "# Condition : NA most likely means Normal\n",
    "replace_na(df, \"Condition1\", value=\"Norm\")\n",
    "replace_na(df, \"Condition2\", value=\"Norm\")\n",
    "\n",
    "# External stuff : NA most likely means average\n",
    "replace_na(df, \"ExterCond\", value=\"TA\")\n",
    "replace_na(df, \"ExterQual\", value=\"TA\")\n",
    "\n",
    "# Fence : data description says NA means \"no fence\"\n",
    "replace_na(df, \"Fence\", value=\"No\")\n",
    "\n",
    "# Functional : data description says NA means typical\n",
    "replace_na(df, \"Functional\", value=\"Typ\")\n",
    "\n",
    "# GarageType etc : data description says NA for garage features is \"no garage\"\n",
    "replace_na(df, \"GarageType\", value=\"No\")\n",
    "replace_na(df, \"GarageFinish\", value=\"No\")\n",
    "replace_na(df, \"GarageQual\", value=\"No\")\n",
    "replace_na(df, \"GarageCond\", value=\"No\")\n",
    "replace_na(df, \"GarageArea\", value=0)\n",
    "replace_na(df, \"GarageCars\", value=0)\n",
    "\n",
    "# HalfBath : NA most likely means no half baths above grade\n",
    "replace_na(df, \"HalfBath\", value=0)\n",
    "\n",
    "# HeatingQC : NA most likely means typical\n",
    "replace_na(df, \"HeatingQC\", value=\"Ta\")\n",
    "\n",
    "# KitchenAbvGr : NA most likely means 0\n",
    "replace_na(df, \"KitchenAbvGr\", value=0)\n",
    "\n",
    "# KitchenQual : NA most likely means typical\n",
    "replace_na(df, \"KitchenQual\", value=\"TA\")\n",
    "\n",
    "# LotFrontage : NA most likely means no lot frontage\n",
    "replace_na(df, \"LotFrontage\", value=0)\n",
    "\n",
    "# LotShape : NA most likely means regular\n",
    "replace_na(df, \"LotShape\", value=\"Reg\")\n",
    "\n",
    "# MasVnrType : NA most likely means no veneer\n",
    "replace_na(df, \"MasVnrType\", value=\"None\")\n",
    "replace_na(df, \"MasVnrArea\", value=0)\n",
    "\n",
    "# MiscFeature : data description says NA means \"no misc feature\"\n",
    "replace_na(df, \"MiscFeature\", value=\"No\")\n",
    "replace_na(df, \"MiscVal\", value=0)\n",
    "\n",
    "# OpenPorchSF : NA most likely means no open porch\n",
    "replace_na(df, \"OpenPorchSF\", value=0)\n",
    "\n",
    "# PavedDrive : NA most likely means not paved\n",
    "replace_na(df, \"PavedDrive\", value=\"N\")\n",
    "\n",
    "# PoolQC : data description says NA means \"no pool\"\n",
    "replace_na(df, \"PoolQC\", value=\"No\")\n",
    "replace_na(df, \"PoolArea\", value=0)\n",
    "\n",
    "# SaleCondition : NA most likely means normal sale\n",
    "replace_na(df, \"SaleCondition\", value=\"Normal\")\n",
    "\n",
    "# ScreenPorch : NA most likely means no screen porch\n",
    "replace_na(df, \"ScreenPorch\", value=0)\n",
    "\n",
    "# TotRmsAbvGrd : NA most likely means 0\n",
    "replace_na(df, \"TotRmsAbvGrd\", value=0)\n",
    "\n",
    "# Utilities : NA most likely means all public utilities\n",
    "replace_na(df, \"Utilities\", value=\"AllPub\")\n",
    "\n",
    "# WoodDeckSF : NA most likely means no wood deck\n",
    "replace_na(df, \"WoodDeckSF\", value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przypadku wykonywania tego typu zmian - o ile istnieje taka możliwość - warto rozważyć różne interpretacje brakujących wartości. Może okazać się, że przyjęte przez nas założenia są błędne i prowadzą do pogorszenia działania modelu. Dlatego warto porównać jakoś predykcji z danymi uzupełnionymi oraz z danymi, w których kolumna z brakującymi wartościami jest po prostu usuwana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 3 (0.5 punktu)**\n",
    "\n",
    "Z pomocą dokumentacji zmiennych w pliku [ames_description.txt](ames_description.txt) zdecyduj, jakie wartości domyślne przypisać zmiennym:\n",
    "- `CentralAir`\n",
    "- `EnclosedPorch`\n",
    "- `FireplaceQu` oraz `Fireplaces`\n",
    "- `SaleCondition`\n",
    "\n",
    "W praktyce niestety zwykle nie jest tak łatwo, że mamy dokumentację i ten krok zajmuje kilka godzin (lub dni) konsultacji z różnymi osobami w firmie :) \n",
    "Czasami w ogóle nie da się ustalić jaka wartość byłaby sensowna, ponieważ nie mamy żadnego dostępu do osób odpowiedzialnych za przygotowanie wykorzystywanego zbioru danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T23:24:29.533701007Z",
     "start_time": "2023-09-15T23:24:29.415321162Z"
    }
   },
   "outputs": [],
   "source": [
    "# your_code\n",
    "replace_na(df, \"CentralAir\", value=\"N\")\n",
    "replace_na(df, \"EnclosedPorch\", value=0)\n",
    "replace_na(df, \"FireplaceQu\", value=\"No\")\n",
    "replace_na(df, \"Fireplaces\", value=0)\n",
    "replace_na(df, \"SaleCondition\", value=\"Normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dane kategoryczne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak już zdążyliśmy zauważyć, istnieją dwa główne rodzaje danych: numeryczne (*numerical data*) oraz kategoryczne (*categorical data*). Ten podział jest bardzo istotny. Dane numeryczne to żadna niespodzianka, po prostu mają swoją wartość, jak np. **GrLivArea**, czyli powierzchnia budynku/apartamentów. Dane kategoryczne to takie, którym w większości przypadków nie można przyporządkować wartości liczbowej (wyjątkiem są dane kategoryczne uporządkowane - *categorical ordinal*).\n",
    "\n",
    "Wyobraź sobie zmienną reprezentującą kolory o wartościach \"red\", \"green\" i \"blue. Jeżeli zakodowałbyś je np. jako $red = 0$, $green = 1$, $blue = 2$, to stwierdzasz tym samym, że w pewnym sensie $red < green < blue$. Raczej nie ma powodu, żeby tak sądzić. Jest to zmienna, która ma skończoną liczbę wartości, ale są one nieuporządkowane. Taki typ to zmienne *categorical nominal*.\n",
    "\n",
    "Szczególnym przypadkiem są zmienne binarne (*boolean*). Jest to u nas kolumna **CentralAir** (Central Air Conditioning). Z opisu w pliku [ames_description.txt](ames_description.txt) wiemy, że przyjmuje ona dokładnie dwie wartości kategoryczne: *No* oraz *Yes*. W takiej sytuacji wolno zakodować te wartości numerycznie jako 0 i 1. Stwierdzasz tym samym, że klimatyzacja albo jest, albo jej nie ma.\n",
    "\n",
    "Sytuacją podobną, chociaż mniej oczywistą, może być zmienna **Street**, opisująca typ drogi wiodącej do nieruchomości. Jeśli znowu spojrzymy do opisu danych, to można zauważyć, że ta zmienna może przyjmować tylko dwie różne wartości - *Grvl* i *Pave*. I tu też możemy sobie pozwolić na zakodowanie tych wartości jako 0 i 1. Stwierdzamy wtedy, że droga jest *utwardzona* (Pave) dla wartości 1. Oczywiście równie dobrze można by zakodować to odwrotnie i stwierdzić, że droga jest *nieutwardzona* (Grvl) gdy wartość wynosi 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W Pandas typy numeryczne są oparte o NumPy (np. `np.int64`), a zmienne kategoryczne, napisy itp. są typu `object` (typ `Categorical` istnieje od pewnego czasu, ale nie jest jeszcze zbyt dobrze wspierany).\n",
    "\n",
    "Zmienne **MSSubClass** oraz **MoSold** są kategoryczne (tak wynika z informacji zawartej w pliku [ames_description.txt](ames_description.txt)), a są w naszych danych wprost liczbami. Przekształćmy je zatem do poprawnego typu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T23:24:31.301156832Z",
     "start_time": "2023-09-15T23:24:31.205139018Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.replace(\n",
    "    {\n",
    "        \"MSSubClass\": {\n",
    "            20: \"SC20\",\n",
    "            30: \"SC30\",\n",
    "            40: \"SC40\",\n",
    "            45: \"SC45\",\n",
    "            50: \"SC50\",\n",
    "            60: \"SC60\",\n",
    "            70: \"SC70\",\n",
    "            75: \"SC75\",\n",
    "            80: \"SC80\",\n",
    "            85: \"SC85\",\n",
    "            90: \"SC90\",\n",
    "            120: \"SC120\",\n",
    "            150: \"SC150\",\n",
    "            160: \"SC160\",\n",
    "            180: \"SC180\",\n",
    "            190: \"SC190\",\n",
    "        },\n",
    "        \"MoSold\": {\n",
    "            1: \"Jan\",\n",
    "            2: \"Feb\",\n",
    "            3: \"Mar\",\n",
    "            4: \"Apr\",\n",
    "            5: \"May\",\n",
    "            6: \"Jun\",\n",
    "            7: \"Jul\",\n",
    "            8: \"Aug\",\n",
    "            9: \"Sep\",\n",
    "            10: \"Oct\",\n",
    "            11: \"Nov\",\n",
    "            12: \"Dec\",\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oprócz tego zakodujemy zmienne kategoryczne uporządkowane (*categorical ordinal*) z tekstowych na kolejne liczby całkowite.\n",
    "\n",
    "Przykładowo zmienna **BsmtCond**, oceniająca stan piwnicy, ma następujące możliwe wartości:\n",
    "* *NA* (No) Basement\n",
    "* *Po* (Poor) - Severe cracking, settling, or wetness\n",
    "* *Fa* (Fair) - dampness or some cracking or settling\n",
    "* *TA* (Typical) - slight dampness allowed\n",
    "* *Gd* (Good)\n",
    "* *Ex* (Excellent)\n",
    "\n",
    "Do następujących wartości możemy dopasować pewną skalę punktową, bo są one naturalnie uporządkowane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T23:44:44.949727320Z",
     "start_time": "2023-09-15T23:44:44.821182382Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.replace(\n",
    "    {\n",
    "        \"Alley\": {\"None\": 0, \"Grvl\": 1, \"Pave\": 2},\n",
    "        \"BsmtCond\": {\"No\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"BsmtExposure\": {\"No\": 0, \"Mn\": 1, \"Av\": 2, \"Gd\": 3},\n",
    "        \"BsmtFinType1\": {\n",
    "            \"No\": 0,\n",
    "            \"Unf\": 1,\n",
    "            \"LwQ\": 2,\n",
    "            \"Rec\": 3,\n",
    "            \"BLQ\": 4,\n",
    "            \"ALQ\": 5,\n",
    "            \"GLQ\": 6,\n",
    "        },\n",
    "        \"BsmtFinType2\": {\n",
    "            \"No\": 0,\n",
    "            \"Unf\": 1,\n",
    "            \"LwQ\": 2,\n",
    "            \"Rec\": 3,\n",
    "            \"BLQ\": 4,\n",
    "            \"ALQ\": 5,\n",
    "            \"GLQ\": 6,\n",
    "        },\n",
    "        \"BsmtQual\": {\"No\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"ExterCond\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"ExterQual\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"FireplaceQu\": {\"No\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"Functional\": {\n",
    "            \"Sal\": 1,\n",
    "            \"Sev\": 2,\n",
    "            \"Maj2\": 3,\n",
    "            \"Maj1\": 4,\n",
    "            \"Mod\": 5,\n",
    "            \"Min2\": 6,\n",
    "            \"Min1\": 7,\n",
    "            \"Typ\": 8,\n",
    "        },\n",
    "        \"GarageCond\": {\"No\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"GarageQual\": {\"No\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"HeatingQC\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"KitchenQual\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"LandSlope\": {\"Sev\": 1, \"Mod\": 2, \"Gtl\": 3},\n",
    "        \"LotShape\": {\"IR3\": 1, \"IR2\": 2, \"IR1\": 3, \"Reg\": 4},\n",
    "        \"PavedDrive\": {\"N\": 0, \"P\": 1, \"Y\": 2},\n",
    "        \"PoolQC\": {\"No\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n",
    "        \"Street\": {\"Grvl\": 0, \"Pave\": 1},\n",
    "        \"Utilities\": {\"ELO\": 1, \"NoSeWa\": 2, \"NoSewr\": 3, \"AllPub\": 4},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie danych do uczenia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nasz zbiór podzielimy na dwa podzbiory: treningowy (70%) i testowy (30%). Zbiór treningowy pozwoli nam utworzyć model regresji liniowej, natomiast testowy - oszacować jego jakość.\n",
    "\n",
    "Pamiętaj, że wyniki uzyskiwane przez model na danych treningowych nie odzwierciedlają tego, jak będzie on sobie radził na danych, których nie ma w zbiorze uczącym. Aby uzyskać taką informację, konieczne jest sprawdzenie, jak model radzi sobie na danych testowych. Daje nam to oszacowanie, jak dobrze model **generalizuje się** dla nowych danych.\n",
    "\n",
    "Wydzielimy sobie równeż zbiory kolumn z danymi numerycznymi i kategorycznymi, co później ułatwi nam odwoływanie się do nich.\n",
    "\n",
    "Funkcja `train_test_split` z biblioteki Scikit-Learn przyjmuje osobno macierze dla cech (*features*) i etykiet (*labels*), dlatego wyodrębniamy sobie z naszej tablicy kolumnę **SalePrice**, która zawiera ceny nieruchomości.\n",
    "\n",
    "---\n",
    "#### *Ciekawostka*\n",
    "\n",
    "Można zauważyć, że zmienna `y` jest małą literą, natomiast `X_train` czy `X_test` są z dużej. Są to konwencje pochodzące z matematyki:\n",
    "\n",
    "* wektor w matematyce często oznaczamy małą pogrubioną literą ($\\textbf{y}$) - w programowaniu natomiast oznaczamy po prostu małą literą - `y`\n",
    "* macierz w matematyce oznaczamy dużą pogrubioną literą ($\\textbf{X}$) - w programowaniu po prostu dużą literą - `X`\n",
    "\n",
    "Zbiór etykiet to w naszym przypadku wektor cen, więc zapisujemy `y` małą literą. Z drugiej strony `X` zawiera kolumny z cechami opisującymi poszczególne rekordy, a więc jest to macierz.\n",
    "\n",
    "---\n",
    "\n",
    "**Uwaga**: w eksperymentach ustalamy na sztywno wartość parametru `random_state`. [Doczytaj](https://scikit-learn.org/stable/glossary.html#term-random_state), dlaczego wykorzystywany jest ten parametr i co się dzieje, gdy jest on równy stałej wartości jak zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df12 = df.copy()\n",
    "y = df.pop(\"SalePrice\")\n",
    "\n",
    "categorical_features = df.select_dtypes(include=\"object\").columns\n",
    "numerical_features = df.select_dtypes(exclude=\"object\").columns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, y, test_size=0.3, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz trzeba dokonać transformacji naszych danych:\n",
    "- zmienne kategoryczne nieuporządkowane trzeba przetworzyć tak, aby nasz algorym był w stanie je obsłużyć, czyli je zakodować za pomocą **one-hot encoding**,\n",
    "- zmienne numeryczne dalej mogą mieć wartości brakujące, więc trzeba je uzupełnić, inaczej **imputować (impute)**,\n",
    "- zmienne numeryczne trzeba przeskalować do zakresu wartości $[0, 1]$ czyli je **znormalizować (normalization)** przez zastosowanie **min-max scaling**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Kodowanie one-hot encoding\n",
    "\n",
    "Powyżej omawialiśmy zmienne kategoryczne. Typ *categorical ordinal* można zakodować kolejnymi liczbami całkowitymi, co jest oczywiście proste. Co jednak ze zmiennymi bez kolejności, typu *categorical nominal*? Trzeba je dalej przekształcić na liczby (żeby model był w stanie je przetworzyć), ale tak, aby nie nadać im implicite kolejności.\n",
    "\n",
    "Spójrzmy na kolumnę **Neighborhood**, oznaczającą poszczególne dzielnice. Dom znajduje się tylko w jednej dzielnicy, a w pozostałych go nie ma. Idea kodowania **one-hot encoding** polega na stworzeniu tylu zmiennych, ile jest możliwych wartości, a następnie w każdym wierszu przypisanie wartości 1 w tej kolumnie, z której była oryginalnie zmienna.\n",
    "\n",
    "Przykładowo, jeżeli mielibyśmy 3 wartości `[\"A\", \"B\", \"C\"]`, to powstają z nich 3 cechy (kolumny macierzy `X`) `[col_A, col_B, col_C]`. Wiersz z pierwotną wartością `\"B\"` będzie miał wartości tych cech `[0, 1, 0]`. W przypadku naszej zmiennej **Neighborhood** pojawią się osobne zmienne **Old Town**, **NoRidge**, **Gilbert** itd., a dla każdego wiersza dokładnie jedna z nich będzie miała wartość 1.\n",
    "\n",
    "#### Dla zainteresowanych\n",
    "\n",
    "Jeżeli mamy dużo możliwych wartości, czyli zmienną o dużej **kardynalności (cardinality)**, to kolumn powstanie bardzo dużo. Do tego są **rzadkie (sparse)**, więc tracimy dużo pamięci na przechowywanie zer. Istnieją inne kodowania, które zajmują mniej miejsca, a implementuje je biblioteka [Category Encoders](https://contrib.scikit-learn.org/category_encoders/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputacja brakujących wartości numerycznych\n",
    "\n",
    "Wcześniej już napotkaliśmy wartości brakujące i postaraliśmy się uzupełnić je jak najlepiej potrafiliśmy, używając dokumentacji naszego zbioru. Nie gwarantuje to jednak usunięcia wszystkich braków. Nie zawsze w praktyce da się też tak łatwo znaleźć wartości do uzupełnienia. W przypadku zwykłych cech numerycznych możemy zastosować jedną z kilku bardzo popularnych strategii radzenia sobie z wartościami brakującymi:\n",
    "\n",
    "1. Usunąć kolumnę, która zawiera brakujące wartości.\n",
    "1. Usunąć wiersze, w których brakuje wartości.\n",
    "1. Zastąpić brakujące wartości innymi, np. średnią z kolumny, medianą albo wartością stałą.\n",
    "1. Przewidzieć brakujące wartości wykorzystując odpowiedni model uczenia maszynowego.\n",
    "\n",
    "Podejście 4 jest często zbyt czasochłonne. Opcje 1 i 2 prowadzą do utraty danych. My wypróbujemy sposób nr 3.\n",
    "\n",
    "Nie znaczy to jednak, że usunięcie wierszy czy kolumny jest zawsze złym podejściem. Usunięcie kolumny jest uzasadnione, jeśli ma ona naprawdę dużo wartości brakujących. W takich wypadkach ciężko z niej wyciągnąć jakąkolwiek sensowną informację. Usunięcie wierszy może być uzasadnione w przypadku, gdy mamy dużo rekordów i tylko niewielka część z nich posiada wartości brakujące (usunięcie kilku wierszy nie powinno powodować problemu).\n",
    "\n",
    "#### Dla zainteresowanych\n",
    "\n",
    "Popularne algorytmy imputacji danych często są oparte [o algorytm najbliższych sąsiadów, czyli najbardziej podobne punkty](https://scikit-learn.org/stable/modules/impute.html#nearest-neighbors-imputation). Innym podejściem, iteracyjnie imputującym wartości, jest [algorytm MICE](https://www.numpyninja.com/post/mice-algorithm-to-impute-missing-values-in-a-dataset).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skalowanie\n",
    "\n",
    "Jest to bardzo ważny krok dla wielu modeli sztucznej inteligencji. Często takie modele mają pewne założenia co do danych wejściowych, a szczególnie popularnym założeniem jest, że wszystkie cechy mają wartości o podobnej skali. W szczególności regresja liniowa i logistyczna też czynią to założenie. Dlatego trzeba przeskalować nasze dane, żeby spełnić to założenie. Najprostsza metoda to `MinMaxScaler`, który przekształca wszystkie wartości do przedziału $[0, 1]$.\n",
    "\n",
    "Istnieją też inne metody, np. standaryzacja, którą możesz pamiętać ze statystyki (jej wynikiem jest Z-score). Polega na odjęciu średniej i podzieleniu przez odchylenie standardowe każdej cechy. Wynikiem przekształcenia są cechy o średniej 0 i odchyleniu standardowym 1.\n",
    "\n",
    "Więcej informacji na temat tego, dlaczego skalowanie jest tak istotne, możesz znaleźć [tutaj](https://analyticsindiamag.com/why-data-scaling-is-important-in-machine-learning-how-to-effectively-do-it/).\n",
    "\n",
    "#### Dla zainteresowanych\n",
    "\n",
    "Porównanie różnych metod skalowania [możesz znaleźć tutaj](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html). Ciekawą metodą jest np. RobustScaler, który jest podobny do StandardScaler, ale używa mediany i kwartyli zamiast średniej i odchylenia standardowego. Są to tzw. robust statistics, czyli miary odporne na występowanie wartości odstających (outliers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przetwarzanie danych z wykorzystaniem Scikit-Learn\n",
    "\n",
    "Mamy zatem do wykonania:\n",
    "- na zmiennych numerycznych 2 operacje do wykonania: imputacja i skalowanie,\n",
    "- na zmiennych kategorycznych: zastosowanie kodowania one-hot encoding.\n",
    "\n",
    "W Scikit-learn służą do tego następujące klasy:\n",
    "- `OneHotEncoder`, `SimpleImputer`, `MinMaxScaler` - transformacje, implementują metody `.fit()` i `.transform()`,\n",
    "- `Pipeline` - do układania transformacji sekwencyjnie,\n",
    "- `ColumnTransformer` - do układania transformacji równolegle, dla różnych kolumn.\n",
    "\n",
    "**Ważne:** jako, że zaraz skorzystamy z regresji liniowej, do klasy `OneHotEncoder` trzeba przekazać `drop=\"first\"`. Stworzy to 1 zmienną mniej, niż typowy one-hot encoding, np. `pd.get_dummies()`, gwarantując brak **idealnie współliniowych zmiennych (perfectly collinear features)**, co byłby niestabilny numerycznie. Dodatkowo, jako że przekształcamy już po podziale na zbiór treningowy i testowy, to możemy spotkać na zbiorze testowym nieliczne przypadki kategorii, których nie ma w zbiorze treningowym - kodujemy je wtedy po prostu jako wektory zer za pomocą `handle_unknown=\"ignore\"`.\n",
    "\n",
    "Na przykładzie `StandardScaler` (standaryzacja) rozpatrzmy, jak działają poszczególne metody.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metoda `.fit()`\n",
    "\n",
    "Do wykonania standaryzacji potrzebujemy dla każdej z cech określić 2 wartości - średnią oraz odchylenie standardowe. Formuła standaryzacji dla przypomnienia:\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "Metodę `.fit()` wykonujemy tylko raz, dla **danych treningowych**. To powoduje, że obliczamy wartości $\\mu$ oraz $\\sigma$ dla każdej cechy, na podstawie wartości ze zbioru treningowego. Wyuczone wartości zostają zapisane w obiekcie `StandardScaler` i mogą być później używane do przeprowadzenia standaryzacji zarówno dla danych treningowych, jak i testowych.\n",
    "\n",
    "**Co, gdyby dla danych testowych przeprowadzić osobną standaryzację?**\n",
    "\n",
    "Będziemy, na przykład, standaryzować kolumnę **GrLivArea** - powierzchnię nieruchomości. Załóżmy, że z danych treningowych wyszłoby, że średnia jest równa $60m^2$, a odchylenie standardowe - $20m^2$. Wtedy wartości z przedziału $[40, 80]$ zostaną przekształcone do $[-1, 1]$. Nasz model wykorzysta to przekształcenie i będzie uważał, że wartości po transformacji w pobliżu $0$ oznaczają średniej wielkości apartamenty.\n",
    "\n",
    "Określiliśmy parametry modelu i dostajemy kilkadziesiąt budynków z jakiejś zamożnej dzielnicy dla predykcji. Średnia powierzchnia dla tych budynków to około $160m^2$. Osobno przeprowadzając standaryzację dla takich danych testowych, zaburzylibyśmy rozkład tej cechy, gdyż tym razem wartości wokół $0$ oznaczałyby dość duże mieszkania. Modele są niezwykle czułe na podobne zaburzenia - musimy przetwarzać dane spójnie, żeby nie doszło do podobnych sytuacji. \n",
    "\n",
    "**Czemu nie wywołać `.fit()` na wszystkich danych, a nie tylko treningowych?**\n",
    "\n",
    "Wydzieliliśmy dane testowe po to, żeby sprawdzać, jak model poradzi sobie z danymi, których do tej pory nigdy nie widział, bo to właśnie takie dane będzie on dostawać w praktyce, po wdrożeniu do realnego systemu. Ta ocena obejmuje też etap preprocessingu, w tym skalowania. Więc jeśli etap preprocessingu zobaczy dane testowe, to nie będziemy w stanie uczciwie estymować jego zachowania na nowych danych.\n",
    "\n",
    "Wykorzystanie danych testowych w procesie treningu to błąd **wycieku danych (data leakage)**. Skutkuje on niepoprawnym, nadmiernie optymistycznym oszacowaniem jakości modelu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metoda `.transform()`\n",
    "\n",
    "Przekształca dane za pomocą parametrów wyznaczonych w `.fit()`.\n",
    "\n",
    "### Metoda `.fit_transform()`\n",
    "\n",
    "Metoda, która najpierw wykonuje `.fit()`, a potem `.transform()` i zwraca wynik ostatniej. W przypadku niektórych transformacji wykorzystuje ich specyfikę i działa szybciej, niż sekwencyjne wywołanie `.fit()` oraz`.transform()`. Trzeba jednak pamiętać, że możemy tego użyć tylko na zbiorze treningowym - na zbiorze testowym wywołujemy już tylko `.transform()`.\n",
    "\n",
    "**Zadanie 4 (0.5 punktu)**\n",
    "\n",
    "Stwórz pipeline'y dla zmiennych kategorycznych i numerycznych. Połącz je następnie z użyciem `ColumnTransformer`. \"Wytrenuj\" go na danych treningowych, a następnie przetransformuj dane treningowe oraz testowe.\n",
    "\n",
    "**Uwaga:** przekaż do `ColumnTransformer` parametr `verbose_feature_names_out=False`, żeby nie zmieniał on nazw cech. Ułatwi nam to późniejszą analizę wyników."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-16T19:33:06.290225746Z",
     "start_time": "2023-09-16T19:33:04.939828812Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wlade\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:170: UserWarning: Found unknown categories in columns [12, 15, 17] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(\n",
    "    drop=\"first\", sparse=False, handle_unknown=\"ignore\")#zmieniłem sparse_output na sparse, bo mam starszą wersję\n",
    "median_imputer = SimpleImputer(strategy=\"median\")\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "categorical_pipeline = Pipeline([('one_hot_encoder', one_hot_encoder)])  # your_code_here\n",
    "\n",
    "numerical_pipeline = Pipeline([('median_imputer', median_imputer),('min_max_scaler', min_max_scaler)])  # your_code_here\n",
    "\n",
    "column_transformer = ColumnTransformer(     [(\"categorical_pipeline\", categorical_pipeline, categorical_features),\n",
    "      (\"numerical_pipeline\", numerical_pipeline, numerical_features)], verbose_feature_names_out=False)  # your_code_here\n",
    "\n",
    "# fit and transform\n",
    "df_transformed = column_transformer.fit_transform(X_train.copy(),y_train.copy())\n",
    "X_train = column_transformer.transform(X_train.copy())\n",
    "#y_train = column_transformer.transform(y_train.copy())\n",
    "X_test = column_transformer.transform(X_test.copy())\n",
    "#y_test = column_transformer.transform(y_test.copy())\n",
    "# your_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('categorical_pipeline',\n",
       "                                 Pipeline(steps=[('one_hot_encoder',\n",
       "                                                  OneHotEncoder(drop='first',\n",
       "                                                                handle_unknown='ignore',\n",
       "                                                                sparse=False))]),\n",
       "                                 Index(['MSSubClass', 'MSZoning', 'LandContour', 'LotConfig', 'Neighborhood',\n",
       "       'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle',\n",
       "       'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foun...\n",
       "       'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr',\n",
       "       'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces',\n",
       "       'FireplaceQu', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'GarageQual',\n",
       "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
       "       'EnclosedPorch', 'X3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
       "       'MiscVal', 'YrSold'],\n",
       "      dtype='object'))],\n",
       "                  verbose_feature_names_out=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresja liniowa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy teraz przejść do przewidywania wartości domów. Naszym narzędziem będzie tutaj **regresja liniowa (linear regression)**, czyli model postaci:\n",
    "$$\n",
    "\\hat{y} = ax + b\n",
    "$$\n",
    "gdzie $\\hat{y}$ to zmienna zależna, $x$ to zmienna niezależna (wartość cechy), a współczynniki obliczane są według wzorów opisanych [tutaj](https://www.vedantu.com/formula/linear-regression-formula), bez wątpienia znanych Ci z algebry liniowej i statystyki.\n",
    "\n",
    "Rozwinięciem regresji liniowej jest wielokrotna regresja liniowa (*multiple linear regression*), która pozwala na wykorzystanie więcej niż jednej cechy do predykcji wartości. W takim modelu predykcja to kombinacja liniowa cech i wag, gdzie każda cecha posiada własną wagę. Więcej o tym mechanizmie możesz przeczytać [tutaj](https://rankia.pl/analizy-gieldowe/co-to-jest-wielokrotna-regresja-liniowa-mlr/). Formalnie jest to model postaci:\n",
    "$$\n",
    "\\hat{y} = \\boldsymbol{w} \\cdot \\boldsymbol{x} + b = \\sum_{i=1}^{d} w_i x_i + b\n",
    "$$\n",
    "gdzie:\n",
    "- $d$ to **wymiarowość (dimensionality)**, czyli liczba cech\n",
    "- $\\boldsymbol{w}$ to wektor wag o długości $d$\n",
    "- $w_i$ to wagi poszczególnych cech\n",
    "- $b$ to **wyraz wolny (bias / intercept)**, punkt przecięcia ze środkiem układu współrzędnych\n",
    "\n",
    "Pozostaje pytanie, jak wyznaczyć wagi $\\boldsymbol{w}$ i wyraz wolny $b$. Można to robić na różne sposoby, przy czym klasyczna regresja liniowa minimalizuje **błąd średniokwadratowy (mean squared error, MSE)**. Jest to przykład **funkcji kosztu (loss function / cost function)**, a konkretnie *squared loss / L2 loss**. Ma on postać:\n",
    "$$\n",
    "L(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^n \\left( y - \\hat{y} \\right)^2\n",
    "$$\n",
    "gdzie $\\hat{y}$ to wartość przewidywana przez model, $y$ - prawdziwa, a $n$ to liczba punktów w zbiorze.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W Scikit-learn ten model implementuje klasa `LinearRegression`. Jej ważne cechy:\n",
    "- domyślnie uwzględnia intercept (bias) przez `fit_intercept=True`; jeżeli nasze dane są już wycentrowane, to jest to niepotrzebne i może powodować problemy numeryczne,\n",
    "- używa implementacji z pseudoodwrotnością Moore'a-Penrose'a (SVD),\n",
    "- nie pozwala na regularyzację, do tego trzeba użyć innych klas.\n",
    "\n",
    "Jak ocenić, jak taki model sobie radzi? Trzeba tutaj użyć pewnej **metryki (metric)**, czyli wyznacznika jakości modelu. Można na to patrzeć z wielu różnych perspektyw, w zależności od charakterystyki problemu. Tradycyjnie używa się **Root MSE (RMSE)**, czyli pierwiastka kwadratowego z MSE. Ma ważne zalety:\n",
    "- regresja liniowa z definicji modelu optymalizuje miarę MSE, więc używamy metryki dobrze związanej z modelem,\n",
    "- dzięki pierwiastkowaniu ma tę samą jednostkę, co przewidywane wartości. .\n",
    "\n",
    "Jest też dość czuła na wartości odstające, ale może to być korzystne, w zależności od zastosowania.\n",
    "\n",
    "$$\n",
    "RMSE(y, \\hat{y}) = \\sqrt{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "W Scikit-learn RMSE liczy się dość specyficznie, bo używa się funkcji do MSE z argumentem `squared=False`.\n",
    "\n",
    "#### Dla zainteresowanych\n",
    "\n",
    "Minimalizując inne rodzaje błędu, otrzymujemy modele liniowe o innych parametrach, ale tej samej postaci funkcji. Typowo modele te są bardziej odporne na wartości odstające, ale bardziej kosztowne w treningu. Są to np. [quantile regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.QuantileRegressor.html) optymalizująca koszt L1 (*mean absolute error*) czy [Huber regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html), optymalizująca tzw. Huber loss (połączenie L1 i L2).\n",
    "\n",
    "Obliczanie regresji liniowej używa pseudoodwrotności Moore'a-Penrose'a i SVD. Objaśnia to dobrze [ten tutorial](https://sthalles.github.io/svd-for-regression/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.1159\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# all variables are in range [0, 1], so we don't need an intercept\n",
    "reg_linear = LinearRegression(fit_intercept=False)\n",
    "reg_linear.fit(X_train, y_train)\n",
    "\n",
    "y_pred = reg_linear.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czy taki błąd to duży, czy mały? Wszystko zależy od skali wartości przewidywanych. Trzeba pamiętać, że dokonaliśmy logarytmowania zmiennej docelowej, więc trzeba to sprawdzić po transformacji odwrotnej `np.expm1`. Po tej operacji wartość błędu będzie wyrażona w dolarach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12289982726531762 $\n",
      "0.11591447076725657\n"
     ]
    }
   ],
   "source": [
    "print(np.expm1(rmse),\"$\")\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zbyt małe i nadmierne dopasowanie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W trakcie trenowania modelu może dojść do sytuacji, w której zostanie on **przeuczony (overfitting)**. W takim wypadku model nadmiernie dostosowuje się do danych treningowych, \"zakuwając\" je. Daje wtedy bardzo dokładne wyniki na zbiorze treningowym, ale kiepskie na zbiorze testowym. Modele przeuczone słabo zatem się **generalizują (generalization)**.\n",
    "\n",
    "Dlatego wcześniej wydzieliliśmy zbiór testowy, za pomocą którego oceniamy skuteczność naszego modelu. Pozwala to uniknąć powyższego błędu. Przeuczenie bardzo często można rozpoznać właśnie po różnym zachowaniu modelu na danych treningowych i testowych. Jeśli z danymi treningowymi model radzi sobie dużo lepiej, niż z testowymi, to istnieje dużo ryzyko, że model został przeuczony i skupił się na zapamiętywaniu konkretnych przykładów, na których się uczył, niż na wyciąganiu z nich uniwersalnych wzorców. Taki model słabo się generalizuje i nie poradzi sobie z nowymi danymi.\n",
    "\n",
    "Sprawdza się to następująco:\n",
    "- obliczamy błąd treningowy oraz testowy,\n",
    "- jeżeli oba błędy są wysokie, to mamy zbyt małe dopasowanie (*underfitting*) i trzeba użyć pojemniejszego modelu,\n",
    "- jeżeli błąd treningowy jest dużo niższy od testowego, to mamy nadmierne dopasowanie (*overfitting*) i model trzeba regularyzować.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W praktyce paradoksalnie często model o większej pojemności z mocną regularyzacją działa lepiej od prostszego modelu ze słabą regularyzacją. Wyjaśnianie, czemu tak jest, to otwarty problem naukowy, szczególnie w kontekście sieci neuronowych.\n",
    "\n",
    "Przeuczenie modelu jest bardzo istotnym problemem w sztucznej inteligencji i istnieje szereg metod, służących zapobieganiu tego zjawiska. Jedną z nich jest regularyzacja - do globalnej funkcji błędu dodawane są \"kary\" za tworzenie zbyt złożonych modeli. Typowe metody regularyzacji to L1 oraz L2, które penalizują wielkość parametrów obliczonych w trakcie treningu. Obie te wartości są tak naprawdę normami (odpowiednio `l1` i `l2`) wektorów wag modelu, przeskalowanymi przez określoną wartość. Dodawanie tych kar ma zapobiec przeuczeniu, bo typowo duże wagi w regresji liniowej i podobnych modelach oznaczają przeuczenie.\n",
    "\n",
    "Czemu tak jest? Przeuczenie bierze się z tego, że nasz model \"zakuwa\" zbiór treningowy, ucząc się **szumu (noise)** w danych, przypisując nadmierne znaczenie niewielkim różnicom w wartościach cech. Jeżeli cecha ma dużą wagę, to nawet niewielka zmiana jej wartości bardzo zmienia finalną predykcję (która jest kombinacją liniową). Dzięki regularyzacji, jeżeli model podczas treningu będzie chciał zwiększyć wagę dla cechy, to musi mu się to opłacać. Innymi słowy, zwiększenie wagi cechy musi zmniejszyć koszt (np. MSE) bardziej, niż wzrośnie kara z regularyzacji.\n",
    "\n",
    "Jak słusznie się domyślić, zbyt duże kary spowoduję z kolei niedouczenie (ang. *underfitting*). Więcej o konstrukcji i zastosowaniach regularyzacji L1 i L2 możesz przeczytać [tutaj](https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261).\n",
    "\n",
    "#### Dla zainteresowanych\n",
    "\n",
    "W praktyce detekcja nadmiernego dopasowania nie musi być wcale taka oczywista. Nasz model może przeuczać się tylko na niektórych segmentach danych, dla nietrywialnych kombinacji cech etc. Testowanie modeli ML i detekcja overfittingu jest otwartym problemem badawczym, ale powstają już pierwsze narzędzia do tego, np. [Giskard](https://github.com/Giskard-AI/giskard)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 5 (1.0 punkt)**\n",
    "\n",
    "Uzupełnij kod funkcji `assess_regression_model` o:\n",
    "- obliczenie predykcji na zbiorze treningowym oraz testowym,\n",
    "- transformacje eksponencjalne, żeby wrócić do oryginalnej jednostki (dolara),\n",
    "- obliczenie RMSE dla zbioru treningowego i testowego,\n",
    "- wypisywanie RMSE, zaokrąglonego do 2 miejsc po przecinku.\n",
    "\n",
    "Skomentuj wyniki. Czy następuje przeuczenie modelu? Oceń także sam błąd, czy subiektywnie to duża wartość, biorąc pod uwagę rozkład zmiennej docelowej (wartości i wykresy w sekcji EDA)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_regression_model(model, X_train, X_test, y_train, y_test) -> None:\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    # predict for train and test\n",
    "    # your_code_here\n",
    "\n",
    "    # exponential transform for y_train, y_test and predictions\n",
    "    y_pred_train = np.expm1(y_pred_train)\n",
    "    y_train = np.expm1(y_train)\n",
    "    y_pred_test = np.expm1(y_pred_test)\n",
    "    y_test = np.expm1(y_test)\n",
    "    # your_code_here\n",
    "\n",
    "    # calculate train and test RMSE\n",
    "\n",
    "    rmse_train = mean_squared_error(y_train, y_pred_train, squared=False)\n",
    "    rmse_test = mean_squared_error(y_test, y_pred_test, squared=False)\n",
    "    # your_code_here\n",
    "    \n",
    "    # print train and test RMSE\n",
    "    \n",
    "    print(rmse_train)\n",
    "    print(rmse_test)\n",
    "    # your_code_here\n",
    "\n",
    "# your_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16751.11470779035\n",
      "21318.614606466956\n"
     ]
    }
   ],
   "source": [
    "assess_regression_model(reg_linear, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# // skomentuj tutaj\n",
    "#### Pierwiastek z błędu średniokwadratowego jest niższy dla zbioru treningowego, następuje pewne przeuczenie modelu. Sam błąd średniokwadratowy również nie jest w obu wypadkach zbyt duży. Pozostaje on kilkukrotnie mniejszy od wartości oczekiwanej zmiennej y , a także od jej wariancji, zatem na pewno nie mamy do czynienia z underfittingiem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresja regularyzowana (ridge, LASSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularyzacja zmniejsza pojemność modelu regresji liniowej, narzucając mniejsze wagi poprzez penalizację dużych wag w funkcji kosztu. Regresja liniowa z regularyzacją L2 nazywa się *ridge regression*, z regularyzacją L1 - *LASSO regression*, a z oboma naraz - *ElasticNet regression*. Formalnie mamy:\n",
    "$$\n",
    "L_{ridge}(y, \\hat{y}) = \\frac{1}{n} (y - \\hat{y})^2 + \\lambda ||\\boldsymbol{w}||_2^2\n",
    "$$\n",
    "$$\n",
    "L_{LASSO}(y, \\hat{y}) = \\frac{1}{n} (y - \\hat{y})^2 + \\alpha ||\\boldsymbol{w}||_1\n",
    "$$\n",
    "$$\n",
    "L_{ElasticNet}(y, \\hat{y}) = \\frac{1}{n} (y - \\hat{y})^2 + \\lambda ||\\boldsymbol{w}||_2^2 + \\alpha ||\\boldsymbol{w}||_1\n",
    "$$\n",
    "\n",
    "Jak widać, regularyzacja dodaje do zwykłego kosztu MSE dodatkowe wyrazy, penalizujące wielkość wag $\\boldsymbol{w}$. **Siłę regularyzacji (regularization strength)**, czyli jak mocna jest taka kara, wyznacza współczynnik, oznaczany typowo $\\lambda$ albo $\\alpha$. Jest to **hiperparametr (hyperparameter)**, czyli stała modelu, którą narzucamy z góry, przed treningiem. Nie jest on uczony z danych. Jak go dobrać, omówimy poniżej.\n",
    "\n",
    "Regresja ridge (L2) zmniejsza wagi i jest różniczkowalna (szybsza i łatwiejsza w treningu). Regresja LASSO (L1) dokonuje **selekcji cech (feature selection)**, zmniejszając często wagi cech dokładnie do zera, eliminując tym samym słabe cechy. Oba naraz realizuje model ElasticNet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W Scikit-learn implementują je klasy `Ridge`, `Lasso` oraz `ElasticNet`. Najważniejszy hiperparametr każdego z tych modeli to siła regularyzacji, która we wszystkich klasach to `alpha`. Scikit-learn definiuje regularyzację ElasticNet dość specyficznie, za pomocą parametru `l1_ratio`, który wyznacza, jaki ułamek siły regularyzacji przypada dla L1, a jaki dla L2:\n",
    "$$\n",
    "L_{ElasticNet}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^n \\left( y - \\hat{y} \\right)^2 + \\alpha \\cdot (1 - L1\\_ratio) \\cdot ||\\boldsymbol{w}||_2^2 + \\alpha \\cdot L1\\_ratio \\cdot ||\\boldsymbol{w}||_1 \\\\\n",
    "$$\n",
    "\n",
    "Inne ważne uwagi:\n",
    "- liczba iteracji `max_iter` wyznacza liczbę iteracji solwera; im więcej, tym dokładniejsze rozwiązanie, ale tym dłuższy czas obliczeń,\n",
    "- jeżeli `max_iter` będzie zbyt mała i algorytm nie osiągnie zbieżności, to dostaniemy ostrzeżenie, wtedy zwykle trzeba po prostu ją zwiększyć, np. 10-krotnie,\n",
    "- jeżeli nie potrzebujemy bardzo precyzyjnego rozwiązania, można ustawić większe `tol` dla przyspieszenia obliczeń.\n",
    "\n",
    "Jako że nasz model jest regularyzowany i nie ma ryzyka problemów numerycznych, to teraz już obliczamy intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16870.911994236554\n",
      "18879.105240626523\n",
      "\n",
      "79579.7870177079\n",
      "80091.98682467625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "reg_ridge = Ridge(random_state=0)\n",
    "reg_lasso = Lasso(random_state=0)\n",
    "\n",
    "reg_ridge.fit(X_train, y_train)\n",
    "reg_lasso.fit(X_train, y_train)\n",
    "\n",
    "assess_regression_model(reg_ridge, X_train, X_test, y_train, y_test)\n",
    "print()\n",
    "assess_regression_model(reg_lasso, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przypadku regularyzacji L2 domyślna siła regularyzacji (`alpha=1.0`) znacząco poprawiła wynik, natomiast w przypadku L1 mamy bardzo silny underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hiperparametrów, zbiór walidacyjny\n",
    "\n",
    "Praktycznie wszystkie modele ML mają hiperparametry, często liczne, które w zauważalny sposób wpływają na wyniki, a szczególnie na underfitting i overfitting. Ich wartości trzeba dobrać zatem dość dokładnie. Jak to zrobić? Proces doboru hiperparametrów nazywa się **tuningiem hiperparametrów** (*hyperparameter tuning*).\n",
    "\n",
    "Istnieje na to wiele sposobów. Większość z nich polega na tym, że trenuje się za każdym razem model z nowym zestawem hiperparametrów i wybiera się ten zestaw, który pozwala uzyskać najlepsze wyniki. Metody głównie różnią się między sobą sposobem doboru kandydujących zestawów hiperparametrów.\n",
    "\n",
    "Najprostsze i najpopularniejsze to:\n",
    "\n",
    "* **pełne przeszukiwanie** (*grid search*) - definiujemy możliwe wartości dla różnych hiperparametrów, a metoda sprawdza ich wszystkie możliwe kombinacje (czyli siatkę),\n",
    "* **losowe przeszukiwanie** (*randomized search*) - definiujemy możliwe wartości jak w pełnym przeszukiwaniu, ale sprawdzamy tylko ograniczoną liczbę losowo wybranych kombinacji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak ocenić, jak dobry jest jakiś zestaw hiperparametrów? Nie możemy sprawdzić tego na zbiorze treningowym - wyniki byłyby zbyt optymistyczne. Nie możemy wykorzystać zbioru testowego - mielibyśmy data leakage, bo wybieralibyśmy model explicite pod nasz zbiór testowy. Trzeba zatem osobnego zbioru, na którym będziemy na bieżąco sprawdzać jakość modeli dla różnych hiperparametrów. Jest to **zbiór walidacyjny** (*validation set*).\n",
    "\n",
    "Zbiór taki wycina się ze zbioru treningowego. Dzielimy zatem nasze dane nie na dwie, ale trzy części: treningową, walidacyjną i testową. Typowe proporcje to 60-20-20% lub 80-10-10%.\n",
    "\n",
    "Metody tuningu hiperparametrów są zaimplementowane w Scikit-Learn jako `GridSearchCV` oraz `RandomizedSearchCV`. Są też bardziej wyspecjalizowane metody dla konkretnych modeli, które są dla nich typowo o wiele szybsze.\n",
    "\n",
    "**Uwaga:** warto zauważyć, że liczba możliwych kombinacji rośnie gwałtownie wraz z liczbą hiperparametrów i ich możliwych wartości. Mając siatkę na 3 hiperparametry po 10 możliwych wartości dla każdego, otrzymujemy 1000 możliwych kombinacji. W pracy w ML płacą nam też za to, że wiemy, jakie siatki dobrać :)\n",
    "\n",
    "#### Dla zainteresowanych\n",
    "\n",
    "Szczególnie inteligentne są metody tuningu z grupy metod optymalizacji bayesowskiej (Bayesian hyperparameter optimization / Bayesian HPO). Są to np. procesy Gaussowskie oraz Tree Parzen Estimator (TPE). Wykorzystują one dość zaawansowaną statystykę, aby zamodelować, jak poszczególne hiperparametry wpływają na wynik i dobierają takie kolejne kombinacje hiperparametrów, które są ich zdaniem najbardziej obiecujące. W szczególności wiele z tych metod traktuje dobór hiperparametrów jak problem regresji, gdzie parametrami są hiperparametry modelu, które dobieramy.\n",
    "\n",
    "Takich metod szczególnie często używa się przy tuningu hiperparametrów dla sieci neuronowej, gdyż jej wytrenowanie jest czasochłonne, a więc nie możemy pozwolić sobie na sprawdzenie licznych kombinacji, bo zbyt dużo by nas to kosztowało.\n",
    "\n",
    "Ta metoda została zaimplementowana w wielu frameworkach, jak np. Optuna czy Hyperopt. Więcej można o nich przeczytać [tutaj](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walidacja skrośna\n",
    "\n",
    "Jednorazowy podział zbioru na części nazywa się *split validation* lub *holdout*. Używamy go, gdy mamy sporo danych, i 10-20% zbioru jako dane walidacyjne czy testowe to dość dużo, żeby mieć przyzwoite oszacowanie. Zbyt mały zbiór walidacyjny czy testowy da nam mało wiarygodne wyniki - nie da się nawet powiedzieć, czy zbyt pesymityczne, czy optymistyczne! W praktyce niestety często mamy mało danych. Trzeba zatem jakiejś magicznej metody, która stworzy nam więcej zbiorów walidacyjnych z tej samej ilości danych.\n",
    "\n",
    "Taką metodą jest **walidacja skrośna** (*cross-validation, CV*). Polega na tym, że dzielimy zbiór na K równych podzbiorów, tzw. *foldów*. Każdy podzbiór po kolei staje się zbiorem walidacyjnym, a pozostałe łączymy w zbiór treningowy. Przykładowo, jeżeli mamy 5 foldów (1, 2, 3, 4, 5), to będziemy mieli po kolei:\n",
    "- zbiór treningowy: (2, 3, 4, 5), walidacyjny: (1)\n",
    "- zbiór treningowy: (1, 3, 4, 5), walidacyjny: (2)\n",
    "- zbiór treningowy: (1, 2, 4, 5), walidacyjny: (3)\n",
    "- zbiór treningowy: (1, 2, 3, 5), walidacyjny: (4)\n",
    "- zbiór treningowy: (1, 2, 3, 4), walidacyjny: (5)\n",
    "\n",
    "Trenujemy zatem K modeli dla tego samego zestawu hiperparametrów i każdy testujemy na zbiorze walidacyjnym. Mamy K wyników dla zbiorów walidacyjnych, które możemy uśrednić (i ew. obliczyć odchylenie standardowe). Takie wyniki są znacznie bardziej wiarygodne zgodnie ze statystyką (moc statystyczna itp.). Typowo używa się 5 lub 10 foldów, co jest dobrym balansem między liczbą modeli do wytrenowania i wielkością zbiorów walidacyjnych.\n",
    "\n",
    "Szczególnym przypadkiem jest Leave-One-Out Cross-Validation (LOOCV), w którym ilość podzbiorów (*foldów*) jest równa ilości rekordów. Czyli w danej chwili tylko 1 przykład jest zbiorem walidacyjnym. Daje to możliwość prawie całkowitego wykorzystania naszych danych (w każdej iteracji musimy wydzielić tylko 1 przykład na zbiór walidacyjny, cała reszta jest naszym zbiorem treningowym), ale wprowadza ogromny koszt obliczeniowy. Jest to opłacalne tylko w szczególnych przypadkach.\n",
    "\n",
    "Można zauważyć, że w nazwach klas do tuningu parametrów, wspomnianych wyżej, mamy sufiks `CV` - to jest właśnie *Cross Validation*.\n",
    "\n",
    "#### Dla zainteresowanych\n",
    "\n",
    "Walidacji skrośnej można użyć także do testowania, tworząc wiele zbiorów testowych. Można połączyć obie techniki, co daje tzw. [nested cross-validation](https://vitalflux.com/python-nested-cross-validation-algorithm-selection/). Jest to bardzo kosztowna, ale jednocześnie bardzo precyzyjna technika."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RidgeCV, LassoCV, ElasticNetCV\n",
    "\n",
    "W przypadku regresji liniowej istnieją bardzo wydajne implementacje walidacji skrośnej, głównie dzięki prostocie tego modelu. W Scikit-learn są to odpowiednio `RidgeCV`, `LassoCV` oraz `ElasticNetCV`.\n",
    "\n",
    "`RidgeCV` domyślnie wykorzystuje efektywną implementację Leave-One-Out Cross-Validation (LOOCV). Jest to możliwe dzięki pewnym sztuczkom opartym na algebrze liniowej, wyjaśnionych [w dokumentacji w kodzie](https://github.com/scikit-learn/scikit-learn/blob/8c9c1f27b7e21201cfffb118934999025fd50cca/sklearn/linear_model/_ridge.py#L1547) (dla zainteresowanych). Co ważne, jest to operacja o wiele szybsza niż osobne grid search + ridge regression, a nawet od `RidgeCV` z mniejszą liczbą foldów.\n",
    "\n",
    "`LassoCV` oraz `ElasticNetCV` iterują od najmniejszych do największych wartości `alpha` (siły regularyzacji), używając rozwiązania dla mniejszej siły regularyzacji jako punktu początkowego dla kolejnej wartości. Odpowiada to po prostu dość inteligentnemu wyborowi punktu startowego w optymalizacji funkcji kosztu, a znacznie obniża koszt obliczeniowy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 6 (1.0 punkt)**\n",
    "\n",
    "Użyj klas `RidgeCV` oraz `LassoCV` do tuningu hiperparametrów.\n",
    "\n",
    "Dla `RidgeCV` sprawdź 1000 wartości `[0.1, 100]` w skali liniowej - przyda się `np.linspace()`. Użyj LOOCV.\n",
    "\n",
    "Dla `LassoCV` Scikit-learn sam dobierze wartości, musisz podać tylko liczbę wartości alfa do sprawdzenia - użyj 1000. Użyj 5-fold CV. Pamiętaj o podaniu `random_state=0` - solver jest niedeterministyczny.\n",
    "\n",
    "Wypisz znalezione optymalne wartości siły regularyzacji `.alpha_` dla obu modeli, zaokrąglone do 4 miejsca po przecinku dla czytelności.\n",
    "\n",
    "---\n",
    "\n",
    "***Ciekawostka***\n",
    "\n",
    "Atrybuty z `_` (*underscore*) na końcu w Scikit-Learn oznaczają, że zostały one wyliczone podczas treningu (`.fit()`). W powyższym przypadku optymalny współczynnik regularyzacji `.alpha_` został wyznaczony dopiero po przeprowadzeniu tuningu hiperparametrów.\n",
    "\n",
    "Jeśli zajrzeć do [dokumentacji](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) dla klasy `LinearRegression`, to można zauważyć takie atrybuty jak `.coef_` przechowujący wyznaczone współczynniki cech, czy `.intercept_` - wyraz wolny.\n",
    "\n",
    "Takie atrybuty pozwalają przeprowadzić dogłębniejszą analizę wytrenowanego modelu.\n",
    "\n",
    "---\n",
    "\n",
    "Przetestuj modele z użyciem `assess_regression_model()`. Skomentuj wyniki. Czy udało się wyeliminować overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeCV alpha: 2.9000\n",
      "LassoCV alpha: 0.0003\n"
     ]
    }
   ],
   "source": [
    "# your_code\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "rcv = RidgeCV(alphas=np.linspace(0.1, 100,1000)).fit(X_train, y_train)\n",
    "lcv = LassoCV(n_alphas=1000,random_state=0).fit(X_train, y_train)\n",
    "print(f\"RidgeCV alpha: {rcv.alpha_:.4f}\")\n",
    "print(f\"LassoCV alpha: {lcv.alpha_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17212.412656167468\n",
      "18758.434421865353\n",
      "\n",
      "18108.042649753825\n",
      "18664.619729687958\n"
     ]
    }
   ],
   "source": [
    "assess_regression_model(rcv, X_train, X_test, y_train, y_test)\n",
    "print()\n",
    "assess_regression_model(lcv, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# // skomentuj tutaj\n",
    "### W przypadku L2, tuning parametru, zmienił wynik uzyskany na zbiorze testowym. Następuje teraz bardzo niewielki overfitting. W przypadku L1 wyniki na zbiorach testowym i treningowym uległy znaczącej poprawie. Nie następuje teraz ani overfitting, ani underfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresja wielomianowa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresja wielomianowa to po prostu dodanie wielomianów cech do naszych danych:\n",
    "$$\n",
    "[a, b, c, d] -> [a, b, c, d, a^2, b^2, c^2, d^2, ab, ac, ad, bc, bd, cd]\n",
    "$$\n",
    "\n",
    "Pozwala to na uwzględnienie bardziej złożonych kombinacji cech, których sama regresja liniowa, ze względu na swoją prostotę, nie jest w stanie uwzględnić.\n",
    "\n",
    "W Scikit-learn regresja wielomianowa składa się z 2 osobnych kroków: wygenerowania cech wielomianowych i użycia zwykłej regresji liniowej. Pozwala to na użycie tej transformacji dla dowolnych algorytmów, nie tylko regresji liniowej.\n",
    "\n",
    "Kwestią sporną jest, czy jest sens przeprowadzać taką transformację dla zmiennych po one-hot encodingu. Potęgi na pewno nie mają sensu, natomiast interakcje realizują po prostu operację koniunkcji (AND), ale łatwo prowadzi to do eksplozji wymiarowości. Dla uproszczenia poniżej zastosujemy transformację dla wszystkich cech.\n",
    "\n",
    "Warto pamiętać, że jeżeli używamy modelu, który sam dodaje intercept (jak regresja liniowa), to trzeba przekazać `include_bias=False`. Żeby wymiarowość zbytnio nam nie urosła, użyjemy `interaction_only=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12856.878895016682\n",
      "18298.30197590179\n",
      "\n",
      "Ridge + polynomial features alpha: 84.8000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "poly_features.fit(X_train)\n",
    "\n",
    "X_train_poly = poly_features.transform(X_train)\n",
    "X_test_poly = poly_features.transform(X_test)\n",
    "\n",
    "reg_ridge_cv_poly = RidgeCV(alphas=np.linspace(0.1, 100, 1000))\n",
    "reg_ridge_cv_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "assess_regression_model(reg_ridge_cv_poly, X_train_poly, X_test_poly, y_train, y_test)\n",
    "print()\n",
    "print(f\"Ridge + polynomial features alpha: {reg_ridge_cv_poly.alpha_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co ciekawe, model bardziej zbliżył się do przeuczenia, ale błąd testowy zmalał. Jest to niezbyt częste, ale możliwe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresja logistyczna\n",
    "\n",
    "Regresja logistyczna jest modelem, który pozwala na przewidywanie wartości zmiennych dychotomicznych w oparciu o jedną lub większą ilość cech. Funkcją bazową regresji logistycznej jest funkcja logistyczna. Bardzo ciekawe podsumowanie dotyczące matematyki stojącej za regresją logistyczną znajdziesz [tu](https://philippmuens.com/logistic-regression-from-scratch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do klasyfikacji wykorzystamy zbiór [Bank Marketing](https://archive.ics.uci.edu/ml/datasets/bank+marketing), w którym przewiduje się, czy dana osoba będzie zainteresowana lokatą terminową w banku. Precyzyjny targetowany marketing jest ważny z perspektywy biznesu, bo w praktyce chce się reklamować tak mało, jak to możliwe. Bank zarabia tylko na tych osobach, które są faktycznie zainteresowane reklamą, a pozostałych można łatwo zrazić zbyt dużą liczbą reklam, więc precyzyjna ocena przynosi tu realne zyski.\n",
    "\n",
    "Zbiór posiada dwie wersje, uproszczoną oraz rozszerzoną o dodatkowe atrybuty socjoekonomiczne (np. sytuację ekonomiczną w planowanym momencie reklamy). Wykorzystamy tę drugą, bo są to bardzo wartościowe cechy. Dodatkowo każda wersja posiada pełny zbiór (ok. 45 tysięcy przykładów) oraz pomniejszony (ok. 4 tysiąca przykładów). Dzięki skalowalności regresji logistycznej możemy bez problemu wykorzystać pełny zbiór z dodatkowymi cechami.\n",
    "\n",
    "Opisy zmiennych znajdują się w pliku [bank_marketing_description.txt](bank_marketing_description.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 7 (1.0 punkt)**\n",
    "\n",
    "*Wczytywanie i czyszczenie danych*\n",
    "\n",
    "1. Załaduj zbiór danych z pliku [bank_marketing_data.csv](bank_marketing_data.csv) do DataFrame'a. Zwróć uwagę, że separatorem jest średnik (argument `sep`).\n",
    "2. Usuń kolumny:\n",
    "    - `default`, czy klient ma zadłużenie na karcie kredytowej; ma tylko 3 wartości `yes`,\n",
    "    - `duration`, czas trwania ostatniego telefonu reklamowego; autorzy sugerują usunięcie w opisie zbioru, bo nie znamy tej wartości przed wykonaniem telefonu,\n",
    "    - `pdays`, liczba dni od ostatniego telefonu reklamowego w ramach danej kampanii marketingowej; jeżeli to pierwszy kontakt, to wartość to 999, i ciężko byłoby włączyć taką cechę do modelu, a mamy już i tak informację o tym, czy to pierwszy kontakt z klientem w zmiennej `previous`,\n",
    "    - `poutcome`, wynik poprzedniej kampanii; w zdecydowanej większości przypadków to `nonexistent`.\n",
    "3. Dokonaj filtrowania wierszy:\n",
    "    - usuń wiersze z `education` na poziomie `illiterate`, jest ich tylko kilkanaście.\n",
    "4. Zakoduj odpowiednio zmienne `education`, `contact`, `month`, `day_of_week` i `y`. Dla ułatwienia słowniki tych zmiennych są w zmiennych poniżej.\n",
    "5. Wyodrębnij kolumnę `y` do zmiennej `y` (pamiętaj o usunięciu jej z DataFrame'a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_mapping = {\n",
    "    \"basic.4y\": \"primary\",\n",
    "    \"basic.6y\": \"primary\",\n",
    "    \"basic.9y\": \"primary\",\n",
    "    \"high.school\": \"secondary\",\n",
    "    \"professional.course\": \"secondary\",\n",
    "    \"university.degree\": \"tertiary\",\n",
    "}\n",
    "\n",
    "contact_mapping = {\n",
    "    \"telephone\": 0,\n",
    "    \"cellular\": 1,\n",
    "}\n",
    "\n",
    "month_mapping = {\n",
    "    \"jan\": 1,\n",
    "    \"feb\": 2,\n",
    "    \"mar\": 3,\n",
    "    \"apr\": 4,\n",
    "    \"may\": 5,\n",
    "    \"jun\": 6,\n",
    "    \"jul\": 7,\n",
    "    \"aug\": 8,\n",
    "    \"sep\": 9,\n",
    "    \"oct\": 10,\n",
    "    \"nov\": 11,\n",
    "    \"dec\": 12,\n",
    "}\n",
    "\n",
    "day_of_week_mapping = {\n",
    "    \"mon\": 1,\n",
    "    \"tue\": 2,\n",
    "    \"wed\": 3,\n",
    "    \"thu\": 4,\n",
    "    \"fri\": 5,\n",
    "}\n",
    "\n",
    "y_mapping = {\n",
    "    \"no\": 0,\n",
    "    \"yes\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41188 entries, 0 to 41187\n",
      "Data columns (total 21 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   age             41188 non-null  int64  \n",
      " 1   job             41188 non-null  object \n",
      " 2   marital         41188 non-null  object \n",
      " 3   education       41188 non-null  object \n",
      " 4   default         41188 non-null  object \n",
      " 5   housing         41188 non-null  object \n",
      " 6   loan            41188 non-null  object \n",
      " 7   contact         41188 non-null  object \n",
      " 8   month           41188 non-null  object \n",
      " 9   day_of_week     41188 non-null  object \n",
      " 10  duration        41188 non-null  int64  \n",
      " 11  campaign        41188 non-null  int64  \n",
      " 12  pdays           41188 non-null  int64  \n",
      " 13  previous        41188 non-null  int64  \n",
      " 14  poutcome        41188 non-null  object \n",
      " 15  emp.var.rate    41188 non-null  float64\n",
      " 16  cons.price.idx  41188 non-null  float64\n",
      " 17  cons.conf.idx   41188 non-null  float64\n",
      " 18  euribor3m       41188 non-null  float64\n",
      " 19  nr.employed     41188 non-null  float64\n",
      " 20  y               41188 non-null  object \n",
      "dtypes: float64(5), int64(5), object(11)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "y1 = y\n",
    "df1 = pd.read_csv(\"bank_marketing_data.csv\",sep=\";\")# your_code\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop([\"default\", \"duration\", \"pdays\", \"poutcome\"], axis=\"columns\")\n",
    "df1 = df1.loc[~df1[\"education\"].isin([\"illiterate\"]), :]\n",
    "df1 = df1.replace({\"education\": education_mapping,\n",
    "                \"contact\":contact_mapping,\n",
    "                \"month\":month_mapping,\n",
    "                \"day_of_week\":day_of_week_mapping,\n",
    "                \"y\":y_mapping})\n",
    "y = df1.pop(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    8617\n",
       "1    8513\n",
       "3    8132\n",
       "2    8085\n",
       "5    7823\n",
       "Name: day_of_week, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['day_of_week'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 8 (0.5 punktu)**\n",
    "\n",
    "*Exploratory Data Analysis (EDA)*\n",
    "\n",
    "1. Sprawdź, czy są jakieś wartości brakujące za pomocą biblioteki `missingno`. Jeżeli tak, to sprawdź w dokumentacji zbioru, jaka byłaby sensowna wartość do ich uzupełnienia.\n",
    "2. Narysuj wykres (bar plot) z częstością klas. Uwzględnij częstość na wykresie ([to może się przydać](https://stackoverflow.com/a/68107610/9472066)). Pamiętaj o tytule i opisaniu osi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               False\n",
       "job               False\n",
       "marital           False\n",
       "education         False\n",
       "housing           False\n",
       "loan              False\n",
       "contact           False\n",
       "month             False\n",
       "day_of_week       False\n",
       "campaign          False\n",
       "previous          False\n",
       "emp.var.rate      False\n",
       "cons.price.idx    False\n",
       "cons.conf.idx     False\n",
       "euribor3m         False\n",
       "nr.employed       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot missing values\n",
    "# your_code\n",
    "msno.bar(df1)\n",
    "#plt.show()\n",
    "# your_code\n",
    "#nie ma żadnych NULLi, ani NaNów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot class frequencies\n",
    "y.value_counts().plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać, będziemy tu mieli do czynienia z problemem klasyfikacji niezbalansowanej. Na szczęście funkcja kosztu w regresji logistycznej pozwala na dodanie **wag klas (class weights)**, aby przypisać większą wagę interesującej nas klasie pozytywnej. Scikit-learn dla wartości `class_weights=\"balanced\"` obliczy wagi odwrotnie proporcjonalne do częstości danej klasy w zbiorze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 9 (1.0 punkt)**\n",
    "\n",
    "*Podział i preprocessing danych*\n",
    "\n",
    "1. Dokonaj podziału zbioru na treningowy i testowy w proporcjach 75%-25%. Pamiętaj o użyciu podziału ze stratyfikacją (argument `stratify`), aby zachować proporcje klas. Ustaw `random_state=0`.\n",
    "2. Stwórz `ColumnTransformer`, przetwarzający zmienne kategoryczne za pomocą `OneHotEncoder` (teraz już nie musimy robić `drop=\"first\"`), a numeryczne za pomocą `StandardScaler`. Zaaplikuj go do odpowiednich kolumn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your_code\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df1, y, test_size=0.25, random_state=0, stratify = y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "one_hot_encoder = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")#zmieniłem sparse_output na sparse, bo mam starszą wersję\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "categorical_features = df1.select_dtypes(include=\"object\").columns\n",
    "numerical_features = df1.select_dtypes(exclude=\"object\").columns\n",
    "\n",
    "categorical_pipeline = Pipeline([('one_hot_encoder', one_hot_encoder)])  # your_code_here\n",
    "\n",
    "numerical_pipeline = Pipeline([('standard_scaler', standard_scaler)])  # your_code_here\n",
    "\n",
    "column_transformer = ColumnTransformer(     [(\"categorical_pipeline\", categorical_pipeline, categorical_features),\n",
    "      (\"numerical_pipeline\", numerical_pipeline, numerical_features)])#, verbose_feature_names_out=False\n",
    "\n",
    "column_transformer.fit_transform(X_train.copy(),y_train.copy())\n",
    "X_train_transformed = column_transformer.transform(X_train.copy())\n",
    "X_test_transformed = column_transformer.transform(X_test.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30877, 37)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metryki klasyfikacji binarnej\n",
    "\n",
    "W klasyfikacji binarnej mamy tylko dwie klasy, z konwencji oznaczamy jedną klasę jako negatywną, a drugą - pozytywną. W naszym przypadku klasą negatywną będą osoby niezainteresowane lokatą - nie chcemy im pokazywać naszych reklam, bo to będzie raczej nieskuteczne, a reklama kosztuje. Naszym targetem będą osoby oznaczone klasą pozytywną.\n",
    "\n",
    "Wytrenowaliśmy model, ale jak sprawdzić jakość jego działania? Metryki z regresji raczej za wiele nam nie pomogą. Potrzebujemy zdefiniować nowe.\n",
    "\n",
    "#### Celność, dokładność (*Accuracy*)\n",
    "\n",
    "Najprostszym sposobem oceny klasyfikacji jest sprawdzić, w ilu przypadkach się mylimy, a w ilu model odpowiada poprawnie. Ta metryka jest zwana ***accuracy***. Ma ona jednak zasadniczą wadę - kompletnie nie radzi sobie z klasami niezbalansowanymi.\n",
    "\n",
    "Prosty przypadek - mamy zbiór danych, który pozwala na podstawie różnych parametrów medycznych wykryć rzadką chorobę, która zdarza się u 0.01% ludzi. Weźmy prosty klasyfikator, który zawsze zwraca klasę negatywną. Niby jest w oczywisty sposób kompletnie nieprzydatny, ale jednak dla losowej próbki ludzi dostanie ***celność*** równą 99.99%, bo, rzeczywiście, u większości tej choroby nie będzie.\n",
    "\n",
    "Potrzebujemy bardziej skomplikowanej metryki, której nie da się tak łatwo oszukać.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Macierz pomyłek (*Confusion Matrix*)\n",
    "\n",
    "Żeby zdefiniować taką metodę oceny klasyfikacji, musimy najpierw rozważyć jakie sytuacje mogą zdarzyć się przy klasyfikacji binarnej. Spójrzmy na tablicę poniżej:\n",
    "\n",
    "<div>\n",
    "<img src=\"confusion-matrix.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Występują tutaj przypadki:\n",
    "* ***prawdziwie pozytywne*** (*true positive*) - model zwrócił klasę pozytywną (*positive*), i jest to prawda (*true*)\n",
    "* ***prawdziwie negatwyne*** (*true negative*) - model zwrócił klasę negatywną (*negative*), i jest to prawda (*true*)\n",
    "* ***fałszywie negatywne*** (*false negative*) - model zwrócił klasę negatywną (*negative*), ale nie jest to prawda (*false*)\n",
    "* ***fałszywie pozytywne*** (*false positive*) - model zwrócił klasę pozytywną (*positive*), ale nie jest to prawda (*false*)\n",
    "\n",
    "Mając powyższe punkty - możemy zdefiniować ***celność*** następująco:\n",
    "\n",
    "$$\n",
    "accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "czyli ilość przypadków, w których poprawnie zidentykowaliśmy klasę, podzieloną przez ilość wszystkich przypadków.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Precyzja (miara predykcyjna dodatnia) i czułość (*Precision & Recall*)\n",
    "\n",
    "Jednak jak zauważyliśmy wcześniej, istnieją sytuacje, w których nie jest to właściwe podejście.\n",
    "\n",
    "Zdecydowanie ciekawszą dla nas metryką może być stwierdzenie jaką część rekordów z klasą pozytywną model poprawnie rozpoznał. Pozwoli to nam powiedzieć, jak czuły jest nasz model na klasę pozytywną. Ta metryka nazywa się czułością (***recall***):\n",
    "\n",
    "$$\n",
    "recall = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Jest o ilość przypadków, w których poprawnie rozpoznaliśmy klasę pozytywną, podzielona przez ilość wszystkich przypadków z klasą pozytywną.\n",
    "\n",
    "Drugą korzystną dla nas metryką będzie stwierdzenie ile z osób, które zakwalifikowaliśmy do klasy pozytywne, rzeczywiście do niej należy. Pozwoli to oszacować, jak często mylimy się oznaczając rekord klasą pozytywną. Ta metryka nazywa się precyzją (***precision***):\n",
    "\n",
    "$$\n",
    "precision = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "Jest to ilość przypadków, w których poprawnie rozpoznaliśmy klasę pozytywną, podzielona przez ilość wszystkich przypadków, w których zwróciliśmy klasę pozytywną.\n",
    "\n",
    "Ta metryka może być bardzo pomocna, na przykład, przy klasyfikacji spamu. Gorzej będzie, jeśli wrzucimy ważnego maila do spamu, niż przegapimy jakąś reklamę. Chcemy, aby jeśli coś zostało zaklasyfikowane jako spam, rzeczywiście nim było - chcemy jak najwyższą precyzję.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score\n",
    "\n",
    "Powyższe metryki mają wadę - pojedynczo można je łatwo oszukać:\n",
    "\n",
    "* Czy chcemy idealną ***precyzję***? - wystarczy zawsze zwracać klasę negatywną (ważny mail nie trafi do spamu, jeśli żadnego z nich tam nie wrzucimy).\n",
    "* Czy chcemy idealną ***czułość***? - zawsze zwracamy klasę pozytywną (na pewno nie pominiemy chorego pacjenta, jeśli każdemu powiemy, że jest chory).\n",
    "\n",
    "Musimy stosować je w parze. Dla prostoty, często agregujemy je do jednej zagregowanej miary za pomocą średniej harmonicznej. W przypadku liczb z zakresu $[0, 1]$ (a z takimi mamy do czynienia), ona ma taką własność, że wartość wynikowa zawsze będzie bliższa mniejszej wartości. I im większa jest między nimi różnica, tym bardziej jest to widoczne. Przykładowo, dla pary $(100\\%, 0\\%)$ średnia harmoniczna wynosi $0\\%$. Średnia harmoniczna z ***precyzji*** i ***czułości*** nazywana jest ***miarą F1*** (*F1 score*):\n",
    "\n",
    "$$\n",
    "F_1 = \\frac{2 \\cdot precision \\cdot recall}{precision + recall}\n",
    "$$\n",
    "\n",
    "[Ten tutorial](https://mlu-explain.github.io/precision-recall/) ma świetne wizualizację, które w interaktywny sposób prezentują działanie powyższych metryk.\n",
    "\n",
    "**Uwaga**:  indeks dolny w mierze $F_1$ oznacza, że mamy do czyninia z miarą, która daje taką samą wagę precyzji i czułości, ale w ogólnym przypadku jest to parametr, za pomocą którego możemy promować miarę, która ma dla nas większe znaczenie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 10 (2.0 punkty)**\n",
    "\n",
    "*Trening, tuning i analiza modeli*\n",
    "\n",
    "1. Wytrenuj podstawowy model regresji logistycznej z użyciem `LogisticRegression`. Użyj wag klas (`class_weights=\"balanced\"`). Przetestuj model, wypisując pecyzję, czułość oraz miarę F1 w procentach. **Uwaga:** Scikit-learn domyślnie stosuje tutaj regularyzację L2, więc przekaż `penalty=\"None\"`.\n",
    "2. Dokonaj tuningu modelu z regularyzacją L2 za pomocą `LogisticRegressionCV`:\n",
    "    - sprawdź 100 wartości, wystarczy podać liczbę do `Cs`,\n",
    "    - użyj 5-krotnej walidacji krzyżowej,\n",
    "    - wybierz najlepszy model według metryki F1 (parametr `scoring`),\n",
    "    - pamiętaj o `class_weights=\"balanced\"` i `random_state=0`,\n",
    "    - użyj `n_jobs=-1` dla przyspieszenia obliczeń (`-1` znaczy, że użyjemy wszystkich rdzeni do obliczeń),\n",
    "    - przetestuj model, wypisując precyzję, czułość i miarę F1 w procentach.\n",
    "    - **uwaga:** Scikit-learn stosuje tutaj konwencję, gdzie parametr `C` to odwrotność siły regularyzacji - im mniejszy, tym silniejsza regularyzacja.\n",
    "3. Dokonaj analogicznego tuningu, ale dla regularyzacji L1. Użyj solwera SAGA. Przetestuj model, wypisując precyzję, czułość i miarę F1 w procentach.\n",
    "4. Dokonaj analizy wytrenowanych modeli:\n",
    "    - Oblicz miarę F1 na zbiorze treningowym modelu bez żadnej regularyzacji i porównaj go z wynikiem testowym; czy występuje tutaj overfitting?\n",
    "    - Czy twoim zdaniem tworzenie modeli z regularyzacją ma sens w tym przypadku?\n",
    "\n",
    "Napisz co, w twojej opinii, jest ważniejsze dla naszego problemu, ***precision*** czy ***recall***? Jak moglibyśmy, nie zmieniając modelu, zmienić ich stosunek?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "def test(test,pred):\n",
    "    precission = precision_score(test, pred)\n",
    "    recall     = recall_score(test, pred)\n",
    "    F1         = f1_score(test, pred)\n",
    "    print(f\"precission: {precission*100:.2f}%\")\n",
    "    print(f\"recall:     {recall*100:.2f}%\")\n",
    "    print(f\"F1:         {F1*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precission: 26.38%\n",
      "recall:     66.87%\n",
      "F1:         37.83%\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "reg_logistic = LogisticRegression(penalty=\"none\",class_weight=\"balanced\")\n",
    "reg_logistic.fit(X_train_transformed, y_train)\n",
    "y_pred = reg_logistic.predict(X_test_transformed)\n",
    "#print(y_test.to_numpy())\n",
    "test(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precission: 26.38%\n",
      "recall:     66.87%\n",
      "F1:         37.83%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "lrcvl2 = LogisticRegressionCV(Cs=100,cv=5,\n",
    "                            scoring ='f1',\n",
    "                            class_weight=\"balanced\",\n",
    "                            random_state=0,\n",
    "                            n_jobs=-1)\n",
    "lrcvl2.fit(X_train_transformed, y_train)\n",
    "y_pred = lrcvl2.predict(X_test_transformed)\n",
    "test(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precission: 26.38%\n",
      "recall:     66.87%\n",
      "F1:         37.83%\n"
     ]
    }
   ],
   "source": [
    "lrcvl1 = LogisticRegressionCV(Cs=100,cv=5,\n",
    "                            scoring ='f1',\n",
    "                            penalty = 'l1',\n",
    "                            class_weight=\"balanced\",\n",
    "                            random_state=0,\n",
    "                            n_jobs=-1,\n",
    "                            solver = 'saga')\n",
    "lrcvl1.fit(X_train_transformed, y_train)\n",
    "y_pred = lrcvl1.predict(X_test_transformed)\n",
    "test(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tuning model on train dataset:\n",
      "precission: 26.88%\n",
      "recall:     68.33%\n",
      "F1:         38.59%\n",
      "L2 tuning model on train dataset:\n",
      "precission: 26.89%\n",
      "recall:     68.33%\n",
      "F1:         38.59%\n",
      "L1 tuning model on train dataset:\n",
      "precission: 26.88%\n",
      "recall:     68.33%\n",
      "F1:         38.59%\n"
     ]
    }
   ],
   "source": [
    "print(\"No tuning model on train dataset:\")\n",
    "y_pred = reg_logistic.predict(X_train_transformed)\n",
    "test(y_train, y_pred)\n",
    "print(\"L2 tuning model on train dataset:\")\n",
    "y_pred = lrcvl2.predict(X_train_transformed)\n",
    "test(y_train, y_pred)\n",
    "print(\"L1 tuning model on train dataset:\")\n",
    "y_pred = lrcvl1.predict(X_train_transformed)\n",
    "test(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj\n",
    "#### Precyzja, dokładność oraz miara F1 okazują się niskie dla każdego z powyższych modeli. dzieje się tak prawdopodobnie ze słabego wyboru modelu. być może niektóre zależności zbioru danych nie są liniowe, lub też dla podobnych albo wręcz tych samych sytuacji, jedni klienci decydowali się na lokatę, a inni nie. W powyższych modelach nie występuje overfitting. Tworzenie modeli z regularyzacją nie ma w tym wypadku wiele sensu, wyniki zupełnie się nie zmieniły\n",
    "\n",
    "## Dla naszego modelu ważniejsza jest precyzja: bank chce zadzwonić do wszystkich, którzy mogliby potencjalnie założyć lokatę. Ich wartości moglibyśmy zmienić ustawiając zdefiniowany przez nas parametr scoring, inny niż F1, tak aby nowa metryka \"premiowała\" wysokie wyniki precyzji bardziej niż wysokie wyniki czułości.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 11 (2.0 punkty)**\n",
    "\n",
    "*Dodanie cech wielomianowych do regresji logistycznej*\n",
    "\n",
    "1. Stwórz nowy pipeline do przetwarzania danych do regresji logistycznej, dodając `PolynomialFeatures` do zmiennych numerycznych przed standaryzacją. Wygeneruj cechy o stopniu 2, interakcje oraz potęgi, nie generuj interceptu.\n",
    "2. Wytrenuj model regresji logistycznej bez regularyzacji na takim powiększonym zbiorze. Wypisz F1 treningowy oraz testowy w procentach.\n",
    "3. Zdecyduj, czy jest sens tworzyć modele z regularyzacją. Jeżeli tak, to wytrenuj i dokonaj tuningu takich modeli. Jeżeli nie, to uzasadnij czemu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10293, 92)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df1, y, test_size=0.25, random_state=0, stratify = y)\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "\n",
    "categorical_pipeline = Pipeline([('one_hot_encoder', one_hot_encoder)])  # your_code_here\n",
    "\n",
    "numerical_pipeline = Pipeline([('polynomial_features',poly_features),('standard_scaler', standard_scaler)])  # your_code_here\n",
    "\n",
    "column_transformer = ColumnTransformer(     [(\"categorical_pipeline\", categorical_pipeline, categorical_features),\n",
    "      (\"numerical_pipeline\", numerical_pipeline, numerical_features)], verbose_feature_names_out=False)  # your_code_here\n",
    "\n",
    "df_transformed = column_transformer.fit_transform(X_train.copy(),y_train.copy())\n",
    "X_train = column_transformer.transform(X_train.copy())\n",
    "#y_train = column_transformer.transform(y_train.copy())\n",
    "X_test_transformed = column_transformer.transform(X_test.copy())\n",
    "#y_test = column_transformer.transform(y_test.copy())\n",
    "X_test_transformed.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precission: 30.18%\n",
      "recall:     65.57%\n",
      "F1:         41.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wlade\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "reg_logistic = LogisticRegression(penalty=\"none\",class_weight=\"balanced\")\n",
    "reg_logistic.fit(X_train, y_train)\n",
    "y_pred = reg_logistic.predict(X_test_transformed)\n",
    "#print(y_test.to_numpy())\n",
    "test(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj\n",
    "### dodatkowa regularyzacja modelu nie przyniesie najprawdopodobniej znaczącego efektu, gdyż model nie jest wystarczająco dopasowany do danych. Być może również  y (decyzja klienta) jest do pewnego stopnia niezależna / niedeterministyczna od parametrów danych. Klient w końcu sam podejmuje decyzję, czy podpisał lokatę terminową."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie dodatkowe (3 punkty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z formalnego, statystycznego punktu widzenia regresja liniowa czyni szereg założeń ([Wikipedia](https://en.wikipedia.org/wiki/Linear_regression#Assumptions)):\n",
    "1. Liniowość - relacja w danych może być reprezentowana jako `y=Xw`.\n",
    "2. Normalność błędów - błędy (rezydua) mają rozkład normalny, wycentrowany na zerze.\n",
    "3. Homoskedastyczność (stała wariancja) - wariancja błędu nie zależy od wartości docelowych `y`. Innymi słowy, nasz błąd będzie w przybliżeniu miał podobny \"rozrzut\" dla małych i dużych wartości `y`.\n",
    "4. Niezależność błędów - błąd i `y` są niezależne (w sensie statystycznym). Innymi słowy, nie ma między nimi bezpośredniej relacji. Jeżeli nie pracujemy z szeregami czasowymi, to to założenie po prostu jest spełnione.\n",
    "5. Brak współliniowości zmiennych - nie ma idealnej korelacji cech.\n",
    "\n",
    "Testowanie tych własności nie zawsze jest oczywiste, a w szczególności Scikit-learn oferuje tutaj dość mało opcji, bo pochodzą one głównie z tradycyjnej statystyki.\n",
    "\n",
    "1. Liniowość:\n",
    "  - numerycznie: wysoki współczynnik dopasowania modelu $R^2$ na zbiorze treningowym, niski błąd (RMSE) na zbiorze treningowym oraz testowym\n",
    "  - testem statystycznym: [Rainbow test](https://www.statsmodels.org/dev/generated/statsmodels.stats.diagnostic.linear_rainbow.html) lub [Harvey Collier test](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.linear_harvey_collier.html)\n",
    "  - graficznie: możliwe kiedy mamy 1/2 zmienne i da się narysować wykres zmiennej zależnej względem cech\n",
    "2. Normalność błędów:\n",
    "  - graficznie: robimy histogram rezyduów, powinien mieć kształt rozkładu normalnego i być wycentrowany na zerze\n",
    "  - testem statystycznym: [Jarque-Bera test](https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test), [Omnibus normality test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html)\n",
    "3. Homoskedastyczność:\n",
    "  - graficznie: robimy scatter plot rezyduów dla wartości przewidywanych od najmniejszej do największej, nie powinno być na nim żadnych widocznych wzorców czy kształtów; [przykład 1](https://towardsdatascience.com/multivariant-linear-regression-e636a4f99b40), [przykład 2](https://www.vexpower.com/brief/homoskedasticity)\n",
    "  - testem statystycznym: [Breusch–Pagan test](https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test) lub [Goldfeld-Quandt test](https://en.wikipedia.org/wiki/Goldfeld%E2%80%93Quandt_test)\n",
    "4. Niezależność błędów - nie omawiam, bo dotyczy tylko szeregów czasowych.\n",
    "5. Brak współliniowości zmiennych: numerycznie, sprawdzić korelacje zmiennych, lub współczynnik uwarunkowania macierzy `X`\n",
    "\n",
    "\n",
    "W ramach zadania wytrenuj model regresji liniowej dla zbioru danych Ames Housing z użyciem biblioteki Statsmodels: [OLS docs](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html), [OLS](https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html), [Regression diagnostics](https://www.statsmodels.org/dev/examples/notebooks/generated/regression_diagnostics.html). Wytrenuj najpierw model bez regularyzacji, a następnie z regularyzacją L2 oraz L1. Nie przeprowadzaj tuningu, użyj tych wartości siły regularyzacji, które wyznaczyliśmy wcześniej.\n",
    "\n",
    "Przetestuj założenia za pomocą testów statystycznych: Harvey Collier, Jarque-Bera, Breusch–Pagan. Współliniowość zmiennych zweryfikuj z użyciem współczynnika uwarunkowania. Zastosuj poziom istotności $\\alpha=0.05$.\n",
    "\n",
    "Czy założenia są spełnione w przypadku podstawowego modelu i/lub modeli z regularyzacją? Czy modele regularyzowane w lepszym stopniu spełniają założenia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.api as sms\n",
    "from patsy import dmatrices\n",
    "from statsmodels.stats.diagnostic import linear_harvey_collier\n",
    "from statsmodels.stats.stattools import jarque_bera\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = OneHotEncoder(drop=\"first\", sparse=False, handle_unknown=\"ignore\")\n",
    "median_imputer = SimpleImputer(strategy=\"median\")\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "categorical_features = df.select_dtypes(include=\"object\").columns\n",
    "numerical_features = df.select_dtypes(exclude=\"object\").columns\n",
    "\n",
    "categorical_pipeline = Pipeline([('one_hot_encoder', one_hot_encoder)])  # your_code_here\n",
    "\n",
    "numerical_pipeline = Pipeline([('median_imputer', median_imputer),('min_max_scaler', min_max_scaler)])\n",
    "\n",
    "column_transformer = ColumnTransformer(     [(\"categorical_pipeline\", categorical_pipeline, categorical_features),\n",
    "      (\"numerical_pipeline\", numerical_pipeline, numerical_features)], verbose_feature_names_out=False)  # your_code_here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fit and transform\n",
    "#df_transformed = column_transformer.fit_transform(X_train.copy(),y_train.copy())\n",
    "#X_train = column_transformer.transform(X_train.copy())\n",
    "#y_train = column_transformer.transform(y_train.copy())\n",
    "#X_test = column_transformer.transform(X_test.copy())\n",
    "#y_test = column_transformer.transform(y_test.copy())\n",
    "# your_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wlade\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:170: UserWarning: Found unknown categories in columns [12, 17] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, y1, test_size=0.25, random_state=0)\n",
    "df_transformed = column_transformer.fit_transform(X_train.copy(),y_train.copy())\n",
    "X_train = column_transformer.transform(X_train.copy())\n",
    "X_test = column_transformer.transform(X_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y_train, X_train)\n",
    "res = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "resL1 = model.fit_regularized(method='elastic_net',\n",
    "                    alpha=lcv.alpha_,\n",
    "                    L1_wt=1.0,\n",
    "                    start_params=None,\n",
    "                    profile_scale=False,\n",
    "                    refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "resL2 = model.fit_regularized(method='elastic_net',\n",
    "                    alpha=rcv.alpha_,\n",
    "                    L1_wt=0.000001,# jeżeli dam 0.0, to jest błąd Not implemrnted error, a method ma tylko 2 możliwe wartości: to lub sqrt_lasso\n",
    "                    start_params=None,\n",
    "                    profile_scale=False,\n",
    "                    refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>SalePrice</td>    <th>  R-squared:         </th> <td>   0.940</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.933</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   132.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 25 Oct 2023</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:17:51</td>     <th>  Log-Likelihood:    </th> <td>  1926.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2191</td>      <th>  AIC:               </th> <td>  -3390.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1959</td>      <th>  BIC:               </th> <td>  -2069.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   231</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>   <td>   -0.2067</td> <td>    0.123</td> <td>   -1.677</td> <td> 0.094</td> <td>   -0.448</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>   <td>   -0.0657</td> <td>    0.034</td> <td>   -1.910</td> <td> 0.056</td> <td>   -0.133</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>   <td>   -0.0333</td> <td>    0.054</td> <td>   -0.618</td> <td> 0.537</td> <td>   -0.139</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>   <td>    0.1136</td> <td>    0.109</td> <td>    1.044</td> <td> 0.297</td> <td>   -0.100</td> <td>    0.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>   <td>    0.0826</td> <td>    0.051</td> <td>    1.618</td> <td> 0.106</td> <td>   -0.018</td> <td>    0.183</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>   <td>    0.0164</td> <td>    0.053</td> <td>    0.307</td> <td> 0.759</td> <td>   -0.088</td> <td>    0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>   <td>    0.0716</td> <td>    0.074</td> <td>    0.965</td> <td> 0.335</td> <td>   -0.074</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>   <td>    0.1073</td> <td>    0.083</td> <td>    1.299</td> <td> 0.194</td> <td>   -0.055</td> <td>    0.269</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>   <td>    0.1229</td> <td>    0.060</td> <td>    2.044</td> <td> 0.041</td> <td>    0.005</td> <td>    0.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>  <td>    0.0506</td> <td>    0.057</td> <td>    0.883</td> <td> 0.378</td> <td>   -0.062</td> <td>    0.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>  <td>    0.1203</td> <td>    0.058</td> <td>    2.058</td> <td> 0.040</td> <td>    0.006</td> <td>    0.235</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>  <td>    0.1098</td> <td>    0.071</td> <td>    1.554</td> <td> 0.120</td> <td>   -0.029</td> <td>    0.248</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>  <td>    0.0032</td> <td>    0.076</td> <td>    0.042</td> <td> 0.967</td> <td>   -0.146</td> <td>    0.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>  <td>    0.0953</td> <td>    0.064</td> <td>    1.498</td> <td> 0.134</td> <td>   -0.029</td> <td>    0.220</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>  <td>    0.0074</td> <td>    0.029</td> <td>    0.253</td> <td> 0.800</td> <td>   -0.050</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>  <td>    1.0350</td> <td>    0.148</td> <td>    7.001</td> <td> 0.000</td> <td>    0.745</td> <td>    1.325</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>  <td>    1.3097</td> <td>    0.148</td> <td>    8.876</td> <td> 0.000</td> <td>    1.020</td> <td>    1.599</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>  <td>    1.2635</td> <td>    0.170</td> <td>    7.451</td> <td> 0.000</td> <td>    0.931</td> <td>    1.596</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>  <td>    1.2585</td> <td>    0.148</td> <td>    8.476</td> <td> 0.000</td> <td>    0.967</td> <td>    1.550</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>  <td>    1.2886</td> <td>    0.145</td> <td>    8.864</td> <td> 0.000</td> <td>    1.004</td> <td>    1.574</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>  <td>    1.2326</td> <td>    0.145</td> <td>    8.516</td> <td> 0.000</td> <td>    0.949</td> <td>    1.516</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>  <td>    0.0254</td> <td>    0.020</td> <td>    1.298</td> <td> 0.195</td> <td>   -0.013</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>  <td>    0.0195</td> <td>    0.024</td> <td>    0.811</td> <td> 0.417</td> <td>   -0.028</td> <td>    0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>  <td>    0.0087</td> <td>    0.014</td> <td>    0.629</td> <td> 0.530</td> <td>   -0.019</td> <td>    0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>  <td>    0.0034</td> <td>    0.012</td> <td>    0.271</td> <td> 0.786</td> <td>   -0.021</td> <td>    0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>  <td>   -0.0263</td> <td>    0.017</td> <td>   -1.577</td> <td> 0.115</td> <td>   -0.059</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>  <td>   -0.0199</td> <td>    0.033</td> <td>   -0.609</td> <td> 0.542</td> <td>   -0.084</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>  <td>   -0.0040</td> <td>    0.007</td> <td>   -0.606</td> <td> 0.545</td> <td>   -0.017</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>  <td>    0.1225</td> <td>    0.060</td> <td>    2.025</td> <td> 0.043</td> <td>    0.004</td> <td>    0.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>  <td>    0.0455</td> <td>    0.043</td> <td>    1.055</td> <td> 0.291</td> <td>   -0.039</td> <td>    0.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>  <td>    0.0839</td> <td>    0.035</td> <td>    2.399</td> <td> 0.017</td> <td>    0.015</td> <td>    0.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>  <td>    0.0553</td> <td>    0.036</td> <td>    1.527</td> <td> 0.127</td> <td>   -0.016</td> <td>    0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>  <td>    0.0072</td> <td>    0.028</td> <td>    0.259</td> <td> 0.795</td> <td>   -0.047</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>  <td>    0.1147</td> <td>    0.032</td> <td>    3.565</td> <td> 0.000</td> <td>    0.052</td> <td>    0.178</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>  <td>   -0.0114</td> <td>    0.030</td> <td>   -0.380</td> <td> 0.704</td> <td>   -0.070</td> <td>    0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>  <td>    0.0126</td> <td>    0.029</td> <td>    0.431</td> <td> 0.667</td> <td>   -0.045</td> <td>    0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>  <td>    0.1030</td> <td>    0.052</td> <td>    1.991</td> <td> 0.047</td> <td>    0.002</td> <td>    0.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>  <td>    0.0246</td> <td>    0.038</td> <td>    0.648</td> <td> 0.517</td> <td>   -0.050</td> <td>    0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>  <td>   -0.0666</td> <td>    0.044</td> <td>   -1.508</td> <td> 0.132</td> <td>   -0.153</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>  <td>    0.0148</td> <td>    0.030</td> <td>    0.491</td> <td> 0.623</td> <td>   -0.044</td> <td>    0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>  <td>    0.0143</td> <td>    0.030</td> <td>    0.482</td> <td> 0.630</td> <td>   -0.044</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>  <td>    0.0853</td> <td>    0.053</td> <td>    1.624</td> <td> 0.105</td> <td>   -0.018</td> <td>    0.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>  <td>    0.0034</td> <td>    0.030</td> <td>    0.113</td> <td> 0.910</td> <td>   -0.056</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>  <td>    0.0631</td> <td>    0.033</td> <td>    1.906</td> <td> 0.057</td> <td>   -0.002</td> <td>    0.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>  <td>    0.0888</td> <td>    0.028</td> <td>    3.167</td> <td> 0.002</td> <td>    0.034</td> <td>    0.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>  <td>    0.0101</td> <td>    0.035</td> <td>    0.288</td> <td> 0.773</td> <td>   -0.059</td> <td>    0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>  <td>    0.0251</td> <td>    0.037</td> <td>    0.671</td> <td> 0.502</td> <td>   -0.048</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>  <td>    0.0325</td> <td>    0.031</td> <td>    1.059</td> <td> 0.290</td> <td>   -0.028</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>  <td>    0.0041</td> <td>    0.029</td> <td>    0.141</td> <td> 0.888</td> <td>   -0.053</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>  <td>    0.0646</td> <td>    0.033</td> <td>    1.940</td> <td> 0.053</td> <td>   -0.001</td> <td>    0.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>  <td>    0.1118</td> <td>    0.031</td> <td>    3.552</td> <td> 0.000</td> <td>    0.050</td> <td>    0.174</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th>  <td>    0.0186</td> <td>    0.031</td> <td>    0.607</td> <td> 0.544</td> <td>   -0.042</td> <td>    0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>  <td>    0.0007</td> <td>    0.039</td> <td>    0.019</td> <td> 0.985</td> <td>   -0.075</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th>  <td>    0.0126</td> <td>    0.018</td> <td>    0.696</td> <td> 0.487</td> <td>   -0.023</td> <td>    0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th>  <td>    0.0601</td> <td>    0.015</td> <td>    4.012</td> <td> 0.000</td> <td>    0.031</td> <td>    0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th>  <td>    0.0790</td> <td>    0.035</td> <td>    2.268</td> <td> 0.023</td> <td>    0.011</td> <td>    0.147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th>  <td>    0.0720</td> <td>    0.026</td> <td>    2.750</td> <td> 0.006</td> <td>    0.021</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th>  <td>    0.0003</td> <td>    0.029</td> <td>    0.011</td> <td> 0.991</td> <td>   -0.056</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th>  <td>    0.0174</td> <td>    0.028</td> <td>    0.627</td> <td> 0.531</td> <td>   -0.037</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60</th>  <td>    0.0092</td> <td>    0.066</td> <td>    0.140</td> <td> 0.889</td> <td>   -0.120</td> <td>    0.139</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x61</th>  <td>    0.0067</td> <td>    0.044</td> <td>    0.154</td> <td> 0.877</td> <td>   -0.079</td> <td>    0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x62</th>  <td>   -0.1014</td> <td>    0.072</td> <td>   -1.412</td> <td> 0.158</td> <td>   -0.242</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x63</th>  <td>   -0.0619</td> <td>    0.060</td> <td>   -1.029</td> <td> 0.304</td> <td>   -0.180</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x64</th>  <td>    0.0066</td> <td>    0.086</td> <td>    0.076</td> <td> 0.939</td> <td>   -0.162</td> <td>    0.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x65</th>  <td>   -0.0361</td> <td>    0.101</td> <td>   -0.358</td> <td> 0.720</td> <td>   -0.234</td> <td>    0.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x66</th>  <td>   -0.1022</td> <td>    0.165</td> <td>   -0.618</td> <td> 0.537</td> <td>   -0.426</td> <td>    0.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x67</th>  <td>   -0.0648</td> <td>    0.126</td> <td>   -0.516</td> <td> 0.606</td> <td>   -0.311</td> <td>    0.182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x68</th>  <td>    0.0138</td> <td>    0.125</td> <td>    0.111</td> <td> 0.912</td> <td>   -0.232</td> <td>    0.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x69</th>  <td>   -0.0365</td> <td>    0.098</td> <td>   -0.371</td> <td> 0.711</td> <td>   -0.230</td> <td>    0.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x70</th>  <td>    0.0074</td> <td>    0.029</td> <td>    0.253</td> <td> 0.800</td> <td>   -0.050</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x71</th>  <td>   -0.0238</td> <td>    0.054</td> <td>   -0.438</td> <td> 0.662</td> <td>   -0.130</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x72</th>  <td>    0.0241</td> <td>    0.051</td> <td>    0.472</td> <td> 0.637</td> <td>   -0.076</td> <td>    0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x73</th>  <td>    0.0293</td> <td>    0.064</td> <td>    0.459</td> <td> 0.646</td> <td>   -0.096</td> <td>    0.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x74</th>  <td>    0.0424</td> <td>    0.033</td> <td>    1.300</td> <td> 0.194</td> <td>   -0.022</td> <td>    0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x75</th>  <td>   -0.0370</td> <td>    0.066</td> <td>   -0.561</td> <td> 0.575</td> <td>   -0.166</td> <td>    0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x76</th>  <td>    0.0683</td> <td>    0.046</td> <td>    1.474</td> <td> 0.141</td> <td>   -0.023</td> <td>    0.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77</th>  <td>    0.0464</td> <td>    0.031</td> <td>    1.515</td> <td> 0.130</td> <td>   -0.014</td> <td>    0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x78</th>  <td>    0.0410</td> <td>    0.044</td> <td>    0.934</td> <td> 0.350</td> <td>   -0.045</td> <td>    0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x79</th>  <td>    0.1049</td> <td>    0.061</td> <td>    1.719</td> <td> 0.086</td> <td>   -0.015</td> <td>    0.225</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x80</th>  <td>    0.0091</td> <td>    0.061</td> <td>    0.150</td> <td> 0.881</td> <td>   -0.110</td> <td>    0.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x81</th>  <td>   -0.0161</td> <td>    0.067</td> <td>   -0.240</td> <td> 0.810</td> <td>   -0.148</td> <td>    0.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x82</th>  <td>    0.0090</td> <td>    0.061</td> <td>    0.147</td> <td> 0.883</td> <td>   -0.111</td> <td>    0.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x83</th>  <td>   -0.1428</td> <td>    0.077</td> <td>   -1.857</td> <td> 0.064</td> <td>   -0.294</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x84</th>  <td>   -0.0215</td> <td>    0.094</td> <td>   -0.229</td> <td> 0.819</td> <td>   -0.205</td> <td>    0.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x85</th>  <td>    0.1668</td> <td>    0.127</td> <td>    1.317</td> <td> 0.188</td> <td>   -0.082</td> <td>    0.415</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x86</th>  <td>    0.0686</td> <td>    0.127</td> <td>    0.540</td> <td> 0.590</td> <td>   -0.181</td> <td>    0.318</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x87</th>  <td>    0.0803</td> <td>    0.114</td> <td>    0.702</td> <td> 0.483</td> <td>   -0.144</td> <td>    0.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x88</th>  <td>   -0.0318</td> <td>    0.056</td> <td>   -0.571</td> <td> 0.568</td> <td>   -0.141</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x89</th>  <td>    0.0184</td> <td>    0.049</td> <td>    0.378</td> <td> 0.705</td> <td>   -0.077</td> <td>    0.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x90</th>  <td>    0.0555</td> <td>    0.054</td> <td>    1.027</td> <td> 0.305</td> <td>   -0.050</td> <td>    0.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x91</th>  <td>   -0.0625</td> <td>    0.121</td> <td>   -0.515</td> <td> 0.607</td> <td>   -0.301</td> <td>    0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x92</th>  <td>    0.0724</td> <td>    0.071</td> <td>    1.021</td> <td> 0.307</td> <td>   -0.067</td> <td>    0.212</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x93</th>  <td>    0.1329</td> <td>    0.044</td> <td>    3.037</td> <td> 0.002</td> <td>    0.047</td> <td>    0.219</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x94</th>  <td>    1.3690</td> <td>    0.176</td> <td>    7.765</td> <td> 0.000</td> <td>    1.023</td> <td>    1.715</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x95</th>  <td>   -0.0024</td> <td>    0.089</td> <td>   -0.027</td> <td> 0.978</td> <td>   -0.176</td> <td>    0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x96</th>  <td>    0.0460</td> <td>    0.042</td> <td>    1.094</td> <td> 0.274</td> <td>   -0.036</td> <td>    0.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x97</th>  <td>   -0.0132</td> <td>    0.121</td> <td>   -0.109</td> <td> 0.913</td> <td>   -0.250</td> <td>    0.223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x98</th>  <td>    0.0520</td> <td>    0.051</td> <td>    1.027</td> <td> 0.305</td> <td>   -0.047</td> <td>    0.151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x99</th>  <td>    0.0582</td> <td>    0.041</td> <td>    1.406</td> <td> 0.160</td> <td>   -0.023</td> <td>    0.139</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x100</th> <td>    0.3099</td> <td>    0.059</td> <td>    5.233</td> <td> 0.000</td> <td>    0.194</td> <td>    0.426</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x101</th> <td>   -0.0475</td> <td>    0.099</td> <td>   -0.480</td> <td> 0.631</td> <td>   -0.242</td> <td>    0.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x102</th> <td>    0.0535</td> <td>    0.050</td> <td>    1.070</td> <td> 0.285</td> <td>   -0.045</td> <td>    0.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x103</th> <td>    0.0679</td> <td>    0.047</td> <td>    1.435</td> <td> 0.151</td> <td>   -0.025</td> <td>    0.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x104</th> <td>    0.0547</td> <td>    0.041</td> <td>    1.346</td> <td> 0.179</td> <td>   -0.025</td> <td>    0.134</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x105</th> <td>    0.0709</td> <td>    0.046</td> <td>    1.553</td> <td> 0.121</td> <td>   -0.019</td> <td>    0.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x106</th> <td>    0.1467</td> <td>    0.093</td> <td>    1.575</td> <td> 0.115</td> <td>   -0.036</td> <td>    0.329</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x107</th> <td>   -0.0400</td> <td>    0.065</td> <td>   -0.620</td> <td> 0.535</td> <td>   -0.167</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x108</th> <td>   -0.0228</td> <td>    0.047</td> <td>   -0.482</td> <td> 0.630</td> <td>   -0.115</td> <td>    0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x109</th> <td>    0.0915</td> <td>    0.089</td> <td>    1.024</td> <td> 0.306</td> <td>   -0.084</td> <td>    0.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x110</th> <td>    0.0145</td> <td>    0.043</td> <td>    0.339</td> <td> 0.735</td> <td>   -0.069</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x111</th> <td>    0.0185</td> <td>    0.052</td> <td>    0.353</td> <td> 0.724</td> <td>   -0.084</td> <td>    0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x112</th> <td>    0.0262</td> <td>    0.051</td> <td>    0.511</td> <td> 0.610</td> <td>   -0.074</td> <td>    0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x113</th> <td>   -0.0792</td> <td>    0.120</td> <td>   -0.662</td> <td> 0.508</td> <td>   -0.314</td> <td>    0.156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x114</th> <td>    0.0021</td> <td>    0.041</td> <td>    0.051</td> <td> 0.959</td> <td>   -0.079</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x115</th> <td>    0.3099</td> <td>    0.059</td> <td>    5.233</td> <td> 0.000</td> <td>    0.194</td> <td>    0.426</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x116</th> <td>    0.0827</td> <td>    0.076</td> <td>    1.086</td> <td> 0.278</td> <td>   -0.067</td> <td>    0.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x117</th> <td>    0.0333</td> <td>    0.050</td> <td>    0.664</td> <td> 0.507</td> <td>   -0.065</td> <td>    0.132</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x118</th> <td>    0.0004</td> <td>    0.048</td> <td>    0.009</td> <td> 0.993</td> <td>   -0.093</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x119</th> <td>    0.0176</td> <td>    0.041</td> <td>    0.424</td> <td> 0.671</td> <td>   -0.064</td> <td>    0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x120</th> <td>   -0.0009</td> <td>    0.045</td> <td>   -0.021</td> <td> 0.983</td> <td>   -0.089</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x121</th> <td>    0.0333</td> <td>    0.026</td> <td>    1.281</td> <td> 0.200</td> <td>   -0.018</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x122</th> <td>   -0.2970</td> <td>    0.136</td> <td>   -2.179</td> <td> 0.029</td> <td>   -0.564</td> <td>   -0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x123</th> <td>    0.0386</td> <td>    0.026</td> <td>    1.482</td> <td> 0.139</td> <td>   -0.013</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x124</th> <td>    0.0541</td> <td>    0.027</td> <td>    1.969</td> <td> 0.049</td> <td>    0.000</td> <td>    0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x125</th> <td>    0.0172</td> <td>    0.011</td> <td>    1.506</td> <td> 0.132</td> <td>   -0.005</td> <td>    0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x126</th> <td>    0.0355</td> <td>    0.013</td> <td>    2.821</td> <td> 0.005</td> <td>    0.011</td> <td>    0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x127</th> <td>    0.0263</td> <td>    0.032</td> <td>    0.828</td> <td> 0.408</td> <td>   -0.036</td> <td>    0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x128</th> <td>    0.0792</td> <td>    0.045</td> <td>    1.748</td> <td> 0.081</td> <td>   -0.010</td> <td>    0.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x129</th> <td>   -0.0604</td> <td>    0.053</td> <td>   -1.145</td> <td> 0.252</td> <td>   -0.164</td> <td>    0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x130</th> <td>    0.1052</td> <td>    0.113</td> <td>    0.933</td> <td> 0.351</td> <td>   -0.116</td> <td>    0.326</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x131</th> <td>    0.1529</td> <td>    0.116</td> <td>    1.322</td> <td> 0.186</td> <td>   -0.074</td> <td>    0.380</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x132</th> <td>    0.0135</td> <td>    0.120</td> <td>    0.112</td> <td> 0.911</td> <td>   -0.221</td> <td>    0.248</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x133</th> <td>    0.0565</td> <td>    0.162</td> <td>    0.350</td> <td> 0.726</td> <td>   -0.260</td> <td>    0.373</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x134</th> <td>    0.1389</td> <td>    0.134</td> <td>    1.038</td> <td> 0.299</td> <td>   -0.123</td> <td>    0.401</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x135</th> <td>    0.0319</td> <td>    0.013</td> <td>    2.454</td> <td> 0.014</td> <td>    0.006</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x136</th> <td>   -0.0266</td> <td>    0.021</td> <td>   -1.259</td> <td> 0.208</td> <td>   -0.068</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x137</th> <td>    0.0304</td> <td>    0.048</td> <td>    0.628</td> <td> 0.530</td> <td>   -0.064</td> <td>    0.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x138</th> <td>   -0.0107</td> <td>    0.011</td> <td>   -0.978</td> <td> 0.328</td> <td>   -0.032</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x139</th> <td>    0.0573</td> <td>    0.110</td> <td>    0.520</td> <td> 0.603</td> <td>   -0.159</td> <td>    0.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x140</th> <td>    0.0500</td> <td>    0.029</td> <td>    1.744</td> <td> 0.081</td> <td>   -0.006</td> <td>    0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x141</th> <td>    0.0623</td> <td>    0.040</td> <td>    1.574</td> <td> 0.116</td> <td>   -0.015</td> <td>    0.140</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x142</th> <td>    0.0468</td> <td>    0.031</td> <td>    1.500</td> <td> 0.134</td> <td>   -0.014</td> <td>    0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x143</th> <td>   -0.0078</td> <td>    0.044</td> <td>   -0.177</td> <td> 0.860</td> <td>   -0.094</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x144</th> <td>    0.0337</td> <td>    0.029</td> <td>    1.177</td> <td> 0.239</td> <td>   -0.022</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x145</th> <td>    0.0416</td> <td>    0.100</td> <td>    0.416</td> <td> 0.678</td> <td>   -0.155</td> <td>    0.238</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x146</th> <td>    0.0871</td> <td>    0.106</td> <td>    0.819</td> <td> 0.413</td> <td>   -0.122</td> <td>    0.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x147</th> <td>   -0.0099</td> <td>    0.007</td> <td>   -1.350</td> <td> 0.177</td> <td>   -0.024</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x148</th> <td>    0.0018</td> <td>    0.009</td> <td>    0.207</td> <td> 0.836</td> <td>   -0.015</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x149</th> <td>   -0.0150</td> <td>    0.017</td> <td>   -0.868</td> <td> 0.386</td> <td>   -0.049</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x150</th> <td>    0.0104</td> <td>    0.014</td> <td>    0.743</td> <td> 0.458</td> <td>   -0.017</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x151</th> <td>   -0.0286</td> <td>    0.047</td> <td>   -0.613</td> <td> 0.540</td> <td>   -0.120</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x152</th> <td>    0.0038</td> <td>    0.013</td> <td>    0.300</td> <td> 0.764</td> <td>   -0.021</td> <td>    0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x153</th> <td>    0.0137</td> <td>    0.101</td> <td>    0.136</td> <td> 0.892</td> <td>   -0.184</td> <td>    0.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x154</th> <td>    0.0518</td> <td>    0.110</td> <td>    0.469</td> <td> 0.639</td> <td>   -0.165</td> <td>    0.268</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x155</th> <td>    0.0055</td> <td>    0.094</td> <td>    0.058</td> <td> 0.954</td> <td>   -0.180</td> <td>    0.191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x156</th> <td>   -0.2412</td> <td>    0.153</td> <td>   -1.573</td> <td> 0.116</td> <td>   -0.542</td> <td>    0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x157</th> <td>   -0.0080</td> <td>    0.012</td> <td>   -0.679</td> <td> 0.497</td> <td>   -0.031</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x158</th> <td>    0.0047</td> <td>    0.015</td> <td>    0.317</td> <td> 0.751</td> <td>   -0.024</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x159</th> <td>   -0.0115</td> <td>    0.013</td> <td>   -0.866</td> <td> 0.387</td> <td>   -0.038</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x160</th> <td>    0.0068</td> <td>    0.014</td> <td>    0.487</td> <td> 0.626</td> <td>   -0.020</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x161</th> <td>    0.0048</td> <td>    0.010</td> <td>    0.486</td> <td> 0.627</td> <td>   -0.015</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x162</th> <td>   -0.0043</td> <td>    0.010</td> <td>   -0.446</td> <td> 0.656</td> <td>   -0.023</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x163</th> <td>   -0.0042</td> <td>    0.012</td> <td>   -0.358</td> <td> 0.721</td> <td>   -0.027</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x164</th> <td>    0.0067</td> <td>    0.010</td> <td>    0.664</td> <td> 0.507</td> <td>   -0.013</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x165</th> <td>   -0.0147</td> <td>    0.013</td> <td>   -1.092</td> <td> 0.275</td> <td>   -0.041</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x166</th> <td>   -0.0130</td> <td>    0.013</td> <td>   -1.016</td> <td> 0.310</td> <td>   -0.038</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x167</th> <td>    0.0048</td> <td>    0.013</td> <td>    0.379</td> <td> 0.704</td> <td>   -0.020</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x168</th> <td>    0.0199</td> <td>    0.037</td> <td>    0.545</td> <td> 0.586</td> <td>   -0.052</td> <td>    0.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x169</th> <td>    0.1293</td> <td>    0.052</td> <td>    2.475</td> <td> 0.013</td> <td>    0.027</td> <td>    0.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x170</th> <td>    0.0458</td> <td>    0.030</td> <td>    1.515</td> <td> 0.130</td> <td>   -0.013</td> <td>    0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x171</th> <td>   -0.0370</td> <td>    0.044</td> <td>   -0.833</td> <td> 0.405</td> <td>   -0.124</td> <td>    0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x172</th> <td>   -0.0033</td> <td>    0.053</td> <td>   -0.062</td> <td> 0.951</td> <td>   -0.107</td> <td>    0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x173</th> <td>    0.1013</td> <td>    0.054</td> <td>    1.863</td> <td> 0.063</td> <td>   -0.005</td> <td>    0.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x174</th> <td>    0.0474</td> <td>    0.047</td> <td>    1.003</td> <td> 0.316</td> <td>   -0.045</td> <td>    0.140</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x175</th> <td>    0.0008</td> <td>    0.111</td> <td>    0.007</td> <td> 0.994</td> <td>   -0.217</td> <td>    0.218</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x176</th> <td>    0.0083</td> <td>    0.015</td> <td>    0.552</td> <td> 0.581</td> <td>   -0.021</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x177</th> <td>    0.2288</td> <td>    0.040</td> <td>    5.677</td> <td> 0.000</td> <td>    0.150</td> <td>    0.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x178</th> <td>    0.1388</td> <td>    0.035</td> <td>    3.949</td> <td> 0.000</td> <td>    0.070</td> <td>    0.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x179</th> <td>    0.0497</td> <td>    0.022</td> <td>    2.288</td> <td> 0.022</td> <td>    0.007</td> <td>    0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x180</th> <td>    0.0965</td> <td>    0.011</td> <td>    9.087</td> <td> 0.000</td> <td>    0.076</td> <td>    0.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x181</th> <td>    0.0386</td> <td>    0.052</td> <td>    0.742</td> <td> 0.458</td> <td>   -0.063</td> <td>    0.141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x182</th> <td>   -0.0006</td> <td>    0.027</td> <td>   -0.024</td> <td> 0.981</td> <td>   -0.053</td> <td>    0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x183</th> <td>    0.4940</td> <td>    0.087</td> <td>    5.651</td> <td> 0.000</td> <td>    0.323</td> <td>    0.665</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x184</th> <td>    0.0110</td> <td>    0.047</td> <td>    0.234</td> <td> 0.815</td> <td>   -0.081</td> <td>    0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x185</th> <td>   -0.0081</td> <td>    0.016</td> <td>   -0.503</td> <td> 0.615</td> <td>   -0.040</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x186</th> <td>    0.0085</td> <td>    0.016</td> <td>    0.546</td> <td> 0.585</td> <td>   -0.022</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x187</th> <td>    0.1713</td> <td>    0.121</td> <td>    1.413</td> <td> 0.158</td> <td>   -0.066</td> <td>    0.409</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x188</th> <td>   -0.0147</td> <td>    0.026</td> <td>   -0.568</td> <td> 0.570</td> <td>   -0.065</td> <td>    0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x189</th> <td>    0.4781</td> <td>    0.034</td> <td>   14.173</td> <td> 0.000</td> <td>    0.412</td> <td>    0.544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x190</th> <td>    0.3253</td> <td>    0.026</td> <td>   12.727</td> <td> 0.000</td> <td>    0.275</td> <td>    0.375</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x191</th> <td>    0.2544</td> <td>    0.042</td> <td>    6.124</td> <td> 0.000</td> <td>    0.173</td> <td>    0.336</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x192</th> <td>    0.0258</td> <td>    0.012</td> <td>    2.108</td> <td> 0.035</td> <td>    0.002</td> <td>    0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x193</th> <td>    0.0571</td> <td>    0.036</td> <td>    1.603</td> <td> 0.109</td> <td>   -0.013</td> <td>    0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x194</th> <td>    0.0269</td> <td>    0.023</td> <td>    1.158</td> <td> 0.247</td> <td>   -0.019</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x195</th> <td>    0.0162</td> <td>    0.030</td> <td>    0.541</td> <td> 0.589</td> <td>   -0.043</td> <td>    0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x196</th> <td>    0.0662</td> <td>    0.031</td> <td>    2.153</td> <td> 0.031</td> <td>    0.006</td> <td>    0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x197</th> <td>    0.0363</td> <td>    0.037</td> <td>    0.984</td> <td> 0.325</td> <td>   -0.036</td> <td>    0.109</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x198</th> <td>    0.0371</td> <td>    0.010</td> <td>    3.864</td> <td> 0.000</td> <td>    0.018</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x199</th> <td>    0.0174</td> <td>    0.012</td> <td>    1.503</td> <td> 0.133</td> <td>   -0.005</td> <td>    0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x200</th> <td>    0.1683</td> <td>    0.022</td> <td>    7.699</td> <td> 0.000</td> <td>    0.125</td> <td>    0.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x201</th> <td>    0.0113</td> <td>    0.026</td> <td>    0.426</td> <td> 0.670</td> <td>   -0.041</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x202</th> <td>    0.0547</td> <td>    0.034</td> <td>    1.601</td> <td> 0.110</td> <td>   -0.012</td> <td>    0.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x203</th> <td>    0.0075</td> <td>    0.019</td> <td>    0.393</td> <td> 0.694</td> <td>   -0.030</td> <td>    0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x204</th> <td>    0.1516</td> <td>    0.027</td> <td>    5.556</td> <td> 0.000</td> <td>    0.098</td> <td>    0.205</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x205</th> <td>    0.0596</td> <td>    0.014</td> <td>    4.222</td> <td> 0.000</td> <td>    0.032</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x206</th> <td>  816.3474</td> <td>   27.220</td> <td>   29.991</td> <td> 0.000</td> <td>  762.965</td> <td>  869.730</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x207</th> <td>  440.7742</td> <td>   14.699</td> <td>   29.987</td> <td> 0.000</td> <td>  411.947</td> <td>  469.601</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x208</th> <td>  251.8819</td> <td>    8.400</td> <td>   29.986</td> <td> 0.000</td> <td>  235.408</td> <td>  268.356</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x209</th> <td> -807.0342</td> <td>   26.943</td> <td>  -29.953</td> <td> 0.000</td> <td> -859.874</td> <td> -754.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x210</th> <td>    0.0402</td> <td>    0.021</td> <td>    1.894</td> <td> 0.058</td> <td>   -0.001</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x211</th> <td>    0.0021</td> <td>    0.021</td> <td>    0.098</td> <td> 0.922</td> <td>   -0.040</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x212</th> <td>    0.0696</td> <td>    0.032</td> <td>    2.180</td> <td> 0.029</td> <td>    0.007</td> <td>    0.132</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x213</th> <td>    0.0393</td> <td>    0.016</td> <td>    2.512</td> <td> 0.012</td> <td>    0.009</td> <td>    0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x214</th> <td>   -0.0926</td> <td>    0.040</td> <td>   -2.320</td> <td> 0.020</td> <td>   -0.171</td> <td>   -0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x215</th> <td>   -0.0878</td> <td>    0.072</td> <td>   -1.212</td> <td> 0.226</td> <td>   -0.230</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x216</th> <td>    0.0699</td> <td>    0.024</td> <td>    2.975</td> <td> 0.003</td> <td>    0.024</td> <td>    0.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x217</th> <td>    0.0414</td> <td>    0.038</td> <td>    1.098</td> <td> 0.273</td> <td>   -0.033</td> <td>    0.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x218</th> <td>    0.2077</td> <td>    0.030</td> <td>    6.936</td> <td> 0.000</td> <td>    0.149</td> <td>    0.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x219</th> <td>    0.0644</td> <td>    0.032</td> <td>    1.993</td> <td> 0.046</td> <td>    0.001</td> <td>    0.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x220</th> <td>    0.0029</td> <td>    0.015</td> <td>    0.195</td> <td> 0.845</td> <td>   -0.026</td> <td>    0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x221</th> <td>    0.0450</td> <td>    0.025</td> <td>    1.795</td> <td> 0.073</td> <td>   -0.004</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x222</th> <td>    0.1366</td> <td>    0.033</td> <td>    4.103</td> <td> 0.000</td> <td>    0.071</td> <td>    0.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x223</th> <td>    0.0328</td> <td>    0.043</td> <td>    0.768</td> <td> 0.443</td> <td>   -0.051</td> <td>    0.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x224</th> <td>    0.0888</td> <td>    0.065</td> <td>    1.374</td> <td> 0.170</td> <td>   -0.038</td> <td>    0.216</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x225</th> <td>    0.1003</td> <td>    0.068</td> <td>    1.485</td> <td> 0.138</td> <td>   -0.032</td> <td>    0.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x226</th> <td>    0.0313</td> <td>    0.012</td> <td>    2.583</td> <td> 0.010</td> <td>    0.008</td> <td>    0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x227</th> <td>    0.0689</td> <td>    0.030</td> <td>    2.266</td> <td> 0.024</td> <td>    0.009</td> <td>    0.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x228</th> <td>    0.0109</td> <td>    0.031</td> <td>    0.351</td> <td> 0.725</td> <td>   -0.050</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x229</th> <td>    0.1806</td> <td>    0.043</td> <td>    4.173</td> <td> 0.000</td> <td>    0.096</td> <td>    0.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x230</th> <td>    0.0715</td> <td>    0.049</td> <td>    1.474</td> <td> 0.141</td> <td>   -0.024</td> <td>    0.167</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x231</th> <td>    0.1387</td> <td>    0.026</td> <td>    5.373</td> <td> 0.000</td> <td>    0.088</td> <td>    0.189</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x232</th> <td>    0.1901</td> <td>    0.153</td> <td>    1.243</td> <td> 0.214</td> <td>   -0.110</td> <td>    0.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x233</th> <td>   -0.1480</td> <td>    0.175</td> <td>   -0.846</td> <td> 0.398</td> <td>   -0.491</td> <td>    0.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x234</th> <td>    0.0697</td> <td>    0.146</td> <td>    0.476</td> <td> 0.634</td> <td>   -0.217</td> <td>    0.356</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x235</th> <td>   -0.0162</td> <td>    0.008</td> <td>   -2.136</td> <td> 0.033</td> <td>   -0.031</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1070.411</td> <th>  Durbin-Watson:     </th> <td>   2.012</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>35537.311</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>-1.670</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>22.445</td>  <th>  Cond. No.          </th> <td>1.17e+16</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 4.06e-28. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              SalePrice   R-squared:                       0.940\n",
       "Model:                            OLS   Adj. R-squared:                  0.933\n",
       "Method:                 Least Squares   F-statistic:                     132.5\n",
       "Date:                Wed, 25 Oct 2023   Prob (F-statistic):               0.00\n",
       "Time:                        22:17:51   Log-Likelihood:                 1926.9\n",
       "No. Observations:                2191   AIC:                            -3390.\n",
       "Df Residuals:                    1959   BIC:                            -2069.\n",
       "Df Model:                         231                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1            -0.2067      0.123     -1.677      0.094      -0.448       0.035\n",
       "x2            -0.0657      0.034     -1.910      0.056      -0.133       0.002\n",
       "x3            -0.0333      0.054     -0.618      0.537      -0.139       0.073\n",
       "x4             0.1136      0.109      1.044      0.297      -0.100       0.327\n",
       "x5             0.0826      0.051      1.618      0.106      -0.018       0.183\n",
       "x6             0.0164      0.053      0.307      0.759      -0.088       0.121\n",
       "x7             0.0716      0.074      0.965      0.335      -0.074       0.217\n",
       "x8             0.1073      0.083      1.299      0.194      -0.055       0.269\n",
       "x9             0.1229      0.060      2.044      0.041       0.005       0.241\n",
       "x10            0.0506      0.057      0.883      0.378      -0.062       0.163\n",
       "x11            0.1203      0.058      2.058      0.040       0.006       0.235\n",
       "x12            0.1098      0.071      1.554      0.120      -0.029       0.248\n",
       "x13            0.0032      0.076      0.042      0.967      -0.146       0.153\n",
       "x14            0.0953      0.064      1.498      0.134      -0.029       0.220\n",
       "x15            0.0074      0.029      0.253      0.800      -0.050       0.065\n",
       "x16            1.0350      0.148      7.001      0.000       0.745       1.325\n",
       "x17            1.3097      0.148      8.876      0.000       1.020       1.599\n",
       "x18            1.2635      0.170      7.451      0.000       0.931       1.596\n",
       "x19            1.2585      0.148      8.476      0.000       0.967       1.550\n",
       "x20            1.2886      0.145      8.864      0.000       1.004       1.574\n",
       "x21            1.2326      0.145      8.516      0.000       0.949       1.516\n",
       "x22            0.0254      0.020      1.298      0.195      -0.013       0.064\n",
       "x23            0.0195      0.024      0.811      0.417      -0.028       0.067\n",
       "x24            0.0087      0.014      0.629      0.530      -0.019       0.036\n",
       "x25            0.0034      0.012      0.271      0.786      -0.021       0.028\n",
       "x26           -0.0263      0.017     -1.577      0.115      -0.059       0.006\n",
       "x27           -0.0199      0.033     -0.609      0.542      -0.084       0.044\n",
       "x28           -0.0040      0.007     -0.606      0.545      -0.017       0.009\n",
       "x29            0.1225      0.060      2.025      0.043       0.004       0.241\n",
       "x30            0.0455      0.043      1.055      0.291      -0.039       0.130\n",
       "x31            0.0839      0.035      2.399      0.017       0.015       0.152\n",
       "x32            0.0553      0.036      1.527      0.127      -0.016       0.126\n",
       "x33            0.0072      0.028      0.259      0.795      -0.047       0.061\n",
       "x34            0.1147      0.032      3.565      0.000       0.052       0.178\n",
       "x35           -0.0114      0.030     -0.380      0.704      -0.070       0.048\n",
       "x36            0.0126      0.029      0.431      0.667      -0.045       0.070\n",
       "x37            0.1030      0.052      1.991      0.047       0.002       0.204\n",
       "x38            0.0246      0.038      0.648      0.517      -0.050       0.099\n",
       "x39           -0.0666      0.044     -1.508      0.132      -0.153       0.020\n",
       "x40            0.0148      0.030      0.491      0.623      -0.044       0.074\n",
       "x41            0.0143      0.030      0.482      0.630      -0.044       0.072\n",
       "x42            0.0853      0.053      1.624      0.105      -0.018       0.188\n",
       "x43            0.0034      0.030      0.113      0.910      -0.056       0.063\n",
       "x44            0.0631      0.033      1.906      0.057      -0.002       0.128\n",
       "x45            0.0888      0.028      3.167      0.002       0.034       0.144\n",
       "x46            0.0101      0.035      0.288      0.773      -0.059       0.079\n",
       "x47            0.0251      0.037      0.671      0.502      -0.048       0.098\n",
       "x48            0.0325      0.031      1.059      0.290      -0.028       0.093\n",
       "x49            0.0041      0.029      0.141      0.888      -0.053       0.061\n",
       "x50            0.0646      0.033      1.940      0.053      -0.001       0.130\n",
       "x51            0.1118      0.031      3.552      0.000       0.050       0.174\n",
       "x52            0.0186      0.031      0.607      0.544      -0.042       0.079\n",
       "x53            0.0007      0.039      0.019      0.985      -0.075       0.077\n",
       "x54            0.0126      0.018      0.696      0.487      -0.023       0.048\n",
       "x55            0.0601      0.015      4.012      0.000       0.031       0.089\n",
       "x56            0.0790      0.035      2.268      0.023       0.011       0.147\n",
       "x57            0.0720      0.026      2.750      0.006       0.021       0.123\n",
       "x58            0.0003      0.029      0.011      0.991      -0.056       0.057\n",
       "x59            0.0174      0.028      0.627      0.531      -0.037       0.072\n",
       "x60            0.0092      0.066      0.140      0.889      -0.120       0.139\n",
       "x61            0.0067      0.044      0.154      0.877      -0.079       0.092\n",
       "x62           -0.1014      0.072     -1.412      0.158      -0.242       0.039\n",
       "x63           -0.0619      0.060     -1.029      0.304      -0.180       0.056\n",
       "x64            0.0066      0.086      0.076      0.939      -0.162       0.175\n",
       "x65           -0.0361      0.101     -0.358      0.720      -0.234       0.161\n",
       "x66           -0.1022      0.165     -0.618      0.537      -0.426       0.222\n",
       "x67           -0.0648      0.126     -0.516      0.606      -0.311       0.182\n",
       "x68            0.0138      0.125      0.111      0.912      -0.232       0.260\n",
       "x69           -0.0365      0.098     -0.371      0.711      -0.230       0.157\n",
       "x70            0.0074      0.029      0.253      0.800      -0.050       0.065\n",
       "x71           -0.0238      0.054     -0.438      0.662      -0.130       0.083\n",
       "x72            0.0241      0.051      0.472      0.637      -0.076       0.124\n",
       "x73            0.0293      0.064      0.459      0.646      -0.096       0.155\n",
       "x74            0.0424      0.033      1.300      0.194      -0.022       0.106\n",
       "x75           -0.0370      0.066     -0.561      0.575      -0.166       0.092\n",
       "x76            0.0683      0.046      1.474      0.141      -0.023       0.159\n",
       "x77            0.0464      0.031      1.515      0.130      -0.014       0.106\n",
       "x78            0.0410      0.044      0.934      0.350      -0.045       0.127\n",
       "x79            0.1049      0.061      1.719      0.086      -0.015       0.225\n",
       "x80            0.0091      0.061      0.150      0.881      -0.110       0.129\n",
       "x81           -0.0161      0.067     -0.240      0.810      -0.148       0.115\n",
       "x82            0.0090      0.061      0.147      0.883      -0.111       0.129\n",
       "x83           -0.1428      0.077     -1.857      0.064      -0.294       0.008\n",
       "x84           -0.0215      0.094     -0.229      0.819      -0.205       0.163\n",
       "x85            0.1668      0.127      1.317      0.188      -0.082       0.415\n",
       "x86            0.0686      0.127      0.540      0.590      -0.181       0.318\n",
       "x87            0.0803      0.114      0.702      0.483      -0.144       0.305\n",
       "x88           -0.0318      0.056     -0.571      0.568      -0.141       0.078\n",
       "x89            0.0184      0.049      0.378      0.705      -0.077       0.114\n",
       "x90            0.0555      0.054      1.027      0.305      -0.050       0.161\n",
       "x91           -0.0625      0.121     -0.515      0.607      -0.301       0.176\n",
       "x92            0.0724      0.071      1.021      0.307      -0.067       0.212\n",
       "x93            0.1329      0.044      3.037      0.002       0.047       0.219\n",
       "x94            1.3690      0.176      7.765      0.000       1.023       1.715\n",
       "x95           -0.0024      0.089     -0.027      0.978      -0.176       0.172\n",
       "x96            0.0460      0.042      1.094      0.274      -0.036       0.128\n",
       "x97           -0.0132      0.121     -0.109      0.913      -0.250       0.223\n",
       "x98            0.0520      0.051      1.027      0.305      -0.047       0.151\n",
       "x99            0.0582      0.041      1.406      0.160      -0.023       0.139\n",
       "x100           0.3099      0.059      5.233      0.000       0.194       0.426\n",
       "x101          -0.0475      0.099     -0.480      0.631      -0.242       0.146\n",
       "x102           0.0535      0.050      1.070      0.285      -0.045       0.152\n",
       "x103           0.0679      0.047      1.435      0.151      -0.025       0.161\n",
       "x104           0.0547      0.041      1.346      0.179      -0.025       0.134\n",
       "x105           0.0709      0.046      1.553      0.121      -0.019       0.160\n",
       "x106           0.1467      0.093      1.575      0.115      -0.036       0.329\n",
       "x107          -0.0400      0.065     -0.620      0.535      -0.167       0.087\n",
       "x108          -0.0228      0.047     -0.482      0.630      -0.115       0.070\n",
       "x109           0.0915      0.089      1.024      0.306      -0.084       0.267\n",
       "x110           0.0145      0.043      0.339      0.735      -0.069       0.098\n",
       "x111           0.0185      0.052      0.353      0.724      -0.084       0.121\n",
       "x112           0.0262      0.051      0.511      0.610      -0.074       0.127\n",
       "x113          -0.0792      0.120     -0.662      0.508      -0.314       0.156\n",
       "x114           0.0021      0.041      0.051      0.959      -0.079       0.083\n",
       "x115           0.3099      0.059      5.233      0.000       0.194       0.426\n",
       "x116           0.0827      0.076      1.086      0.278      -0.067       0.232\n",
       "x117           0.0333      0.050      0.664      0.507      -0.065       0.132\n",
       "x118           0.0004      0.048      0.009      0.993      -0.093       0.094\n",
       "x119           0.0176      0.041      0.424      0.671      -0.064       0.099\n",
       "x120          -0.0009      0.045     -0.021      0.983      -0.089       0.087\n",
       "x121           0.0333      0.026      1.281      0.200      -0.018       0.084\n",
       "x122          -0.2970      0.136     -2.179      0.029      -0.564      -0.030\n",
       "x123           0.0386      0.026      1.482      0.139      -0.013       0.090\n",
       "x124           0.0541      0.027      1.969      0.049       0.000       0.108\n",
       "x125           0.0172      0.011      1.506      0.132      -0.005       0.040\n",
       "x126           0.0355      0.013      2.821      0.005       0.011       0.060\n",
       "x127           0.0263      0.032      0.828      0.408      -0.036       0.088\n",
       "x128           0.0792      0.045      1.748      0.081      -0.010       0.168\n",
       "x129          -0.0604      0.053     -1.145      0.252      -0.164       0.043\n",
       "x130           0.1052      0.113      0.933      0.351      -0.116       0.326\n",
       "x131           0.1529      0.116      1.322      0.186      -0.074       0.380\n",
       "x132           0.0135      0.120      0.112      0.911      -0.221       0.248\n",
       "x133           0.0565      0.162      0.350      0.726      -0.260       0.373\n",
       "x134           0.1389      0.134      1.038      0.299      -0.123       0.401\n",
       "x135           0.0319      0.013      2.454      0.014       0.006       0.057\n",
       "x136          -0.0266      0.021     -1.259      0.208      -0.068       0.015\n",
       "x137           0.0304      0.048      0.628      0.530      -0.064       0.125\n",
       "x138          -0.0107      0.011     -0.978      0.328      -0.032       0.011\n",
       "x139           0.0573      0.110      0.520      0.603      -0.159       0.273\n",
       "x140           0.0500      0.029      1.744      0.081      -0.006       0.106\n",
       "x141           0.0623      0.040      1.574      0.116      -0.015       0.140\n",
       "x142           0.0468      0.031      1.500      0.134      -0.014       0.108\n",
       "x143          -0.0078      0.044     -0.177      0.860      -0.094       0.078\n",
       "x144           0.0337      0.029      1.177      0.239      -0.022       0.090\n",
       "x145           0.0416      0.100      0.416      0.678      -0.155       0.238\n",
       "x146           0.0871      0.106      0.819      0.413      -0.122       0.296\n",
       "x147          -0.0099      0.007     -1.350      0.177      -0.024       0.004\n",
       "x148           0.0018      0.009      0.207      0.836      -0.015       0.019\n",
       "x149          -0.0150      0.017     -0.868      0.386      -0.049       0.019\n",
       "x150           0.0104      0.014      0.743      0.458      -0.017       0.038\n",
       "x151          -0.0286      0.047     -0.613      0.540      -0.120       0.063\n",
       "x152           0.0038      0.013      0.300      0.764      -0.021       0.029\n",
       "x153           0.0137      0.101      0.136      0.892      -0.184       0.211\n",
       "x154           0.0518      0.110      0.469      0.639      -0.165       0.268\n",
       "x155           0.0055      0.094      0.058      0.954      -0.180       0.191\n",
       "x156          -0.2412      0.153     -1.573      0.116      -0.542       0.059\n",
       "x157          -0.0080      0.012     -0.679      0.497      -0.031       0.015\n",
       "x158           0.0047      0.015      0.317      0.751      -0.024       0.034\n",
       "x159          -0.0115      0.013     -0.866      0.387      -0.038       0.015\n",
       "x160           0.0068      0.014      0.487      0.626      -0.020       0.034\n",
       "x161           0.0048      0.010      0.486      0.627      -0.015       0.024\n",
       "x162          -0.0043      0.010     -0.446      0.656      -0.023       0.015\n",
       "x163          -0.0042      0.012     -0.358      0.721      -0.027       0.019\n",
       "x164           0.0067      0.010      0.664      0.507      -0.013       0.027\n",
       "x165          -0.0147      0.013     -1.092      0.275      -0.041       0.012\n",
       "x166          -0.0130      0.013     -1.016      0.310      -0.038       0.012\n",
       "x167           0.0048      0.013      0.379      0.704      -0.020       0.030\n",
       "x168           0.0199      0.037      0.545      0.586      -0.052       0.091\n",
       "x169           0.1293      0.052      2.475      0.013       0.027       0.232\n",
       "x170           0.0458      0.030      1.515      0.130      -0.013       0.105\n",
       "x171          -0.0370      0.044     -0.833      0.405      -0.124       0.050\n",
       "x172          -0.0033      0.053     -0.062      0.951      -0.107       0.100\n",
       "x173           0.1013      0.054      1.863      0.063      -0.005       0.208\n",
       "x174           0.0474      0.047      1.003      0.316      -0.045       0.140\n",
       "x175           0.0008      0.111      0.007      0.994      -0.217       0.218\n",
       "x176           0.0083      0.015      0.552      0.581      -0.021       0.038\n",
       "x177           0.2288      0.040      5.677      0.000       0.150       0.308\n",
       "x178           0.1388      0.035      3.949      0.000       0.070       0.208\n",
       "x179           0.0497      0.022      2.288      0.022       0.007       0.092\n",
       "x180           0.0965      0.011      9.087      0.000       0.076       0.117\n",
       "x181           0.0386      0.052      0.742      0.458      -0.063       0.141\n",
       "x182          -0.0006      0.027     -0.024      0.981      -0.053       0.052\n",
       "x183           0.4940      0.087      5.651      0.000       0.323       0.665\n",
       "x184           0.0110      0.047      0.234      0.815      -0.081       0.103\n",
       "x185          -0.0081      0.016     -0.503      0.615      -0.040       0.023\n",
       "x186           0.0085      0.016      0.546      0.585      -0.022       0.039\n",
       "x187           0.1713      0.121      1.413      0.158      -0.066       0.409\n",
       "x188          -0.0147      0.026     -0.568      0.570      -0.065       0.036\n",
       "x189           0.4781      0.034     14.173      0.000       0.412       0.544\n",
       "x190           0.3253      0.026     12.727      0.000       0.275       0.375\n",
       "x191           0.2544      0.042      6.124      0.000       0.173       0.336\n",
       "x192           0.0258      0.012      2.108      0.035       0.002       0.050\n",
       "x193           0.0571      0.036      1.603      0.109      -0.013       0.127\n",
       "x194           0.0269      0.023      1.158      0.247      -0.019       0.072\n",
       "x195           0.0162      0.030      0.541      0.589      -0.043       0.075\n",
       "x196           0.0662      0.031      2.153      0.031       0.006       0.127\n",
       "x197           0.0363      0.037      0.984      0.325      -0.036       0.109\n",
       "x198           0.0371      0.010      3.864      0.000       0.018       0.056\n",
       "x199           0.0174      0.012      1.503      0.133      -0.005       0.040\n",
       "x200           0.1683      0.022      7.699      0.000       0.125       0.211\n",
       "x201           0.0113      0.026      0.426      0.670      -0.041       0.063\n",
       "x202           0.0547      0.034      1.601      0.110      -0.012       0.122\n",
       "x203           0.0075      0.019      0.393      0.694      -0.030       0.045\n",
       "x204           0.1516      0.027      5.556      0.000       0.098       0.205\n",
       "x205           0.0596      0.014      4.222      0.000       0.032       0.087\n",
       "x206         816.3474     27.220     29.991      0.000     762.965     869.730\n",
       "x207         440.7742     14.699     29.987      0.000     411.947     469.601\n",
       "x208         251.8819      8.400     29.986      0.000     235.408     268.356\n",
       "x209        -807.0342     26.943    -29.953      0.000    -859.874    -754.194\n",
       "x210           0.0402      0.021      1.894      0.058      -0.001       0.082\n",
       "x211           0.0021      0.021      0.098      0.922      -0.040       0.044\n",
       "x212           0.0696      0.032      2.180      0.029       0.007       0.132\n",
       "x213           0.0393      0.016      2.512      0.012       0.009       0.070\n",
       "x214          -0.0926      0.040     -2.320      0.020      -0.171      -0.014\n",
       "x215          -0.0878      0.072     -1.212      0.226      -0.230       0.054\n",
       "x216           0.0699      0.024      2.975      0.003       0.024       0.116\n",
       "x217           0.0414      0.038      1.098      0.273      -0.033       0.115\n",
       "x218           0.2077      0.030      6.936      0.000       0.149       0.266\n",
       "x219           0.0644      0.032      1.993      0.046       0.001       0.128\n",
       "x220           0.0029      0.015      0.195      0.845      -0.026       0.032\n",
       "x221           0.0450      0.025      1.795      0.073      -0.004       0.094\n",
       "x222           0.1366      0.033      4.103      0.000       0.071       0.202\n",
       "x223           0.0328      0.043      0.768      0.443      -0.051       0.117\n",
       "x224           0.0888      0.065      1.374      0.170      -0.038       0.216\n",
       "x225           0.1003      0.068      1.485      0.138      -0.032       0.233\n",
       "x226           0.0313      0.012      2.583      0.010       0.008       0.055\n",
       "x227           0.0689      0.030      2.266      0.024       0.009       0.128\n",
       "x228           0.0109      0.031      0.351      0.725      -0.050       0.072\n",
       "x229           0.1806      0.043      4.173      0.000       0.096       0.265\n",
       "x230           0.0715      0.049      1.474      0.141      -0.024       0.167\n",
       "x231           0.1387      0.026      5.373      0.000       0.088       0.189\n",
       "x232           0.1901      0.153      1.243      0.214      -0.110       0.490\n",
       "x233          -0.1480      0.175     -0.846      0.398      -0.491       0.195\n",
       "x234           0.0697      0.146      0.476      0.634      -0.217       0.356\n",
       "x235          -0.0162      0.008     -2.136      0.033      -0.031      -0.001\n",
       "==============================================================================\n",
       "Omnibus:                     1070.411   Durbin-Watson:                   2.012\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            35537.311\n",
       "Skew:                          -1.670   Prob(JB):                         0.00\n",
       "Kurtosis:                      22.445   Cond. No.                     1.17e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 4.06e-28. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>SalePrice</td>    <th>  R-squared:         </th> <td>   0.781</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.768</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   57.57</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 25 Oct 2023</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:17:51</td>     <th>  Log-Likelihood:    </th> <td>  512.97</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2191</td>      <th>  AIC:               </th> <td>  -767.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2063</td>      <th>  BIC:               </th> <td>  -33.65</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   128</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>   <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>   <td>   -0.0249</td> <td>    0.058</td> <td>   -0.433</td> <td> 0.665</td> <td>   -0.138</td> <td>    0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>   <td>   -0.5086</td> <td>    0.092</td> <td>   -5.522</td> <td> 0.000</td> <td>   -0.689</td> <td>   -0.328</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>   <td>    1.2641</td> <td>    0.177</td> <td>    7.125</td> <td> 0.000</td> <td>    0.916</td> <td>    1.612</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>   <td>    1.3179</td> <td>    0.087</td> <td>   15.081</td> <td> 0.000</td> <td>    1.147</td> <td>    1.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>   <td>    1.3508</td> <td>    0.091</td> <td>   14.864</td> <td> 0.000</td> <td>    1.173</td> <td>    1.529</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>   <td>    1.5386</td> <td>    0.127</td> <td>   12.103</td> <td> 0.000</td> <td>    1.289</td> <td>    1.788</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>   <td>    1.5448</td> <td>    0.142</td> <td>   10.847</td> <td> 0.000</td> <td>    1.265</td> <td>    1.824</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>   <td>    1.7007</td> <td>    0.100</td> <td>   17.064</td> <td> 0.000</td> <td>    1.505</td> <td>    1.896</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>  <td>    1.2934</td> <td>    0.098</td> <td>   13.166</td> <td> 0.000</td> <td>    1.101</td> <td>    1.486</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>  <td>    1.4962</td> <td>    0.099</td> <td>   15.038</td> <td> 0.000</td> <td>    1.301</td> <td>    1.691</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>  <td>    1.3712</td> <td>    0.121</td> <td>   11.376</td> <td> 0.000</td> <td>    1.135</td> <td>    1.608</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>  <td>    0.3686</td> <td>    0.138</td> <td>    2.669</td> <td> 0.008</td> <td>    0.098</td> <td>    0.639</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>  <td>    1.1000</td> <td>    0.112</td> <td>    9.846</td> <td> 0.000</td> <td>    0.881</td> <td>    1.319</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>  <td>    0.5820</td> <td>    0.048</td> <td>   12.203</td> <td> 0.000</td> <td>    0.489</td> <td>    0.676</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>  <td>    0.7471</td> <td>    0.122</td> <td>    6.115</td> <td> 0.000</td> <td>    0.508</td> <td>    0.987</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>  <td>    1.1066</td> <td>    0.118</td> <td>    9.391</td> <td> 0.000</td> <td>    0.876</td> <td>    1.338</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>  <td>    1.0173</td> <td>    0.126</td> <td>    8.080</td> <td> 0.000</td> <td>    0.770</td> <td>    1.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>  <td>    1.0801</td> <td>    0.115</td> <td>    9.431</td> <td> 0.000</td> <td>    0.855</td> <td>    1.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>  <td>    1.0123</td> <td>    0.115</td> <td>    8.778</td> <td> 0.000</td> <td>    0.786</td> <td>    1.238</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>  <td>    0.1317</td> <td>    0.034</td> <td>    3.886</td> <td> 0.000</td> <td>    0.065</td> <td>    0.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>  <td>    0.2503</td> <td>    0.041</td> <td>    6.076</td> <td> 0.000</td> <td>    0.170</td> <td>    0.331</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>  <td>    0.0565</td> <td>    0.025</td> <td>    2.304</td> <td> 0.021</td> <td>    0.008</td> <td>    0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>  <td>    0.0670</td> <td>    0.020</td> <td>    3.363</td> <td> 0.001</td> <td>    0.028</td> <td>    0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>  <td>    0.0055</td> <td>    0.029</td> <td>    0.188</td> <td> 0.851</td> <td>   -0.052</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>  <td>    0.1218</td> <td>    0.042</td> <td>    2.922</td> <td> 0.004</td> <td>    0.040</td> <td>    0.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>  <td>   -0.0663</td> <td>    0.020</td> <td>   -3.247</td> <td> 0.001</td> <td>   -0.106</td> <td>   -0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>  <td>   -0.1056</td> <td>    0.023</td> <td>   -4.564</td> <td> 0.000</td> <td>   -0.151</td> <td>   -0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>  <td>   -0.0745</td> <td>    0.026</td> <td>   -2.836</td> <td> 0.005</td> <td>   -0.126</td> <td>   -0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>  <td>    0.0964</td> <td>    0.083</td> <td>    1.161</td> <td> 0.246</td> <td>   -0.066</td> <td>    0.259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>  <td>   -0.2299</td> <td>    0.057</td> <td>   -4.041</td> <td> 0.000</td> <td>   -0.342</td> <td>   -0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>  <td>   -0.0695</td> <td>    0.026</td> <td>   -2.661</td> <td> 0.008</td> <td>   -0.121</td> <td>   -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>  <td>   -0.0784</td> <td>    0.021</td> <td>   -3.815</td> <td> 0.000</td> <td>   -0.119</td> <td>   -0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>  <td>   -0.0202</td> <td>    0.078</td> <td>   -0.258</td> <td> 0.796</td> <td>   -0.174</td> <td>    0.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>  <td>   -0.0938</td> <td>    0.026</td> <td>   -3.634</td> <td> 0.000</td> <td>   -0.144</td> <td>   -0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>  <td>    0.0538</td> <td>    0.025</td> <td>    2.168</td> <td> 0.030</td> <td>    0.005</td> <td>    0.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>  <td>   -0.0633</td> <td>    0.026</td> <td>   -2.435</td> <td> 0.015</td> <td>   -0.114</td> <td>   -0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>  <td>   -0.0439</td> <td>    0.043</td> <td>   -1.025</td> <td> 0.305</td> <td>   -0.128</td> <td>    0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>  <td>   -0.0766</td> <td>    0.026</td> <td>   -2.924</td> <td> 0.003</td> <td>   -0.128</td> <td>   -0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>  <td>   -0.1033</td> <td>    0.025</td> <td>   -4.077</td> <td> 0.000</td> <td>   -0.153</td> <td>   -0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>  <td>    0.1190</td> <td>    0.037</td> <td>    3.235</td> <td> 0.001</td> <td>    0.047</td> <td>    0.191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th>  <td>    0.1467</td> <td>    0.054</td> <td>    2.705</td> <td> 0.007</td> <td>    0.040</td> <td>    0.253</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th>  <td>   -0.0638</td> <td>    0.045</td> <td>   -1.403</td> <td> 0.161</td> <td>   -0.153</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x61</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x62</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x63</th>  <td>    0.2978</td> <td>    0.045</td> <td>    6.615</td> <td> 0.000</td> <td>    0.210</td> <td>    0.386</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x64</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x65</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x66</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x67</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x68</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x69</th>  <td>    0.1804</td> <td>    0.157</td> <td>    1.152</td> <td> 0.249</td> <td>   -0.127</td> <td>    0.487</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x70</th>  <td>    0.5820</td> <td>    0.048</td> <td>   12.203</td> <td> 0.000</td> <td>    0.489</td> <td>    0.676</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x71</th>  <td>    1.1936</td> <td>    0.093</td> <td>   12.833</td> <td> 0.000</td> <td>    1.011</td> <td>    1.376</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x72</th>  <td>    1.2340</td> <td>    0.088</td> <td>   14.081</td> <td> 0.000</td> <td>    1.062</td> <td>    1.406</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x73</th>  <td>    0.2847</td> <td>    0.111</td> <td>    2.555</td> <td> 0.011</td> <td>    0.066</td> <td>    0.503</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x74</th>  <td>    0.3397</td> <td>    0.055</td> <td>    6.193</td> <td> 0.000</td> <td>    0.232</td> <td>    0.447</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x75</th>  <td>    0.3084</td> <td>    0.110</td> <td>    2.794</td> <td> 0.005</td> <td>    0.092</td> <td>    0.525</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x76</th>  <td>    0.4910</td> <td>    0.078</td> <td>    6.292</td> <td> 0.000</td> <td>    0.338</td> <td>    0.644</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77</th>  <td>    0.3246</td> <td>    0.052</td> <td>    6.232</td> <td> 0.000</td> <td>    0.222</td> <td>    0.427</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x78</th>  <td>    0.5492</td> <td>    0.074</td> <td>    7.421</td> <td> 0.000</td> <td>    0.404</td> <td>    0.694</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x79</th>  <td>    1.2735</td> <td>    0.103</td> <td>   12.377</td> <td> 0.000</td> <td>    1.072</td> <td>    1.475</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x80</th>  <td>    0.5282</td> <td>    0.058</td> <td>    9.177</td> <td> 0.000</td> <td>    0.415</td> <td>    0.641</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x81</th>  <td>    0.4772</td> <td>    0.076</td> <td>    6.260</td> <td> 0.000</td> <td>    0.328</td> <td>    0.627</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x82</th>  <td>    0.5255</td> <td>    0.059</td> <td>    8.961</td> <td> 0.000</td> <td>    0.410</td> <td>    0.640</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x83</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x84</th>  <td>    0.4181</td> <td>    0.109</td> <td>    3.821</td> <td> 0.000</td> <td>    0.203</td> <td>    0.633</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x85</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x86</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x87</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x88</th>  <td>    0.3301</td> <td>    0.069</td> <td>    4.762</td> <td> 0.000</td> <td>    0.194</td> <td>    0.466</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x89</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x90</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x91</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x92</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x93</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x94</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x95</th>  <td>    0.1475</td> <td>    0.144</td> <td>    1.021</td> <td> 0.307</td> <td>   -0.136</td> <td>    0.431</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x96</th>  <td>    0.0039</td> <td>    0.027</td> <td>    0.142</td> <td> 0.887</td> <td>   -0.050</td> <td>    0.058</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x97</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x98</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x99</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x100</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x101</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x102</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x103</th> <td>   -0.0059</td> <td>    0.021</td> <td>   -0.274</td> <td> 0.784</td> <td>   -0.048</td> <td>    0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x104</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x105</th> <td>   -0.0021</td> <td>    0.035</td> <td>   -0.062</td> <td> 0.951</td> <td>   -0.070</td> <td>    0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x106</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x107</th> <td>   -0.0162</td> <td>    0.081</td> <td>   -0.200</td> <td> 0.841</td> <td>   -0.175</td> <td>    0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x108</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x109</th> <td>   -0.0954</td> <td>    0.146</td> <td>   -0.654</td> <td> 0.513</td> <td>   -0.381</td> <td>    0.191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x110</th> <td>    0.0204</td> <td>    0.031</td> <td>    0.664</td> <td> 0.507</td> <td>   -0.040</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x111</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x112</th> <td>    0.0235</td> <td>    0.021</td> <td>    1.119</td> <td> 0.263</td> <td>   -0.018</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x113</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x114</th> <td>    0.0284</td> <td>    0.023</td> <td>    1.240</td> <td> 0.215</td> <td>   -0.016</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x115</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x116</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x117</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x118</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x119</th> <td>    0.0559</td> <td>    0.020</td> <td>    2.734</td> <td> 0.006</td> <td>    0.016</td> <td>    0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x120</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x121</th> <td>   -0.0050</td> <td>    0.018</td> <td>   -0.287</td> <td> 0.774</td> <td>   -0.039</td> <td>    0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x122</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x123</th> <td>    0.0223</td> <td>    0.018</td> <td>    1.243</td> <td> 0.214</td> <td>   -0.013</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x124</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x125</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x126</th> <td>    0.0182</td> <td>    0.015</td> <td>    1.197</td> <td> 0.232</td> <td>   -0.012</td> <td>    0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x127</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x128</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x129</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x130</th> <td>    0.5825</td> <td>    0.063</td> <td>    9.208</td> <td> 0.000</td> <td>    0.458</td> <td>    0.707</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x131</th> <td>    0.6172</td> <td>    0.077</td> <td>    8.004</td> <td> 0.000</td> <td>    0.466</td> <td>    0.768</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x132</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x133</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x134</th> <td>    0.6128</td> <td>    0.125</td> <td>    4.903</td> <td> 0.000</td> <td>    0.368</td> <td>    0.858</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x135</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x136</th> <td>   -0.0495</td> <td>    0.038</td> <td>   -1.312</td> <td> 0.190</td> <td>   -0.123</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x137</th> <td>   -0.0350</td> <td>    0.086</td> <td>   -0.408</td> <td> 0.683</td> <td>   -0.203</td> <td>    0.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x138</th> <td>   -0.0345</td> <td>    0.019</td> <td>   -1.774</td> <td> 0.076</td> <td>   -0.073</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x139</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x140</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x141</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x142</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x143</th> <td>    0.0023</td> <td>    0.061</td> <td>    0.037</td> <td> 0.970</td> <td>   -0.118</td> <td>    0.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x144</th> <td>   -0.0007</td> <td>    0.015</td> <td>   -0.049</td> <td> 0.961</td> <td>   -0.031</td> <td>    0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x145</th> <td>    0.4862</td> <td>    0.064</td> <td>    7.584</td> <td> 0.000</td> <td>    0.360</td> <td>    0.612</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x146</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x147</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x148</th> <td>    0.0086</td> <td>    0.013</td> <td>    0.648</td> <td> 0.517</td> <td>   -0.017</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x149</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x150</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x151</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x152</th> <td>    0.0301</td> <td>    0.012</td> <td>    2.587</td> <td> 0.010</td> <td>    0.007</td> <td>    0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x153</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x154</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x155</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x156</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x157</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x158</th> <td>    0.0210</td> <td>    0.024</td> <td>    0.876</td> <td> 0.381</td> <td>   -0.026</td> <td>    0.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x159</th> <td>   -0.0092</td> <td>    0.021</td> <td>   -0.439</td> <td> 0.661</td> <td>   -0.050</td> <td>    0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x160</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x161</th> <td>   -0.0102</td> <td>    0.013</td> <td>   -0.760</td> <td> 0.448</td> <td>   -0.037</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x162</th> <td>   -0.0110</td> <td>    0.013</td> <td>   -0.856</td> <td> 0.392</td> <td>   -0.036</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x163</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x164</th> <td>   -0.0053</td> <td>    0.014</td> <td>   -0.377</td> <td> 0.706</td> <td>   -0.033</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x165</th> <td>   -0.0266</td> <td>    0.021</td> <td>   -1.254</td> <td> 0.210</td> <td>   -0.068</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x166</th> <td>   -0.0194</td> <td>    0.020</td> <td>   -0.985</td> <td> 0.325</td> <td>   -0.058</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x167</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x168</th> <td>   -0.0408</td> <td>    0.063</td> <td>   -0.648</td> <td> 0.517</td> <td>   -0.164</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x169</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x170</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x171</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x172</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x173</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x174</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x175</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x176</th> <td>   -0.0370</td> <td>    0.017</td> <td>   -2.136</td> <td> 0.033</td> <td>   -0.071</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x177</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x178</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x179</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x180</th> <td>    0.0565</td> <td>    0.015</td> <td>    3.765</td> <td> 0.000</td> <td>    0.027</td> <td>    0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x181</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x182</th> <td>    0.0781</td> <td>    0.048</td> <td>    1.635</td> <td> 0.102</td> <td>   -0.016</td> <td>    0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x183</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x184</th> <td>    0.2395</td> <td>    0.080</td> <td>    2.989</td> <td> 0.003</td> <td>    0.082</td> <td>    0.397</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x185</th> <td>   -0.0220</td> <td>    0.029</td> <td>   -0.762</td> <td> 0.446</td> <td>   -0.079</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x186</th> <td>    0.0713</td> <td>    0.027</td> <td>    2.624</td> <td> 0.009</td> <td>    0.018</td> <td>    0.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x187</th> <td>    4.7601</td> <td>    0.162</td> <td>   29.446</td> <td> 0.000</td> <td>    4.443</td> <td>    5.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x188</th> <td>    0.2053</td> <td>    0.044</td> <td>    4.629</td> <td> 0.000</td> <td>    0.118</td> <td>    0.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x189</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x190</th> <td>    0.4472</td> <td>    0.044</td> <td>   10.174</td> <td> 0.000</td> <td>    0.361</td> <td>    0.533</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x191</th> <td>    0.5039</td> <td>    0.068</td> <td>    7.453</td> <td> 0.000</td> <td>    0.371</td> <td>    0.636</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x192</th> <td>   -0.0087</td> <td>    0.021</td> <td>   -0.407</td> <td> 0.684</td> <td>   -0.051</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x193</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x194</th> <td>    0.1155</td> <td>    0.039</td> <td>    2.974</td> <td> 0.003</td> <td>    0.039</td> <td>    0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x195</th> <td>    0.0137</td> <td>    0.054</td> <td>    0.256</td> <td> 0.798</td> <td>   -0.092</td> <td>    0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x196</th> <td>    0.1413</td> <td>    0.053</td> <td>    2.669</td> <td> 0.008</td> <td>    0.037</td> <td>    0.245</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x197</th> <td>    0.0494</td> <td>    0.062</td> <td>    0.800</td> <td> 0.424</td> <td>   -0.072</td> <td>    0.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x198</th> <td>    0.0561</td> <td>    0.017</td> <td>    3.300</td> <td> 0.001</td> <td>    0.023</td> <td>    0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x199</th> <td>    0.0451</td> <td>    0.020</td> <td>    2.208</td> <td> 0.027</td> <td>    0.005</td> <td>    0.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x200</th> <td>    0.1282</td> <td>    0.063</td> <td>    2.035</td> <td> 0.042</td> <td>    0.005</td> <td>    0.252</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x201</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x202</th> <td>    0.1279</td> <td>    0.052</td> <td>    2.451</td> <td> 0.014</td> <td>    0.026</td> <td>    0.230</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x203</th> <td>   -0.0058</td> <td>    0.059</td> <td>   -0.098</td> <td> 0.922</td> <td>   -0.122</td> <td>    0.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x204</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x205</th> <td>    0.1004</td> <td>    0.025</td> <td>    4.011</td> <td> 0.000</td> <td>    0.051</td> <td>    0.149</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x206</th> <td>    1.5062</td> <td>    0.104</td> <td>   14.527</td> <td> 0.000</td> <td>    1.303</td> <td>    1.710</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x207</th> <td>    0.5337</td> <td>    0.064</td> <td>    8.366</td> <td> 0.000</td> <td>    0.409</td> <td>    0.659</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x208</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x209</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x210</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x211</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x212</th> <td>    0.0990</td> <td>    0.056</td> <td>    1.773</td> <td> 0.076</td> <td>   -0.011</td> <td>    0.209</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x213</th> <td>    0.0653</td> <td>    0.027</td> <td>    2.377</td> <td> 0.018</td> <td>    0.011</td> <td>    0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x214</th> <td>   -0.0630</td> <td>    0.070</td> <td>   -0.897</td> <td> 0.370</td> <td>   -0.201</td> <td>    0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x215</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x216</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x217</th> <td>    0.0964</td> <td>    0.065</td> <td>    1.491</td> <td> 0.136</td> <td>   -0.030</td> <td>    0.223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x218</th> <td>    0.4996</td> <td>    0.052</td> <td>    9.607</td> <td> 0.000</td> <td>    0.398</td> <td>    0.602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x219</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x220</th> <td>    0.0623</td> <td>    0.016</td> <td>    3.928</td> <td> 0.000</td> <td>    0.031</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x221</th> <td>    0.0449</td> <td>    0.044</td> <td>    1.025</td> <td> 0.305</td> <td>   -0.041</td> <td>    0.131</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x222</th> <td>    0.1917</td> <td>    0.059</td> <td>    3.241</td> <td> 0.001</td> <td>    0.076</td> <td>    0.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x223</th> <td>   -0.0316</td> <td>    0.076</td> <td>   -0.418</td> <td> 0.676</td> <td>   -0.180</td> <td>    0.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x224</th> <td>    0.8140</td> <td>    0.095</td> <td>    8.524</td> <td> 0.000</td> <td>    0.627</td> <td>    1.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x225</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x226</th> <td>    0.0311</td> <td>    0.021</td> <td>    1.469</td> <td> 0.142</td> <td>   -0.010</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x227</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x228</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x229</th> <td>    0.2230</td> <td>    0.075</td> <td>    2.960</td> <td> 0.003</td> <td>    0.075</td> <td>    0.371</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x230</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x231</th> <td>    0.1970</td> <td>    0.046</td> <td>    4.309</td> <td> 0.000</td> <td>    0.107</td> <td>    0.287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x232</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x233</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x234</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x235</th> <td>    0.0061</td> <td>    0.013</td> <td>    0.453</td> <td> 0.650</td> <td>   -0.020</td> <td>    0.032</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>2520.021</td> <th>  Durbin-Watson:     </th>  <td>   2.020</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>893704.385</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 5.351</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>101.362</td> <th>  Cond. No.          </th>  <td>1.17e+16</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 4.06e-28. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              SalePrice   R-squared:                       0.781\n",
       "Model:                            OLS   Adj. R-squared:                  0.768\n",
       "Method:                 Least Squares   F-statistic:                     57.57\n",
       "Date:                Wed, 25 Oct 2023   Prob (F-statistic):               0.00\n",
       "Time:                        22:17:51   Log-Likelihood:                 512.97\n",
       "No. Observations:                2191   AIC:                            -767.9\n",
       "Df Residuals:                    2063   BIC:                            -33.65\n",
       "Df Model:                         128                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1                  0          0        nan        nan           0           0\n",
       "x2            -0.0249      0.058     -0.433      0.665      -0.138       0.088\n",
       "x3            -0.5086      0.092     -5.522      0.000      -0.689      -0.328\n",
       "x4             1.2641      0.177      7.125      0.000       0.916       1.612\n",
       "x5             1.3179      0.087     15.081      0.000       1.147       1.489\n",
       "x6             1.3508      0.091     14.864      0.000       1.173       1.529\n",
       "x7             1.5386      0.127     12.103      0.000       1.289       1.788\n",
       "x8             1.5448      0.142     10.847      0.000       1.265       1.824\n",
       "x9             1.7007      0.100     17.064      0.000       1.505       1.896\n",
       "x10            1.2934      0.098     13.166      0.000       1.101       1.486\n",
       "x11            1.4962      0.099     15.038      0.000       1.301       1.691\n",
       "x12            1.3712      0.121     11.376      0.000       1.135       1.608\n",
       "x13            0.3686      0.138      2.669      0.008       0.098       0.639\n",
       "x14            1.1000      0.112      9.846      0.000       0.881       1.319\n",
       "x15            0.5820      0.048     12.203      0.000       0.489       0.676\n",
       "x16            0.7471      0.122      6.115      0.000       0.508       0.987\n",
       "x17            1.1066      0.118      9.391      0.000       0.876       1.338\n",
       "x18                 0          0        nan        nan           0           0\n",
       "x19            1.0173      0.126      8.080      0.000       0.770       1.264\n",
       "x20            1.0801      0.115      9.431      0.000       0.855       1.305\n",
       "x21            1.0123      0.115      8.778      0.000       0.786       1.238\n",
       "x22            0.1317      0.034      3.886      0.000       0.065       0.198\n",
       "x23            0.2503      0.041      6.076      0.000       0.170       0.331\n",
       "x24            0.0565      0.025      2.304      0.021       0.008       0.105\n",
       "x25            0.0670      0.020      3.363      0.001       0.028       0.106\n",
       "x26                 0          0        nan        nan           0           0\n",
       "x27                 0          0        nan        nan           0           0\n",
       "x28                 0          0        nan        nan           0           0\n",
       "x29                 0          0        nan        nan           0           0\n",
       "x30                 0          0        nan        nan           0           0\n",
       "x31            0.0055      0.029      0.188      0.851      -0.052       0.063\n",
       "x32            0.1218      0.042      2.922      0.004       0.040       0.204\n",
       "x33           -0.0663      0.020     -3.247      0.001      -0.106      -0.026\n",
       "x34                 0          0        nan        nan           0           0\n",
       "x35           -0.1056      0.023     -4.564      0.000      -0.151      -0.060\n",
       "x36           -0.0745      0.026     -2.836      0.005      -0.126      -0.023\n",
       "x37            0.0964      0.083      1.161      0.246      -0.066       0.259\n",
       "x38                 0          0        nan        nan           0           0\n",
       "x39           -0.2299      0.057     -4.041      0.000      -0.342      -0.118\n",
       "x40           -0.0695      0.026     -2.661      0.008      -0.121      -0.018\n",
       "x41           -0.0784      0.021     -3.815      0.000      -0.119      -0.038\n",
       "x42           -0.0202      0.078     -0.258      0.796      -0.174       0.133\n",
       "x43           -0.0938      0.026     -3.634      0.000      -0.144      -0.043\n",
       "x44                 0          0        nan        nan           0           0\n",
       "x45            0.0538      0.025      2.168      0.030       0.005       0.102\n",
       "x46           -0.0633      0.026     -2.435      0.015      -0.114      -0.012\n",
       "x47           -0.0439      0.043     -1.025      0.305      -0.128       0.040\n",
       "x48           -0.0766      0.026     -2.924      0.003      -0.128      -0.025\n",
       "x49           -0.1033      0.025     -4.077      0.000      -0.153      -0.054\n",
       "x50                 0          0        nan        nan           0           0\n",
       "x51            0.1190      0.037      3.235      0.001       0.047       0.191\n",
       "x52                 0          0        nan        nan           0           0\n",
       "x53                 0          0        nan        nan           0           0\n",
       "x54                 0          0        nan        nan           0           0\n",
       "x55                 0          0        nan        nan           0           0\n",
       "x56            0.1467      0.054      2.705      0.007       0.040       0.253\n",
       "x57                 0          0        nan        nan           0           0\n",
       "x58           -0.0638      0.045     -1.403      0.161      -0.153       0.025\n",
       "x59                 0          0        nan        nan           0           0\n",
       "x60                 0          0        nan        nan           0           0\n",
       "x61                 0          0        nan        nan           0           0\n",
       "x62                 0          0        nan        nan           0           0\n",
       "x63            0.2978      0.045      6.615      0.000       0.210       0.386\n",
       "x64                 0          0        nan        nan           0           0\n",
       "x65                 0          0        nan        nan           0           0\n",
       "x66                 0          0        nan        nan           0           0\n",
       "x67                 0          0        nan        nan           0           0\n",
       "x68                 0          0        nan        nan           0           0\n",
       "x69            0.1804      0.157      1.152      0.249      -0.127       0.487\n",
       "x70            0.5820      0.048     12.203      0.000       0.489       0.676\n",
       "x71            1.1936      0.093     12.833      0.000       1.011       1.376\n",
       "x72            1.2340      0.088     14.081      0.000       1.062       1.406\n",
       "x73            0.2847      0.111      2.555      0.011       0.066       0.503\n",
       "x74            0.3397      0.055      6.193      0.000       0.232       0.447\n",
       "x75            0.3084      0.110      2.794      0.005       0.092       0.525\n",
       "x76            0.4910      0.078      6.292      0.000       0.338       0.644\n",
       "x77            0.3246      0.052      6.232      0.000       0.222       0.427\n",
       "x78            0.5492      0.074      7.421      0.000       0.404       0.694\n",
       "x79            1.2735      0.103     12.377      0.000       1.072       1.475\n",
       "x80            0.5282      0.058      9.177      0.000       0.415       0.641\n",
       "x81            0.4772      0.076      6.260      0.000       0.328       0.627\n",
       "x82            0.5255      0.059      8.961      0.000       0.410       0.640\n",
       "x83                 0          0        nan        nan           0           0\n",
       "x84            0.4181      0.109      3.821      0.000       0.203       0.633\n",
       "x85                 0          0        nan        nan           0           0\n",
       "x86                 0          0        nan        nan           0           0\n",
       "x87                 0          0        nan        nan           0           0\n",
       "x88            0.3301      0.069      4.762      0.000       0.194       0.466\n",
       "x89                 0          0        nan        nan           0           0\n",
       "x90                 0          0        nan        nan           0           0\n",
       "x91                 0          0        nan        nan           0           0\n",
       "x92                 0          0        nan        nan           0           0\n",
       "x93                 0          0        nan        nan           0           0\n",
       "x94                 0          0        nan        nan           0           0\n",
       "x95            0.1475      0.144      1.021      0.307      -0.136       0.431\n",
       "x96            0.0039      0.027      0.142      0.887      -0.050       0.058\n",
       "x97                 0          0        nan        nan           0           0\n",
       "x98                 0          0        nan        nan           0           0\n",
       "x99                 0          0        nan        nan           0           0\n",
       "x100                0          0        nan        nan           0           0\n",
       "x101                0          0        nan        nan           0           0\n",
       "x102                0          0        nan        nan           0           0\n",
       "x103          -0.0059      0.021     -0.274      0.784      -0.048       0.036\n",
       "x104                0          0        nan        nan           0           0\n",
       "x105          -0.0021      0.035     -0.062      0.951      -0.070       0.066\n",
       "x106                0          0        nan        nan           0           0\n",
       "x107          -0.0162      0.081     -0.200      0.841      -0.175       0.142\n",
       "x108                0          0        nan        nan           0           0\n",
       "x109          -0.0954      0.146     -0.654      0.513      -0.381       0.191\n",
       "x110           0.0204      0.031      0.664      0.507      -0.040       0.081\n",
       "x111                0          0        nan        nan           0           0\n",
       "x112           0.0235      0.021      1.119      0.263      -0.018       0.065\n",
       "x113                0          0        nan        nan           0           0\n",
       "x114           0.0284      0.023      1.240      0.215      -0.016       0.073\n",
       "x115                0          0        nan        nan           0           0\n",
       "x116                0          0        nan        nan           0           0\n",
       "x117                0          0        nan        nan           0           0\n",
       "x118                0          0        nan        nan           0           0\n",
       "x119           0.0559      0.020      2.734      0.006       0.016       0.096\n",
       "x120                0          0        nan        nan           0           0\n",
       "x121          -0.0050      0.018     -0.287      0.774      -0.039       0.029\n",
       "x122                0          0        nan        nan           0           0\n",
       "x123           0.0223      0.018      1.243      0.214      -0.013       0.057\n",
       "x124                0          0        nan        nan           0           0\n",
       "x125                0          0        nan        nan           0           0\n",
       "x126           0.0182      0.015      1.197      0.232      -0.012       0.048\n",
       "x127                0          0        nan        nan           0           0\n",
       "x128                0          0        nan        nan           0           0\n",
       "x129                0          0        nan        nan           0           0\n",
       "x130           0.5825      0.063      9.208      0.000       0.458       0.707\n",
       "x131           0.6172      0.077      8.004      0.000       0.466       0.768\n",
       "x132                0          0        nan        nan           0           0\n",
       "x133                0          0        nan        nan           0           0\n",
       "x134           0.6128      0.125      4.903      0.000       0.368       0.858\n",
       "x135                0          0        nan        nan           0           0\n",
       "x136          -0.0495      0.038     -1.312      0.190      -0.123       0.024\n",
       "x137          -0.0350      0.086     -0.408      0.683      -0.203       0.133\n",
       "x138          -0.0345      0.019     -1.774      0.076      -0.073       0.004\n",
       "x139                0          0        nan        nan           0           0\n",
       "x140                0          0        nan        nan           0           0\n",
       "x141                0          0        nan        nan           0           0\n",
       "x142                0          0        nan        nan           0           0\n",
       "x143           0.0023      0.061      0.037      0.970      -0.118       0.122\n",
       "x144          -0.0007      0.015     -0.049      0.961      -0.031       0.029\n",
       "x145           0.4862      0.064      7.584      0.000       0.360       0.612\n",
       "x146                0          0        nan        nan           0           0\n",
       "x147                0          0        nan        nan           0           0\n",
       "x148           0.0086      0.013      0.648      0.517      -0.017       0.035\n",
       "x149                0          0        nan        nan           0           0\n",
       "x150                0          0        nan        nan           0           0\n",
       "x151                0          0        nan        nan           0           0\n",
       "x152           0.0301      0.012      2.587      0.010       0.007       0.053\n",
       "x153                0          0        nan        nan           0           0\n",
       "x154                0          0        nan        nan           0           0\n",
       "x155                0          0        nan        nan           0           0\n",
       "x156                0          0        nan        nan           0           0\n",
       "x157                0          0        nan        nan           0           0\n",
       "x158           0.0210      0.024      0.876      0.381      -0.026       0.068\n",
       "x159          -0.0092      0.021     -0.439      0.661      -0.050       0.032\n",
       "x160                0          0        nan        nan           0           0\n",
       "x161          -0.0102      0.013     -0.760      0.448      -0.037       0.016\n",
       "x162          -0.0110      0.013     -0.856      0.392      -0.036       0.014\n",
       "x163                0          0        nan        nan           0           0\n",
       "x164          -0.0053      0.014     -0.377      0.706      -0.033       0.022\n",
       "x165          -0.0266      0.021     -1.254      0.210      -0.068       0.015\n",
       "x166          -0.0194      0.020     -0.985      0.325      -0.058       0.019\n",
       "x167                0          0        nan        nan           0           0\n",
       "x168          -0.0408      0.063     -0.648      0.517      -0.164       0.083\n",
       "x169                0          0        nan        nan           0           0\n",
       "x170                0          0        nan        nan           0           0\n",
       "x171                0          0        nan        nan           0           0\n",
       "x172                0          0        nan        nan           0           0\n",
       "x173                0          0        nan        nan           0           0\n",
       "x174                0          0        nan        nan           0           0\n",
       "x175                0          0        nan        nan           0           0\n",
       "x176          -0.0370      0.017     -2.136      0.033      -0.071      -0.003\n",
       "x177                0          0        nan        nan           0           0\n",
       "x178                0          0        nan        nan           0           0\n",
       "x179                0          0        nan        nan           0           0\n",
       "x180           0.0565      0.015      3.765      0.000       0.027       0.086\n",
       "x181                0          0        nan        nan           0           0\n",
       "x182           0.0781      0.048      1.635      0.102      -0.016       0.172\n",
       "x183                0          0        nan        nan           0           0\n",
       "x184           0.2395      0.080      2.989      0.003       0.082       0.397\n",
       "x185          -0.0220      0.029     -0.762      0.446      -0.079       0.035\n",
       "x186           0.0713      0.027      2.624      0.009       0.018       0.125\n",
       "x187           4.7601      0.162     29.446      0.000       4.443       5.077\n",
       "x188           0.2053      0.044      4.629      0.000       0.118       0.292\n",
       "x189                0          0        nan        nan           0           0\n",
       "x190           0.4472      0.044     10.174      0.000       0.361       0.533\n",
       "x191           0.5039      0.068      7.453      0.000       0.371       0.636\n",
       "x192          -0.0087      0.021     -0.407      0.684      -0.051       0.033\n",
       "x193                0          0        nan        nan           0           0\n",
       "x194           0.1155      0.039      2.974      0.003       0.039       0.192\n",
       "x195           0.0137      0.054      0.256      0.798      -0.092       0.119\n",
       "x196           0.1413      0.053      2.669      0.008       0.037       0.245\n",
       "x197           0.0494      0.062      0.800      0.424      -0.072       0.170\n",
       "x198           0.0561      0.017      3.300      0.001       0.023       0.089\n",
       "x199           0.0451      0.020      2.208      0.027       0.005       0.085\n",
       "x200           0.1282      0.063      2.035      0.042       0.005       0.252\n",
       "x201                0          0        nan        nan           0           0\n",
       "x202           0.1279      0.052      2.451      0.014       0.026       0.230\n",
       "x203          -0.0058      0.059     -0.098      0.922      -0.122       0.110\n",
       "x204                0          0        nan        nan           0           0\n",
       "x205           0.1004      0.025      4.011      0.000       0.051       0.149\n",
       "x206           1.5062      0.104     14.527      0.000       1.303       1.710\n",
       "x207           0.5337      0.064      8.366      0.000       0.409       0.659\n",
       "x208                0          0        nan        nan           0           0\n",
       "x209                0          0        nan        nan           0           0\n",
       "x210                0          0        nan        nan           0           0\n",
       "x211                0          0        nan        nan           0           0\n",
       "x212           0.0990      0.056      1.773      0.076      -0.011       0.209\n",
       "x213           0.0653      0.027      2.377      0.018       0.011       0.119\n",
       "x214          -0.0630      0.070     -0.897      0.370      -0.201       0.075\n",
       "x215                0          0        nan        nan           0           0\n",
       "x216                0          0        nan        nan           0           0\n",
       "x217           0.0964      0.065      1.491      0.136      -0.030       0.223\n",
       "x218           0.4996      0.052      9.607      0.000       0.398       0.602\n",
       "x219                0          0        nan        nan           0           0\n",
       "x220           0.0623      0.016      3.928      0.000       0.031       0.093\n",
       "x221           0.0449      0.044      1.025      0.305      -0.041       0.131\n",
       "x222           0.1917      0.059      3.241      0.001       0.076       0.308\n",
       "x223          -0.0316      0.076     -0.418      0.676      -0.180       0.117\n",
       "x224           0.8140      0.095      8.524      0.000       0.627       1.001\n",
       "x225                0          0        nan        nan           0           0\n",
       "x226           0.0311      0.021      1.469      0.142      -0.010       0.073\n",
       "x227                0          0        nan        nan           0           0\n",
       "x228                0          0        nan        nan           0           0\n",
       "x229           0.2230      0.075      2.960      0.003       0.075       0.371\n",
       "x230                0          0        nan        nan           0           0\n",
       "x231           0.1970      0.046      4.309      0.000       0.107       0.287\n",
       "x232                0          0        nan        nan           0           0\n",
       "x233                0          0        nan        nan           0           0\n",
       "x234                0          0        nan        nan           0           0\n",
       "x235           0.0061      0.013      0.453      0.650      -0.020       0.032\n",
       "==============================================================================\n",
       "Omnibus:                     2520.021   Durbin-Watson:                   2.020\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           893704.385\n",
       "Skew:                           5.351   Prob(JB):                         0.00\n",
       "Kurtosis:                     101.362   Cond. No.                     1.17e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 4.06e-28. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resL1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>SalePrice</td>    <th>  R-squared:         </th> <td>   0.940</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.933</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   130.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 25 Oct 2023</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:17:52</td>     <th>  Log-Likelihood:    </th> <td>  1926.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2191</td>      <th>  AIC:               </th> <td>  -3382.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1956</td>      <th>  BIC:               </th> <td>  -2038.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   235</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>   <td>   -0.2067</td> <td>    0.123</td> <td>   -1.676</td> <td> 0.094</td> <td>   -0.449</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>   <td>   -0.0657</td> <td>    0.034</td> <td>   -1.908</td> <td> 0.057</td> <td>   -0.133</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>   <td>   -0.0333</td> <td>    0.054</td> <td>   -0.617</td> <td> 0.537</td> <td>   -0.139</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>   <td>    0.1136</td> <td>    0.109</td> <td>    1.043</td> <td> 0.297</td> <td>   -0.100</td> <td>    0.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>   <td>    0.0826</td> <td>    0.051</td> <td>    1.617</td> <td> 0.106</td> <td>   -0.018</td> <td>    0.183</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>   <td>    0.0164</td> <td>    0.054</td> <td>    0.307</td> <td> 0.759</td> <td>   -0.089</td> <td>    0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>   <td>    0.0716</td> <td>    0.074</td> <td>    0.964</td> <td> 0.335</td> <td>   -0.074</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>   <td>    0.1073</td> <td>    0.083</td> <td>    1.298</td> <td> 0.195</td> <td>   -0.055</td> <td>    0.269</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>   <td>    0.1229</td> <td>    0.060</td> <td>    2.042</td> <td> 0.041</td> <td>    0.005</td> <td>    0.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>  <td>    0.0506</td> <td>    0.057</td> <td>    0.882</td> <td> 0.378</td> <td>   -0.062</td> <td>    0.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>  <td>    0.1203</td> <td>    0.058</td> <td>    2.056</td> <td> 0.040</td> <td>    0.006</td> <td>    0.235</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>  <td>    0.1098</td> <td>    0.071</td> <td>    1.552</td> <td> 0.121</td> <td>   -0.029</td> <td>    0.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>  <td>    0.0032</td> <td>    0.076</td> <td>    0.042</td> <td> 0.967</td> <td>   -0.147</td> <td>    0.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>  <td>    0.0953</td> <td>    0.064</td> <td>    1.497</td> <td> 0.135</td> <td>   -0.030</td> <td>    0.220</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>  <td>    0.0074</td> <td>    0.029</td> <td>    0.253</td> <td> 0.801</td> <td>   -0.050</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>  <td>    1.0350</td> <td>    0.148</td> <td>    6.996</td> <td> 0.000</td> <td>    0.745</td> <td>    1.325</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>  <td>    1.3097</td> <td>    0.148</td> <td>    8.870</td> <td> 0.000</td> <td>    1.020</td> <td>    1.599</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>  <td>    1.2635</td> <td>    0.170</td> <td>    7.445</td> <td> 0.000</td> <td>    0.931</td> <td>    1.596</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>  <td>    1.2585</td> <td>    0.149</td> <td>    8.470</td> <td> 0.000</td> <td>    0.967</td> <td>    1.550</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>  <td>    1.2886</td> <td>    0.145</td> <td>    8.857</td> <td> 0.000</td> <td>    1.003</td> <td>    1.574</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>  <td>    1.2326</td> <td>    0.145</td> <td>    8.510</td> <td> 0.000</td> <td>    0.949</td> <td>    1.517</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>  <td>    0.0254</td> <td>    0.020</td> <td>    1.297</td> <td> 0.195</td> <td>   -0.013</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>  <td>    0.0195</td> <td>    0.024</td> <td>    0.811</td> <td> 0.418</td> <td>   -0.028</td> <td>    0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>  <td>    0.0087</td> <td>    0.014</td> <td>    0.628</td> <td> 0.530</td> <td>   -0.019</td> <td>    0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>  <td>    0.0034</td> <td>    0.012</td> <td>    0.271</td> <td> 0.786</td> <td>   -0.021</td> <td>    0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>  <td>   -0.0263</td> <td>    0.017</td> <td>   -1.576</td> <td> 0.115</td> <td>   -0.059</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>  <td>   -0.0199</td> <td>    0.033</td> <td>   -0.609</td> <td> 0.543</td> <td>   -0.084</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>  <td>   -0.0040</td> <td>    0.007</td> <td>   -0.606</td> <td> 0.545</td> <td>   -0.017</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>  <td>    0.1225</td> <td>    0.061</td> <td>    2.023</td> <td> 0.043</td> <td>    0.004</td> <td>    0.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>  <td>    0.0455</td> <td>    0.043</td> <td>    1.055</td> <td> 0.292</td> <td>   -0.039</td> <td>    0.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>  <td>    0.0839</td> <td>    0.035</td> <td>    2.397</td> <td> 0.017</td> <td>    0.015</td> <td>    0.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>  <td>    0.0553</td> <td>    0.036</td> <td>    1.526</td> <td> 0.127</td> <td>   -0.016</td> <td>    0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>  <td>    0.0072</td> <td>    0.028</td> <td>    0.259</td> <td> 0.796</td> <td>   -0.047</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>  <td>    0.1147</td> <td>    0.032</td> <td>    3.562</td> <td> 0.000</td> <td>    0.052</td> <td>    0.178</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>  <td>   -0.0114</td> <td>    0.030</td> <td>   -0.380</td> <td> 0.704</td> <td>   -0.070</td> <td>    0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>  <td>    0.0126</td> <td>    0.029</td> <td>    0.430</td> <td> 0.667</td> <td>   -0.045</td> <td>    0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>  <td>    0.1030</td> <td>    0.052</td> <td>    1.990</td> <td> 0.047</td> <td>    0.001</td> <td>    0.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>  <td>    0.0246</td> <td>    0.038</td> <td>    0.647</td> <td> 0.517</td> <td>   -0.050</td> <td>    0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>  <td>   -0.0666</td> <td>    0.044</td> <td>   -1.506</td> <td> 0.132</td> <td>   -0.153</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>  <td>    0.0148</td> <td>    0.030</td> <td>    0.491</td> <td> 0.624</td> <td>   -0.044</td> <td>    0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>  <td>    0.0143</td> <td>    0.030</td> <td>    0.482</td> <td> 0.630</td> <td>   -0.044</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>  <td>    0.0853</td> <td>    0.053</td> <td>    1.622</td> <td> 0.105</td> <td>   -0.018</td> <td>    0.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>  <td>    0.0034</td> <td>    0.030</td> <td>    0.113</td> <td> 0.910</td> <td>   -0.056</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>  <td>    0.0631</td> <td>    0.033</td> <td>    1.905</td> <td> 0.057</td> <td>   -0.002</td> <td>    0.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>  <td>    0.0888</td> <td>    0.028</td> <td>    3.164</td> <td> 0.002</td> <td>    0.034</td> <td>    0.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>  <td>    0.0101</td> <td>    0.035</td> <td>    0.288</td> <td> 0.774</td> <td>   -0.059</td> <td>    0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>  <td>    0.0251</td> <td>    0.037</td> <td>    0.671</td> <td> 0.503</td> <td>   -0.048</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>  <td>    0.0325</td> <td>    0.031</td> <td>    1.059</td> <td> 0.290</td> <td>   -0.028</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>  <td>    0.0041</td> <td>    0.029</td> <td>    0.141</td> <td> 0.888</td> <td>   -0.053</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>  <td>    0.0646</td> <td>    0.033</td> <td>    1.938</td> <td> 0.053</td> <td>   -0.001</td> <td>    0.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>  <td>    0.1118</td> <td>    0.032</td> <td>    3.550</td> <td> 0.000</td> <td>    0.050</td> <td>    0.174</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th>  <td>    0.0186</td> <td>    0.031</td> <td>    0.606</td> <td> 0.544</td> <td>   -0.042</td> <td>    0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>  <td>    0.0007</td> <td>    0.039</td> <td>    0.019</td> <td> 0.985</td> <td>   -0.075</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th>  <td>    0.0126</td> <td>    0.018</td> <td>    0.695</td> <td> 0.487</td> <td>   -0.023</td> <td>    0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th>  <td>    0.0601</td> <td>    0.015</td> <td>    4.009</td> <td> 0.000</td> <td>    0.031</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th>  <td>    0.0790</td> <td>    0.035</td> <td>    2.266</td> <td> 0.024</td> <td>    0.011</td> <td>    0.147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th>  <td>    0.0720</td> <td>    0.026</td> <td>    2.748</td> <td> 0.006</td> <td>    0.021</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th>  <td>    0.0003</td> <td>    0.029</td> <td>    0.011</td> <td> 0.991</td> <td>   -0.057</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th>  <td>    0.0174</td> <td>    0.028</td> <td>    0.627</td> <td> 0.531</td> <td>   -0.037</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60</th>  <td>    0.0092</td> <td>    0.066</td> <td>    0.140</td> <td> 0.889</td> <td>   -0.120</td> <td>    0.139</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x61</th>  <td>    0.0067</td> <td>    0.044</td> <td>    0.154</td> <td> 0.877</td> <td>   -0.079</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x62</th>  <td>   -0.1014</td> <td>    0.072</td> <td>   -1.411</td> <td> 0.158</td> <td>   -0.242</td> <td>    0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x63</th>  <td>   -0.0619</td> <td>    0.060</td> <td>   -1.028</td> <td> 0.304</td> <td>   -0.180</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x64</th>  <td>    0.0066</td> <td>    0.086</td> <td>    0.076</td> <td> 0.939</td> <td>   -0.162</td> <td>    0.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x65</th>  <td>   -0.0361</td> <td>    0.101</td> <td>   -0.358</td> <td> 0.720</td> <td>   -0.234</td> <td>    0.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x66</th>  <td>   -0.1022</td> <td>    0.165</td> <td>   -0.617</td> <td> 0.537</td> <td>   -0.427</td> <td>    0.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x67</th>  <td>   -0.0648</td> <td>    0.126</td> <td>   -0.516</td> <td> 0.606</td> <td>   -0.311</td> <td>    0.182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x68</th>  <td>    0.0138</td> <td>    0.125</td> <td>    0.110</td> <td> 0.912</td> <td>   -0.232</td> <td>    0.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x69</th>  <td>   -0.0365</td> <td>    0.099</td> <td>   -0.370</td> <td> 0.711</td> <td>   -0.230</td> <td>    0.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x70</th>  <td>    0.0074</td> <td>    0.029</td> <td>    0.253</td> <td> 0.801</td> <td>   -0.050</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x71</th>  <td>   -0.0238</td> <td>    0.054</td> <td>   -0.438</td> <td> 0.662</td> <td>   -0.130</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x72</th>  <td>    0.0241</td> <td>    0.051</td> <td>    0.472</td> <td> 0.637</td> <td>   -0.076</td> <td>    0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x73</th>  <td>    0.0293</td> <td>    0.064</td> <td>    0.458</td> <td> 0.647</td> <td>   -0.096</td> <td>    0.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x74</th>  <td>    0.0424</td> <td>    0.033</td> <td>    1.299</td> <td> 0.194</td> <td>   -0.022</td> <td>    0.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x75</th>  <td>   -0.0370</td> <td>    0.066</td> <td>   -0.561</td> <td> 0.575</td> <td>   -0.166</td> <td>    0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x76</th>  <td>    0.0683</td> <td>    0.046</td> <td>    1.473</td> <td> 0.141</td> <td>   -0.023</td> <td>    0.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77</th>  <td>    0.0464</td> <td>    0.031</td> <td>    1.514</td> <td> 0.130</td> <td>   -0.014</td> <td>    0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x78</th>  <td>    0.0410</td> <td>    0.044</td> <td>    0.933</td> <td> 0.351</td> <td>   -0.045</td> <td>    0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x79</th>  <td>    0.1049</td> <td>    0.061</td> <td>    1.718</td> <td> 0.086</td> <td>   -0.015</td> <td>    0.225</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x80</th>  <td>    0.0091</td> <td>    0.061</td> <td>    0.150</td> <td> 0.881</td> <td>   -0.111</td> <td>    0.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x81</th>  <td>   -0.0161</td> <td>    0.067</td> <td>   -0.240</td> <td> 0.810</td> <td>   -0.148</td> <td>    0.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x82</th>  <td>    0.0090</td> <td>    0.061</td> <td>    0.147</td> <td> 0.883</td> <td>   -0.111</td> <td>    0.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x83</th>  <td>   -0.1428</td> <td>    0.077</td> <td>   -1.855</td> <td> 0.064</td> <td>   -0.294</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x84</th>  <td>   -0.0215</td> <td>    0.094</td> <td>   -0.228</td> <td> 0.819</td> <td>   -0.206</td> <td>    0.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x85</th>  <td>    0.1668</td> <td>    0.127</td> <td>    1.316</td> <td> 0.188</td> <td>   -0.082</td> <td>    0.415</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x86</th>  <td>    0.0686</td> <td>    0.127</td> <td>    0.539</td> <td> 0.590</td> <td>   -0.181</td> <td>    0.318</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x87</th>  <td>    0.0803</td> <td>    0.114</td> <td>    0.701</td> <td> 0.483</td> <td>   -0.144</td> <td>    0.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x88</th>  <td>   -0.0318</td> <td>    0.056</td> <td>   -0.570</td> <td> 0.568</td> <td>   -0.141</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x89</th>  <td>    0.0184</td> <td>    0.049</td> <td>    0.378</td> <td> 0.706</td> <td>   -0.077</td> <td>    0.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x90</th>  <td>    0.0555</td> <td>    0.054</td> <td>    1.026</td> <td> 0.305</td> <td>   -0.051</td> <td>    0.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x91</th>  <td>   -0.0625</td> <td>    0.122</td> <td>   -0.514</td> <td> 0.607</td> <td>   -0.301</td> <td>    0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x92</th>  <td>    0.0724</td> <td>    0.071</td> <td>    1.021</td> <td> 0.308</td> <td>   -0.067</td> <td>    0.212</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x93</th>  <td>    0.1329</td> <td>    0.044</td> <td>    3.034</td> <td> 0.002</td> <td>    0.047</td> <td>    0.219</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x94</th>  <td>    1.3690</td> <td>    0.176</td> <td>    7.759</td> <td> 0.000</td> <td>    1.023</td> <td>    1.715</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x95</th>  <td>   -0.0024</td> <td>    0.089</td> <td>   -0.027</td> <td> 0.978</td> <td>   -0.177</td> <td>    0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x96</th>  <td>    0.0460</td> <td>    0.042</td> <td>    1.093</td> <td> 0.274</td> <td>   -0.037</td> <td>    0.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x97</th>  <td>   -0.0132</td> <td>    0.121</td> <td>   -0.109</td> <td> 0.913</td> <td>   -0.250</td> <td>    0.224</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x98</th>  <td>    0.0520</td> <td>    0.051</td> <td>    1.026</td> <td> 0.305</td> <td>   -0.047</td> <td>    0.151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x99</th>  <td>    0.0582</td> <td>    0.041</td> <td>    1.404</td> <td> 0.160</td> <td>   -0.023</td> <td>    0.139</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x100</th> <td>    0.3099</td> <td>    0.059</td> <td>    5.229</td> <td> 0.000</td> <td>    0.194</td> <td>    0.426</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x101</th> <td>   -0.0475</td> <td>    0.099</td> <td>   -0.480</td> <td> 0.631</td> <td>   -0.242</td> <td>    0.147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x102</th> <td>    0.0535</td> <td>    0.050</td> <td>    1.069</td> <td> 0.285</td> <td>   -0.045</td> <td>    0.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x103</th> <td>    0.0679</td> <td>    0.047</td> <td>    1.434</td> <td> 0.152</td> <td>   -0.025</td> <td>    0.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x104</th> <td>    0.0547</td> <td>    0.041</td> <td>    1.345</td> <td> 0.179</td> <td>   -0.025</td> <td>    0.134</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x105</th> <td>    0.0709</td> <td>    0.046</td> <td>    1.552</td> <td> 0.121</td> <td>   -0.019</td> <td>    0.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x106</th> <td>    0.1467</td> <td>    0.093</td> <td>    1.574</td> <td> 0.116</td> <td>   -0.036</td> <td>    0.329</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x107</th> <td>   -0.0400</td> <td>    0.065</td> <td>   -0.619</td> <td> 0.536</td> <td>   -0.167</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x108</th> <td>   -0.0228</td> <td>    0.047</td> <td>   -0.482</td> <td> 0.630</td> <td>   -0.115</td> <td>    0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x109</th> <td>    0.0915</td> <td>    0.089</td> <td>    1.023</td> <td> 0.306</td> <td>   -0.084</td> <td>    0.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x110</th> <td>    0.0145</td> <td>    0.043</td> <td>    0.339</td> <td> 0.735</td> <td>   -0.069</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x111</th> <td>    0.0185</td> <td>    0.052</td> <td>    0.353</td> <td> 0.724</td> <td>   -0.084</td> <td>    0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x112</th> <td>    0.0262</td> <td>    0.051</td> <td>    0.510</td> <td> 0.610</td> <td>   -0.074</td> <td>    0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x113</th> <td>   -0.0792</td> <td>    0.120</td> <td>   -0.661</td> <td> 0.509</td> <td>   -0.314</td> <td>    0.156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x114</th> <td>    0.0021</td> <td>    0.041</td> <td>    0.051</td> <td> 0.959</td> <td>   -0.079</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x115</th> <td>    0.3099</td> <td>    0.059</td> <td>    5.229</td> <td> 0.000</td> <td>    0.194</td> <td>    0.426</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x116</th> <td>    0.0827</td> <td>    0.076</td> <td>    1.085</td> <td> 0.278</td> <td>   -0.067</td> <td>    0.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x117</th> <td>    0.0333</td> <td>    0.050</td> <td>    0.663</td> <td> 0.507</td> <td>   -0.065</td> <td>    0.132</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x118</th> <td>    0.0004</td> <td>    0.048</td> <td>    0.009</td> <td> 0.993</td> <td>   -0.093</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x119</th> <td>    0.0176</td> <td>    0.041</td> <td>    0.424</td> <td> 0.672</td> <td>   -0.064</td> <td>    0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x120</th> <td>   -0.0009</td> <td>    0.045</td> <td>   -0.021</td> <td> 0.983</td> <td>   -0.089</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x121</th> <td>    0.0333</td> <td>    0.026</td> <td>    1.280</td> <td> 0.201</td> <td>   -0.018</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x122</th> <td>   -0.2970</td> <td>    0.136</td> <td>   -2.177</td> <td> 0.030</td> <td>   -0.564</td> <td>   -0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x123</th> <td>    0.0386</td> <td>    0.026</td> <td>    1.481</td> <td> 0.139</td> <td>   -0.013</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x124</th> <td>    0.0541</td> <td>    0.027</td> <td>    1.967</td> <td> 0.049</td> <td>    0.000</td> <td>    0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x125</th> <td>    0.0172</td> <td>    0.011</td> <td>    1.505</td> <td> 0.133</td> <td>   -0.005</td> <td>    0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x126</th> <td>    0.0355</td> <td>    0.013</td> <td>    2.819</td> <td> 0.005</td> <td>    0.011</td> <td>    0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x127</th> <td>    0.0263</td> <td>    0.032</td> <td>    0.827</td> <td> 0.408</td> <td>   -0.036</td> <td>    0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x128</th> <td>    0.0792</td> <td>    0.045</td> <td>    1.747</td> <td> 0.081</td> <td>   -0.010</td> <td>    0.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x129</th> <td>   -0.0604</td> <td>    0.053</td> <td>   -1.144</td> <td> 0.253</td> <td>   -0.164</td> <td>    0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x130</th> <td>    0.1052</td> <td>    0.113</td> <td>    0.932</td> <td> 0.351</td> <td>   -0.116</td> <td>    0.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x131</th> <td>    0.1529</td> <td>    0.116</td> <td>    1.321</td> <td> 0.187</td> <td>   -0.074</td> <td>    0.380</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x132</th> <td>    0.0135</td> <td>    0.120</td> <td>    0.112</td> <td> 0.911</td> <td>   -0.221</td> <td>    0.248</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x133</th> <td>    0.0565</td> <td>    0.162</td> <td>    0.350</td> <td> 0.727</td> <td>   -0.260</td> <td>    0.374</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x134</th> <td>    0.1389</td> <td>    0.134</td> <td>    1.037</td> <td> 0.300</td> <td>   -0.124</td> <td>    0.402</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x135</th> <td>    0.0319</td> <td>    0.013</td> <td>    2.452</td> <td> 0.014</td> <td>    0.006</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x136</th> <td>   -0.0266</td> <td>    0.021</td> <td>   -1.258</td> <td> 0.208</td> <td>   -0.068</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x137</th> <td>    0.0304</td> <td>    0.048</td> <td>    0.627</td> <td> 0.531</td> <td>   -0.065</td> <td>    0.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x138</th> <td>   -0.0107</td> <td>    0.011</td> <td>   -0.977</td> <td> 0.329</td> <td>   -0.032</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x139</th> <td>    0.0573</td> <td>    0.110</td> <td>    0.520</td> <td> 0.603</td> <td>   -0.159</td> <td>    0.274</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x140</th> <td>    0.0500</td> <td>    0.029</td> <td>    1.743</td> <td> 0.082</td> <td>   -0.006</td> <td>    0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x141</th> <td>    0.0623</td> <td>    0.040</td> <td>    1.573</td> <td> 0.116</td> <td>   -0.015</td> <td>    0.140</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x142</th> <td>    0.0468</td> <td>    0.031</td> <td>    1.498</td> <td> 0.134</td> <td>   -0.014</td> <td>    0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x143</th> <td>   -0.0078</td> <td>    0.044</td> <td>   -0.177</td> <td> 0.860</td> <td>   -0.094</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x144</th> <td>    0.0337</td> <td>    0.029</td> <td>    1.176</td> <td> 0.240</td> <td>   -0.022</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x145</th> <td>    0.0416</td> <td>    0.100</td> <td>    0.415</td> <td> 0.678</td> <td>   -0.155</td> <td>    0.238</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x146</th> <td>    0.0871</td> <td>    0.107</td> <td>    0.818</td> <td> 0.414</td> <td>   -0.122</td> <td>    0.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x147</th> <td>   -0.0099</td> <td>    0.007</td> <td>   -1.349</td> <td> 0.178</td> <td>   -0.024</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x148</th> <td>    0.0018</td> <td>    0.009</td> <td>    0.207</td> <td> 0.836</td> <td>   -0.015</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x149</th> <td>   -0.0150</td> <td>    0.017</td> <td>   -0.867</td> <td> 0.386</td> <td>   -0.049</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x150</th> <td>    0.0104</td> <td>    0.014</td> <td>    0.742</td> <td> 0.458</td> <td>   -0.017</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x151</th> <td>   -0.0286</td> <td>    0.047</td> <td>   -0.613</td> <td> 0.540</td> <td>   -0.120</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x152</th> <td>    0.0038</td> <td>    0.013</td> <td>    0.300</td> <td> 0.764</td> <td>   -0.021</td> <td>    0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x153</th> <td>    0.0137</td> <td>    0.101</td> <td>    0.136</td> <td> 0.892</td> <td>   -0.184</td> <td>    0.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x154</th> <td>    0.0518</td> <td>    0.111</td> <td>    0.469</td> <td> 0.639</td> <td>   -0.165</td> <td>    0.269</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x155</th> <td>    0.0055</td> <td>    0.094</td> <td>    0.058</td> <td> 0.954</td> <td>   -0.180</td> <td>    0.191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x156</th> <td>   -0.2412</td> <td>    0.153</td> <td>   -1.572</td> <td> 0.116</td> <td>   -0.542</td> <td>    0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x157</th> <td>   -0.0080</td> <td>    0.012</td> <td>   -0.679</td> <td> 0.497</td> <td>   -0.031</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x158</th> <td>    0.0047</td> <td>    0.015</td> <td>    0.317</td> <td> 0.751</td> <td>   -0.024</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x159</th> <td>   -0.0115</td> <td>    0.013</td> <td>   -0.865</td> <td> 0.387</td> <td>   -0.038</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x160</th> <td>    0.0068</td> <td>    0.014</td> <td>    0.487</td> <td> 0.627</td> <td>   -0.020</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x161</th> <td>    0.0048</td> <td>    0.010</td> <td>    0.486</td> <td> 0.627</td> <td>   -0.015</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x162</th> <td>   -0.0043</td> <td>    0.010</td> <td>   -0.445</td> <td> 0.656</td> <td>   -0.023</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x163</th> <td>   -0.0042</td> <td>    0.012</td> <td>   -0.357</td> <td> 0.721</td> <td>   -0.027</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x164</th> <td>    0.0067</td> <td>    0.010</td> <td>    0.664</td> <td> 0.507</td> <td>   -0.013</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x165</th> <td>   -0.0147</td> <td>    0.014</td> <td>   -1.091</td> <td> 0.275</td> <td>   -0.041</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x166</th> <td>   -0.0130</td> <td>    0.013</td> <td>   -1.015</td> <td> 0.310</td> <td>   -0.038</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x167</th> <td>    0.0048</td> <td>    0.013</td> <td>    0.379</td> <td> 0.705</td> <td>   -0.020</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x168</th> <td>    0.0199</td> <td>    0.037</td> <td>    0.544</td> <td> 0.586</td> <td>   -0.052</td> <td>    0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x169</th> <td>    0.1293</td> <td>    0.052</td> <td>    2.473</td> <td> 0.013</td> <td>    0.027</td> <td>    0.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x170</th> <td>    0.0458</td> <td>    0.030</td> <td>    1.514</td> <td> 0.130</td> <td>   -0.014</td> <td>    0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x171</th> <td>   -0.0370</td> <td>    0.044</td> <td>   -0.833</td> <td> 0.405</td> <td>   -0.124</td> <td>    0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x172</th> <td>   -0.0033</td> <td>    0.053</td> <td>   -0.062</td> <td> 0.951</td> <td>   -0.107</td> <td>    0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x173</th> <td>    0.1013</td> <td>    0.054</td> <td>    1.861</td> <td> 0.063</td> <td>   -0.005</td> <td>    0.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x174</th> <td>    0.0474</td> <td>    0.047</td> <td>    1.002</td> <td> 0.317</td> <td>   -0.045</td> <td>    0.140</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x175</th> <td>    0.0008</td> <td>    0.111</td> <td>    0.007</td> <td> 0.994</td> <td>   -0.217</td> <td>    0.218</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x176</th> <td>    0.0083</td> <td>    0.015</td> <td>    0.552</td> <td> 0.581</td> <td>   -0.021</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x177</th> <td>    0.2288</td> <td>    0.040</td> <td>    5.672</td> <td> 0.000</td> <td>    0.150</td> <td>    0.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x178</th> <td>    0.1388</td> <td>    0.035</td> <td>    3.946</td> <td> 0.000</td> <td>    0.070</td> <td>    0.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x179</th> <td>    0.0497</td> <td>    0.022</td> <td>    2.286</td> <td> 0.022</td> <td>    0.007</td> <td>    0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x180</th> <td>    0.0965</td> <td>    0.011</td> <td>    9.080</td> <td> 0.000</td> <td>    0.076</td> <td>    0.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x181</th> <td>    0.0386</td> <td>    0.052</td> <td>    0.742</td> <td> 0.458</td> <td>   -0.063</td> <td>    0.141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x182</th> <td>   -0.0006</td> <td>    0.027</td> <td>   -0.024</td> <td> 0.981</td> <td>   -0.053</td> <td>    0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x183</th> <td>    0.4940</td> <td>    0.087</td> <td>    5.646</td> <td> 0.000</td> <td>    0.322</td> <td>    0.666</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x184</th> <td>    0.0110</td> <td>    0.047</td> <td>    0.234</td> <td> 0.815</td> <td>   -0.081</td> <td>    0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x185</th> <td>   -0.0081</td> <td>    0.016</td> <td>   -0.503</td> <td> 0.615</td> <td>   -0.040</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x186</th> <td>    0.0085</td> <td>    0.016</td> <td>    0.546</td> <td> 0.585</td> <td>   -0.022</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x187</th> <td>    0.1713</td> <td>    0.121</td> <td>    1.412</td> <td> 0.158</td> <td>   -0.067</td> <td>    0.409</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x188</th> <td>   -0.0147</td> <td>    0.026</td> <td>   -0.568</td> <td> 0.570</td> <td>   -0.066</td> <td>    0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x189</th> <td>    0.4781</td> <td>    0.034</td> <td>   14.162</td> <td> 0.000</td> <td>    0.412</td> <td>    0.544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x190</th> <td>    0.3253</td> <td>    0.026</td> <td>   12.717</td> <td> 0.000</td> <td>    0.275</td> <td>    0.376</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x191</th> <td>    0.2544</td> <td>    0.042</td> <td>    6.119</td> <td> 0.000</td> <td>    0.173</td> <td>    0.336</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x192</th> <td>    0.0258</td> <td>    0.012</td> <td>    2.106</td> <td> 0.035</td> <td>    0.002</td> <td>    0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x193</th> <td>    0.0571</td> <td>    0.036</td> <td>    1.602</td> <td> 0.109</td> <td>   -0.013</td> <td>    0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x194</th> <td>    0.0269</td> <td>    0.023</td> <td>    1.157</td> <td> 0.247</td> <td>   -0.019</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x195</th> <td>    0.0162</td> <td>    0.030</td> <td>    0.541</td> <td> 0.589</td> <td>   -0.043</td> <td>    0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x196</th> <td>    0.0662</td> <td>    0.031</td> <td>    2.151</td> <td> 0.032</td> <td>    0.006</td> <td>    0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x197</th> <td>    0.0363</td> <td>    0.037</td> <td>    0.983</td> <td> 0.326</td> <td>   -0.036</td> <td>    0.109</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x198</th> <td>    0.0371</td> <td>    0.010</td> <td>    3.861</td> <td> 0.000</td> <td>    0.018</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x199</th> <td>    0.0174</td> <td>    0.012</td> <td>    1.502</td> <td> 0.133</td> <td>   -0.005</td> <td>    0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x200</th> <td>    0.1683</td> <td>    0.022</td> <td>    7.693</td> <td> 0.000</td> <td>    0.125</td> <td>    0.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x201</th> <td>    0.0113</td> <td>    0.026</td> <td>    0.426</td> <td> 0.670</td> <td>   -0.041</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x202</th> <td>    0.0547</td> <td>    0.034</td> <td>    1.600</td> <td> 0.110</td> <td>   -0.012</td> <td>    0.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x203</th> <td>    0.0075</td> <td>    0.019</td> <td>    0.393</td> <td> 0.694</td> <td>   -0.030</td> <td>    0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x204</th> <td>    0.1516</td> <td>    0.027</td> <td>    5.552</td> <td> 0.000</td> <td>    0.098</td> <td>    0.205</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x205</th> <td>    0.0596</td> <td>    0.014</td> <td>    4.218</td> <td> 0.000</td> <td>    0.032</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x206</th> <td>  816.3474</td> <td>   27.240</td> <td>   29.968</td> <td> 0.000</td> <td>  762.924</td> <td>  869.771</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x207</th> <td>  440.7742</td> <td>   14.710</td> <td>   29.964</td> <td> 0.000</td> <td>  411.925</td> <td>  469.623</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x208</th> <td>  251.8819</td> <td>    8.406</td> <td>   29.963</td> <td> 0.000</td> <td>  235.395</td> <td>  268.368</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x209</th> <td> -807.0342</td> <td>   26.964</td> <td>  -29.930</td> <td> 0.000</td> <td> -859.915</td> <td> -754.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x210</th> <td>    0.0402</td> <td>    0.021</td> <td>    1.893</td> <td> 0.059</td> <td>   -0.001</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x211</th> <td>    0.0021</td> <td>    0.021</td> <td>    0.098</td> <td> 0.922</td> <td>   -0.040</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x212</th> <td>    0.0696</td> <td>    0.032</td> <td>    2.178</td> <td> 0.030</td> <td>    0.007</td> <td>    0.132</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x213</th> <td>    0.0393</td> <td>    0.016</td> <td>    2.510</td> <td> 0.012</td> <td>    0.009</td> <td>    0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x214</th> <td>   -0.0926</td> <td>    0.040</td> <td>   -2.318</td> <td> 0.021</td> <td>   -0.171</td> <td>   -0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x215</th> <td>   -0.0878</td> <td>    0.072</td> <td>   -1.211</td> <td> 0.226</td> <td>   -0.230</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x216</th> <td>    0.0699</td> <td>    0.024</td> <td>    2.972</td> <td> 0.003</td> <td>    0.024</td> <td>    0.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x217</th> <td>    0.0414</td> <td>    0.038</td> <td>    1.097</td> <td> 0.273</td> <td>   -0.033</td> <td>    0.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x218</th> <td>    0.2077</td> <td>    0.030</td> <td>    6.931</td> <td> 0.000</td> <td>    0.149</td> <td>    0.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x219</th> <td>    0.0644</td> <td>    0.032</td> <td>    1.992</td> <td> 0.047</td> <td>    0.001</td> <td>    0.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x220</th> <td>    0.0029</td> <td>    0.015</td> <td>    0.195</td> <td> 0.846</td> <td>   -0.026</td> <td>    0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x221</th> <td>    0.0450</td> <td>    0.025</td> <td>    1.793</td> <td> 0.073</td> <td>   -0.004</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x222</th> <td>    0.1366</td> <td>    0.033</td> <td>    4.100</td> <td> 0.000</td> <td>    0.071</td> <td>    0.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x223</th> <td>    0.0328</td> <td>    0.043</td> <td>    0.767</td> <td> 0.443</td> <td>   -0.051</td> <td>    0.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x224</th> <td>    0.0888</td> <td>    0.065</td> <td>    1.373</td> <td> 0.170</td> <td>   -0.038</td> <td>    0.216</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x225</th> <td>    0.1003</td> <td>    0.068</td> <td>    1.484</td> <td> 0.138</td> <td>   -0.032</td> <td>    0.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x226</th> <td>    0.0313</td> <td>    0.012</td> <td>    2.581</td> <td> 0.010</td> <td>    0.008</td> <td>    0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x227</th> <td>    0.0689</td> <td>    0.030</td> <td>    2.265</td> <td> 0.024</td> <td>    0.009</td> <td>    0.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x228</th> <td>    0.0109</td> <td>    0.031</td> <td>    0.351</td> <td> 0.726</td> <td>   -0.050</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x229</th> <td>    0.1806</td> <td>    0.043</td> <td>    4.169</td> <td> 0.000</td> <td>    0.096</td> <td>    0.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x230</th> <td>    0.0715</td> <td>    0.049</td> <td>    1.473</td> <td> 0.141</td> <td>   -0.024</td> <td>    0.167</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x231</th> <td>    0.1387</td> <td>    0.026</td> <td>    5.368</td> <td> 0.000</td> <td>    0.088</td> <td>    0.189</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x232</th> <td>    0.1901</td> <td>    0.153</td> <td>    1.242</td> <td> 0.214</td> <td>   -0.110</td> <td>    0.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x233</th> <td>   -0.1480</td> <td>    0.175</td> <td>   -0.845</td> <td> 0.398</td> <td>   -0.491</td> <td>    0.196</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x234</th> <td>    0.0697</td> <td>    0.146</td> <td>    0.476</td> <td> 0.634</td> <td>   -0.217</td> <td>    0.357</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x235</th> <td>   -0.0162</td> <td>    0.008</td> <td>   -2.135</td> <td> 0.033</td> <td>   -0.031</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1070.411</td> <th>  Durbin-Watson:     </th> <td>   2.012</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>35537.311</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>-1.670</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>22.445</td>  <th>  Cond. No.          </th> <td>1.17e+16</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 4.06e-28. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              SalePrice   R-squared:                       0.940\n",
       "Model:                            OLS   Adj. R-squared:                  0.933\n",
       "Method:                 Least Squares   F-statistic:                     130.0\n",
       "Date:                Wed, 25 Oct 2023   Prob (F-statistic):               0.00\n",
       "Time:                        22:17:52   Log-Likelihood:                 1926.9\n",
       "No. Observations:                2191   AIC:                            -3382.\n",
       "Df Residuals:                    1956   BIC:                            -2038.\n",
       "Df Model:                         235                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1            -0.2067      0.123     -1.676      0.094      -0.449       0.035\n",
       "x2            -0.0657      0.034     -1.908      0.057      -0.133       0.002\n",
       "x3            -0.0333      0.054     -0.617      0.537      -0.139       0.073\n",
       "x4             0.1136      0.109      1.043      0.297      -0.100       0.327\n",
       "x5             0.0826      0.051      1.617      0.106      -0.018       0.183\n",
       "x6             0.0164      0.054      0.307      0.759      -0.089       0.121\n",
       "x7             0.0716      0.074      0.964      0.335      -0.074       0.217\n",
       "x8             0.1073      0.083      1.298      0.195      -0.055       0.269\n",
       "x9             0.1229      0.060      2.042      0.041       0.005       0.241\n",
       "x10            0.0506      0.057      0.882      0.378      -0.062       0.163\n",
       "x11            0.1203      0.058      2.056      0.040       0.006       0.235\n",
       "x12            0.1098      0.071      1.552      0.121      -0.029       0.249\n",
       "x13            0.0032      0.076      0.042      0.967      -0.147       0.153\n",
       "x14            0.0953      0.064      1.497      0.135      -0.030       0.220\n",
       "x15            0.0074      0.029      0.253      0.801      -0.050       0.065\n",
       "x16            1.0350      0.148      6.996      0.000       0.745       1.325\n",
       "x17            1.3097      0.148      8.870      0.000       1.020       1.599\n",
       "x18            1.2635      0.170      7.445      0.000       0.931       1.596\n",
       "x19            1.2585      0.149      8.470      0.000       0.967       1.550\n",
       "x20            1.2886      0.145      8.857      0.000       1.003       1.574\n",
       "x21            1.2326      0.145      8.510      0.000       0.949       1.517\n",
       "x22            0.0254      0.020      1.297      0.195      -0.013       0.064\n",
       "x23            0.0195      0.024      0.811      0.418      -0.028       0.067\n",
       "x24            0.0087      0.014      0.628      0.530      -0.019       0.036\n",
       "x25            0.0034      0.012      0.271      0.786      -0.021       0.028\n",
       "x26           -0.0263      0.017     -1.576      0.115      -0.059       0.006\n",
       "x27           -0.0199      0.033     -0.609      0.543      -0.084       0.044\n",
       "x28           -0.0040      0.007     -0.606      0.545      -0.017       0.009\n",
       "x29            0.1225      0.061      2.023      0.043       0.004       0.241\n",
       "x30            0.0455      0.043      1.055      0.292      -0.039       0.130\n",
       "x31            0.0839      0.035      2.397      0.017       0.015       0.152\n",
       "x32            0.0553      0.036      1.526      0.127      -0.016       0.126\n",
       "x33            0.0072      0.028      0.259      0.796      -0.047       0.061\n",
       "x34            0.1147      0.032      3.562      0.000       0.052       0.178\n",
       "x35           -0.0114      0.030     -0.380      0.704      -0.070       0.048\n",
       "x36            0.0126      0.029      0.430      0.667      -0.045       0.070\n",
       "x37            0.1030      0.052      1.990      0.047       0.001       0.204\n",
       "x38            0.0246      0.038      0.647      0.517      -0.050       0.099\n",
       "x39           -0.0666      0.044     -1.506      0.132      -0.153       0.020\n",
       "x40            0.0148      0.030      0.491      0.624      -0.044       0.074\n",
       "x41            0.0143      0.030      0.482      0.630      -0.044       0.072\n",
       "x42            0.0853      0.053      1.622      0.105      -0.018       0.188\n",
       "x43            0.0034      0.030      0.113      0.910      -0.056       0.063\n",
       "x44            0.0631      0.033      1.905      0.057      -0.002       0.128\n",
       "x45            0.0888      0.028      3.164      0.002       0.034       0.144\n",
       "x46            0.0101      0.035      0.288      0.774      -0.059       0.079\n",
       "x47            0.0251      0.037      0.671      0.503      -0.048       0.098\n",
       "x48            0.0325      0.031      1.059      0.290      -0.028       0.093\n",
       "x49            0.0041      0.029      0.141      0.888      -0.053       0.061\n",
       "x50            0.0646      0.033      1.938      0.053      -0.001       0.130\n",
       "x51            0.1118      0.032      3.550      0.000       0.050       0.174\n",
       "x52            0.0186      0.031      0.606      0.544      -0.042       0.079\n",
       "x53            0.0007      0.039      0.019      0.985      -0.075       0.077\n",
       "x54            0.0126      0.018      0.695      0.487      -0.023       0.048\n",
       "x55            0.0601      0.015      4.009      0.000       0.031       0.090\n",
       "x56            0.0790      0.035      2.266      0.024       0.011       0.147\n",
       "x57            0.0720      0.026      2.748      0.006       0.021       0.123\n",
       "x58            0.0003      0.029      0.011      0.991      -0.057       0.057\n",
       "x59            0.0174      0.028      0.627      0.531      -0.037       0.072\n",
       "x60            0.0092      0.066      0.140      0.889      -0.120       0.139\n",
       "x61            0.0067      0.044      0.154      0.877      -0.079       0.093\n",
       "x62           -0.1014      0.072     -1.411      0.158      -0.242       0.040\n",
       "x63           -0.0619      0.060     -1.028      0.304      -0.180       0.056\n",
       "x64            0.0066      0.086      0.076      0.939      -0.162       0.175\n",
       "x65           -0.0361      0.101     -0.358      0.720      -0.234       0.162\n",
       "x66           -0.1022      0.165     -0.617      0.537      -0.427       0.222\n",
       "x67           -0.0648      0.126     -0.516      0.606      -0.311       0.182\n",
       "x68            0.0138      0.125      0.110      0.912      -0.232       0.260\n",
       "x69           -0.0365      0.099     -0.370      0.711      -0.230       0.157\n",
       "x70            0.0074      0.029      0.253      0.801      -0.050       0.065\n",
       "x71           -0.0238      0.054     -0.438      0.662      -0.130       0.083\n",
       "x72            0.0241      0.051      0.472      0.637      -0.076       0.124\n",
       "x73            0.0293      0.064      0.458      0.647      -0.096       0.155\n",
       "x74            0.0424      0.033      1.299      0.194      -0.022       0.107\n",
       "x75           -0.0370      0.066     -0.561      0.575      -0.166       0.092\n",
       "x76            0.0683      0.046      1.473      0.141      -0.023       0.159\n",
       "x77            0.0464      0.031      1.514      0.130      -0.014       0.106\n",
       "x78            0.0410      0.044      0.933      0.351      -0.045       0.127\n",
       "x79            0.1049      0.061      1.718      0.086      -0.015       0.225\n",
       "x80            0.0091      0.061      0.150      0.881      -0.111       0.129\n",
       "x81           -0.0161      0.067     -0.240      0.810      -0.148       0.115\n",
       "x82            0.0090      0.061      0.147      0.883      -0.111       0.129\n",
       "x83           -0.1428      0.077     -1.855      0.064      -0.294       0.008\n",
       "x84           -0.0215      0.094     -0.228      0.819      -0.206       0.163\n",
       "x85            0.1668      0.127      1.316      0.188      -0.082       0.415\n",
       "x86            0.0686      0.127      0.539      0.590      -0.181       0.318\n",
       "x87            0.0803      0.114      0.701      0.483      -0.144       0.305\n",
       "x88           -0.0318      0.056     -0.570      0.568      -0.141       0.078\n",
       "x89            0.0184      0.049      0.378      0.706      -0.077       0.114\n",
       "x90            0.0555      0.054      1.026      0.305      -0.051       0.161\n",
       "x91           -0.0625      0.122     -0.514      0.607      -0.301       0.176\n",
       "x92            0.0724      0.071      1.021      0.308      -0.067       0.212\n",
       "x93            0.1329      0.044      3.034      0.002       0.047       0.219\n",
       "x94            1.3690      0.176      7.759      0.000       1.023       1.715\n",
       "x95           -0.0024      0.089     -0.027      0.978      -0.177       0.172\n",
       "x96            0.0460      0.042      1.093      0.274      -0.037       0.129\n",
       "x97           -0.0132      0.121     -0.109      0.913      -0.250       0.224\n",
       "x98            0.0520      0.051      1.026      0.305      -0.047       0.151\n",
       "x99            0.0582      0.041      1.404      0.160      -0.023       0.139\n",
       "x100           0.3099      0.059      5.229      0.000       0.194       0.426\n",
       "x101          -0.0475      0.099     -0.480      0.631      -0.242       0.147\n",
       "x102           0.0535      0.050      1.069      0.285      -0.045       0.152\n",
       "x103           0.0679      0.047      1.434      0.152      -0.025       0.161\n",
       "x104           0.0547      0.041      1.345      0.179      -0.025       0.134\n",
       "x105           0.0709      0.046      1.552      0.121      -0.019       0.160\n",
       "x106           0.1467      0.093      1.574      0.116      -0.036       0.329\n",
       "x107          -0.0400      0.065     -0.619      0.536      -0.167       0.087\n",
       "x108          -0.0228      0.047     -0.482      0.630      -0.115       0.070\n",
       "x109           0.0915      0.089      1.023      0.306      -0.084       0.267\n",
       "x110           0.0145      0.043      0.339      0.735      -0.069       0.098\n",
       "x111           0.0185      0.052      0.353      0.724      -0.084       0.121\n",
       "x112           0.0262      0.051      0.510      0.610      -0.074       0.127\n",
       "x113          -0.0792      0.120     -0.661      0.509      -0.314       0.156\n",
       "x114           0.0021      0.041      0.051      0.959      -0.079       0.083\n",
       "x115           0.3099      0.059      5.229      0.000       0.194       0.426\n",
       "x116           0.0827      0.076      1.085      0.278      -0.067       0.232\n",
       "x117           0.0333      0.050      0.663      0.507      -0.065       0.132\n",
       "x118           0.0004      0.048      0.009      0.993      -0.093       0.094\n",
       "x119           0.0176      0.041      0.424      0.672      -0.064       0.099\n",
       "x120          -0.0009      0.045     -0.021      0.983      -0.089       0.087\n",
       "x121           0.0333      0.026      1.280      0.201      -0.018       0.084\n",
       "x122          -0.2970      0.136     -2.177      0.030      -0.564      -0.029\n",
       "x123           0.0386      0.026      1.481      0.139      -0.013       0.090\n",
       "x124           0.0541      0.027      1.967      0.049       0.000       0.108\n",
       "x125           0.0172      0.011      1.505      0.133      -0.005       0.040\n",
       "x126           0.0355      0.013      2.819      0.005       0.011       0.060\n",
       "x127           0.0263      0.032      0.827      0.408      -0.036       0.089\n",
       "x128           0.0792      0.045      1.747      0.081      -0.010       0.168\n",
       "x129          -0.0604      0.053     -1.144      0.253      -0.164       0.043\n",
       "x130           0.1052      0.113      0.932      0.351      -0.116       0.327\n",
       "x131           0.1529      0.116      1.321      0.187      -0.074       0.380\n",
       "x132           0.0135      0.120      0.112      0.911      -0.221       0.248\n",
       "x133           0.0565      0.162      0.350      0.727      -0.260       0.374\n",
       "x134           0.1389      0.134      1.037      0.300      -0.124       0.402\n",
       "x135           0.0319      0.013      2.452      0.014       0.006       0.057\n",
       "x136          -0.0266      0.021     -1.258      0.208      -0.068       0.015\n",
       "x137           0.0304      0.048      0.627      0.531      -0.065       0.125\n",
       "x138          -0.0107      0.011     -0.977      0.329      -0.032       0.011\n",
       "x139           0.0573      0.110      0.520      0.603      -0.159       0.274\n",
       "x140           0.0500      0.029      1.743      0.082      -0.006       0.106\n",
       "x141           0.0623      0.040      1.573      0.116      -0.015       0.140\n",
       "x142           0.0468      0.031      1.498      0.134      -0.014       0.108\n",
       "x143          -0.0078      0.044     -0.177      0.860      -0.094       0.078\n",
       "x144           0.0337      0.029      1.176      0.240      -0.022       0.090\n",
       "x145           0.0416      0.100      0.415      0.678      -0.155       0.238\n",
       "x146           0.0871      0.107      0.818      0.414      -0.122       0.296\n",
       "x147          -0.0099      0.007     -1.349      0.178      -0.024       0.004\n",
       "x148           0.0018      0.009      0.207      0.836      -0.015       0.019\n",
       "x149          -0.0150      0.017     -0.867      0.386      -0.049       0.019\n",
       "x150           0.0104      0.014      0.742      0.458      -0.017       0.038\n",
       "x151          -0.0286      0.047     -0.613      0.540      -0.120       0.063\n",
       "x152           0.0038      0.013      0.300      0.764      -0.021       0.029\n",
       "x153           0.0137      0.101      0.136      0.892      -0.184       0.211\n",
       "x154           0.0518      0.111      0.469      0.639      -0.165       0.269\n",
       "x155           0.0055      0.094      0.058      0.954      -0.180       0.191\n",
       "x156          -0.2412      0.153     -1.572      0.116      -0.542       0.060\n",
       "x157          -0.0080      0.012     -0.679      0.497      -0.031       0.015\n",
       "x158           0.0047      0.015      0.317      0.751      -0.024       0.034\n",
       "x159          -0.0115      0.013     -0.865      0.387      -0.038       0.015\n",
       "x160           0.0068      0.014      0.487      0.627      -0.020       0.034\n",
       "x161           0.0048      0.010      0.486      0.627      -0.015       0.024\n",
       "x162          -0.0043      0.010     -0.445      0.656      -0.023       0.015\n",
       "x163          -0.0042      0.012     -0.357      0.721      -0.027       0.019\n",
       "x164           0.0067      0.010      0.664      0.507      -0.013       0.027\n",
       "x165          -0.0147      0.014     -1.091      0.275      -0.041       0.012\n",
       "x166          -0.0130      0.013     -1.015      0.310      -0.038       0.012\n",
       "x167           0.0048      0.013      0.379      0.705      -0.020       0.030\n",
       "x168           0.0199      0.037      0.544      0.586      -0.052       0.092\n",
       "x169           0.1293      0.052      2.473      0.013       0.027       0.232\n",
       "x170           0.0458      0.030      1.514      0.130      -0.014       0.105\n",
       "x171          -0.0370      0.044     -0.833      0.405      -0.124       0.050\n",
       "x172          -0.0033      0.053     -0.062      0.951      -0.107       0.100\n",
       "x173           0.1013      0.054      1.861      0.063      -0.005       0.208\n",
       "x174           0.0474      0.047      1.002      0.317      -0.045       0.140\n",
       "x175           0.0008      0.111      0.007      0.994      -0.217       0.218\n",
       "x176           0.0083      0.015      0.552      0.581      -0.021       0.038\n",
       "x177           0.2288      0.040      5.672      0.000       0.150       0.308\n",
       "x178           0.1388      0.035      3.946      0.000       0.070       0.208\n",
       "x179           0.0497      0.022      2.286      0.022       0.007       0.092\n",
       "x180           0.0965      0.011      9.080      0.000       0.076       0.117\n",
       "x181           0.0386      0.052      0.742      0.458      -0.063       0.141\n",
       "x182          -0.0006      0.027     -0.024      0.981      -0.053       0.052\n",
       "x183           0.4940      0.087      5.646      0.000       0.322       0.666\n",
       "x184           0.0110      0.047      0.234      0.815      -0.081       0.103\n",
       "x185          -0.0081      0.016     -0.503      0.615      -0.040       0.023\n",
       "x186           0.0085      0.016      0.546      0.585      -0.022       0.039\n",
       "x187           0.1713      0.121      1.412      0.158      -0.067       0.409\n",
       "x188          -0.0147      0.026     -0.568      0.570      -0.066       0.036\n",
       "x189           0.4781      0.034     14.162      0.000       0.412       0.544\n",
       "x190           0.3253      0.026     12.717      0.000       0.275       0.376\n",
       "x191           0.2544      0.042      6.119      0.000       0.173       0.336\n",
       "x192           0.0258      0.012      2.106      0.035       0.002       0.050\n",
       "x193           0.0571      0.036      1.602      0.109      -0.013       0.127\n",
       "x194           0.0269      0.023      1.157      0.247      -0.019       0.072\n",
       "x195           0.0162      0.030      0.541      0.589      -0.043       0.075\n",
       "x196           0.0662      0.031      2.151      0.032       0.006       0.127\n",
       "x197           0.0363      0.037      0.983      0.326      -0.036       0.109\n",
       "x198           0.0371      0.010      3.861      0.000       0.018       0.056\n",
       "x199           0.0174      0.012      1.502      0.133      -0.005       0.040\n",
       "x200           0.1683      0.022      7.693      0.000       0.125       0.211\n",
       "x201           0.0113      0.026      0.426      0.670      -0.041       0.063\n",
       "x202           0.0547      0.034      1.600      0.110      -0.012       0.122\n",
       "x203           0.0075      0.019      0.393      0.694      -0.030       0.045\n",
       "x204           0.1516      0.027      5.552      0.000       0.098       0.205\n",
       "x205           0.0596      0.014      4.218      0.000       0.032       0.087\n",
       "x206         816.3474     27.240     29.968      0.000     762.924     869.771\n",
       "x207         440.7742     14.710     29.964      0.000     411.925     469.623\n",
       "x208         251.8819      8.406     29.963      0.000     235.395     268.368\n",
       "x209        -807.0342     26.964    -29.930      0.000    -859.915    -754.153\n",
       "x210           0.0402      0.021      1.893      0.059      -0.001       0.082\n",
       "x211           0.0021      0.021      0.098      0.922      -0.040       0.044\n",
       "x212           0.0696      0.032      2.178      0.030       0.007       0.132\n",
       "x213           0.0393      0.016      2.510      0.012       0.009       0.070\n",
       "x214          -0.0926      0.040     -2.318      0.021      -0.171      -0.014\n",
       "x215          -0.0878      0.072     -1.211      0.226      -0.230       0.054\n",
       "x216           0.0699      0.024      2.972      0.003       0.024       0.116\n",
       "x217           0.0414      0.038      1.097      0.273      -0.033       0.116\n",
       "x218           0.2077      0.030      6.931      0.000       0.149       0.267\n",
       "x219           0.0644      0.032      1.992      0.047       0.001       0.128\n",
       "x220           0.0029      0.015      0.195      0.846      -0.026       0.032\n",
       "x221           0.0450      0.025      1.793      0.073      -0.004       0.094\n",
       "x222           0.1366      0.033      4.100      0.000       0.071       0.202\n",
       "x223           0.0328      0.043      0.767      0.443      -0.051       0.117\n",
       "x224           0.0888      0.065      1.373      0.170      -0.038       0.216\n",
       "x225           0.1003      0.068      1.484      0.138      -0.032       0.233\n",
       "x226           0.0313      0.012      2.581      0.010       0.008       0.055\n",
       "x227           0.0689      0.030      2.265      0.024       0.009       0.128\n",
       "x228           0.0109      0.031      0.351      0.726      -0.050       0.072\n",
       "x229           0.1806      0.043      4.169      0.000       0.096       0.266\n",
       "x230           0.0715      0.049      1.473      0.141      -0.024       0.167\n",
       "x231           0.1387      0.026      5.368      0.000       0.088       0.189\n",
       "x232           0.1901      0.153      1.242      0.214      -0.110       0.490\n",
       "x233          -0.1480      0.175     -0.845      0.398      -0.491       0.196\n",
       "x234           0.0697      0.146      0.476      0.634      -0.217       0.357\n",
       "x235          -0.0162      0.008     -2.135      0.033      -0.031      -0.001\n",
       "==============================================================================\n",
       "Omnibus:                     1070.411   Durbin-Watson:                   2.012\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            35537.311\n",
       "Skew:                          -1.670   Prob(JB):                         0.00\n",
       "Kurtosis:                      22.445   Cond. No.                     1.17e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 4.06e-28. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resL2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value of Jarque Bera test for no regularisation model:  0.0\n"
     ]
    }
   ],
   "source": [
    "jb_test = jarque_bera(res.resid)\n",
    "print(\"p-value of Jarque Bera test for no regularisation model: \",jb_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value of Jarque Bera test for L1 regularisation model:  0.0\n"
     ]
    }
   ],
   "source": [
    "jb_test = jarque_bera(resL1.resid)\n",
    "print(\"p-value of Jarque Bera test for L1 regularisation model: \",jb_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value of Jarque Bera test for L2 regularisation model:  0.0\n"
     ]
    }
   ],
   "source": [
    "jb_test = jarque_bera(resL2.resid)\n",
    "print(\"p-value of Jarque Bera test for L2 regularisation model: \",jb_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(res.resid,bins = 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(resL1.resid,bins = 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(resL2.resid,bins = 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value of Breusch–Pagan test for no regularisation model:  1.2641469145364133e-12\n"
     ]
    }
   ],
   "source": [
    "hb_test = het_breuschpagan(res.resid,res.model.exog, robust=True)\n",
    "print(\"p-value of Breusch–Pagan test for no regularisation model: \", hb_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value of Breusch–Pagan test for L1 regularisation model:  1.2458713017539086e-285\n"
     ]
    }
   ],
   "source": [
    "hb_test = het_breuschpagan(resL1.resid,res.model.exog, robust=True)\n",
    "print(\"p-value of Breusch–Pagan test for L1 regularisation model: \", hb_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value of Breusch–Pagan test for L2 regularisation model:  1.2641469145364133e-12\n"
     ]
    }
   ],
   "source": [
    "hb_test = het_breuschpagan(resL2.resid,res.model.exog, robust=True)\n",
    "print(\"p-value of Breusch–Pagan test for L2 regularisation model: \", hb_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is X matrix condition lower than 0.05?\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"is X matrix condition number lower than 0.05?\")\n",
    "print(np.linalg.cond(X_train) < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Udało mi się przeprowadzić 3 z 4 testów w zadaniu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
